{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f650ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "sys.path.append(\"/home/bengtegard/github/data-science-rag/src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1e0da",
   "metadata": {},
   "source": [
    "### Data Ingestion: Load and preprocess the text data in various formats (pdfs, text files, markdown etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efe81d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 784 documents from /home/bengtegard/github/data-science-rag/data/machine_learning/Hands-On Machine Learning with Scikit-Learn, Keras, and - Aurélien Géron _3, 2022 _O'Reilly Media.pdf\n",
      "Loaded 216 documents from /home/bengtegard/github/data-science-rag/data/machine_learning/CS229 Lecture Notes by Andrew Ng.pdf\n",
      "Loaded 163 documents from /home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf\n",
      "Loaded 152 documents from /home/bengtegard/github/data-science-rag/data/machine_learning/Burkov_the_hundred-page_Machine_Learning.pdf\n",
      "Loaded 361 documents from /home/bengtegard/github/data-science-rag/data/machine_learning/Generative AI with Langchain.pdf\n",
      "Loaded 1 documents from /home/bengtegard/github/data-science-rag/data/personal_notes/stat_book.ipynb\n",
      "Loaded 24 documents from /home/bengtegard/github/data-science-rag/data/data_science/tidy-data.pdf\n",
      "Loaded 18 documents from /home/bengtegard/github/data-science-rag/data/data_science/Breiman - two cultures of statistical modeling.pdf\n",
      "Loaded 26 documents from /home/bengtegard/github/data-science-rag/data/data_science/layered-grammar.pdf\n",
      "Loaded 285 documents from /home/bengtegard/github/data-science-rag/data/data_science/The Data Science Handbook.pdf\n",
      "Loaded 29 documents from /home/bengtegard/github/data-science-rag/data/data_science/The_Art_of_Turning_Data_Into_Product_DJ.pdf\n",
      "Loaded 5 documents from /home/bengtegard/github/data-science-rag/data/data_science/same-stats-different-graphs.pdf_rec2hRjLLGgM7Cn2T.pdf\n",
      "Loaded 973 documents from /home/bengtegard/github/data-science-rag/data/statistics/Stats Data and Models, 3rd Edition- Richard D de Veaux.pdf\n",
      "Loaded 225 documents from /home/bengtegard/github/data-science-rag/data/statistics/[Data] Think Stats (2014).pdf\n",
      "Loaded 213 documents from /home/bengtegard/github/data-science-rag/data/statistics/thinkbayes.pdf\n",
      "Loaded 5 documents from /home/bengtegard/github/data-science-rag/data/statistics/same-stats-different-graphs.pdf_rec2hRjLLGgM7Cn2T.pdf\n",
      "Loaded 291 documents from /home/bengtegard/github/data-science-rag/data/sql/Renee M Teate - SQL for Data Scientists_ A Beginner's Guide for Building Datasets for Analysis (2021, Wiley) - libgen.li.pdf\n",
      "Loaded 212 documents from /home/bengtegard/github/data-science-rag/data/linear_algebra/Jason Brownlee-Basics for Linear Algebra for Machine Learning - Discover the Mathematical Language of Data in Python (2018).pdf\n",
      "Loaded 504 documents from /home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf\n",
      "Loaded 548 documents from /home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf\n",
      "Loaded 1011 documents from /home/bengtegard/github/data-science-rag/data/python/luciano-ramalho-fluent-python_-clear-concise-and-effective-programming-oreilly-media-2022.pdf\n",
      "Loaded 520 documents from /home/bengtegard/github/data-science-rag/data/r/R for Data Science - Hadley Wickham & Garrett Grolemund - (Data Science Books)-1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 376 0 (offset 0)\n",
      "Ignoring wrong pointing object 393 0 (offset 0)\n",
      "Ignoring wrong pointing object 425 0 (offset 0)\n",
      "Ignoring wrong pointing object 427 0 (offset 0)\n",
      "Ignoring wrong pointing object 434 0 (offset 0)\n",
      "Ignoring wrong pointing object 652 0 (offset 0)\n",
      "Ignoring wrong pointing object 678 0 (offset 0)\n",
      "Ignoring wrong pointing object 781 0 (offset 0)\n",
      "Ignoring wrong pointing object 837 0 (offset 0)\n",
      "Ignoring wrong pointing object 840 0 (offset 0)\n",
      "Ignoring wrong pointing object 843 0 (offset 0)\n",
      "Ignoring wrong pointing object 854 0 (offset 0)\n",
      "Ignoring wrong pointing object 885 0 (offset 0)\n",
      "Ignoring wrong pointing object 929 0 (offset 0)\n",
      "Ignoring wrong pointing object 1050 0 (offset 0)\n",
      "Ignoring wrong pointing object 1092 0 (offset 0)\n",
      "Ignoring wrong pointing object 1125 0 (offset 0)\n",
      "Ignoring wrong pointing object 1138 0 (offset 0)\n",
      "Ignoring wrong pointing object 1140 0 (offset 0)\n",
      "Ignoring wrong pointing object 1149 0 (offset 0)\n",
      "Ignoring wrong pointing object 1157 0 (offset 0)\n",
      "Ignoring wrong pointing object 1161 0 (offset 0)\n",
      "Ignoring wrong pointing object 1163 0 (offset 0)\n",
      "Ignoring wrong pointing object 1173 0 (offset 0)\n",
      "Ignoring wrong pointing object 1200 0 (offset 0)\n",
      "Ignoring wrong pointing object 1202 0 (offset 0)\n",
      "Ignoring wrong pointing object 1276 0 (offset 0)\n",
      "Ignoring wrong pointing object 1280 0 (offset 0)\n",
      "Ignoring wrong pointing object 1290 0 (offset 0)\n",
      "Ignoring wrong pointing object 1425 0 (offset 0)\n",
      "Ignoring wrong pointing object 1452 0 (offset 0)\n",
      "Ignoring wrong pointing object 1512 0 (offset 0)\n",
      "Ignoring wrong pointing object 1551 0 (offset 0)\n",
      "Ignoring wrong pointing object 1578 0 (offset 0)\n",
      "Ignoring wrong pointing object 1588 0 (offset 0)\n",
      "Ignoring wrong pointing object 1645 0 (offset 0)\n",
      "Ignoring wrong pointing object 1650 0 (offset 0)\n",
      "Ignoring wrong pointing object 1674 0 (offset 0)\n",
      "Ignoring wrong pointing object 1677 0 (offset 0)\n",
      "Ignoring wrong pointing object 1725 0 (offset 0)\n",
      "Ignoring wrong pointing object 1735 0 (offset 0)\n",
      "Ignoring wrong pointing object 1753 0 (offset 0)\n",
      "Ignoring wrong pointing object 1755 0 (offset 0)\n",
      "Ignoring wrong pointing object 1793 0 (offset 0)\n",
      "Ignoring wrong pointing object 1795 0 (offset 0)\n",
      "Ignoring wrong pointing object 1952 0 (offset 0)\n",
      "Ignoring wrong pointing object 1969 0 (offset 0)\n",
      "Ignoring wrong pointing object 1978 0 (offset 0)\n",
      "Ignoring wrong pointing object 1990 0 (offset 0)\n",
      "Ignoring wrong pointing object 2059 0 (offset 0)\n",
      "Ignoring wrong pointing object 2068 0 (offset 0)\n",
      "Ignoring wrong pointing object 2076 0 (offset 0)\n",
      "Ignoring wrong pointing object 2079 0 (offset 0)\n",
      "Ignoring wrong pointing object 2081 0 (offset 0)\n",
      "Ignoring wrong pointing object 2083 0 (offset 0)\n",
      "Ignoring wrong pointing object 2085 0 (offset 0)\n",
      "Ignoring wrong pointing object 2087 0 (offset 0)\n",
      "Ignoring wrong pointing object 2089 0 (offset 0)\n",
      "Ignoring wrong pointing object 2091 0 (offset 0)\n",
      "Ignoring wrong pointing object 2093 0 (offset 0)\n",
      "Ignoring wrong pointing object 2095 0 (offset 0)\n",
      "Ignoring wrong pointing object 2097 0 (offset 0)\n",
      "Ignoring wrong pointing object 2099 0 (offset 0)\n",
      "Ignoring wrong pointing object 2101 0 (offset 0)\n",
      "Ignoring wrong pointing object 2103 0 (offset 0)\n",
      "Ignoring wrong pointing object 2105 0 (offset 0)\n",
      "Ignoring wrong pointing object 2107 0 (offset 0)\n",
      "Ignoring wrong pointing object 2280 0 (offset 0)\n",
      "Ignoring wrong pointing object 2298 0 (offset 0)\n",
      "Ignoring wrong pointing object 2300 0 (offset 0)\n",
      "Ignoring wrong pointing object 2302 0 (offset 0)\n",
      "Ignoring wrong pointing object 2306 0 (offset 0)\n",
      "Ignoring wrong pointing object 2310 0 (offset 0)\n",
      "Ignoring wrong pointing object 2329 0 (offset 0)\n",
      "Ignoring wrong pointing object 2331 0 (offset 0)\n",
      "Ignoring wrong pointing object 2333 0 (offset 0)\n",
      "Ignoring wrong pointing object 2337 0 (offset 0)\n",
      "Ignoring wrong pointing object 2369 0 (offset 0)\n",
      "Ignoring wrong pointing object 2371 0 (offset 0)\n",
      "Ignoring wrong pointing object 2373 0 (offset 0)\n",
      "Ignoring wrong pointing object 2377 0 (offset 0)\n",
      "Ignoring wrong pointing object 2427 0 (offset 0)\n",
      "Ignoring wrong pointing object 2436 0 (offset 0)\n",
      "Ignoring wrong pointing object 2443 0 (offset 0)\n",
      "Ignoring wrong pointing object 2456 0 (offset 0)\n",
      "Ignoring wrong pointing object 2459 0 (offset 0)\n",
      "Ignoring wrong pointing object 2472 0 (offset 0)\n",
      "Ignoring wrong pointing object 2474 0 (offset 0)\n",
      "Ignoring wrong pointing object 2488 0 (offset 0)\n",
      "Ignoring wrong pointing object 2490 0 (offset 0)\n",
      "Ignoring wrong pointing object 2493 0 (offset 0)\n",
      "Ignoring wrong pointing object 2545 0 (offset 0)\n",
      "Ignoring wrong pointing object 2559 0 (offset 0)\n",
      "Ignoring wrong pointing object 2569 0 (offset 0)\n",
      "Ignoring wrong pointing object 2594 0 (offset 0)\n",
      "Ignoring wrong pointing object 2629 0 (offset 0)\n",
      "Ignoring wrong pointing object 2773 0 (offset 0)\n",
      "Ignoring wrong pointing object 2886 0 (offset 0)\n",
      "Ignoring wrong pointing object 2921 0 (offset 0)\n",
      "Ignoring wrong pointing object 2949 0 (offset 0)\n",
      "Ignoring wrong pointing object 2968 0 (offset 0)\n",
      "Ignoring wrong pointing object 2974 0 (offset 0)\n",
      "Ignoring wrong pointing object 2996 0 (offset 0)\n",
      "Ignoring wrong pointing object 3012 0 (offset 0)\n",
      "Ignoring wrong pointing object 3128 0 (offset 0)\n",
      "Ignoring wrong pointing object 3159 0 (offset 0)\n",
      "Ignoring wrong pointing object 3171 0 (offset 0)\n",
      "Ignoring wrong pointing object 3189 0 (offset 0)\n",
      "Ignoring wrong pointing object 3254 0 (offset 0)\n",
      "Ignoring wrong pointing object 3257 0 (offset 0)\n",
      "Ignoring wrong pointing object 3260 0 (offset 0)\n",
      "Ignoring wrong pointing object 3263 0 (offset 0)\n",
      "Ignoring wrong pointing object 3329 0 (offset 0)\n",
      "Ignoring wrong pointing object 3331 0 (offset 0)\n",
      "Ignoring wrong pointing object 3336 0 (offset 0)\n",
      "Ignoring wrong pointing object 3338 0 (offset 0)\n",
      "Ignoring wrong pointing object 3399 0 (offset 0)\n",
      "Ignoring wrong pointing object 3411 0 (offset 0)\n",
      "Ignoring wrong pointing object 3434 0 (offset 0)\n",
      "Ignoring wrong pointing object 3464 0 (offset 0)\n",
      "Ignoring wrong pointing object 3474 0 (offset 0)\n",
      "Ignoring wrong pointing object 3485 0 (offset 0)\n",
      "Ignoring wrong pointing object 3516 0 (offset 0)\n",
      "Ignoring wrong pointing object 3528 0 (offset 0)\n",
      "Ignoring wrong pointing object 3577 0 (offset 0)\n",
      "Ignoring wrong pointing object 3593 0 (offset 0)\n",
      "Ignoring wrong pointing object 3603 0 (offset 0)\n",
      "Ignoring wrong pointing object 3624 0 (offset 0)\n",
      "Ignoring wrong pointing object 3639 0 (offset 0)\n",
      "Ignoring wrong pointing object 3644 0 (offset 0)\n",
      "Ignoring wrong pointing object 3735 0 (offset 0)\n",
      "Ignoring wrong pointing object 3885 0 (offset 0)\n",
      "Ignoring wrong pointing object 3889 0 (offset 0)\n",
      "Ignoring wrong pointing object 3911 0 (offset 0)\n",
      "Ignoring wrong pointing object 3934 0 (offset 0)\n",
      "Ignoring wrong pointing object 3969 0 (offset 0)\n",
      "Ignoring wrong pointing object 3979 0 (offset 0)\n",
      "Ignoring wrong pointing object 4033 0 (offset 0)\n",
      "Ignoring wrong pointing object 4036 0 (offset 0)\n",
      "Ignoring wrong pointing object 4038 0 (offset 0)\n",
      "Ignoring wrong pointing object 4102 0 (offset 0)\n",
      "Ignoring wrong pointing object 4311 0 (offset 0)\n",
      "Ignoring wrong pointing object 4313 0 (offset 0)\n",
      "Ignoring wrong pointing object 4315 0 (offset 0)\n",
      "Ignoring wrong pointing object 4356 0 (offset 0)\n",
      "Ignoring wrong pointing object 4434 0 (offset 0)\n",
      "Ignoring wrong pointing object 4436 0 (offset 0)\n",
      "Ignoring wrong pointing object 4439 0 (offset 0)\n",
      "Ignoring wrong pointing object 4443 0 (offset 0)\n",
      "Ignoring wrong pointing object 4446 0 (offset 0)\n",
      "Ignoring wrong pointing object 4449 0 (offset 0)\n",
      "Ignoring wrong pointing object 4452 0 (offset 0)\n",
      "Ignoring wrong pointing object 4455 0 (offset 0)\n",
      "Ignoring wrong pointing object 4612 0 (offset 0)\n",
      "Ignoring wrong pointing object 4631 0 (offset 0)\n",
      "Ignoring wrong pointing object 4674 0 (offset 0)\n",
      "Ignoring wrong pointing object 4720 0 (offset 0)\n",
      "Ignoring wrong pointing object 4723 0 (offset 0)\n",
      "Ignoring wrong pointing object 4731 0 (offset 0)\n",
      "Ignoring wrong pointing object 4734 0 (offset 0)\n",
      "Ignoring wrong pointing object 4803 0 (offset 0)\n",
      "Ignoring wrong pointing object 4826 0 (offset 0)\n",
      "Ignoring wrong pointing object 4852 0 (offset 0)\n",
      "Ignoring wrong pointing object 4946 0 (offset 0)\n",
      "Ignoring wrong pointing object 4969 0 (offset 0)\n",
      "Ignoring wrong pointing object 5060 0 (offset 0)\n",
      "Ignoring wrong pointing object 5106 0 (offset 0)\n",
      "Ignoring wrong pointing object 5108 0 (offset 0)\n",
      "Ignoring wrong pointing object 5144 0 (offset 0)\n",
      "Ignoring wrong pointing object 5168 0 (offset 0)\n",
      "Ignoring wrong pointing object 5177 0 (offset 0)\n",
      "Ignoring wrong pointing object 5180 0 (offset 0)\n",
      "Ignoring wrong pointing object 5213 0 (offset 0)\n",
      "Ignoring wrong pointing object 5237 0 (offset 0)\n",
      "Ignoring wrong pointing object 5252 0 (offset 0)\n",
      "Ignoring wrong pointing object 5270 0 (offset 0)\n",
      "Ignoring wrong pointing object 5334 0 (offset 0)\n",
      "Ignoring wrong pointing object 5338 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 835 documents from /home/bengtegard/github/data-science-rag/data/r/The Book of R_ A First Course in Programming and Statistics ( PDFDrive.com ).pdf\n",
      "Loaded 615 documents from /home/bengtegard/github/data-science-rag/data/r/ISLRv2_corrected_June_2023.pdf\n",
      "Loaded 404 documents from /home/bengtegard/github/data-science-rag/data/r/The Art of R Programming.pdf\n"
     ]
    }
   ],
   "source": [
    "from document_processer import DocumentProcessor\n",
    "from utils import load_config\n",
    "\n",
    "# Load config.yml\n",
    "config = load_config('/home/bengtegard/github/data-science-rag/config.yml')\n",
    "\n",
    "# Initialize DocumentProcessor\n",
    "processor = DocumentProcessor(config['documents'])\n",
    "\n",
    "# Load curated material from folders\n",
    "local_docs = processor.load_documents(\"/home/bengtegard/github/data-science-rag/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b63de49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded content from https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\n",
      "Loaded content from https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html\n",
      "Loaded content from https://scott.fortmann-roe.com/docs/BiasVariance.html\n",
      "Loaded content from https://scott.fortmann-roe.com/docs/MeasuringError.html\n"
     ]
    }
   ],
   "source": [
    "url = [\n",
    "    \"https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\",\n",
    "    \"https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html\",\n",
    "    \"https://scott.fortmann-roe.com/docs/BiasVariance.html\",\n",
    "    \"https://scott.fortmann-roe.com/docs/MeasuringError.html\"\n",
    "]\n",
    "\n",
    "# Load urls\n",
    "web_docs = processor.load_web_pages(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb8663e",
   "metadata": {},
   "source": [
    "### Chunking: Break the data into smaller, overlapping chunks to improve retrieval performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58845308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2400 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Chunk 1602 token count 1018 exceeds chunk_size 1000\n",
      "Warning: Chunk 2048 token count 1051 exceeds chunk_size 1000\n",
      "Warning: Chunk 3271 token count 1011 exceeds chunk_size 1000\n",
      "Warning: Chunk 3274 token count 1009 exceeds chunk_size 1000\n",
      "Warning: Chunk 3285 token count 1012 exceeds chunk_size 1000\n",
      "Warning: Chunk 9168 token count 1199 exceeds chunk_size 1000\n",
      "Warning: Chunk 9169 token count 1180 exceeds chunk_size 1000\n",
      "Warning: Chunk 9173 token count 1295 exceeds chunk_size 1000\n",
      "Warning: Chunk 9174 token count 1061 exceeds chunk_size 1000\n",
      "Warning: Chunk 9176 token count 1265 exceeds chunk_size 1000\n",
      "Warning: Chunk 9178 token count 1029 exceeds chunk_size 1000\n",
      "Warning: Chunk 9180 token count 1455 exceeds chunk_size 1000\n",
      "Created 9198 chunks\n"
     ]
    }
   ],
   "source": [
    "all_docs = local_docs + web_docs\n",
    "\n",
    "chunks = processor.create_chunks(all_docs)\n",
    "print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378ff98",
   "metadata": {},
   "source": [
    "Check chunks from the folder python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c8a9c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 4331 ---\n",
      "Content:\n",
      "Natural Language Processing with Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 2, 'page_label': 'i', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4331}\n",
      "\n",
      "--- Chunk 4332 ---\n",
      "Content:\n",
      "Natural Language Processing\n",
      "with Python\n",
      "Steven Bird, Ewan Klein, and Edward Loper\n",
      "Beijing • Cambridge • Farnham • Köln • Sebastopol • Taipei • Tokyo...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 4, 'page_label': 'iii', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4332}\n",
      "\n",
      "--- Chunk 4333 ---\n",
      "Content:\n",
      "Natural Language Processing with Python\n",
      "by Steven Bird, Ewan Klein, and Edward Loper\n",
      "Copyright © 2009 Steven Bird, Ewan Klein, and Edward Loper. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O’Reilly \n",
      "books may be purchased for educational, business, or sales promotional use. Online editions\n",
      "are also available for most titles ( http://my.safaribooksonline.com). For more information, contact our\n",
      "corporate/institutional sales department: (800) 998-9938 or corporate@oreilly.com.\n",
      "Editor: Julie Steele\n",
      "Production Editor: Loranah Dimant\n",
      "Copyeditor: Genevieve d’Entremont\n",
      "Proofreader: Loranah Dimant\n",
      "Indexer: Ellen Troutman Zaig\n",
      "Cover Designer: Karen Montgomery\n",
      "Interior Designer: David Futato\n",
      "Illustrator: Robert Romano\n",
      "Printing History:\n",
      "June 2009:\n",
      "First Edition. \n",
      "Nutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of\n",
      "O’Reilly \n",
      "Media, Inc. Natural Language Processing with Python, the image of a right whale, and related\n",
      "trade dress are trademarks of O’Reilly Media, Inc.\n",
      "Many of the designations used by manufacturers and sellers to distinguish their products are claimed as\n",
      "trademarks. Where those designations appear in this book, and O’Reilly Media, Inc. was aware of a\n",
      "trademark claim, the designations have been printed in caps or initial caps.\n",
      "While every precaution has been taken in the preparation of this book, the publisher and authors assume\n",
      "no responsibility for errors or omissions, or for damages resulting from the use of the information con-\n",
      "tained herein.\n",
      "ISBN: 978-0-596-51649-9\n",
      "[M]\n",
      "1244726609...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 5, 'page_label': 'iv', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4333}\n",
      "\n",
      "--- Chunk 4334 ---\n",
      "Content:\n",
      "Table of Contents\n",
      "Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\n",
      "1. Language Processing and Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\n",
      "1.1 Computing with Language: Texts and Words 1\n",
      "1.2 A Closer Look at Python: Texts as Lists of Words 10\n",
      "1.3 Computing with Language: Simple Statistics 16\n",
      "1.4 Back to Python: Making Decisions and Taking Control 22\n",
      "1.5 Automatic Natural Language Understanding 27\n",
      "1.6 Summary 33\n",
      "1.7 Further Reading 34\n",
      "1.8 Exercises 35\n",
      "2. Accessing Text Corpora and Lexical Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "2.1 Accessing Text Corpora 39\n",
      "2.2 Conditional Frequency Distributions 52\n",
      "2.3 More Python: Reusing Code 56\n",
      "2.4 Lexical Resources 59\n",
      "2.5 WordNet 67\n",
      "2.6 Summary 73\n",
      "2.7 Further Reading 73\n",
      "2.8 Exercises 74\n",
      "3. Processing Raw Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  79\n",
      "3.1 Accessing Text from the Web and from Disk 80\n",
      "3.2 Strings: Text Processing at the Lowest Level 87\n",
      "3.3 Text Processing with Unicode 93\n",
      "3.4 Regular Expressions for Detecting Word Patterns 97\n",
      "3.5 Useful Applications of Regular Expressions 102\n",
      "3.6 Normalizing Text 107\n",
      "3.7 Regular Expressions for Tokenizing Text 109\n",
      "3.8 Segmentation 112\n",
      "3.9 Formatting: From Lists to Strings 116\n",
      "v...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 6, 'page_label': 'v', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4334}\n",
      "\n",
      "--- Chunk 4335 ---\n",
      "Content:\n",
      "3.10 Summary 121\n",
      "3.11 Further Reading 122\n",
      "3.12 Exercises 123\n",
      "4. Writing Structured Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  129\n",
      "4.1 Back to the Basics 130\n",
      "4.2 Sequences 133\n",
      "4.3 Questions of Style 138\n",
      "4.4 Functions: The Foundation of Structured Programming 142\n",
      "4.5 Doing More with Functions 149\n",
      "4.6 Program Development 154\n",
      "4.7 Algorithm Design 160\n",
      "4.8 A Sample of Python Libraries 167\n",
      "4.9 Summary 172\n",
      "4.10 Further Reading 173\n",
      "4.11 Exercises 173\n",
      "5. Categorizing and Tagging Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  179\n",
      "5.1 Using a Tagger 179\n",
      "5.2 Tagged Corpora 181\n",
      "5.3 Mapping Words to Properties Using Python Dictionaries 189\n",
      "5.4 Automatic Tagging 198\n",
      "5.5 N-Gram Tagging 202\n",
      "5.6 Transformation-Based Tagging 208\n",
      "5.7 How to Determine the Category of a Word 210\n",
      "5.8 Summary 213\n",
      "5.9 Further Reading 214\n",
      "5.10 Exercises 215\n",
      "6. Learning to Classify Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  221\n",
      "6.1 Supervised Classification 221\n",
      "6.2 Further Examples of Supervised Classification 233\n",
      "6.3 Evaluation 237\n",
      "6.4 Decision Trees 242\n",
      "6.5 Naive Bayes Classifiers 245\n",
      "6.6 Maximum Entropy Classifiers 250\n",
      "6.7 Modeling Linguistic Patterns 254\n",
      "6.8 Summary 256\n",
      "6.9 Further Reading 256\n",
      "6.10 Exercises 257\n",
      "7. Extracting Information from Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  261\n",
      "7.1 Information Extraction 261\n",
      "vi | Table of Contents...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 7, 'page_label': 'vi', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4335}\n",
      "\n",
      "--- Chunk 4336 ---\n",
      "Content:\n",
      "7.2 Chunking 264\n",
      "7.3 Developing and Evaluating Chunkers 270\n",
      "7.4 Recursion in Linguistic Structure 277\n",
      "7.5 Named Entity Recognition 281\n",
      "7.6 Relation Extraction 284\n",
      "7.7 Summary 285\n",
      "7.8 Further Reading 286\n",
      "7.9 Exercises 286\n",
      "8. Analyzing Sentence Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  291\n",
      "8.1 Some Grammatical Dilemmas 292\n",
      "8.2 What’s the Use of Syntax? 295\n",
      "8.3 Context-Free Grammar 298\n",
      "8.4 Parsing with Context-Free Grammar 302\n",
      "8.5 Dependencies and Dependency Grammar 310\n",
      "8.6 Grammar Development 315\n",
      "8.7 Summary 321\n",
      "8.8 Further Reading 322\n",
      "8.9 Exercises 322\n",
      "9. Building Feature-Based Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  327\n",
      "9.1 Grammatical Features 327\n",
      "9.2 Processing Feature Structures 337\n",
      "9.3 Extending a Feature-Based Grammar 344\n",
      "9.4 Summary 356\n",
      "9.5 Further Reading 357\n",
      "9.6 Exercises 358\n",
      "10. Analyzing the Meaning of Sentences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  361\n",
      "10.1 Natural Language Understanding 361\n",
      "10.2 Propositional Logic 368\n",
      "10.3 First-Order Logic 372\n",
      "10.4 The Semantics of English Sentences 385\n",
      "10.5 Discourse Semantics 397\n",
      "10.6 Summary 402\n",
      "10.7 Further Reading 403\n",
      "10.8 Exercises 404\n",
      "11. Managing Linguistic Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  407\n",
      "11.1 Corpus Structure: A Case Study 407\n",
      "11.2 The Life Cycle of a Corpus 412\n",
      "11.3 Acquiring Data 416\n",
      "11.4 Working with XML 425\n",
      "Table of Contents | vii...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 8, 'page_label': 'vii', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4336}\n",
      "\n",
      "--- Chunk 4337 ---\n",
      "Content:\n",
      "11.5 Working with Toolbox Data 431\n",
      "11.6 Describing Language Resources Using OLAC Metadata 435\n",
      "11.7 Summary 437\n",
      "11.8 Further Reading 437\n",
      "11.9 Exercises 438\n",
      "Afterword: The Language Challenge .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  441\n",
      "Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  449\n",
      "NLTK Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  459\n",
      "General Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  463\n",
      "viii | Table of Contents...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 9, 'page_label': 'viii', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4337}\n",
      "\n",
      "--- Chunk 4338 ---\n",
      "Content:\n",
      "Preface\n",
      "This is a book about Natural Language Processing. By “natural language” we mean a\n",
      "language \n",
      "that is used for everyday communication by humans; languages such as Eng-\n",
      "lish, Hindi, or Portuguese. In contrast to artificial languages such as programming lan-\n",
      "guages and mathematical notations, natural languages have evolved as they pass from\n",
      "generation to generation, and are hard to pin down with explicit rules. We will take\n",
      "Natural Language Processing—or NLP for short—in a wide sense to cover any kind of\n",
      "computer manipulation of natural language. At one extreme, it could be as simple as\n",
      "counting word frequencies to compare different writing styles. At the other extreme,\n",
      "NLP involves “understanding” complete human utterances, at least to the extent of\n",
      "being able to give useful responses to them.\n",
      "Technologies based on NLP are becoming increasingly widespread. For example,\n",
      "phones and handheld computers support predictive text and handwriting recognition;\n",
      "web search engines give access to information locked up in unstructured text; machine\n",
      "translation allows us to retrieve texts written in Chinese and read them in Spanish. By\n",
      "providing more natural human-machine interfaces, and more sophisticated access to\n",
      "stored information, language processing has come to play a central role in the multi-\n",
      "lingual information society.\n",
      "This book provides a highly accessible introduction to the field of NLP. It can be used\n",
      "for individual study or as the textbook for a course on natural language processing or\n",
      "computational linguistics, or as a supplement to courses in artificial intelligence, text\n",
      "mining, or corpus linguistics. The book is intensely practical, containing hundreds of\n",
      "fully worked examples and graded exercises.\n",
      "The book is based on the Python programming language together with an open source\n",
      "library called the Natural Language Toolkit  (NLTK). NLTK includes extensive soft-\n",
      "ware, data, and documentation, all freely downloadable from http://www.nltk.org/.\n",
      "Distributions are provided for Windows, Macintosh, and Unix platforms. We strongly\n",
      "encourage you to download Python and NLTK, and try out the examples and exercises\n",
      "along the way.\n",
      "ix...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 10, 'page_label': 'ix', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4338}\n",
      "\n",
      "--- Chunk 4339 ---\n",
      "Content:\n",
      "Audience\n",
      "NLP is important for scientific, economic, social, and cultural reasons. NLP is experi-\n",
      "encing \n",
      "rapid growth as its theories and methods are deployed in a variety of new lan-\n",
      "guage technologies. For this reason it is important for a wide range of people to have a\n",
      "working knowledge of NLP. Within industry, this includes people in human-computer\n",
      "interaction, business information analysis, and web software development. Within\n",
      "academia, it includes people in areas from humanities computing and corpus linguistics\n",
      "through to computer science and artificial intelligence. (To many people in academia,\n",
      "NLP is known by the name of “Computational Linguistics.”)\n",
      "This book is intended for a diverse range of people who want to learn how to write\n",
      "programs that analyze written language, regardless of previous programming\n",
      "experience:\n",
      "New to programming?\n",
      "The early chapters of the book are suitable for readers with no prior knowledge of\n",
      "programming, so long as you aren’t afraid to tackle new concepts and develop new\n",
      "computing skills. The book is full of examples that you can copy and try for your-\n",
      "self, together with hundreds of graded exercises. If you need a more general intro-\n",
      "duction to Python, see the list of Python resources at http://docs.python.org/.\n",
      "New to Python?\n",
      "Experienced programmers can quickly learn enough Python using this book to get\n",
      "immersed in natural language processing. All relevant Python features are carefully\n",
      "explained and exemplified, and you will quickly come to appreciate Python’s suit-\n",
      "ability for this application area. The language index will help you locate relevant\n",
      "discussions in the book.\n",
      "Already dreaming in Python?\n",
      "Skim the Python examples and dig into the interesting language analysis material\n",
      "that starts in Chapter 1. You’ll soon be applying your skills to this fascinating\n",
      "domain.\n",
      "Emphasis\n",
      "This book is a practical introduction to NLP. You will learn by example, write real\n",
      "programs, and grasp the value of being able to test an idea through implementation. If\n",
      "you haven’t learned already, this book will teach you programming. Unlike other\n",
      "programming books, we provide extensive illustrations and exercises from NLP. The\n",
      "approach we have taken is also principled, in that we cover the theoretical underpin-\n",
      "nings and don’t shy away from careful linguistic and computational analysis. We have\n",
      "tried to be pragmatic in striking a balance between theory and application, identifying\n",
      "the connections and the tensions. Finally, we recognize that you won’t get through this\n",
      "unless it is also pleasurable, so we have tried to include many applications and ex-\n",
      "amples that are interesting and entertaining, and sometimes whimsical.\n",
      "x | Preface...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 11, 'page_label': 'x', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4339}\n",
      "\n",
      "--- Chunk 4340 ---\n",
      "Content:\n",
      "Note that this book is not a reference work. Its coverage of Python and NLP is selective,\n",
      "and presented in a tutorial style. For reference material, please consult the substantial\n",
      "quantity \n",
      "of searchable resources available at http://python.org/ and http://www.nltk\n",
      ".org/.\n",
      "This book is not an advanced computer science text. The content ranges from intro-\n",
      "ductory to intermediate, and is directed at readers who want to learn how to analyze\n",
      "text using Python and the Natural Language Toolkit. To learn about advanced algo-\n",
      "rithms implemented in NLTK, you can examine the Python code linked from http://\n",
      "www.nltk.org/, and consult the other materials cited in this book.\n",
      "What You Will Learn\n",
      "By digging into the material presented here, you will learn:\n",
      "• How simple programs can help you manipulate and analyze language data, and\n",
      "how to write these programs\n",
      "• How key concepts from NLP and linguistics are used to describe and analyze\n",
      "language\n",
      "• How data structures and algorithms are used in NLP\n",
      "• How language data is stored in standard formats, and how data can be used to\n",
      "evaluate the performance of NLP techniques\n",
      "Depending on your background, and your motivation for being interested in NLP, you\n",
      "will gain different kinds of skills and knowledge from this book, as set out in Table P-1.\n",
      "Table P-1. Skills and knowledge to be gained from reading this book, depending on readers’ goals and\n",
      "background\n",
      "Goals Background in arts and humanities Background in science and engineering\n",
      "Language\n",
      "analysis\n",
      "Manipulating large corpora, exploring linguistic\n",
      "models, and testing empirical claims.\n",
      "Using techniques in data modeling, data mining, and\n",
      "knowledge discovery to analyze natural language.\n",
      "Language\n",
      "technology\n",
      "Building robust systems to perform linguistic tasks\n",
      "with technological applications.\n",
      "Using linguistic algorithms and data structures in robust\n",
      "language processing software.\n",
      "Organization\n",
      "The \n",
      "early chapters are organized in order of conceptual difficulty, starting with a prac-\n",
      "tical introduction to language processing that shows how to explore interesting bodies\n",
      "of text using tiny Python programs (Chapters 1–3). This is followed by a chapter on\n",
      "structured programming (Chapter 4) that consolidates the programming topics scat-\n",
      "tered across the preceding chapters. After this, the pace picks up, and we move on to\n",
      "a series of chapters covering fundamental topics in language processing: tagging, clas-\n",
      "sification, and information extraction (Chapters 5–7). The next three chapters look at\n",
      "Preface | xi...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 12, 'page_label': 'xi', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4340}\n",
      "\n",
      "--- Chunk 4341 ---\n",
      "Content:\n",
      "ways to parse a sentence, recognize its syntactic structure, and construct representa-\n",
      "tions \n",
      "of meaning (Chapters 8–10). The final chapter is devoted to linguistic data and\n",
      "how it can be managed effectively ( Chapter 11). The book concludes with an After-\n",
      "word, briefly discussing the past and future of the field.\n",
      "Within each chapter, we switch between different styles of presentation. In one style,\n",
      "natural language is the driver. We analyze language, explore linguistic concepts, and\n",
      "use programming examples to support the discussion. We often employ Python con-\n",
      "structs that have not been introduced systematically, so you can see their purpose before\n",
      "delving into the details of how and why they work. This is just like learning idiomatic\n",
      "expressions in a foreign language: you’re able to buy a nice pastry without first having\n",
      "learned the intricacies of question formation. In the other style of presentation, the\n",
      "programming language will be the driver. We’ll analyze programs, explore algorithms,\n",
      "and the linguistic examples will play a supporting role.\n",
      "Each chapter ends with a series of graded exercises, which are useful for consolidating\n",
      "the material. The exercises are graded according to the following scheme: ○ is for easy\n",
      "exercises that involve minor modifications to supplied code samples or other simple\n",
      "activities; ◑ is for intermediate exercises that explore an aspect of the material in more\n",
      "depth, requiring careful analysis and design; ● is for difficult, open-ended tasks that\n",
      "will challenge your understanding of the material and force you to think independently\n",
      "(readers new to programming should skip these).\n",
      "Each chapter has a further reading section and an online “extras” section at http://www\n",
      ".nltk.org/, with pointers to more advanced materials and online resources. Online ver-\n",
      "sions of all the code examples are also available there.\n",
      "Why Python?\n",
      "Python is a simple yet powerful programming language with excellent functionality for\n",
      "processing linguistic data. Python can be downloaded for free from http://www.python\n",
      ".org/. Installers are available for all platforms.\n",
      "Here is a five-line Python program that processes file.txt and prints all the words ending\n",
      "in ing:\n",
      ">>> for line in open(\"file.txt\"):\n",
      "...     for word in line.split():\n",
      "...         if word.endswith('ing'):\n",
      "...             print word\n",
      "This program illustrates some of the main features of Python. First, whitespace is used\n",
      "to nest lines of code; thus the line starting with if falls inside the scope of the previous\n",
      "line starting with for; this ensures that the ing test is performed for each word. Second,\n",
      "Python is object-oriented; each variable is an entity that has certain defined attributes\n",
      "and methods. For example, the value of the variable line is more than a sequence of\n",
      "characters. It is a string object that has a “method” (or operation) called split() that\n",
      "xii | Preface...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 13, 'page_label': 'xii', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4341}\n",
      "\n",
      "--- Chunk 4342 ---\n",
      "Content:\n",
      "we can use to break a line into its words. To apply a method to an object, we write the\n",
      "object \n",
      "name, followed by a period, followed by the method name, i.e., line.split().\n",
      "Third, methods have arguments expressed inside parentheses. For instance, in the ex-\n",
      "ample, word.endswith('ing') had the argument 'ing' to indicate that we wanted words\n",
      "ending with ing and not something else. Finally—and most importantly—Python is\n",
      "highly readable, so much so that it is fairly easy to guess what this program does even\n",
      "if you have never written a program before.\n",
      "We chose Python because it has a shallow learning curve, its syntax and semantics are\n",
      "transparent, and it has good string-handling functionality. As an interpreted language,\n",
      "Python facilitates interactive exploration. As an object-oriented language, Python per-\n",
      "mits data and methods to be encapsulated and re-used easily. As a dynamic language,\n",
      "Python permits attributes to be added to objects on the fly, and permits variables to be\n",
      "typed dynamically, facilitating rapid development. Python comes with an extensive\n",
      "standard library, including components for graphical programming, numerical pro-\n",
      "cessing, and web connectivity.\n",
      "Python is heavily used in industry, scientific research, and education around the world.\n",
      "Python is often praised for the way it facilitates productivity, quality, and main-\n",
      "tainability of software. A collection of Python success stories is posted at http://www\n",
      ".python.org/about/success/.\n",
      "NLTK defines an infrastructure that can be used to build NLP programs in Python. It\n",
      "provides basic classes for representing data relevant to natural language processing;\n",
      "standard interfaces for performing tasks such as part-of-speech tagging, syntactic pars-\n",
      "ing, and text classification; and standard implementations for each task that can be\n",
      "combined to solve complex problems.\n",
      "NLTK comes with extensive documentation. In addition to this book, the website at\n",
      "http://www.nltk.org/ provides API documentation that covers every module, class, and\n",
      "function in the toolkit, specifying parameters and giving examples of usage. The website\n",
      "also provides many HOWTOs with extensive examples and test cases, intended for\n",
      "users, developers, and instructors.\n",
      "Software Requirements\n",
      "To get the most out of this book, you should install several free software packages.\n",
      "Current download pointers and instructions are available at http://www.nltk.org/.\n",
      "Python\n",
      "The material presented in this book assumes that you are using Python version 2.4\n",
      "or 2.5. We are committed to porting NLTK to Python 3.0 once the libraries that\n",
      "NLTK depends on have been ported.\n",
      "NLTK\n",
      "The code examples in this book use NLTK version 2.0. Subsequent releases of\n",
      "NLTK will be backward-compatible.\n",
      "Preface | xiii...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 14, 'page_label': 'xiii', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4342}\n",
      "\n",
      "--- Chunk 4343 ---\n",
      "Content:\n",
      "NLTK-Data\n",
      "This contains the linguistic corpora that are analyzed and processed in the book.\n",
      "NumPy (recommended)\n",
      "This \n",
      "is a scientific computing library with support for multidimensional arrays and\n",
      "linear algebra, required for certain probability, tagging, clustering, and classifica-\n",
      "tion tasks.\n",
      "Matplotlib (recommended)\n",
      "This is a 2D plotting library for data visualization, and is used in some of the book’s\n",
      "code samples that produce line graphs and bar charts.\n",
      "NetworkX (optional)\n",
      "This is a library for storing and manipulating network structures consisting of\n",
      "nodes and edges. For visualizing semantic networks, also install the Graphviz\n",
      "library.\n",
      "Prover9 (optional)\n",
      "This is an automated theorem prover for first-order and equational logic, used to\n",
      "support inference in language processing.\n",
      "Natural Language Toolkit (NLTK)\n",
      "NLTK was originally created in 2001 as part of a computational linguistics course in\n",
      "the Department of Computer and Information Science at the University of Pennsylva-\n",
      "nia. Since then it has been developed and expanded with the help of dozens of con-\n",
      "tributors. It has now been adopted in courses in dozens of universities, and serves as\n",
      "the basis of many research projects. Table P-2 lists the most important NLTK modules.\n",
      "Table P-2. Language processing tasks and corresponding NLTK modules with examples of\n",
      "functionality\n",
      "Language processing task NLTK modules Functionality\n",
      "Accessing corpora nltk.corpus Standardized interfaces to corpora and lexicons\n",
      "String processing nltk.tokenize, nltk.stem Tokenizers, sentence tokenizers, stemmers\n",
      "Collocation discovery nltk.collocations t-test, chi-squared, point-wise mutual information\n",
      "Part-of-speech tagging nltk.tag n-gram, backoff, Brill, HMM, TnT\n",
      "Classification nltk.classify, nltk.cluster Decision tree, maximum entropy, naive Bayes, EM, k-means\n",
      "Chunking nltk.chunk Regular expression, n-gram, named entity\n",
      "Parsing nltk.parse Chart, feature-based, unification, probabilistic, dependency\n",
      "Semantic interpretation nltk.sem, nltk.inference Lambda calculus, first-order logic, model checking\n",
      "Evaluation metrics nltk.metrics Precision, recall, agreement coefficients\n",
      "Probability and estimation nltk.probability Frequency distributions, smoothed probability distributions\n",
      "Applications nltk.app, nltk.chat Graphical concordancer, parsers, WordNet browser, chatbots\n",
      "xiv | Preface...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 15, 'page_label': 'xiv', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4343}\n",
      "\n",
      "--- Chunk 4344 ---\n",
      "Content:\n",
      "Language processing task NLTK modules Functionality\n",
      "Linguistic fieldwork nltk.toolbox Manipulate data in SIL Toolbox format\n",
      "NLTK was designed with four primary goals in mind:\n",
      "Simplicity\n",
      "To \n",
      "provide an intuitive framework along with substantial building blocks, giving\n",
      "users a practical knowledge of NLP without getting bogged down in the tedious\n",
      "house-keeping usually associated with processing annotated language data\n",
      "Consistency\n",
      "To provide a uniform framework with consistent interfaces and data structures,\n",
      "and easily guessable method names\n",
      "Extensibility\n",
      "To provide a structure into which new software modules can be easily accommo-\n",
      "dated, including alternative implementations and competing approaches to the\n",
      "same task\n",
      "Modularity\n",
      "To provide components that can be used independently without needing to un-\n",
      "derstand the rest of the toolkit\n",
      "Contrasting with these goals are three non-requirements—potentially useful qualities\n",
      "that we have deliberately avoided. First, while the toolkit provides a wide range of\n",
      "functions, it is not encyclopedic; it is a toolkit, not a system, and it will continue to\n",
      "evolve with the field of NLP. Second, while the toolkit is efficient enough to support\n",
      "meaningful tasks, it is not highly optimized for runtime performance; such optimiza-\n",
      "tions often involve more complex algorithms, or implementations in lower-level pro-\n",
      "gramming languages such as C or C++. This would make the software less readable\n",
      "and more difficult to install. Third, we have tried to avoid clever programming tricks,\n",
      "since we believe that clear implementations are preferable to ingenious yet indecipher-\n",
      "able ones.\n",
      "For Instructors\n",
      "Natural Language Processing is often taught within the confines of a single-semester\n",
      "course at the advanced undergraduate level or postgraduate level. Many instructors\n",
      "have found that it is difficult to cover both the theoretical and practical sides of the\n",
      "subject in such a short span of time. Some courses focus on theory to the exclusion of\n",
      "practical exercises, and deprive students of the challenge and excitement of writing\n",
      "programs to automatically process language. Other courses are simply designed to\n",
      "teach programming for linguists, and do not manage to cover any significant NLP con-\n",
      "tent. NLTK was originally developed to address this problem, making it feasible to\n",
      "cover a substantial amount of theory and practice within a single-semester course, even\n",
      "if students have no prior programming experience.\n",
      "Preface | xv...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 16, 'page_label': 'xv', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4344}\n",
      "\n",
      "--- Chunk 4345 ---\n",
      "Content:\n",
      "A significant fraction of any NLP syllabus deals with algorithms and data structures.\n",
      "On \n",
      "their own these can be rather dry, but NLTK brings them to life with the help of\n",
      "interactive graphical user interfaces that make it possible to view algorithms step-by-\n",
      "step. Most NLTK components include a demonstration that performs an interesting\n",
      "task without requiring any special input from the user. An effective way to deliver the\n",
      "materials is through interactive presentation of the examples in this book, entering\n",
      "them in a Python session, observing what they do, and modifying them to explore some\n",
      "empirical or theoretical issue.\n",
      "This book contains hundreds of exercises that can be used as the basis for student\n",
      "assignments. The simplest exercises involve modifying a supplied program fragment in\n",
      "a specified way in order to answer a concrete question. At the other end of the spectrum,\n",
      "NLTK provides a flexible framework for graduate-level research projects, with standard\n",
      "implementations of all the basic data structures and algorithms, interfaces to dozens\n",
      "of widely used datasets (corpora), and a flexible and extensible architecture. Additional\n",
      "support for teaching using NLTK is available on the NLTK website.\n",
      "We believe this book is unique in providing a comprehensive framework for students\n",
      "to learn about NLP in the context of learning to program. What sets these materials\n",
      "apart is the tight coupling of the chapters and exercises with NLTK, giving students—\n",
      "even those with no prior programming experience—a practical introduction to NLP.\n",
      "After completing these materials, students will be ready to attempt one of the more\n",
      "advanced textbooks, such as Speech and Language Processing, by Jurafsky and Martin\n",
      "(Prentice Hall, 2008).\n",
      "This book presents programming concepts in an unusual order, beginning with a non-\n",
      "trivial data type—lists of strings—then introducing non-trivial control structures such\n",
      "as comprehensions and conditionals. These idioms permit us to do useful language\n",
      "processing from the start. Once this motivation is in place, we return to a systematic\n",
      "presentation of fundamental concepts such as strings, loops, files, and so forth. In this\n",
      "way, we cover the same ground as more conventional approaches, without expecting\n",
      "readers to be interested in the programming language for its own sake.\n",
      "Two possible course plans are illustrated in Table P-3. The first one presumes an arts/\n",
      "humanities audience, whereas the second one presumes a science/engineering audi-\n",
      "ence. Other course plans could cover the first five chapters, then devote the remaining\n",
      "time to a single area, such as text classification (Chapters 6 and 7), syntax (Chapters\n",
      "8 and 9), semantics (Chapter 10), or linguistic data management (Chapter 11).\n",
      "Table P-3. Suggested course plans; approximate number of lectures per chapter\n",
      "Chapter Arts and Humanities Science and Engineering\n",
      "Chapter 1, Language Processing and Python 2–4 2\n",
      "Chapter 2, Accessing Text Corpora and Lexical Resources 2–4 2\n",
      "Chapter 3, Processing Raw Text 2–4 2\n",
      "Chapter 4, Writing Structured Programs 2–4 1–2\n",
      "xvi | Preface...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 17, 'page_label': 'xvi', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4345}\n",
      "\n",
      "--- Chunk 4346 ---\n",
      "Content:\n",
      "Chapter Arts and Humanities Science and Engineering\n",
      "Chapter 5, Categorizing and Tagging Words 2–4 2–4\n",
      "Chapter 6, Learning to Classify Text 0–2 2–4\n",
      "Chapter 7, Extracting Information from Text 2 2–4\n",
      "Chapter 8, Analyzing Sentence Structure 2–4 2–4\n",
      "Chapter 9, Building Feature-Based Grammars 2–4 1–4\n",
      "Chapter 10, Analyzing the Meaning of Sentences 1–2 1–4\n",
      "Chapter 11, Managing Linguistic Data 1–2 1–4\n",
      "Total 18–36 18–36\n",
      "Conventions Used in This Book\n",
      "The following typographical conventions are used in this book:\n",
      "Bold\n",
      "Indicates new terms.\n",
      "Italic\n",
      "Used \n",
      "within paragraphs to refer to linguistic examples, the names of texts, and\n",
      "URLs; also used for filenames and file extensions.\n",
      "Constant width\n",
      "Used for program listings, as well as within paragraphs to refer to program elements\n",
      "such as variable or function names, statements, and keywords; also used for pro-\n",
      "gram names.\n",
      "Constant width italic\n",
      "Shows text that should be replaced with user-supplied values or by values deter-\n",
      "mined by context; also used for metavariables within program code examples.\n",
      "This icon signifies a tip, suggestion, or general note.\n",
      "This icon indicates a warning or caution.\n",
      "Using Code Examples\n",
      "This \n",
      "book is here to help you get your job done. In general, you may use the code in\n",
      "this book in your programs and documentation. You do not need to contact us for\n",
      "permission unless you’re reproducing a significant portion of the code. For example,\n",
      "Preface | xvii...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 18, 'page_label': 'xvii', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4346}\n",
      "\n",
      "--- Chunk 4347 ---\n",
      "Content:\n",
      "writing a program that uses several chunks of code from this book does not require\n",
      "permission. \n",
      "Selling or distributing a CD-ROM of examples from O’Reilly books does\n",
      "require permission. Answering a question by citing this book and quoting example\n",
      "code does not require permission. Incorporating a significant amount of example code\n",
      "from this book into your product’s documentation does require permission.\n",
      "We appreciate, but do not require, attribution. An attribution usually includes the title,\n",
      "author, publisher, and ISBN. For example: “ Natural Language Processing with Py-\n",
      "thon, by Steven Bird, Ewan Klein, and Edward Loper. Copyright 2009 Steven Bird,\n",
      "Ewan Klein, and Edward Loper, 978-0-596-51649-9.”\n",
      "If you feel your use of code examples falls outside fair use or the permission given above,\n",
      "feel free to contact us at permissions@oreilly.com.\n",
      "Safari® Books Online\n",
      "When you see a Safari® Books Online icon on the cover of your favorite\n",
      "technology \n",
      "book, that means the book is available online through the\n",
      "O’Reilly Network Safari Bookshelf.\n",
      "Safari offers a solution that’s better than e-books. It’s a virtual library that lets you easily\n",
      "search thousands of top tech books, cut and paste code samples, download chapters,\n",
      "and find quick answers when you need the most accurate, current information. Try it\n",
      "for free at http://my.safaribooksonline.com.\n",
      "How to Contact Us\n",
      "Please address comments and questions concerning this book to the publisher:\n",
      "O’Reilly Media, Inc.\n",
      "1005 Gravenstein Highway North\n",
      "Sebastopol, CA 95472\n",
      "800-998-9938 (in the United States or Canada)\n",
      "707-829-0515 (international or local)\n",
      "707-829-0104 (fax)\n",
      "We have a web page for this book, where we list errata, examples, and any additional\n",
      "information. You can access this page at:\n",
      "http://www.oreilly.com/catalog/9780596516499\n",
      "xviii | Preface...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 19, 'page_label': 'xviii', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4347}\n",
      "\n",
      "--- Chunk 4348 ---\n",
      "Content:\n",
      "The authors provide additional materials for each chapter via the NLTK website at:\n",
      "http://www.nltk.org/\n",
      "To comment or ask technical questions about this book, send email to:\n",
      "bookquestions@oreilly.com\n",
      "For more information about our books, conferences, Resource Centers, and the\n",
      "O’Reilly Network, see our website at:\n",
      "http://www.oreilly.com\n",
      "Acknowledgments\n",
      "The \n",
      "authors are indebted to the following people for feedback on earlier drafts of this\n",
      "book: Doug Arnold, Michaela Atterer, Greg Aumann, Kenneth Beesley, Steven Bethard,\n",
      "Ondrej Bojar, Chris Cieri, Robin Cooper, Grev Corbett, James Curran, Dan Garrette,\n",
      "Jean Mark Gawron, Doug Hellmann, Nitin Indurkhya, Mark Liberman, Peter Ljunglöf,\n",
      "Stefan Müller, Robin Munn, Joel Nothman, Adam Przepiorkowski, Brandon Rhodes,\n",
      "Stuart Robinson, Jussi Salmela, Kyle Schlansker, Rob Speer, and Richard Sproat. We\n",
      "are thankful to many students and colleagues for their comments on the class materials\n",
      "that evolved into these chapters, including participants at NLP and linguistics summer\n",
      "schools in Brazil, India, and the USA. This book would not exist without the members\n",
      "of the nltk-dev developer community, named on the NLTK website, who have given\n",
      "so freely of their time and expertise in building and extending NLTK.\n",
      "We are grateful to the U.S. National Science Foundation, the Linguistic Data Consor-\n",
      "tium, an Edward Clarence Dyason Fellowship, and the Universities of Pennsylvania,\n",
      "Edinburgh, and Melbourne for supporting our work on this book.\n",
      "We thank Julie Steele, Abby Fox, Loranah Dimant, and the rest of the O’Reilly team,\n",
      "for organizing comprehensive reviews of our drafts from people across the NLP and\n",
      "Python communities, for cheerfully customizing O’Reilly’s production tools to accom-\n",
      "modate our needs, and for meticulous copyediting work.\n",
      "Finally, we owe a huge debt of gratitude to our partners, Kay, Mimo, and Jee, for their\n",
      "love, patience, and support over the many years that we worked on this book. We hope\n",
      "that our children—Andrew, Alison, Kirsten, Leonie, and Maaike—catch our enthusi-\n",
      "asm for language and computation from these pages.\n",
      "Royalties\n",
      "Royalties from the sale of this book are being used to support the development of the\n",
      "Natural Language Toolkit.\n",
      "Preface | xix...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 20, 'page_label': 'xix', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4348}\n",
      "\n",
      "--- Chunk 4349 ---\n",
      "Content:\n",
      "Figure P-1. Edward Loper, Ewan Klein, and Steven Bird, Stanford, July 2007\n",
      "xx | Preface...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 21, 'page_label': 'xx', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4349}\n",
      "\n",
      "--- Chunk 4350 ---\n",
      "Content:\n",
      "CHAPTER 1\n",
      "Language Processing and Python\n",
      "It is easy to get our hands on millions of words of text. What can we do with it, assuming\n",
      "we \n",
      "can write some simple programs? In this chapter, we’ll address the following\n",
      "questions:\n",
      "1. What can we achieve by combining simple programming techniques with large\n",
      "quantities of text?\n",
      "2. How can we automatically extract key words and phrases that sum up the style\n",
      "and content of a text?\n",
      "3. What tools and techniques does the Python programming language provide for\n",
      "such work?\n",
      "4. What are some of the interesting challenges of natural language processing?\n",
      "This chapter is divided into sections that skip between two quite different styles. In the\n",
      "“computing with language” sections, we will take on some linguistically motivated\n",
      "programming tasks without necessarily explaining how they work. In the “closer look\n",
      "at Python” sections we will systematically review key programming concepts. We’ll\n",
      "flag the two styles in the section titles, but later chapters will mix both styles without\n",
      "being so up-front about it. We hope this style of introduction gives you an authentic\n",
      "taste of what will come later, while covering a range of elementary concepts in linguis-\n",
      "tics and computer science. If you have basic familiarity with both areas, you can skip\n",
      "to Section 1.5; we will repeat any important points in later chapters, and if you miss\n",
      "anything you can easily consult the online reference material at http://www.nltk.org/. If\n",
      "the material is completely new to you, this chapter will raise more questions than it\n",
      "answers, questions that are addressed in the rest of this book.\n",
      "1.1  Computing with Language: Texts and Words\n",
      "We’re all very familiar with text, since we read and write it every day. Here we will treat\n",
      "text as raw data for the programs we write, programs that manipulate and analyze it in\n",
      "a variety of interesting ways. But before we can do this, we have to get started with the\n",
      "Python interpreter.\n",
      "1...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 22, 'page_label': '1', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4350}\n",
      "\n",
      "--- Chunk 4351 ---\n",
      "Content:\n",
      "Getting Started with Python\n",
      "One \n",
      "of the friendly things about Python is that it allows you to type directly into the\n",
      "interactive interpreter—the program that will be running your Python programs. You\n",
      "can access the Python interpreter using a simple graphical interface called the In-\n",
      "teractive DeveLopment Environment (IDLE). On a Mac you can find this under Ap-\n",
      "plications→MacPython, and on Windows under All Programs →Python. Under Unix\n",
      "you can run Python from the shell by typing idle (if this is not installed, try typing\n",
      "python). The interpreter will print a blurb about your Python version; simply check that\n",
      "you are running Python 2.4 or 2.5 (here it is 2.5.1):\n",
      "Python 2.5.1 (r251:54863, Apr 15 2008, 22:57:26)\n",
      "[GCC 4.0.1 (Apple Inc. build 5465)] on darwin\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>>\n",
      "If you are unable to run the Python interpreter, you probably don’t have\n",
      "Python \n",
      "installed correctly. Please visit http://python.org/ for detailed in-\n",
      "structions.\n",
      "The >>> prompt indicates that the Python interpreter is now waiting for input. When\n",
      "copying examples from this book, don’t type the “>>>” yourself. Now, let’s begin by\n",
      "using Python as a calculator:\n",
      ">>> 1 + 5 * 2 - 3\n",
      "8\n",
      ">>>\n",
      "Once the interpreter has finished calculating the answer and displaying it, the prompt\n",
      "reappears. This means the Python interpreter is waiting for another instruction.\n",
      "Your Turn: Enter a few more expressions of your own. You can use\n",
      "asterisk (*) for multiplication and slash (/) for division, and parentheses\n",
      "for bracketing expressions. Note that division doesn’t always behave as\n",
      "you might expect—it does integer division (with rounding of fractions\n",
      "downwards) when you type 1/3 and “floating-point” (or decimal) divi-\n",
      "sion when you type 1.0/3.0. In order to get the expected behavior of\n",
      "division (standard in Python 3.0), you need to type: from __future__\n",
      "import division.\n",
      "The preceding examples demonstrate how you can work interactively with the Python\n",
      "interpreter, experimenting with various expressions in the language to see what they\n",
      "do. Now let’s try a non-sensical expression to see how the interpreter handles it:\n",
      "2 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 23, 'page_label': '2', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4351}\n",
      "\n",
      "--- Chunk 4352 ---\n",
      "Content:\n",
      ">>> 1 +\n",
      "  File \"<stdin>\", line 1\n",
      "    1 +\n",
      "      ^\n",
      "SyntaxError: invalid syntax\n",
      ">>>\n",
      "This produced a syntax error. \n",
      "In Python, it doesn’t make sense to end an instruction\n",
      "with a plus sign. The Python interpreter indicates the line where the problem occurred\n",
      "(line 1 of <stdin>, which stands for “standard input”).\n",
      "Now that we can use the Python interpreter, we’re ready to start working with language\n",
      "data.\n",
      "Getting Started with NLTK\n",
      "Before going further you should install NLTK, downloadable for free from http://www\n",
      ".nltk.org/. Follow the instructions there to download the version required for your\n",
      "platform.\n",
      "Once you’ve installed NLTK, start up the Python interpreter as before, and install the\n",
      "data required for the book by typing the following two commands at the Python\n",
      "prompt, then selecting the book collection as shown in Figure 1-1.\n",
      ">>> import nltk\n",
      ">>> nltk.download()\n",
      "Figure 1-1. Downloading the NLTK Book Collection: Browse the available packages using\n",
      "n\n",
      "ltk.download(). The Collections tab on the downloader shows how the packages are grouped into\n",
      "sets, and you should select the line labeled book to obtain all data required for the examples and\n",
      "exercises in this book. It consists of about 30 compressed files requiring about 100Mb disk space. The\n",
      "full collection of data (i.e., all in the downloader) is about five times this size (at the time of writing)\n",
      "and continues to expand.\n",
      "Once the data is downloaded to your machine, you can load some of it using the Python\n",
      "interpreter. The first step is to type a special command at the Python prompt, which\n",
      "1.1  Computing with Language: Texts and Words | 3...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 24, 'page_label': '3', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4352}\n",
      "\n",
      "--- Chunk 4353 ---\n",
      "Content:\n",
      "tells the interpreter to load some texts for us to explore: from nltk.book import *. This\n",
      "says “from NLTK’s book module, load all items.” The book module contains all the data\n",
      "you will need as you read this chapter. After printing a welcome message, it loads the\n",
      "text of several books (this will take a few seconds). Here’s the command again, together\n",
      "with the output that you will see. Take care to get spelling and punctuation right, and\n",
      "remember that you don’t type the >>>.\n",
      ">>> from nltk.book import *\n",
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      ">>>\n",
      "Any time we want to find out about these texts, we just have to enter their names at\n",
      "the Python prompt:\n",
      ">>> text1\n",
      "<Text: Moby Dick by Herman Melville 1851>\n",
      ">>> text2\n",
      "<Text: Sense and Sensibility by Jane Austen 1811>\n",
      ">>>\n",
      "Now that we can use the Python interpreter, and have some data to work with, we’re\n",
      "ready to get started.\n",
      "Searching Text\n",
      "There are many ways to examine the context of a text apart from simply reading it. A\n",
      "concordance view shows us every occurrence of a given word, together with some\n",
      "context. Here we look up the word monstrous in Moby Dick by entering text1 followed\n",
      "by a period, then the term concordance, and then placing \"monstrous\" in parentheses:\n",
      ">>> text1.concordance(\"monstrous\")\n",
      "Building index...\n",
      "Displaying 11 of 11 matches:\n",
      "ong the former , one was of a most monstrous size . ... This came towards us ,\n",
      "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
      "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
      "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
      "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
      "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
      "th of Radney .'\" CHAPTER 55 Of the monstrous Pictures of Whales . I shall ere l\n",
      "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
      "ere to enter upon those still more monstrous stories of them which are to be fo\n",
      "4 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 25, 'page_label': '4', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4353}\n",
      "\n",
      "--- Chunk 4354 ---\n",
      "Content:\n",
      "ght have been rummaged out of this monstrous cabinet there is no telling . But\n",
      "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n",
      ">>>\n",
      "Your Turn: Try searching for other words; to save re-typing, you might\n",
      "be able to use up-arrow, Ctrl-up-arrow, or Alt-p to access the previous\n",
      "command and modify the word being searched. You can also try search-\n",
      "es on some of the other texts we have included. For example, search\n",
      "Sense and Sensibility  for the word affection, using text2.concord\n",
      "ance(\"affection\"). Search the book of Genesis to find out how long\n",
      "some people lived, using: text3.concordance(\"lived\"). You could look\n",
      "at text4, the Inaugural Address Corpus, to see examples of English going\n",
      "back to 1789, and search for words like nation, terror, god to see how\n",
      "these words have been used differently over time. We’ve also included\n",
      "text5, the NPS Chat Corpus: search this for unconventional words like\n",
      "im, ur, lol. (Note that this corpus is uncensored!)\n",
      "Once you’ve spent a little while examining these texts, we hope you have a new sense\n",
      "of the richness and diversity of language. In the next chapter you will learn how to\n",
      "access a broader range of text, including text in languages other than English.\n",
      "A concordance permits us to see words in context. For example, we saw that mon-\n",
      "strous occurred in contexts such as the ___ pictures and the ___ size. What other words\n",
      "appear in a similar range of contexts? We can find out by appending the term\n",
      "similar to the name of the text in question, then inserting the relevant word in\n",
      "parentheses:\n",
      ">>> text1.similar(\"monstrous\")\n",
      "Building word-context index...\n",
      "subtly impalpable pitiable curious imperial perilous trustworthy\n",
      "abundant untoward singular lamentable few maddens horrible loving lazy\n",
      "mystifying christian exasperate puzzled\n",
      ">>> text2.similar(\"monstrous\")\n",
      "Building word-context index...\n",
      "very exceedingly so heartily a great good amazingly as sweet\n",
      "remarkably extremely vast\n",
      ">>>\n",
      "Observe that we get different results for different texts. Austen uses this word quite\n",
      "differently from Melville; for her, monstrous has positive connotations, and sometimes\n",
      "functions as an intensifier like the word very.\n",
      "The term common_contexts allows us to examine just the contexts that are shared by\n",
      "two or more words, such as monstrous and very. We have to enclose these words by\n",
      "square brackets as well as parentheses, and separate them with a comma:\n",
      ">>> text2.common_contexts([\"monstrous\", \"very\"])\n",
      "be_glad am_glad a_pretty is_pretty a_lucky\n",
      ">>>\n",
      "1.1  Computing with Language: Texts and Words | 5...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 26, 'page_label': '5', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4354}\n",
      "\n",
      "--- Chunk 4355 ---\n",
      "Content:\n",
      "Your Turn: Pick another pair of words and compare their usage in two\n",
      "different texts, using the similar() and common_contexts() functions.\n",
      "It is one thing to automatically detect that a particular word occurs in a text, and to\n",
      "display some words that appear in the same context. However, we can also determine\n",
      "the location of a word in the text: how many words from the beginning it appears. This\n",
      "positional information can be displayed using a dispersion plot. Each stripe represents\n",
      "an instance of a word, and each row represents the entire text. In Figure 1-2 we see\n",
      "some striking patterns of word usage over the last 220 years (in an artificial text con-\n",
      "structed by joining the texts of the Inaugural Address Corpus end-to-end). You can\n",
      "produce this plot as shown below. You might like to try more words (e.g., liberty,\n",
      "constitution) and different texts. Can you predict the dispersion of a word before you\n",
      "view it? As before, take care to get the quotes, commas, brackets, and parentheses\n",
      "exactly right.\n",
      ">>> text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])\n",
      ">>>\n",
      "Important: You need to have Python’s NumPy and Matplotlib pack-\n",
      "ages installed in order to produce the graphical plots used in this book.\n",
      "Please see http://www.nltk.org/ for installation instructions.\n",
      "Now, just for fun, let’s try generating some random text in the various styles we have\n",
      "just seen. To do this, we type the name of the text followed by the term generate. (We\n",
      "need to include the parentheses, but there’s nothing that goes between them.)\n",
      "Figure 1-2. Lexical dispersion plot for words in U.S. Presidential Inaugural Addresses: This can be\n",
      "used to investigate changes in language use over time.\n",
      "6 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 27, 'page_label': '6', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4355}\n",
      "\n",
      "--- Chunk 4356 ---\n",
      "Content:\n",
      ">>> text3.generate()\n",
      "In the beginning of his brother is a hairy man , whose top may reach\n",
      "unto heaven ; and ye shall sow the land of Egypt there was no bread in\n",
      "all that he was taken out of the month , upon the earth . So shall thy\n",
      "wages be ? And they made their father ; and Isaac was old , and kissed\n",
      "him : and Laban with his cattle in the midst of the hands of Esau thy\n",
      "first born , and Phichol the chief butler unto his son Isaac , she\n",
      ">>>\n",
      "Note \n",
      "that the first time you run this command, it is slow because it gathers statistics\n",
      "about word sequences. Each time you run it, you will get different output text. Now\n",
      "try generating random text in the style of an inaugural address or an Internet chat room.\n",
      "Although the text is random, it reuses common words and phrases from the source text\n",
      "and gives us a sense of its style and content. (What is lacking in this randomly generated\n",
      "text?)\n",
      "When generate produces its output, punctuation is split off from the\n",
      "preceding word. While this is not correct formatting for English text,\n",
      "we do it to make clear that words and punctuation are independent of\n",
      "one another. You will learn more about this in Chapter 3.\n",
      "Counting Vocabulary\n",
      "The most obvious fact about texts that emerges from the preceding examples is that\n",
      "they differ in the vocabulary they use. In this section, we will see how to use the com-\n",
      "puter to count the words in a text in a variety of useful ways. As before, you will jump\n",
      "right in and experiment with the Python interpreter, even though you may not have\n",
      "studied Python systematically yet. Test your understanding by modifying the examples,\n",
      "and trying the exercises at the end of the chapter.\n",
      "Let’s begin by finding out the length of a text from start to finish, in terms of the words\n",
      "and punctuation symbols that appear. We use the term len to get the length of some-\n",
      "thing, which we’ll apply here to the book of Genesis:\n",
      ">>> len(text3)\n",
      "44764\n",
      ">>>\n",
      "So Genesis has 44,764 words and punctuation symbols, or “tokens.” A token is the\n",
      "technical name for a sequence of characters—such as hairy, his, or :)—that we want\n",
      "to treat as a group. When we count the number of tokens in a text, say, the phrase to\n",
      "be or not to be, we are counting occurrences of these sequences. Thus, in our example\n",
      "phrase there are two occurrences of to, two of be, and one each of or and not. But there\n",
      "are only four distinct vocabulary items in this phrase. How many distinct words does\n",
      "the book of Genesis contain? To work this out in Python, we have to pose the question\n",
      "slightly differently. The vocabulary of a text is just the set of tokens that it uses, since\n",
      "in a set, all duplicates are collapsed together. In Python we can obtain the vocabulary\n",
      "1.1  Computing with Language: Texts and Words | 7...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 28, 'page_label': '7', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4356}\n",
      "\n",
      "--- Chunk 4357 ---\n",
      "Content:\n",
      "items of text3 with the command: set(text3). When you do this, many screens of\n",
      "words will fly past. Now try the following:\n",
      ">>> sorted(set(text3)) \n",
      "['!', \"'\", '(', ')', ',', ',)', '.', '.)', ':', ';', ';)', '?', '?)',\n",
      "'A', 'Abel', 'Abelmizraim', 'Abidah', 'Abide', 'Abimael', 'Abimelech',\n",
      "'Abr', 'Abrah', 'Abraham', 'Abram', 'Accad', 'Achbor', 'Adah', ...]\n",
      ">>> len(set(text3)) \n",
      "2789\n",
      ">>>\n",
      "By \n",
      "wrapping sorted() around the Python expression set(text3) \n",
      " , we obtain a sorted\n",
      "list \n",
      "of vocabulary items, beginning with various punctuation symbols and continuing\n",
      "with words starting with A. All capitalized words precede lowercase words. We dis-\n",
      "cover the size of the vocabulary indirectly, by asking for the number of items in the set,\n",
      "and again we can use len to obtain this number \n",
      " . Although it has 44,764 tokens, this\n",
      "book \n",
      "has only 2,789 distinct words, or “word types.” A word type  is the form or\n",
      "spelling of the word independently of its specific occurrences in a text—that is, the\n",
      "word considered as a unique item of vocabulary. Our count of 2,789 items will include\n",
      "punctuation symbols, so we will generally call these unique items types instead of word\n",
      "types.\n",
      "Now, let’s calculate a measure of the lexical richness of the text. The next example\n",
      "shows us that each word is used 16 times on average (we need to make sure Python\n",
      "uses floating-point division):\n",
      ">>> from __future__ import division\n",
      ">>> len(text3) / len(set(text3))\n",
      "16.050197203298673\n",
      ">>>\n",
      "Next, let’s focus on particular words. We can count how often a word occurs in a text,\n",
      "and compute what percentage of the text is taken up by a specific word:\n",
      ">>> text3.count(\"smote\")\n",
      "5\n",
      ">>> 100 * text4.count('a') / len(text4)\n",
      "1.4643016433938312\n",
      ">>>\n",
      "Your Turn: How many times does the word lol appear in text5? How\n",
      "much is this as a percentage of the total number of words in this text?\n",
      "You may want to repeat such calculations on several texts, but it is tedious to keep\n",
      "retyping the formula. Instead, you can come up with your own name for a task, like\n",
      "“lexical_diversity” or “percentage”, and associate it with a block of code. Now you\n",
      "only have to type a short name instead of one or more complete lines of Python code,\n",
      "and you can reuse it as often as you like. The block of code that does a task for us is\n",
      "8 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 29, 'page_label': '8', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4357}\n",
      "\n",
      "--- Chunk 4358 ---\n",
      "Content:\n",
      "called a function, and we define a short name for our function with the keyword def.\n",
      "The \n",
      "next example shows how to define two new functions, lexical_diversity() and\n",
      "percentage():\n",
      ">>> def lexical_diversity(text): \n",
      "...     return len(text) / len(set(text)) \n",
      "...\n",
      ">>> def percentage(count, total): \n",
      "...     return 100 * count / total\n",
      "...\n",
      "Caution!\n",
      "The \n",
      "Python interpreter changes the prompt from >>> to ... after en-\n",
      "countering the colon at the end of the first line. The ... prompt indicates\n",
      "that Python expects an indented code block to appear next. It is up to\n",
      "you to do the indentation, by typing four spaces or hitting the Tab key.\n",
      "To finish the indented block, just enter a blank line.\n",
      "In the definition of lexical diversity() \n",
      " , we specify a parameter labeled text. This\n",
      "parameter is a “placeholder” for the actual text whose lexical diversity we want to\n",
      "compute, and reoccurs in the block of code that will run when the function is used, in\n",
      "line \n",
      ". Similarly, percentage() is defined to take two parameters, labeled count and\n",
      "total \n",
      " .\n",
      "Once \n",
      "Python knows that lexical_diversity() and percentage() are the names for spe-\n",
      "cific blocks of code, we can go ahead and use these functions:\n",
      ">>> lexical_diversity(text3)\n",
      "16.050197203298673\n",
      ">>> lexical_diversity(text5)\n",
      "7.4200461589185629\n",
      ">>> percentage(4, 5)\n",
      "80.0\n",
      ">>> percentage(text4.count('a'), len(text4))\n",
      "1.4643016433938312\n",
      ">>>\n",
      "To recap, we use or call a function such as lexical_diversity() by typing its name,\n",
      "followed by an open parenthesis, the name of the text, and then a close parenthesis.\n",
      "These parentheses will show up often; their role is to separate the name of a task—such\n",
      "as lexical_diversity()—from the data that the task is to be performed on—such as\n",
      "text3. The data value that we place in the parentheses when we call a function is an\n",
      "argument to the function.\n",
      "You have already encountered several functions in this chapter, such as len(), set(),\n",
      "and sorted(). By convention, we will always add an empty pair of parentheses after a\n",
      "function name, as in len(), just to make clear that what we are talking about is a func-\n",
      "tion rather than some other kind of Python expression. Functions are an important\n",
      "concept in programming, and we only mention them at the outset to give newcomers\n",
      "1.1  Computing with Language: Texts and Words | 9...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 30, 'page_label': '9', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4358}\n",
      "\n",
      "--- Chunk 4359 ---\n",
      "Content:\n",
      "a sense of the power and creativity of programming. Don’t worry if you find it a bit\n",
      "confusing right now.\n",
      "Later \n",
      "we’ll see how to use functions when tabulating data, as in Table 1-1. Each row\n",
      "of the table will involve the same computation but with different data, and we’ll do this\n",
      "repetitive work using a function.\n",
      "Table 1-1. Lexical diversity of various genres in the Brown Corpus\n",
      "Genre Tokens Types Lexical diversity\n",
      "skill and hobbies 82345 11935 6.9\n",
      "humor 21695 5017 4.3\n",
      "fiction: science 14470 3233 4.5\n",
      "press: reportage 100554 14394 7.0\n",
      "fiction: romance 70022 8452 8.3\n",
      "religion\n",
      "39399 6373 6.2\n",
      "1.2  A Closer Look at Python: Texts as Lists of Words\n",
      "You’ve \n",
      "seen some important elements of the Python programming language. Let’s take\n",
      "a few moments to review them systematically.\n",
      "Lists\n",
      "What is a text? At one level, it is a sequence of symbols on a page such as this one. At\n",
      "another level, it is a sequence of chapters, made up of a sequence of sections, where\n",
      "each section is a sequence of paragraphs, and so on. However, for our purposes, we\n",
      "will think of a text as nothing more than a sequence of words and punctuation. Here’s\n",
      "how we represent text in Python, in this case the opening sentence of Moby Dick:\n",
      ">>> sent1 = ['Call', 'me', 'Ishmael', '.']\n",
      ">>>\n",
      "After the prompt we’ve given a name we made up, sent1, followed by the equals sign,\n",
      "and then some quoted words, separated with commas, and surrounded with brackets.\n",
      "This bracketed material is known as a list in Python: it is how we store a text. We can\n",
      "inspect it by typing the name \n",
      ". We can ask for its length \n",
      " . We can even apply our\n",
      "own lexical_diversity() function to it \n",
      " .\n",
      ">>> sent1 \n",
      "['Call', 'me', 'Ishmael', '.']\n",
      ">>> len(sent1) \n",
      "4\n",
      ">>> lexical_diversity(sent1) \n",
      "1.0\n",
      ">>>\n",
      "10 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 31, 'page_label': '10', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4359}\n",
      "\n",
      "--- Chunk 4360 ---\n",
      "Content:\n",
      "Some more lists have been defined for you, one for the opening sentence of each of our\n",
      "texts, sent2 \n",
      "… sent9. We inspect two of them here; you can see the rest for yourself\n",
      "using the Python interpreter (if you get an error saying that sent2 is not defined, you\n",
      "need to first type from nltk.book import *).\n",
      ">>> sent2\n",
      "['The', 'family', 'of', 'Dashwood', 'had', 'long',\n",
      "'been', 'settled', 'in', 'Sussex', '.']\n",
      ">>> sent3\n",
      "['In', 'the', 'beginning', 'God', 'created', 'the',\n",
      "'heaven', 'and', 'the', 'earth', '.']\n",
      ">>>\n",
      "Your Turn: Make up a few sentences of your own, by typing a name,\n",
      "equals sign, and a list of words, like this: ex1 = ['Monty', 'Python',\n",
      "'and', 'the', 'Holy', 'Grail']. Repeat some of the other Python op-\n",
      "erations we saw earlier in Section 1.1, e.g., sorted(ex1), len(set(ex1)),\n",
      "ex1.count('the').\n",
      "A pleasant surprise is that we can use Python’s addition operator on lists. Adding two\n",
      "lists \n",
      "  creates a new list with everything from the first list, followed by everything from\n",
      "the second list:\n",
      ">>> ['Monty', 'Python'] + ['and', 'the', 'Holy', 'Grail'] \n",
      "['Monty', 'Python', 'and', 'the', 'Holy', 'Grail']\n",
      "This special use of the addition operation is called concatenation; it\n",
      "combines the lists together into a single list. We can concatenate sen-\n",
      "tences to build up a text.\n",
      "We don’t have to literally type the lists either; we can use short names that refer to pre-\n",
      "defined lists.\n",
      ">>> sent4 + sent1\n",
      "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the',\n",
      "'House', 'of', 'Representatives', ':', 'Call', 'me', 'Ishmael', '.']\n",
      ">>>\n",
      "What if we want to add a single item to a list? This is known as appending. When we\n",
      "append() to a list, the list itself is updated as a result of the operation.\n",
      ">>> sent1.append(\"Some\")\n",
      ">>> sent1\n",
      "['Call', 'me', 'Ishmael', '.', 'Some']\n",
      ">>>\n",
      "1.2  A Closer Look at Python: Texts as Lists of Words | 11...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 32, 'page_label': '11', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4360}\n",
      "\n",
      "--- Chunk 4361 ---\n",
      "Content:\n",
      "Indexing Lists\n",
      "As \n",
      "we have seen, a text in Python is a list of words, represented using a combination\n",
      "of brackets and quotes. Just as with an ordinary page of text, we can count up the total\n",
      "number of words in text1 with len(text1), and count the occurrences in a text of a\n",
      "particular word—say, heaven—using text1.count('heaven').\n",
      "With some patience, we can pick out the 1st, 173rd, or even 14,278th word in a printed\n",
      "text. Analogously, we can identify the elements of a Python list by their order of oc-\n",
      "currence in the list. The number that represents this position is the item’s index. We\n",
      "instruct Python to show us the item that occurs at an index such as 173 in a text by\n",
      "writing the name of the text followed by the index inside square brackets:\n",
      ">>> text4[173]\n",
      "'awaken'\n",
      ">>>\n",
      "We can do the converse; given a word, find the index of when it first occurs:\n",
      ">>> text4.index('awaken')\n",
      "173\n",
      ">>>\n",
      "Indexes are a common way to access the words of a text, or, more generally, the ele-\n",
      "ments of any list. Python permits us to access sublists as well, extracting manageable\n",
      "pieces of language from large texts, a technique known as slicing.\n",
      ">>> text5[16715:16735]\n",
      "['U86', 'thats', 'why', 'something', 'like', 'gamefly', 'is', 'so', 'good',\n",
      "'because', 'you', 'can', 'actually', 'play', 'a', 'full', 'game', 'without',\n",
      "'buying', 'it']\n",
      ">>> text6[1600:1625]\n",
      "['We', \"'\", 're', 'an', 'anarcho', '-', 'syndicalist', 'commune', '.', 'We',\n",
      "'take', 'it', 'in', 'turns', 'to', 'act', 'as', 'a', 'sort', 'of', 'executive',\n",
      "'officer', 'for', 'the', 'week']\n",
      ">>>\n",
      "Indexes have some subtleties, and we’ll explore these with the help of an artificial\n",
      "sentence:\n",
      ">>> sent = ['word1', 'word2', 'word3', 'word4', 'word5',\n",
      "...         'word6', 'word7', 'word8', 'word9', 'word10']\n",
      ">>> sent[0]\n",
      "'word1'\n",
      ">>> sent[9]\n",
      "'word10'\n",
      ">>>\n",
      "Notice that our indexes start from zero: sent element zero, written sent[0], is the first\n",
      "word, 'word1', whereas sent element 9 is 'word10'. The reason is simple: the moment\n",
      "Python accesses the content of a list from the computer’s memory, it is already at the\n",
      "first element; we have to tell it how many elements forward to go. Thus, zero steps\n",
      "forward leaves it at the first element.\n",
      "12 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 33, 'page_label': '12', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4361}\n",
      "\n",
      "--- Chunk 4362 ---\n",
      "Content:\n",
      "This practice of counting from zero is initially confusing, but typical of\n",
      "modern \n",
      "programming languages. You’ll quickly get the hang of it if\n",
      "you’ve mastered the system of counting centuries where 19XY is a year\n",
      "in the 20th century, or if you live in a country where the floors of a\n",
      "building are numbered from 1, and so walking up n-1 flights of stairs\n",
      "takes you to level n.\n",
      "Now, if we accidentally use an index that is too large, we get an error:\n",
      ">>> sent[10]\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in ?\n",
      "IndexError: list index out of range\n",
      ">>>\n",
      "This time it is not a syntax error, because the program fragment is syntactically correct.\n",
      "Instead, it is a runtime error, and it produces a Traceback message that shows the\n",
      "context of the error, followed by the name of the error, IndexError, and a brief\n",
      "explanation.\n",
      "Let’s take a closer look at slicing, using our artificial sentence again. Here we verify that\n",
      "the slice 5:8 includes sent elements at indexes 5, 6, and 7:\n",
      ">>> sent[5:8]\n",
      "['word6', 'word7', 'word8']\n",
      ">>> sent[5]\n",
      "'word6'\n",
      ">>> sent[6]\n",
      "'word7'\n",
      ">>> sent[7]\n",
      "'word8'\n",
      ">>>\n",
      "By convention, m:n means elements m…n-1. As the next example shows, we can omit\n",
      "the first number if the slice begins at the start of the list \n",
      ", and we can omit the second\n",
      "number if the slice goes to the end \n",
      " :\n",
      ">>> sent[:3] \n",
      "['word1', 'word2', 'word3']\n",
      ">>> text2[141525:] \n",
      "['among', 'the', 'merits', 'and', 'the', 'happiness', 'of', 'Elinor', 'and', 'Marianne',\n",
      "',', 'let', 'it', 'not', 'be', 'ranked', 'as', 'the', 'least', 'considerable', ',',\n",
      "'that', 'though', 'sisters', ',', 'and', 'living', 'almost', 'within', 'sight', 'of',\n",
      "'each', 'other', ',', 'they', 'could', 'live', 'without', 'disagreement', 'between',\n",
      "'themselves', ',', 'or', 'producing', 'coolness', 'between', 'their', 'husbands', '.',\n",
      "'THE', 'END']\n",
      ">>>\n",
      "We \n",
      "can modify an element of a list by assigning to one of its index values. In the next\n",
      "example, we put sent[0] on the left of the equals sign \n",
      " . We can also replace an entire\n",
      "slice \n",
      "with new material \n",
      " . A consequence of this last change is that the list only has\n",
      "four elements, and accessing a later value generates an error \n",
      " .\n",
      "1.2  A Closer Look at Python: Texts as Lists of Words | 13...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 34, 'page_label': '13', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4362}\n",
      "\n",
      "--- Chunk 4363 ---\n",
      "Content:\n",
      ">>> sent[0] = 'First' \n",
      ">>> sent[9] = 'Last'\n",
      ">>> len(sent)\n",
      "10\n",
      ">>> sent[1:9] = ['Second', 'Third'] \n",
      ">>> sent\n",
      "['First', 'Second', 'Third', 'Last']\n",
      ">>> sent[9] \n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in ?\n",
      "IndexError: list index out of range\n",
      ">>>\n",
      "Your Turn: Take a few minutes to define a sentence of your own and\n",
      "modify individual words and groups of words (slices) using the same\n",
      "methods used earlier. Check your understanding by trying the exercises\n",
      "on lists at the end of this chapter.\n",
      "Variables\n",
      "From the start of Section 1.1, you have had access to texts called text1, text2, and so\n",
      "on. It saved a lot of typing to be able to refer to a 250,000-word book with a short name\n",
      "like this! In general, we can make up names for anything we care to calculate. We did\n",
      "this ourselves in the previous sections, e.g., defining a variable sent1, as follows:\n",
      ">>> sent1 = ['Call', 'me', 'Ishmael', '.']\n",
      ">>>\n",
      "Such lines have the form: variable = expression. Python will evaluate the expression,\n",
      "and save its result to the variable. This process is called assignment. It does not gen-\n",
      "erate any output; you have to type the variable on a line of its own to inspect its contents.\n",
      "The equals sign is slightly misleading, since information is moving from the right side\n",
      "to the left. It might help to think of it as a left-arrow. The name of the variable can be\n",
      "anything you like, e.g., my_sent, sentence, xyzzy. It must start with a letter, and can\n",
      "include numbers and underscores. Here are some examples of variables and\n",
      "assignments:\n",
      ">>> my_sent = ['Bravely', 'bold', 'Sir', 'Robin', ',', 'rode',\n",
      "... 'forth', 'from', 'Camelot', '.']\n",
      ">>> noun_phrase = my_sent[1:4]\n",
      ">>> noun_phrase\n",
      "['bold', 'Sir', 'Robin']\n",
      ">>> wOrDs = sorted(noun_phrase)\n",
      ">>> wOrDs\n",
      "['Robin', 'Sir', 'bold']\n",
      ">>>\n",
      "Remember that capitalized words appear before lowercase words in sorted lists.\n",
      "14 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 35, 'page_label': '14', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4363}\n",
      "\n",
      "--- Chunk 4364 ---\n",
      "Content:\n",
      "Notice in the previous example that we split the definition of my_sent\n",
      "over \n",
      "two lines. Python expressions can be split across multiple lines, so\n",
      "long as this happens within any kind of brackets. Python uses the ...\n",
      "prompt to indicate that more input is expected. It doesn’t matter how\n",
      "much indentation is used in these continuation lines, but some inden-\n",
      "tation usually makes them easier to read.\n",
      "It is good to choose meaningful variable names to remind you—and to help anyone\n",
      "else who reads your Python code—what your code is meant to do. Python does not try\n",
      "to make sense of the names; it blindly follows your instructions, and does not object if\n",
      "you do something confusing, such as one = 'two' or two = 3. The only restriction is\n",
      "that a variable name cannot be any of Python’s reserved words, such as def, if, not,\n",
      "and import. If you use a reserved word, Python will produce a syntax error:\n",
      ">>> not = 'Camelot'\n",
      "File \"<stdin>\", line 1\n",
      "    not = 'Camelot'\n",
      "        ^\n",
      "SyntaxError: invalid syntax\n",
      ">>>\n",
      "We will often use variables to hold intermediate steps of a computation, especially\n",
      "when this makes the code easier to follow. Thus len(set(text1)) could also be written:\n",
      ">>> vocab = set(text1)\n",
      ">>> vocab_size = len(vocab)\n",
      ">>> vocab_size\n",
      "19317\n",
      ">>>\n",
      "Caution!\n",
      "Take \n",
      "care with your choice of names (or identifiers) for Python varia-\n",
      "bles. First, you should start the name with a letter, optionally followed\n",
      "by digits ( 0 to 9) or letters. Thus, abc23 is fine, but 23abc will cause a\n",
      "syntax error. Names are case-sensitive, which means that myVar and\n",
      "myvar are distinct variables. Variable names cannot contain whitespace,\n",
      "but you can separate words using an underscore, e.g., my_var. Be careful\n",
      "not to insert a hyphen instead of an underscore: my-var is wrong, since\n",
      "Python interprets the - as a minus sign.\n",
      "Strings\n",
      "Some of the methods we used to access the elements of a list also work with individual\n",
      "words, or strings. For example, we can assign a string to a variable \n",
      " , index a string\n",
      ", and slice a string \n",
      " .\n",
      "1.2  A Closer Look at Python: Texts as Lists of Words | 15...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 36, 'page_label': '15', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4364}\n",
      "\n",
      "--- Chunk 4365 ---\n",
      "Content:\n",
      ">>> name = 'Monty' \n",
      ">>> name[0] \n",
      "'M'\n",
      ">>> name[:4] \n",
      "'Mont'\n",
      ">>>\n",
      "We can also perform multiplication and addition with strings:\n",
      ">>> name * 2\n",
      "'MontyMonty'\n",
      ">>> name + '!'\n",
      "'Monty!'\n",
      ">>>\n",
      "We \n",
      "can join the words of a list to make a single string, or split a string into a list, as\n",
      "follows:\n",
      ">>> ' '.join(['Monty', 'Python'])\n",
      "'Monty Python'\n",
      ">>> 'Monty Python'.split()\n",
      "['Monty', 'Python']\n",
      ">>>\n",
      "We will come back to the topic of strings in Chapter 3. For the time being, we have\n",
      "two important building blocks—lists and strings—and are ready to get back to some\n",
      "language analysis.\n",
      "1.3  Computing with Language: Simple Statistics\n",
      "Let’s return to our exploration of the ways we can bring our computational resources\n",
      "to bear on large quantities of text. We began this discussion in Section 1.1, and saw\n",
      "how to search for words in context, how to compile the vocabulary of a text, how to\n",
      "generate random text in the same style, and so on.\n",
      "In this section, we pick up the question of what makes a text distinct, and use automatic\n",
      "methods to find characteristic words and expressions of a text. As in Section 1.1, you\n",
      "can try new features of the Python language by copying them into the interpreter, and\n",
      "you’ll learn about these features systematically in the following section.\n",
      "Before continuing further, you might like to check your understanding of the last sec-\n",
      "tion by predicting the output of the following code. You can use the interpreter to check\n",
      "whether you got it right. If you’re not sure how to do this task, it would be a good idea\n",
      "to review the previous section before continuing further.\n",
      ">>> saying = ['After', 'all', 'is', 'said', 'and', 'done',\n",
      "...           'more', 'is', 'said', 'than', 'done']\n",
      ">>> tokens = set(saying)\n",
      ">>> tokens = sorted(tokens)\n",
      ">>> tokens[-2:]\n",
      "what output do you expect here?\n",
      ">>>\n",
      "16 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 37, 'page_label': '16', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4365}\n",
      "\n",
      "--- Chunk 4366 ---\n",
      "Content:\n",
      "Frequency Distributions\n",
      "How \n",
      "can we automatically identify the words of a text that are most informative about\n",
      "the topic and genre of the text? Imagine how you might go about finding the 50 most\n",
      "frequent words of a book. One method would be to keep a tally for each vocabulary\n",
      "item, like that shown in Figure 1-3. The tally would need thousands of rows, and it\n",
      "would be an exceedingly laborious process—so laborious that we would rather assign\n",
      "the task to a machine.\n",
      "Figure 1-3. Counting words appearing in a text (a frequency distribution).\n",
      "The \n",
      "table in Figure 1-3 is known as a frequency distribution , and it tells us the\n",
      "frequency of each vocabulary item in the text. (In general, it could count any kind of\n",
      "observable event.) It is a “distribution” since it tells us how the total number of word\n",
      "tokens in the text are distributed across the vocabulary items. Since we often need\n",
      "frequency distributions in language processing, NLTK provides built-in support for\n",
      "them. Let’s use a FreqDist to find the 50 most frequent words of Moby Dick. Try to\n",
      "work out what is going on here, then read the explanation that follows.\n",
      ">>> fdist1 = FreqDist(text1) \n",
      ">>> fdist1 \n",
      "<FreqDist with 260819 outcomes>\n",
      ">>> vocabulary1 = fdist1.keys() \n",
      ">>> vocabulary1[:50] \n",
      "[',', 'the', '.', 'of', 'and', 'a', 'to', ';', 'in', 'that', \"'\", '-',\n",
      "'his', 'it', 'I', 's', 'is', 'he', 'with', 'was', 'as', '\"', 'all', 'for',\n",
      "'this', '!', 'at', 'by', 'but', 'not', '--', 'him', 'from', 'be', 'on',\n",
      "'so', 'whale', 'one', 'you', 'had', 'have', 'there', 'But', 'or', 'were',\n",
      "'now', 'which', '?', 'me', 'like']\n",
      ">>> fdist1['whale']\n",
      "906\n",
      ">>>\n",
      "When \n",
      "we first invoke FreqDist, we pass the name of the text as an argument \n",
      " . We\n",
      "can \n",
      "inspect the total number of words (“outcomes”) that have been counted up \n",
      " —\n",
      "260,819 \n",
      "in the case of Moby Dick. The expression keys() gives us a list of all the distinct\n",
      "types in the text \n",
      " , and we can look at the first 50 of these by slicing the list \n",
      " .\n",
      "1.3  Computing with Language: Simple Statistics | 17...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 38, 'page_label': '17', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4366}\n",
      "\n",
      "--- Chunk 4367 ---\n",
      "Content:\n",
      "Your Turn: Try the preceding frequency distribution example for your-\n",
      "self, for text2. Be careful to use the correct parentheses and uppercase\n",
      "letters. If you get an error message NameError: name 'FreqDist' is not\n",
      "defined, you need to start your work with from nltk.book import *.\n",
      "Do any words produced in the last example help us grasp the topic or genre of this text?\n",
      "Only one word, whale, is slightly informative! It occurs over 900 times. The rest of the\n",
      "words tell us nothing about the text; they’re just English “plumbing.” What proportion\n",
      "of the text is taken up with such words? We can generate a cumulative frequency plot\n",
      "for these words, using fdist1.plot(50, cumulative=True), to produce the graph in\n",
      "Figure 1-4. These 50 words account for nearly half the book!\n",
      "Figure 1-4. Cumulative frequency plot for the 50 most frequently used words in Moby Dick, which\n",
      "account for nearly half of the tokens.\n",
      "18 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 39, 'page_label': '18', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4367}\n",
      "\n",
      "--- Chunk 4368 ---\n",
      "Content:\n",
      "If the frequent words don’t help us, how about the words that occur once only, the so-\n",
      "called hapaxes? \n",
      "View them by typing fdist1.hapaxes(). This list contains\n",
      "lexicographer, cetological, contraband, expostulations, and about 9,000 others. It seems\n",
      "that there are too many rare words, and without seeing the context we probably can’t\n",
      "guess what half of the hapaxes mean in any case! Since neither frequent nor infrequent\n",
      "words help, we need to try something else.\n",
      "Fine-Grained Selection of Words\n",
      "Next, let’s look at the long words of a text; perhaps these will be more characteristic\n",
      "and informative. For this we adapt some notation from set theory. We would like to\n",
      "find the words from the vocabulary of the text that are more than 15 characters long.\n",
      "Let’s call this property P, so that P(w) is true if and only if w is more than 15 characters\n",
      "long. Now we can express the words of interest using mathematical set notation as\n",
      "shown in (1a). This means “the set of all w such that w is an element of V (the vocabu-\n",
      "lary) and w has property P.”\n",
      "(1) a. { w | w ∈ V & P(w)}\n",
      "b. [w for w in V if p(w)]\n",
      "The corresponding Python expression is given in (1b). (Note that it produces a list, not\n",
      "a set, which means that duplicates are possible.) Observe how similar the two notations\n",
      "are. Let’s go one more step and write executable Python code:\n",
      ">>> V = set(text1)\n",
      ">>> long_words = [w for w in V if len(w) > 15]\n",
      ">>> sorted(long_words)\n",
      "['CIRCUMNAVIGATION', 'Physiognomically', 'apprehensiveness', 'cannibalistically',\n",
      "'characteristically', 'circumnavigating', 'circumnavigation', 'circumnavigations',\n",
      "'comprehensiveness', 'hermaphroditical', 'indiscriminately', 'indispensableness',\n",
      "'irresistibleness', 'physiognomically', 'preternaturalness', 'responsibilities',\n",
      "'simultaneousness', 'subterraneousness', 'supernaturalness', 'superstitiousness',\n",
      "'uncomfortableness', 'uncompromisedness', 'undiscriminating', 'uninterpenetratingly']\n",
      ">>>\n",
      "For each word w in the vocabulary V, we check whether len(w) is greater than 15; all\n",
      "other words will be ignored. We will discuss this syntax more carefully later.\n",
      "Your Turn: Try out the previous statements in the Python interpreter,\n",
      "and experiment with changing the text and changing the length condi-\n",
      "tion. Does it make an difference to your results if you change the variable\n",
      "names, e.g., using [word for word in vocab if ...]?\n",
      "1.3  Computing with Language: Simple Statistics | 19...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 40, 'page_label': '19', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4368}\n",
      "\n",
      "--- Chunk 4369 ---\n",
      "Content:\n",
      "Let’s return to our task of finding words that characterize a text. Notice that the long\n",
      "words \n",
      "in text4 reflect its national focus— constitutionally, transcontinental—whereas\n",
      "those in text5 reflect its informal content: boooooooooooglyyyyyy and\n",
      "yuuuuuuuuuuuummmmmmmmmmmm. Have we succeeded in automatically extract-\n",
      "ing words that typify a text? Well, these very long words are often hapaxes (i.e., unique)\n",
      "and perhaps it would be better to find frequently occurring long words. This seems\n",
      "promising since it eliminates frequent short words (e.g., the) and infrequent long words\n",
      "(e.g., antiphilosophists). Here are all words from the chat corpus that are longer than\n",
      "seven characters, that occur more than seven times:\n",
      ">>> fdist5 = FreqDist(text5)\n",
      ">>> sorted([w for w in set(text5) if len(w) > 7 and fdist5[w] > 7])\n",
      "['#14-19teens', '#talkcity_adults', '((((((((((', '........', 'Question',\n",
      "'actually', 'anything', 'computer', 'cute.-ass', 'everyone', 'football',\n",
      "'innocent', 'listening', 'remember', 'seriously', 'something', 'together',\n",
      "'tomorrow', 'watching']\n",
      ">>>\n",
      "Notice how we have used two conditions: len(w) > 7 ensures that the words are longer\n",
      "than seven letters, and fdist5[w] > 7 ensures that these words occur more than seven\n",
      "times. At last we have managed to automatically identify the frequently occurring con-\n",
      "tent-bearing words of the text. It is a modest but important milestone: a tiny piece of\n",
      "code, processing tens of thousands of words, produces some informative output.\n",
      "Collocations and Bigrams\n",
      "A collocation is a sequence of words that occur together unusually often. Thus red\n",
      "wine is a collocation, whereas the wine is not. A characteristic of collocations is that\n",
      "they are resistant to substitution with words that have similar senses; for example,\n",
      "maroon wine sounds very odd.\n",
      "To get a handle on collocations, we start off by extracting from a text a list of word\n",
      "pairs, also known as bigrams. This is easily accomplished with the function bigrams():\n",
      ">>> bigrams(['more', 'is', 'said', 'than', 'done'])\n",
      "[('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]\n",
      ">>>\n",
      "Here we see that the pair of words than-done is a bigram, and we write it in Python as\n",
      "('than', 'done'). Now, collocations are essentially just frequent bigrams, except that\n",
      "we want to pay more attention to the cases that involve rare words. In particular, we\n",
      "want to find bigrams that occur more often than we would expect based on the fre-\n",
      "quency of individual words. The collocations() function does this for us (we will see\n",
      "how it works later):\n",
      ">>> text4.collocations()\n",
      "Building collocations list\n",
      "United States; fellow citizens; years ago; Federal Government; General\n",
      "Government; American people; Vice President; Almighty God; Fellow\n",
      "citizens; Chief Magistrate; Chief Justice; God bless; Indian tribes;\n",
      "public debt; foreign nations; political parties; State governments;\n",
      "20 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 41, 'page_label': '20', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4369}\n",
      "\n",
      "--- Chunk 4370 ---\n",
      "Content:\n",
      "National Government; United Nations; public money\n",
      ">>> text8.collocations()\n",
      "Building collocations list\n",
      "medium build; social drinker; quiet nights; long term; age open;\n",
      "financially secure; fun times; similar interests; Age open; poss\n",
      "rship; single mum; permanent relationship; slim build; seeks lady;\n",
      "Late 30s; Photo pls; Vibrant personality; European background; ASIAN\n",
      "LADY; country drives\n",
      ">>>\n",
      "The \n",
      "collocations that emerge are very specific to the genre of the texts. In order to find\n",
      "red wine as a collocation, we would need to process a much larger body of text.\n",
      "Counting Other Things\n",
      "Counting words is useful, but we can count other things too. For example, we can look\n",
      "at the distribution of word lengths in a text, by creating a FreqDist out of a long list of\n",
      "numbers, where each number is the length of the corresponding word in the text:\n",
      ">>> [len(w) for w in text1] \n",
      "[1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]\n",
      ">>> fdist = FreqDist([len(w) for w in text1])  \n",
      ">>> fdist  \n",
      "<FreqDist with 260819 outcomes>\n",
      ">>> fdist.keys()\n",
      "[3, 1, 4, 2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20]\n",
      ">>>\n",
      "We \n",
      "start by deriving a list of the lengths of words in text1 \n",
      " , and the FreqDist then\n",
      "counts the number of times each of these occurs \n",
      " . The result \n",
      "  is a distribution\n",
      "containing \n",
      "a quarter of a million items, each of which is a number corresponding to a\n",
      "word token in the text. But there are only 20 distinct items being counted, the numbers\n",
      "1 through 20, because there are only 20 different word lengths. I.e., there are words\n",
      "consisting of just 1 character, 2 characters, ..., 20 characters, but none with 21 or more\n",
      "characters. One might wonder how frequent the different lengths of words are (e.g.,\n",
      "how many words of length 4 appear in the text, are there more words of length 5 than\n",
      "length 4, etc.). We can do this as follows:\n",
      ">>> fdist.items()\n",
      "[(3, 50223), (1, 47933), (4, 42345), (2, 38513), (5, 26597), (6, 17111), (7, 14399),\n",
      "(8, 9966), (9, 6428), (10, 3528), (11, 1873), (12, 1053), (13, 567), (14, 177),\n",
      "(15, 70), (16, 22), (17, 12), (18, 1), (20, 1)]\n",
      ">>> fdist.max()\n",
      "3\n",
      ">>> fdist[3]\n",
      "50223\n",
      ">>> fdist.freq(3)\n",
      "0.19255882431878046\n",
      ">>>\n",
      "From this we see that the most frequent word length is 3, and that words of length 3\n",
      "account for roughly 50,000 (or 20%) of the words making up the book. Although we\n",
      "will not pursue it here, further analysis of word length might help us understand\n",
      "1.3  Computing with Language: Simple Statistics | 21...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 42, 'page_label': '21', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4370}\n",
      "\n",
      "--- Chunk 4371 ---\n",
      "Content:\n",
      "differences between authors, genres, or languages. Table 1-2 summarizes the functions\n",
      "defined in frequency distributions.\n",
      "Table 1-2. Functions defined for NLTK’s frequency distributions\n",
      "Example Description\n",
      "fdist = FreqDist(samples) Create a frequency distribution containing the given samples\n",
      "fdist.inc(sample) Increment the count for this sample\n",
      "fdist['monstrous'] Count of the number of times a given sample occurred\n",
      "fdist.freq('monstrous') Frequency of a given sample\n",
      "fdist.N() Total number of samples\n",
      "fdist.keys() The samples sorted in order of decreasing frequency\n",
      "for sample in fdist: Iterate over the samples, in order of decreasing frequency\n",
      "fdist.max() Sample with the greatest count\n",
      "fdist.tabulate() Tabulate the frequency distribution\n",
      "fdist.plot() Graphical plot of the frequency distribution\n",
      "fdist.plot(cumulative=True) Cumulative plot of the frequency distribution\n",
      "fdist1 < fdist2 Test if samples in fdist1 occur less frequently than in fdist2\n",
      "Our discussion of frequency distributions has introduced some important Python con-\n",
      "cepts, and we will look at them systematically in Section 1.4.\n",
      "1.4  Back to Python: Making Decisions and Taking Control\n",
      "So \n",
      "far, our little programs have had some interesting qualities: the ability to work with\n",
      "language, and the potential to save human effort through automation. A key feature of\n",
      "programming is the ability of machines to make decisions on our behalf, executing\n",
      "instructions when certain conditions are met, or repeatedly looping through text data\n",
      "until some condition is satisfied. This feature is known as control, and is the focus of\n",
      "this section.\n",
      "Conditionals\n",
      "Python supports a wide range of operators, such as < and >=, for testing the relationship\n",
      "between values. The full set of these relational operators are shown in Table 1-3.\n",
      "Table 1-3. Numerical comparison operators\n",
      "Operator Relationship\n",
      "< Less than\n",
      "<= Less than or equal to\n",
      "== Equal to (note this is two “=”signs, not one)\n",
      "22 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 43, 'page_label': '22', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4371}\n",
      "\n",
      "--- Chunk 4372 ---\n",
      "Content:\n",
      "Operator Relationship\n",
      "!= Not equal to\n",
      "> Greater than\n",
      ">= Greater than or equal to\n",
      "We can use these to select different words from a sentence of news text. Here are some\n",
      "examples—notice \n",
      "only the operator is changed from one line to the next. They all use\n",
      "sent7, the first sentence from text7 (Wall Street Journal). As before, if you get an error\n",
      "saying that sent7 is undefined, you need to first type: from nltk.book import *.\n",
      ">>> sent7\n",
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the',\n",
      "'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      ">>> [w for w in sent7 if len(w) < 4]\n",
      "[',', '61', 'old', ',', 'the', 'as', 'a', '29', '.']\n",
      ">>> [w for w in sent7 if len(w) <= 4]\n",
      "[',', '61', 'old', ',', 'will', 'join', 'the', 'as', 'a', 'Nov.', '29', '.']\n",
      ">>> [w for w in sent7 if len(w) == 4]\n",
      "['will', 'join', 'Nov.']\n",
      ">>> [w for w in sent7 if len(w) != 4]\n",
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'the', 'board',\n",
      "'as', 'a', 'nonexecutive', 'director', '29', '.']\n",
      ">>>\n",
      "There is a common pattern to all of these examples: [w for w in text if condition],\n",
      "where condition is a Python “test” that yields either true or false. In the cases shown\n",
      "in the previous code example, the condition is always a numerical comparison. How-\n",
      "ever, we can also test various properties of words, using the functions listed in Table 1-4.\n",
      "Table 1-4. Some word comparison operators\n",
      "Function Meaning\n",
      "s.startswith(t) Test if s starts with t\n",
      "s.endswith(t) Test if s ends with t\n",
      "t in s Test if t is contained inside s\n",
      "s.islower() Test if all cased characters in s are lowercase\n",
      "s.isupper() Test if all cased characters in s are uppercase\n",
      "s.isalpha() Test if all characters in s are alphabetic\n",
      "s.isalnum() Test if all characters in s are alphanumeric\n",
      "s.isdigit() Test if all characters in s are digits\n",
      "s.istitle() Test if s is titlecased (all words in s have initial capitals)\n",
      "Here are some examples of these operators being used to select words from our texts:\n",
      "words \n",
      "ending with -ableness; words containing gnt; words having an initial capital; and\n",
      "words consisting entirely of digits.\n",
      "1.4  Back to Python: Making Decisions and Taking Control | 23...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 44, 'page_label': '23', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4372}\n",
      "\n",
      "--- Chunk 4373 ---\n",
      "Content:\n",
      ">>> sorted([w for w in set(text1) if w.endswith('ableness')])\n",
      "['comfortableness', 'honourableness', 'immutableness', 'indispensableness', ...]\n",
      ">>> sorted([term for term in set(text4) if 'gnt' in term])\n",
      "['Sovereignty', 'sovereignties', 'sovereignty']\n",
      ">>> sorted([item for item in set(text6) if item.istitle()])\n",
      "['A', 'Aaaaaaaaah', 'Aaaaaaaah', 'Aaaaaah', 'Aaaah', 'Aaaaugh', 'Aaagh', ...]\n",
      ">>> sorted([item for item in set(sent7) if item.isdigit()])\n",
      "['29', '61']\n",
      ">>>\n",
      "We \n",
      "can also create more complex conditions. If c is a condition, then not c is also a\n",
      "condition. If we have two conditions c1 and c2, then we can combine them to form a\n",
      "new condition using conjunction and disjunction: c1 and c2, c1 or c2.\n",
      "Your Turn: Run the following examples and try to explain what is going\n",
      "on in each one. Next, try to make up some conditions of your own.\n",
      ">>> sorted([w for w in set(text7) if '-' in w and 'index' in w])\n",
      ">>> sorted([wd for wd in set(text3) if wd.istitle() and len(wd) > 10])\n",
      ">>> sorted([w for w in set(sent7) if not w.islower()])\n",
      ">>> sorted([t for t in set(text2) if 'cie' in t or 'cei' in t])\n",
      "Operating on Every Element\n",
      "In Section 1.3, we saw some examples of counting items other than words. Let’s take\n",
      "a closer look at the notation we used:\n",
      ">>> [len(w) for w in text1]\n",
      "[1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]\n",
      ">>> [w.upper() for w in text1]\n",
      "['[', 'MOBY', 'DICK', 'BY', 'HERMAN', 'MELVILLE', '1851', ']', 'ETYMOLOGY', '.', ...]\n",
      ">>>\n",
      "These expressions have the form [f(w) for ...] or [w.f() for ...], where f is a\n",
      "function that operates on a word to compute its length, or to convert it to uppercase.\n",
      "For now, you don’t need to understand the difference between the notations f(w) and\n",
      "w.f(). Instead, simply learn this Python idiom which performs the same operation on\n",
      "every element of a list. In the preceding examples, it goes through each word in\n",
      "text1, assigning each one in turn to the variable w and performing the specified oper-\n",
      "ation on the variable.\n",
      "The notation just described is called a “list comprehension.” This is our\n",
      "first \n",
      "example of a Python idiom, a fixed notation that we use habitually\n",
      "without bothering to analyze each time. Mastering such idioms is an\n",
      "important part of becoming a fluent Python programmer.\n",
      "Let’s return to the question of vocabulary size, and apply the same idiom here:\n",
      ">>> len(text1)\n",
      "260819\n",
      "24 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 45, 'page_label': '24', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4373}\n",
      "\n",
      "--- Chunk 4374 ---\n",
      "Content:\n",
      ">>> len(set(text1))\n",
      "19317\n",
      ">>> len(set([word.lower() for word in text1]))\n",
      "17231\n",
      ">>>\n",
      "Now that we are not double-counting words like This \n",
      " and this, which differ only in\n",
      "capitalization, we’ve wiped 2,000 off the vocabulary count! We can go a step further\n",
      "and eliminate numbers and punctuation from the vocabulary count by filtering out any\n",
      "non-alphabetic items:\n",
      ">>> len(set([word.lower() for word in text1 if word.isalpha()]))\n",
      "16948\n",
      ">>>\n",
      "This example is slightly complicated: it lowercases all the purely alphabetic items. Per-\n",
      "haps it would have been simpler just to count the lowercase-only items, but this gives\n",
      "the wrong answer (why?).\n",
      "Don’t worry if you don’t feel confident with list comprehensions yet, since you’ll see\n",
      "many more examples along with explanations in the following chapters.\n",
      "Nested Code Blocks\n",
      "Most programming languages permit us to execute a block of code when a conditional\n",
      "expression, or if statement, is satisfied. We already saw examples of conditional tests\n",
      "in code like [w for w in sent7 if len(w) < 4]. In the following program, we have\n",
      "created a variable called word containing the string value 'cat'. The if statement checks\n",
      "whether the test len(word) < 5 is true. It is, so the body of the if statement is invoked\n",
      "and the print statement is executed, displaying a message to the user. Remember to\n",
      "indent the print statement by typing four spaces.\n",
      ">>> word = 'cat'\n",
      ">>> if len(word) < 5:\n",
      "...     print 'word length is less than 5'\n",
      "...   \n",
      "word length is less than 5\n",
      ">>>\n",
      "When \n",
      "we use the Python interpreter we have to add an extra blank line \n",
      "  in order for\n",
      "it to detect that the nested block is complete.\n",
      "If \n",
      "we change the conditional test to len(word) >= 5, to check that the length of word is\n",
      "greater than or equal to 5, then the test will no longer be true. This time, the body of\n",
      "the if statement will not be executed, and no message is shown to the user:\n",
      ">>> if len(word) >= 5:\n",
      "...   print 'word length is greater than or equal to 5'\n",
      "...\n",
      ">>>\n",
      "1.4  Back to Python: Making Decisions and Taking Control | 25...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 46, 'page_label': '25', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4374}\n",
      "\n",
      "--- Chunk 4375 ---\n",
      "Content:\n",
      "An if statement is known as a control structure because it controls whether the code\n",
      "in the indented block will be run. Another control structure is the for loop. Try the\n",
      "following, and remember to include the colon and the four spaces:\n",
      ">>> for word in ['Call', 'me', 'Ishmael', '.']:\n",
      "...     print word\n",
      "...\n",
      "Call\n",
      "me\n",
      "Ishmael\n",
      ".\n",
      ">>>\n",
      "This \n",
      "is called a loop because Python executes the code in circular fashion. It starts by\n",
      "performing the assignment word = 'Call', effectively using the word variable to name\n",
      "the first item of the list. Then, it displays the value of word to the user. Next, it goes\n",
      "back to the for statement, and performs the assignment word = 'me' before displaying\n",
      "this new value to the user, and so on. It continues in this fashion until every item of the\n",
      "list has been processed.\n",
      "Looping with Conditions\n",
      "Now we can combine the if and for statements. We will loop over every item of the\n",
      "list, and print the item only if it ends with the letter l. We’ll pick another name for the\n",
      "variable to demonstrate that Python doesn’t try to make sense of variable names.\n",
      ">>> sent1 = ['Call', 'me', 'Ishmael', '.']\n",
      ">>> for xyzzy in sent1:\n",
      "...     if xyzzy.endswith('l'):\n",
      "...         print xyzzy\n",
      "...\n",
      "Call\n",
      "Ishmael\n",
      ">>>\n",
      "You will notice that if and for statements have a colon at the end of the line, before\n",
      "the indentation begins. In fact, all Python control structures end with a colon. The\n",
      "colon indicates that the current statement relates to the indented block that follows.\n",
      "We can also specify an action to be taken if the condition of the if statement is not\n",
      "met. Here we see the elif (else if) statement, and the else statement. Notice that these\n",
      "also have colons before the indented code.\n",
      ">>> for token in sent1:\n",
      "...     if token.islower():\n",
      "...         print token, 'is a lowercase word'\n",
      "...     elif token.istitle():\n",
      "...         print token, 'is a titlecase word'\n",
      "...     else:\n",
      "...         print token, 'is punctuation'\n",
      "...\n",
      "Call is a titlecase word\n",
      "me is a lowercase word\n",
      "26 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 47, 'page_label': '26', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4375}\n",
      "\n",
      "--- Chunk 4376 ---\n",
      "Content:\n",
      "Ishmael is a titlecase word\n",
      ". is punctuation\n",
      ">>>\n",
      "As \n",
      "you can see, even with this small amount of Python knowledge, you can start to\n",
      "build multiline Python programs. It’s important to develop such programs in pieces,\n",
      "testing that each piece does what you expect before combining them into a program.\n",
      "This is why the Python interactive interpreter is so invaluable, and why you should get\n",
      "comfortable using it.\n",
      "Finally, let’s combine the idioms we’ve been exploring. First, we create a list of cie and\n",
      "cei words, then we loop over each item and print it. Notice the comma at the end of\n",
      "the print statement, which tells Python to produce its output on a single line.\n",
      ">>> tricky = sorted([w for w in set(text2) if 'cie' in w or 'cei' in w])\n",
      ">>> for word in tricky:\n",
      "...     print word,\n",
      "ancient ceiling conceit conceited conceive conscience\n",
      "conscientious conscientiously deceitful deceive ...\n",
      ">>>\n",
      "1.5  Automatic Natural Language Understanding\n",
      "We have been exploring language bottom-up, with the help of texts and the Python\n",
      "programming language. However, we’re also interested in exploiting our knowledge of\n",
      "language and computation by building useful language technologies. We’ll take the\n",
      "opportunity now to step back from the nitty-gritty of code in order to paint a bigger\n",
      "picture of natural language processing.\n",
      "At a purely practical level, we all need help to navigate the universe of information\n",
      "locked up in text on the Web. Search engines have been crucial to the growth and\n",
      "popularity of the Web, but have some shortcomings. It takes skill, knowledge, and\n",
      "some luck, to extract answers to such questions as: What tourist sites can I visit between\n",
      "Philadelphia and Pittsburgh on a limited budget? What do experts say about digital SLR\n",
      "cameras? What predictions about the steel market were made by credible commentators\n",
      "in the past week? Getting a computer to answer them automatically involves a range of\n",
      "language processing tasks, including information extraction, inference, and summari-\n",
      "zation, and would need to be carried out on a scale and with a level of robustness that\n",
      "is still beyond our current capabilities.\n",
      "On a more philosophical level, a long-standing challenge within artificial intelligence\n",
      "has been to build intelligent machines, and a major part of intelligent behavior is un-\n",
      "derstanding language. For many years this goal has been seen as too difficult. However,\n",
      "as NLP technologies become more mature, and robust methods for analyzing unre-\n",
      "stricted text become more widespread, the prospect of natural language understanding\n",
      "has re-emerged as a plausible goal.\n",
      "1.5  Automatic Natural Language Understanding | 27...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 48, 'page_label': '27', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4376}\n",
      "\n",
      "--- Chunk 4377 ---\n",
      "Content:\n",
      "In this section we describe some language understanding technologies, to give you a\n",
      "sense of the interesting challenges that are waiting for you.\n",
      "Word Sense Disambiguation\n",
      "In word \n",
      "sense disambiguation we want to work out which sense of a word was in-\n",
      "tended in a given context. Consider the ambiguous words serve and dish:\n",
      "(2) a. serve: help with food or drink; hold an office; put ball into play\n",
      "b. dish: plate; course of a meal; communications device\n",
      "In a sentence containing the phrase: he served the dish, you can detect that both serve\n",
      "and dish are being used with their food meanings. It’s unlikely that the topic of discus-\n",
      "sion shifted from sports to crockery in the space of three words. This would force you\n",
      "to invent bizarre images, like a tennis pro taking out his frustrations on a china tea-set\n",
      "laid out beside the court. In other words, we automatically disambiguate words using\n",
      "context, exploiting the simple fact that nearby words have closely related meanings. As\n",
      "another example of this contextual effect, consider the word by, which has several\n",
      "meanings, for example, the book by Chesterton (agentive—Chesterton was the author\n",
      "of the book); the cup by the stove (locative—the stove is where the cup is); and submit\n",
      "by Friday (temporal—Friday is the time of the submitting). Observe in (3) that the\n",
      "meaning of the italicized word helps us interpret the meaning of by.\n",
      "(3) a. The lost children were found by the searchers (agentive)\n",
      "b. The lost children were found by the mountain (locative)\n",
      "c. The lost children were found by the afternoon (temporal)\n",
      "Pronoun Resolution\n",
      "A deeper kind of language understanding is to work out “who did what to whom,” i.e.,\n",
      "to detect the subjects and objects of verbs. You learned to do this in elementary school,\n",
      "but it’s harder than you might think. In the sentence the thieves stole the paintings, it is\n",
      "easy to tell who performed the stealing action. Consider three possible following sen-\n",
      "tences in (4), and try to determine what was sold, caught, and found (one case is\n",
      "ambiguous).\n",
      "(4) a. The thieves stole the paintings. They were subsequently sold.\n",
      "b. The thieves stole the paintings. They were subsequently caught.\n",
      "c. The thieves stole the paintings. They were subsequently found.\n",
      "Answering this question involves finding the antecedent of the pronoun they, either\n",
      "thieves or paintings. Computational techniques for tackling this problem include ana-\n",
      "phora resolution —identifying what a pronoun or noun phrase refers to—and\n",
      "28 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 49, 'page_label': '28', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4377}\n",
      "\n",
      "--- Chunk 4378 ---\n",
      "Content:\n",
      "semantic role labeling—identifying how a noun phrase relates to the verb (as agent,\n",
      "patient, instrument, and so on).\n",
      "Generating Language Output\n",
      "If we can automatically solve such problems of language understanding, we will be able\n",
      "to move on to tasks that involve generating language output, such as question\n",
      "answering and machine translation. In the first case, a machine should be able to\n",
      "answer a user’s questions relating to collection of texts:\n",
      "(5) a. Text: ... The thieves stole the paintings. They were subsequently sold. ...\n",
      "b. Human: Who or what was sold?\n",
      "c. Machine: The paintings.\n",
      "The machine’s answer demonstrates that it has correctly worked out that they refers to\n",
      "paintings and not to thieves. In the second case, the machine should be able to translate\n",
      "the text into another language, accurately conveying the meaning of the original text.\n",
      "In translating the example text into French, we are forced to choose the gender of the\n",
      "pronoun in the second sentence: ils (masculine) if the thieves are sold, and elles (fem-\n",
      "inine) if the paintings are sold. Correct translation actually depends on correct under-\n",
      "standing of the pronoun.\n",
      "(6) a. The thieves stole the paintings. They were subsequently found.\n",
      "b. Les voleurs ont volé les peintures. Ils ont été trouvés plus tard. (the thieves)\n",
      "c. Les voleurs ont volé les peintures. Elles ont été trouvées plus tard. (the\n",
      "paintings)\n",
      "In all of these examples, working out the sense of a word, the subject of a verb, and the\n",
      "antecedent of a pronoun are steps in establishing the meaning of a sentence, things we\n",
      "would expect a language understanding system to be able to do.\n",
      "Machine Translation\n",
      "For a long time now, machine translation (MT) has been the holy grail of language\n",
      "understanding, ultimately seeking to provide high-quality, idiomatic translation be-\n",
      "tween any pair of languages. Its roots go back to the early days of the Cold War, when\n",
      "the promise of automatic translation led to substantial government sponsorship, and\n",
      "with it, the genesis of NLP itself.\n",
      "Today, practical translation systems exist for particular pairs of languages, and some\n",
      "are integrated into web search engines. However, these systems have some serious\n",
      "shortcomings. We can explore them with the help of NLTK’s “babelizer” (which is\n",
      "automatically loaded when you import this chapter’s materials using from nltk.book\n",
      "import *). This program submits a sentence for translation into a specified language,\n",
      "1.5  Automatic Natural Language Understanding | 29...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 50, 'page_label': '29', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4378}\n",
      "\n",
      "--- Chunk 4379 ---\n",
      "Content:\n",
      "then submits the resulting sentence for translation back into English. It stops after 12\n",
      "iterations, or if it receives a translation that was produced already (indicating a loop):\n",
      ">>> babelize_shell()\n",
      "NLTK Babelizer: type 'help' for a list of commands.\n",
      "Babel> how long before the next flight to Alice Springs?\n",
      "Babel> german\n",
      "Babel> run\n",
      "0> how long before the next flight to Alice Springs?\n",
      "1> wie lang vor dem folgenden Flug zu Alice Springs?\n",
      "2> how long before the following flight to Alice jump?\n",
      "3> wie lang vor dem folgenden Flug zu Alice springen Sie?\n",
      "4> how long before the following flight to Alice do you jump?\n",
      "5> wie lang, bevor der folgende Flug zu Alice tun, Sie springen?\n",
      "6> how long, before the following flight to Alice does, do you jump?\n",
      "7> wie lang bevor der folgende Flug zu Alice tut, tun Sie springen?\n",
      "8> how long before the following flight to Alice does, do you jump?\n",
      "9> wie lang, bevor der folgende Flug zu Alice tut, tun Sie springen?\n",
      "10> how long, before the following flight does to Alice, do do you jump?\n",
      "11> wie lang bevor der folgende Flug zu Alice tut, Sie tun Sprung?\n",
      "12> how long before the following flight does leap to Alice, does you?\n",
      "Observe \n",
      "that the system correctly translates Alice Springs from English to German (in\n",
      "the line starting 1>), but on the way back to English, this ends up as Alice jump\n",
      "(line 2). The preposition before is initially translated into the corresponding German\n",
      "preposition vor, but later into the conjunction bevor (line 5). After line 5 the sentences\n",
      "become non-sensical (but notice the various phrasings indicated by the commas, and\n",
      "the change from jump to leap). The translation system did not recognize when a word\n",
      "was part of a proper name, and it misinterpreted the grammatical structure. The gram-\n",
      "matical problems are more obvious in the following example. Did John find the pig, or\n",
      "did the pig find John?\n",
      ">>> babelize_shell()\n",
      "Babel> The pig that John found looked happy\n",
      "Babel> german\n",
      "Babel> run\n",
      "0> The pig that John found looked happy\n",
      "1> Das Schwein, das John fand, schaute gl?cklich\n",
      "2> The pig, which found John, looked happy\n",
      "Machine translation is difficult because a given word could have several possible trans-\n",
      "lations (depending on its meaning), and because word order must be changed in keep-\n",
      "ing with the grammatical structure of the target language. Today these difficulties are\n",
      "being faced by collecting massive quantities of parallel texts from news and government\n",
      "websites that publish documents in two or more languages. Given a document in Ger-\n",
      "man and English, and possibly a bilingual dictionary, we can automatically pair up the\n",
      "sentences, a process called text alignment. Once we have a million or more sentence\n",
      "pairs, we can detect corresponding words and phrases, and build a model that can be\n",
      "used for translating new text.\n",
      "30 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 51, 'page_label': '30', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4379}\n",
      "\n",
      "--- Chunk 4380 ---\n",
      "Content:\n",
      "Spoken Dialogue Systems\n",
      "In \n",
      "the history of artificial intelligence, the chief measure of intelligence has been a lin-\n",
      "guistic one, namely the Turing Test: can a dialogue system, responding to a user’s text\n",
      "input, perform so naturally that we cannot distinguish it from a human-generated re-\n",
      "sponse? In contrast, today’s commercial dialogue systems are very limited, but still\n",
      "perform useful functions in narrowly defined domains, as we see here:\n",
      "S: How may I help you?\n",
      "U: When is Saving Private Ryan playing?\n",
      "S: For what theater?\n",
      "U: The Paramount theater.\n",
      "S: Saving Private Ryan is not playing at the Paramount theater, but\n",
      "it’s playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30.\n",
      "You could not ask this system to provide driving instructions or details of nearby res-\n",
      "taurants \n",
      "unless the required information had already been stored and suitable question-\n",
      "answer pairs had been incorporated into the language processing system.\n",
      "Observe that this system seems to understand the user’s goals: the user asks when a\n",
      "movie is showing and the system correctly determines from this that the user wants to\n",
      "see the movie. This inference seems so obvious that you probably didn’t notice it was\n",
      "made, yet a natural language system needs to be endowed with this capability in order\n",
      "to interact naturally. Without it, when asked, Do you know when Saving Private Ryan\n",
      "is playing?, a system might unhelpfully respond with a cold Yes. However, the devel-\n",
      "opers of commercial dialogue systems use contextual assumptions and business logic\n",
      "to ensure that the different ways in which a user might express requests or provide\n",
      "information are handled in a way that makes sense for the particular application. So,\n",
      "if you type When is ..., or I want to know when ... , or Can you tell me when ... , simple\n",
      "rules will always yield screening times. This is enough for the system to provide a useful\n",
      "service.\n",
      "Dialogue systems give us an opportunity to mention the commonly assumed pipeline\n",
      "for NLP. Figure 1-5 shows the architecture of a simple dialogue system. Along the top\n",
      "of the diagram, moving from left to right, is a “pipeline” of some language understand-\n",
      "ing components. These map from speech input via syntactic parsing to some kind of\n",
      "meaning representation. Along the middle, moving from right to left, is the reverse\n",
      "pipeline of components for converting concepts to speech. These components make\n",
      "up the dynamic aspects of the system. At the bottom of the diagram are some repre-\n",
      "sentative bodies of static information: the repositories of language-related data that the\n",
      "processing components draw on to do their work.\n",
      "Your Turn: For an example of a primitive dialogue system, try having\n",
      "a conversation with an NLTK chatbot. To see the available chatbots,\n",
      "run nltk.chat.chatbots(). (Remember to import nltk first.)\n",
      "1.5  Automatic Natural Language Understanding | 31...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 52, 'page_label': '31', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4380}\n",
      "\n",
      "--- Chunk 4381 ---\n",
      "Content:\n",
      "Textual Entailment\n",
      "The \n",
      "challenge of language understanding has been brought into focus in recent years\n",
      "by a public “shared task” called Recognizing Textual Entailment (RTE). The basic\n",
      "scenario is simple. Suppose you want to find evidence to support the hypothesis: Sandra\n",
      "Goudie was defeated by Max Purnell, and that you have another short text that seems\n",
      "to be relevant, for example, Sandra Goudie was first elected to Parliament in the 2002\n",
      "elections, narrowly winning the seat of Coromandel by defeating Labour candidate Max\n",
      "Purnell and pushing incumbent Green MP Jeanette Fitzsimons into third place. Does the\n",
      "text provide enough evidence for you to accept the hypothesis? In this particular case,\n",
      "the answer will be “No.” You can draw this conclusion easily, but it is very hard to\n",
      "come up with automated methods for making the right decision. The RTE Challenges\n",
      "provide data that allow competitors to develop their systems, but not enough data for\n",
      "“brute force” machine learning techniques (a topic we will cover in Chapter 6). Con-\n",
      "sequently, some linguistic analysis is crucial. In the previous example, it is important\n",
      "for the system to note that Sandra Goudie names the person being defeated in the\n",
      "hypothesis, not the person doing the defeating in the text. As another illustration of\n",
      "the difficulty of the task, consider the following text-hypothesis pair:\n",
      "(7) a. Text: David Golinkin is the editor or author of 18 books, and over 150\n",
      "responsa, articles, sermons and books\n",
      "b. Hypothesis: Golinkin has written 18 books\n",
      "Figure 1-5. Simple pipeline architecture for a spoken dialogue system: Spoken input (top left) is\n",
      "analyzed, \n",
      "words are recognized, sentences are parsed and interpreted in context, application-specific\n",
      "actions take place (top right); a response is planned, realized as a syntactic structure, then to suitably\n",
      "inflected words, and finally to spoken output; different types of linguistic knowledge inform each stage\n",
      "of the process.\n",
      "32 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 53, 'page_label': '32', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4381}\n",
      "\n",
      "--- Chunk 4382 ---\n",
      "Content:\n",
      "In order to determine whether the hypothesis is supported by the text, the system needs\n",
      "the \n",
      "following background knowledge: (i) if someone is an author of a book, then he/\n",
      "she has written that book; (ii) if someone is an editor of a book, then he/she has not\n",
      "written (all of) that book; (iii) if someone is editor or author of 18 books, then one\n",
      "cannot conclude that he/she is author of 18 books.\n",
      "Limitations of NLP\n",
      "Despite the research-led advances in tasks such as RTE, natural language systems that\n",
      "have been deployed for real-world applications still cannot perform common-sense\n",
      "reasoning or draw on world knowledge in a general and robust manner. We can wait\n",
      "for these difficult artificial intelligence problems to be solved, but in the meantime it is\n",
      "necessary to live with some severe limitations on the reasoning and knowledge capa-\n",
      "bilities of natural language systems. Accordingly, right from the beginning, an impor-\n",
      "tant goal of NLP research has been to make progress on the difficult task of building\n",
      "technologies that “understand language,” using superficial yet powerful techniques\n",
      "instead of unrestricted knowledge and reasoning capabilities. Indeed, this is one of the\n",
      "goals of this book, and we hope to equip you with the knowledge and skills to build\n",
      "useful NLP systems, and to contribute to the long-term aspiration of building intelligent\n",
      "machines.\n",
      "1.6  Summary\n",
      "• Texts are represented in Python using lists: ['Monty', 'Python']. We can use in-\n",
      "dexing, slicing, and the len() function on lists.\n",
      "• A word “token” is a particular appearance of a given word in a text; a word “type”\n",
      "is the unique form of the word as a particular sequence of letters. We count word\n",
      "tokens using len(text) and word types using len(set(text)).\n",
      "• We obtain the vocabulary of a text t using sorted(set(t)).\n",
      "• We operate on each item of a text using [f(x) for x in text].\n",
      "• To derive the vocabulary, collapsing case distinctions and ignoring punctuation,\n",
      "we can write set([w.lower() for w in text if w.isalpha()]).\n",
      "• We process each word in a text using a for statement, such as for w in t: or for\n",
      "word in text:. This must be followed by the colon character and an indented block\n",
      "of code, to be executed each time through the loop.\n",
      "• We test a condition using an if statement: if len(word) < 5:. This must be fol-\n",
      "lowed by the colon character and an indented block of code, to be executed only\n",
      "if the condition is true.\n",
      "• A frequency distribution is a collection of items along with their frequency counts\n",
      "(e.g., the words of a text and their frequency of appearance).\n",
      "1.6  Summary | 33...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 54, 'page_label': '33', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4382}\n",
      "\n",
      "--- Chunk 4383 ---\n",
      "Content:\n",
      "• A function is a block of code that has been assigned a name and can be reused.\n",
      "Functions are defined using the def keyword, as in def mult(x, y); x and y are\n",
      "parameters of the function, and act as placeholders for actual data values.\n",
      "• A function is called by specifying its name followed by one or more arguments\n",
      "inside parentheses, like this: mult(3, 4), e.g., len(text1).\n",
      "1.7  Further Reading\n",
      "This \n",
      "chapter has introduced new concepts in programming, natural language process-\n",
      "ing, and linguistics, all mixed in together. Many of them are consolidated in the fol-\n",
      "lowing chapters. However, you may also want to consult the online materials provided\n",
      "with this chapter (at http://www.nltk.org/), including links to additional background\n",
      "materials, and links to online NLP systems. You may also like to read up on some\n",
      "linguistics and NLP-related concepts in Wikipedia (e.g., collocations, the Turing Test,\n",
      "the type-token distinction).\n",
      "You should acquaint yourself with the Python documentation available at http://docs\n",
      ".python.org/, including the many tutorials and comprehensive reference materials\n",
      "linked there. A Beginner’s Guide to Python is available at http://wiki.python.org/moin/\n",
      "BeginnersGuide. Miscellaneous questions about Python might be answered in the FAQ\n",
      "at http://www.python.org/doc/faq/general/.\n",
      "As you delve into NLTK, you might want to subscribe to the mailing list where new\n",
      "releases of the toolkit are announced. There is also an NLTK-Users mailing list, where\n",
      "users help each other as they learn how to use Python and NLTK for language analysis\n",
      "work. Details of these lists are available at http://www.nltk.org/.\n",
      "For more information on the topics covered in Section 1.5, and on NLP more generally,\n",
      "you might like to consult one of the following excellent books:\n",
      "• Indurkhya, Nitin and Fred Damerau (eds., 2010) Handbook of Natural Language\n",
      "Processing (second edition), Chapman & Hall/CRC.\n",
      "• Jurafsky, Daniel and James Martin (2008) Speech and Language Processing (second\n",
      "edition), Prentice Hall.\n",
      "• Mitkov, Ruslan (ed., 2002) The Oxford Handbook of Computational Linguistics .\n",
      "Oxford University Press. (second edition expected in 2010).\n",
      "The Association for Computational Linguistics is the international organization that\n",
      "represents the field of NLP. The ACL website hosts many useful resources, including:\n",
      "information about international and regional conferences and workshops; the ACL\n",
      "Wiki with links to hundreds of useful resources; and the ACL Anthology, which contains\n",
      "most of the NLP research literature from the past 50 years, fully indexed and freely\n",
      "downloadable.\n",
      "34 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 55, 'page_label': '34', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4383}\n",
      "\n",
      "--- Chunk 4384 ---\n",
      "Content:\n",
      "Some excellent introductory linguistics textbooks are: (Finegan, 2007), (O’Grady et\n",
      "al., \n",
      "2004), (OSU, 2007). You might like to consult LanguageLog, a popular linguistics\n",
      "blog with occasional posts that use the techniques described in this book.\n",
      "1.8  Exercises\n",
      "1. ○ Try using the Python interpreter as a calculator, and typing expressions like 12 /\n",
      "(4 + 1).\n",
      "2. ○ Given an alphabet of 26 letters, there are 26 to the power 10, or 26 ** 10, 10-\n",
      "letter strings we can form. That works out to 141167095653376L (the L at the end\n",
      "just indicates that this is Python’s long-number format). How many hundred-letter\n",
      "strings are possible?\n",
      "3. ○ The Python multiplication operation can be applied to lists. What happens when\n",
      "you type ['Monty', 'Python'] * 20, or 3 * sent1?\n",
      "4. ○ Review Section 1.1 on computing with language. How many words are there in\n",
      "text2? How many distinct words are there?\n",
      "5. ○ Compare the lexical diversity scores for humor and romance fiction in Ta-\n",
      "ble 1-1. Which genre is more lexically diverse?\n",
      "6. ○ Produce a dispersion plot of the four main protagonists in Sense and Sensibility:\n",
      "Elinor, Marianne, Edward, and Willoughby. What can you observe about the\n",
      "different roles played by the males and females in this novel? Can you identify the\n",
      "couples?\n",
      "7. ○ Find the collocations in text5.\n",
      "8. ○ Consider the following Python expression: len(set(text4)). State the purpose\n",
      "of this expression. Describe the two steps involved in performing this computation.\n",
      "9. ○ Review Section 1.2 on lists and strings.\n",
      "a. Define a string and assign it to a variable, e.g., my_string = 'My String' (but\n",
      "put something more interesting in the string). Print the contents of this variable\n",
      "in two ways, first by simply typing the variable name and pressing Enter, then\n",
      "by using the print statement.\n",
      "b. Try adding the string to itself using my_string + my_string, or multiplying it\n",
      "by a number, e.g., my_string * 3. Notice that the strings are joined together\n",
      "without any spaces. How could you fix this?\n",
      "10. ○ Define a variable my_sent to be a list of words, using the syntax my_sent = [\"My\",\n",
      "\"sent\"] (but with your own words, or a favorite saying).\n",
      "a. Use ' '.join(my_sent) to convert this into a string.\n",
      "b. Use split() to split the string back into the list form you had to start with.\n",
      "11. ○ Define several variables containing lists of words, e.g., phrase1, phrase2, and so\n",
      "on. Join them together in various combinations (using the plus operator) to form\n",
      "1.8  Exercises | 35...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 56, 'page_label': '35', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4384}\n",
      "\n",
      "--- Chunk 4385 ---\n",
      "Content:\n",
      "whole sentences. What is the relationship between len(phrase1 + phrase2) and\n",
      "len(phrase1) + len(phrase2)?\n",
      "12. ○ Consider the following two expressions, which have the same value. Which one\n",
      "will typically be more relevant in NLP? Why?\n",
      "a. \"Monty Python\"[6:12]\n",
      "b. [\"Monty\", \"Python\"][1]\n",
      "13. ○ We have seen how to represent a sentence as a list of words, where each word is\n",
      "a sequence of characters. What does sent1[2][2] do? Why? Experiment with other\n",
      "index values.\n",
      "14. ○ The first sentence of text3 is provided to you in the variable sent3. The index of\n",
      "the in sent3 is 1, because sent3[1] gives us 'the'. What are the indexes of the two\n",
      "other occurrences of this word in sent3?\n",
      "15. ○ Review the discussion of conditionals in Section 1.4. Find all words in the Chat\n",
      "Corpus (text5) starting with the letter b. Show them in alphabetical order.\n",
      "16. ○ Type the expression range(10) at the interpreter prompt. Now try range(10,\n",
      "20), range(10, 20, 2), and range(20, 10, -2). We will see a variety of uses for this\n",
      "built-in function in later chapters.\n",
      "17. ◑ Use text9.index() to find the index of the word sunset. You’ll need to insert this\n",
      "word as an argument between the parentheses. By a process of trial and error, find\n",
      "the slice for the complete sentence that contains this word.\n",
      "18. ◑ Using list addition, and the set and sorted operations, compute the vocabulary\n",
      "of the sentences sent1 ... sent8.\n",
      "19. ◑ What is the difference between the following two lines? Which one will give a\n",
      "larger value? Will this be the case for other texts?\n",
      ">>> sorted(set([w.lower() for w in text1]))\n",
      ">>> sorted([w.lower() for w in set(text1)])\n",
      "20. ◑ What is the difference between the following two tests: w.isupper() and not\n",
      "w.islower()?\n",
      "21. ◑ Write the slice expression that extracts the last two words of text2.\n",
      "22. ◑ Find all the four-letter words in the Chat Corpus ( text5). With the help of a\n",
      "frequency distribution ( FreqDist), show these words in decreasing order of fre-\n",
      "quency.\n",
      "23. ◑ Review the discussion of looping with conditions in Section 1.4. Use a combi-\n",
      "nation of for and if statements to loop over the words of the movie script for\n",
      "Monty Python and the Holy Grail  (text6) and print all the uppercase words, one\n",
      "per line.\n",
      "24. ◑ Write expressions for finding all words in text6 that meet the following condi-\n",
      "tions. The result should be in the form of a list of words: ['word1', 'word2', ...].\n",
      "36 | Chapter 1:  Language Processing and Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 57, 'page_label': '36', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4385}\n",
      "\n",
      "--- Chunk 4386 ---\n",
      "Content:\n",
      "a. Ending in ize\n",
      "b.\n",
      "Containing the letter z\n",
      "c. Containing the sequence of letters pt\n",
      "d. All lowercase letters except for an initial capital (i.e., titlecase)\n",
      "25. ◑ Define sent to be the list of words ['she', 'sells', 'sea', 'shells', 'by',\n",
      "'the', 'sea', 'shore']. Now write code to perform the following tasks:\n",
      "a. Print all words beginning with sh.\n",
      "b. Print all words longer than four characters\n",
      "26. ◑ What does the following Python code do? sum([len(w) for w in text1]) Can\n",
      "you use it to work out the average word length of a text?\n",
      "27. ◑ Define a function called vocab_size(text) that has a single parameter for the\n",
      "text, and which returns the vocabulary size of the text.\n",
      "28. ◑ Define a function percent(word, text) that calculates how often a given word\n",
      "occurs in a text and expresses the result as a percentage.\n",
      "29. ◑ We have been using sets to store vocabularies. Try the following Python expres-\n",
      "sion: set(sent3) < set(text1). Experiment with this using different arguments to\n",
      "set(). What does it do? Can you think of a practical application for this?\n",
      "1.8  Exercises | 37...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 58, 'page_label': '37', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4386}\n",
      "\n",
      "--- Chunk 4387 ---\n",
      "Content:\n",
      "CHAPTER 2\n",
      "Accessing Text Corpora\n",
      "and Lexical Resources\n",
      "Practical work in Natural Language Processing typically uses large bodies of linguistic\n",
      "data, or corpora. The goal of this chapter is to answer the following questions:\n",
      "1. What \n",
      "are some useful text corpora and lexical resources, and how can we access\n",
      "them with Python?\n",
      "2. Which Python constructs are most helpful for this work?\n",
      "3. How do we avoid repeating ourselves when writing Python code?\n",
      "This chapter continues to present programming concepts by example, in the context\n",
      "of a linguistic processing task. We will wait until later before exploring each Python\n",
      "construct systematically. Don’t worry if you see an example that contains something\n",
      "unfamiliar; simply try it out and see what it does, and—if you’re game—modify it by\n",
      "substituting some part of the code with a different text or word. This way you will\n",
      "associate a task with a programming idiom, and learn the hows and whys later.\n",
      "2.1  Accessing Text Corpora\n",
      "As just mentioned, a text corpus is a large body of text. Many corpora are designed to\n",
      "contain a careful balance of material in one or more genres. We examined some small\n",
      "text collections in Chapter 1, such as the speeches known as the US Presidential Inau-\n",
      "gural Addresses. This particular corpus actually contains dozens of individual texts—\n",
      "one per address—but for convenience we glued them end-to-end and treated them as\n",
      "a single text. Chapter 1 also used various predefined texts that we accessed by typing\n",
      "from book import *. However, since we want to be able to work with other texts, this\n",
      "section examines a variety of text corpora. We’ll see how to select individual texts, and\n",
      "how to work with them.\n",
      "39...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 60, 'page_label': '39', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4387}\n",
      "\n",
      "--- Chunk 4388 ---\n",
      "Content:\n",
      "Gutenberg Corpus\n",
      "NLTK \n",
      "includes a small selection of texts from the Project Gutenberg electronic text\n",
      "archive, which contains some 25,000 free electronic books, hosted at http://www.gu\n",
      "tenberg.org/. We begin by getting the Python interpreter to load the NLTK package,\n",
      "then ask to see nltk.corpus.gutenberg.fileids(), the file identifiers in this corpus:\n",
      ">>> import nltk\n",
      ">>> nltk.corpus.gutenberg.fileids()\n",
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',\n",
      "'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt',\n",
      "'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt',\n",
      "'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt',\n",
      "'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt',\n",
      "'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "Let’s pick out the first of these texts—Emma by Jane Austen—and give it a short name,\n",
      "emma, then find out how many words it contains:\n",
      ">>> emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
      ">>> len(emma)\n",
      "192427\n",
      "In Section 1.1, we showed how you could carry out concordancing of a\n",
      "text such as text1 with the command text1.concordance(). However,\n",
      "this assumes that you are using one of the nine texts obtained as a result\n",
      "of doing from nltk.book import *. Now that you have started examining\n",
      "data from nltk.corpus, as in the previous example, you have to employ\n",
      "the following pair of statements to perform concordancing and other\n",
      "tasks from Section 1.1:\n",
      ">>> emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
      ">>> emma.concordance(\"surprize\")\n",
      "When we defined emma, we invoked the words() function of the gutenberg object in\n",
      "NLTK’s corpus package. But since it is cumbersome to type such long names all the\n",
      "time, Python provides another version of the import statement, as follows:\n",
      ">>> from nltk.corpus import gutenberg\n",
      ">>> gutenberg.fileids()\n",
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', ...]\n",
      ">>> emma = gutenberg.words('austen-emma.txt')\n",
      "Let’s write a short program to display other information about each text, by looping\n",
      "over all the values of fileid corresponding to the gutenberg file identifiers listed earlier\n",
      "and then computing statistics for each text. For a compact output display, we will make\n",
      "sure that the numbers are all integers, using int().\n",
      ">>> for fileid in gutenberg.fileids():\n",
      "...     num_chars = len(gutenberg.raw(fileid)) \n",
      "...     num_words = len(gutenberg.words(fileid))\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 61, 'page_label': '40', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4388}\n",
      "\n",
      "--- Chunk 4389 ---\n",
      "Content:\n",
      ". For a compact output display, we will make\n",
      "sure that the numbers are all integers, using int().\n",
      ">>> for fileid in gutenberg.fileids():\n",
      "...     num_chars = len(gutenberg.raw(fileid)) \n",
      "...     num_words = len(gutenberg.words(fileid))\n",
      "...     num_sents = len(gutenberg.sents(fileid))\n",
      "40 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 61, 'page_label': '40', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4389}\n",
      "\n",
      "--- Chunk 4390 ---\n",
      "Content:\n",
      "...     num_vocab = len(set([w.lower() for w in gutenberg.words(fileid)]))\n",
      "...     print int(num_chars/num_words), int(num_words/num_sents), int(num_words/num_vocab), \n",
      "        fileid\n",
      "...\n",
      "4 21 26 austen-emma.txt\n",
      "4 23 16 austen-persuasion.txt\n",
      "4 24 22 austen-sense.txt\n",
      "4 33 79 bible-kjv.txt\n",
      "4 18 5 blake-poems.txt\n",
      "4 17 14 bryant-stories.txt\n",
      "4 17 12 burgess-busterbrown.txt\n",
      "4 16 12 carroll-alice.txt\n",
      "4 17 11 chesterton-ball.txt\n",
      "4 19 11 chesterton-brown.txt\n",
      "4 16 10 chesterton-thursday.txt\n",
      "4 18 24 edgeworth-parents.txt\n",
      "4 24 15 melville-moby_dick.txt\n",
      "4 52 10 milton-paradise.txt\n",
      "4 12 8 shakespeare-caesar.txt\n",
      "4 13 7 shakespeare-hamlet.txt\n",
      "4 13 6 shakespeare-macbeth.txt\n",
      "4 35 12 whitman-leaves.txt\n",
      "This \n",
      "program displays three statistics for each text: average word length, average sen-\n",
      "tence length, and the number of times each vocabulary item appears in the text on\n",
      "average (our lexical diversity score). Observe that average word length appears to be a\n",
      "general property of English, since it has a recurrent value of 4. (In fact, the average word\n",
      "length is really 3, not 4, since the num_chars variable counts space characters.) By con-\n",
      "trast average sentence length and lexical diversity appear to be characteristics of par-\n",
      "ticular authors.\n",
      "The previous example also showed how we can access the “raw” text of the book \n",
      ",\n",
      "not \n",
      "split up into tokens. The raw() function gives us the contents of the file without\n",
      "any linguistic processing. So, for example, len(gutenberg.raw('blake-poems.txt') tells\n",
      "us how many letters occur in the text, including the spaces between words. The\n",
      "sents() function divides the text up into its sentences, where each sentence is a list of\n",
      "words:\n",
      ">>> macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
      ">>> macbeth_sentences\n",
      "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare',\n",
      "'1603', ']'], ['Actus', 'Primus', '.'], ...]\n",
      ">>> macbeth_sentences[1037]\n",
      "['Double', ',', 'double', ',', 'toile', 'and', 'trouble', ';',\n",
      "'Fire', 'burne', ',', 'and', 'Cauldron', 'bubble']\n",
      ">>> longest_len = max([len(s) for s in macbeth_sentences])\n",
      ">>> [s for s in macbeth_sentences if len(s) == longest_len]\n",
      "[['Doubtfull', 'it', 'stood', ',', 'As', 'two', 'spent', 'Swimmers', ',', 'that',\n",
      "'doe', 'cling', 'together', ',', 'And', 'choake', 'their', 'Art', ':', 'The',\n",
      "'mercilesse', 'Macdonwald', ...], ...]\n",
      "2.1  Accessing Text Corpora | 41...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 62, 'page_label': '41', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4390}\n",
      "\n",
      "--- Chunk 4391 ---\n",
      "Content:\n",
      "Most NLTK corpus readers include a variety of access methods apart\n",
      "from words(), raw(), and sents(). Richer linguistic content is available\n",
      "from \n",
      "some corpora, such as part-of-speech tags, dialogue tags, syntactic\n",
      "trees, and so forth; we will see these in later chapters.\n",
      "Web and Chat Text\n",
      "Although Project Gutenberg contains thousands of books, it represents established\n",
      "literature. It is important to consider less formal language as well. NLTK’s small col-\n",
      "lection of web text includes content from a Firefox discussion forum, conversations\n",
      "overheard in New York, the movie script of Pirates of the Carribean , personal adver-\n",
      "tisements, and wine reviews:\n",
      ">>> from nltk.corpus import webtext\n",
      ">>> for fileid in webtext.fileids():\n",
      "...     print fileid, webtext.raw(fileid)[:65], '...'\n",
      "...\n",
      "firefox.txt Cookie Manager: \"Don't allow sites that set removed cookies to se...\n",
      "grail.txt SCENE 1: [wind] [clop clop clop] KING ARTHUR: Whoa there!  [clop...\n",
      "overheard.txt White guy: So, do you have any plans for this evening? Asian girl...\n",
      "pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terr...\n",
      "singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encoun...\n",
      "wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawb...\n",
      "There is also a corpus of instant messaging chat sessions, originally collected by the\n",
      "Naval Postgraduate School for research on automatic detection of Internet predators.\n",
      "The corpus contains over 10,000 posts, anonymized by replacing usernames with\n",
      "generic names of the form “UserNNN”, and manually edited to remove any other\n",
      "identifying information. The corpus is organized into 15 files, where each file contains\n",
      "several hundred posts collected on a given date, for an age-specific chatroom (teens,\n",
      "20s, 30s, 40s, plus a generic adults chatroom). The filename contains the date, chat-\n",
      "room, and number of posts; e.g., 10-19-20s_706posts.xml contains 706 posts gathered\n",
      "from the 20s chat room on 10/19/2006.\n",
      ">>> from nltk.corpus import nps_chat\n",
      ">>> chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
      ">>> chatroom[123]\n",
      "['i', 'do', \"n't\", 'want', 'hot', 'pics', 'of', 'a', 'female', ',',\n",
      "'I', 'can', 'look', 'in', 'a', 'mirror', '.']\n",
      "Brown Corpus\n",
      "The Brown Corpus was the first million-word electronic corpus of English, created in\n",
      "1961 at Brown University. This corpus contains text from 500 sources, and the sources\n",
      "have been categorized by genre, such as news, editorial, and so on. Table 2-1 gives an\n",
      "example of each genre (for a complete list, see http://icame.uib.no/brown/bcm-los.html).\n",
      "42 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 63, 'page_label': '42', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4391}\n",
      "\n",
      "--- Chunk 4392 ---\n",
      "Content:\n",
      "Table 2-1. Example document for each section of the Brown Corpus\n",
      "ID File Genre Description\n",
      "A16 ca16 news Chicago Tribune: Society Reportage\n",
      "B02 cb02 editorial Christian Science Monitor: Editorials\n",
      "C17 cc17 reviews Time Magazine: Reviews\n",
      "D12 cd12 religion Underwood: Probing the Ethics of Realtors\n",
      "E36 ce36 hobbies Norling: Renting a Car in Europe\n",
      "F25 cf25 lore Boroff: Jewish Teenage Culture\n",
      "G22 cg22 belles_lettres Reiner: Coping with Runaway Technology\n",
      "H15 ch15 government US Office of Civil and Defence Mobilization: The Family Fallout Shelter\n",
      "J17 cj19 learned Mosteller: Probability with Statistical Applications\n",
      "K04 ck04 fiction W.E.B. Du Bois: Worlds of Color\n",
      "L13 cl13 mystery Hitchens: Footsteps in the Night\n",
      "M01 cm01 science_fiction Heinlein: Stranger in a Strange Land\n",
      "N14 cn15 adventure Field: Rattlesnake Ridge\n",
      "P12 cp12 romance Callaghan: A Passion in Rome\n",
      "R06 cr06 humor Thurber: The Future, If Any, of Comedy\n",
      "We can access the corpus as a list of words or a list of sentences (where each sentence\n",
      "is \n",
      "itself just a list of words). We can optionally specify particular categories or files to\n",
      "read:\n",
      ">>> from nltk.corpus import brown\n",
      ">>> brown.categories()\n",
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
      "'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance',\n",
      "'science_fiction']\n",
      ">>> brown.words(categories='news')\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      ">>> brown.words(fileids=['cg22'])\n",
      "['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...]\n",
      ">>> brown.sents(categories=['news', 'editorial', 'reviews'])\n",
      "[['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]\n",
      "The Brown Corpus is a convenient resource for studying systematic differences between\n",
      "genres, a kind of linguistic inquiry known as stylistics. Let’s compare genres in their\n",
      "usage of modal verbs. The first step is to produce the counts for a particular genre.\n",
      "Remember to import nltk before doing the following:\n",
      ">>> from nltk.corpus import brown\n",
      ">>> news_text = brown.words(categories='news')\n",
      ">>> fdist = nltk.FreqDist([w.lower() for w in news_text])\n",
      ">>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
      ">>> for m in modals:\n",
      "...     print m + ':', fdist[m],\n",
      "2.1  Accessing Text Corpora | 43...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 64, 'page_label': '43', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4392}\n",
      "\n",
      "--- Chunk 4393 ---\n",
      "Content:\n",
      "...\n",
      "can: 94 could: 87 may: 93 might: 38 must: 53 will: 389\n",
      "Your Turn: Choose a different section of the Brown Corpus, and adapt\n",
      "the preceding example to count a selection of wh words, such as what,\n",
      "when, where, who and why.\n",
      "Next, we need to obtain counts for each genre of interest. We’ll use NLTK’s support\n",
      "for conditional frequency distributions. These are presented systematically in Sec-\n",
      "tion 2.2, where we also unpick the following code line by line. For the moment, you\n",
      "can ignore the details and just concentrate on the output.\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (genre, word)\n",
      "...           for genre in brown.categories()\n",
      "...           for word in brown.words(categories=genre))\n",
      ">>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
      ">>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
      ">>> cfd.tabulate(conditions=genres, samples=modals)\n",
      "                 can could  may might must will\n",
      "           news   93   86   66   38   50  389\n",
      "       religion   82   59   78   12   54   71\n",
      "        hobbies  268   58  131   22   83  264\n",
      "science_fiction   16   49    4   12    8   16\n",
      "        romance   74  193   11   51   45   43\n",
      "          humor   16   30    8    8    9   13\n",
      "Observe that the most frequent modal in the news genre is will, while the most frequent\n",
      "modal in the romance genre is could. Would you have predicted this? The idea that\n",
      "word counts might distinguish genres will be taken up again in Chapter 6.\n",
      "Reuters Corpus\n",
      "The Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The\n",
      "documents have been classified into 90 topics, and grouped into two sets, called “train-\n",
      "ing” and “test”; thus, the text with fileid 'test/14826' is a document drawn from the\n",
      "test set...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 65, 'page_label': '44', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4393}\n",
      "\n",
      "--- Chunk 4394 ---\n",
      "Content:\n",
      ". Would you have predicted this? The idea that\n",
      "word counts might distinguish genres will be taken up again in Chapter 6.\n",
      "Reuters Corpus\n",
      "The Reuters Corpus contains 10,788 news documents totaling 1.3 million words. The\n",
      "documents have been classified into 90 topics, and grouped into two sets, called “train-\n",
      "ing” and “test”; thus, the text with fileid 'test/14826' is a document drawn from the\n",
      "test set. This split is for training and testing algorithms that automatically detect the\n",
      "topic of a document, as we will see in Chapter 6.\n",
      ">>> from nltk.corpus import reuters\n",
      ">>> reuters.fileids()\n",
      "['test/14826', 'test/14828', 'test/14829', 'test/14832', ...]\n",
      ">>> reuters.categories()\n",
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa',\n",
      "'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn',\n",
      "'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', ...]\n",
      "Unlike the Brown Corpus, categories in the Reuters Corpus overlap with each other,\n",
      "simply because a news story often covers multiple topics. We can ask for the topics\n",
      "44 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 65, 'page_label': '44', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4394}\n",
      "\n",
      "--- Chunk 4395 ---\n",
      "Content:\n",
      "covered by one or more documents, or for the documents included in one or more\n",
      "categories. \n",
      "For convenience, the corpus methods accept a single fileid or a list of fileids.\n",
      ">>> reuters.categories('training/9865')\n",
      "['barley', 'corn', 'grain', 'wheat']\n",
      ">>> reuters.categories(['training/9865', 'training/9880'])\n",
      "['barley', 'corn', 'grain', 'money-fx', 'wheat']\n",
      ">>> reuters.fileids('barley')\n",
      "['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', ...]\n",
      ">>> reuters.fileids(['barley', 'corn'])\n",
      "['test/14832', 'test/14858', 'test/15033', 'test/15043', 'test/15106',\n",
      "'test/15287', 'test/15341', 'test/15618', 'test/15618', 'test/15648', ...]\n",
      "Similarly, we can specify the words or sentences we want in terms of files or categories.\n",
      "The first handful of words in each of these texts are the titles, which by convention are\n",
      "stored as uppercase.\n",
      ">>> reuters.words('training/9865')[:14]\n",
      "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', 'BIDS',\n",
      "'DETAILED', 'French', 'operators', 'have', 'requested', 'licences', 'to', 'export']\n",
      ">>> reuters.words(['training/9865', 'training/9880'])\n",
      "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]\n",
      ">>> reuters.words(categories='barley')\n",
      "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]\n",
      ">>> reuters.words(categories=['barley', 'corn'])\n",
      "['THAI', 'TRADE', 'DEFICIT', 'WIDENS', 'IN', 'FIRST', ...]\n",
      "Inaugural Address Corpus\n",
      "In Section 1.1, we looked at the Inaugural Address Corpus, but treated it as a single\n",
      "text. The graph in Figure 1-2 used “word offset” as one of the axes; this is the numerical\n",
      "index of the word in the corpus, counting from the first word of the first address.\n",
      "However, the corpus is actually a collection of 55 texts, one for each presidential ad-\n",
      "dress. An interesting property of this collection is its time dimension:\n",
      ">>> from nltk.corpus import inaugural\n",
      ">>> inaugural.fileids()\n",
      "['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', ...]\n",
      ">>> [fileid[:4] for fileid in inaugural.fileids()]\n",
      "['1789', '1793', '1797', '1801', '1805', '1809', '1813', '1817', '1821', ...]\n",
      "Notice that the year of each text appears in its filename. To get the year out of the\n",
      "filename, we extracted the first four characters, using fileid[:4].\n",
      "Let’s look at how the words America and citizen are used over time...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 66, 'page_label': '45', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4395}\n",
      "\n",
      "--- Chunk 4396 ---\n",
      "Content:\n",
      ". To get the year out of the\n",
      "filename, we extracted the first four characters, using fileid[:4].\n",
      "Let’s look at how the words America and citizen are used over time. The following code\n",
      "converts the words in the Inaugural corpus to lowercase using w.lower() \n",
      ", then checks\n",
      "whether \n",
      "they start with either of the “targets” america or citizen using startswith()\n",
      ". Thus it will count words such as American’s and Citizens. We’ll learn about condi-\n",
      "tional frequency distributions in Section 2.2; for now, just consider the output, shown\n",
      "in Figure 2-1.\n",
      "2.1  Accessing Text Corpora | 45...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 66, 'page_label': '45', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4396}\n",
      "\n",
      "--- Chunk 4397 ---\n",
      "Content:\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (target, file[:4])\n",
      "...           for fileid in inaugural.fileids()\n",
      "...           for w in inaugural.words(fileid)\n",
      "...           for target in ['america', 'citizen']\n",
      "...           if w.lower().startswith(target)) \n",
      ">>> cfd.plot()\n",
      "Figure 2-1. Plot of a conditional frequency distribution: All words in the Inaugural Address Corpus\n",
      "that \n",
      "begin with america or citizen are counted; separate counts are kept for each address; these are\n",
      "plotted so that trends in usage over time can be observed; counts are not normalized for document\n",
      "length.\n",
      "Annotated Text Corpora\n",
      "Many text corpora contain linguistic annotations, representing part-of-speech tags,\n",
      "named entities, syntactic structures, semantic roles, and so forth. NLTK provides\n",
      "convenient ways to access several of these corpora, and has data packages containing\n",
      "corpora and corpus samples, freely downloadable for use in teaching and research.\n",
      "Table 2-2  lists some of the corpora. For information about downloading them, see\n",
      "http://www.nltk.org/data. For more examples of how to access NLTK corpora, please\n",
      "consult the Corpus HOWTO at http://www.nltk.org/howto.\n",
      "Table 2-2. Some of the corpora and corpus samples distributed with NLTK\n",
      "Corpus Compiler Contents\n",
      "Brown Corpus Francis, Kucera 15 genres, 1.15M words, tagged, categorized\n",
      "CESS Treebanks CLiC-UB 1M words, tagged and parsed (Catalan, Spanish)\n",
      "Chat-80 Data Files Pereira & Warren World Geographic Database\n",
      "CMU Pronouncing Dictionary CMU 127k entries\n",
      "CoNLL 2000 Chunking Data CoNLL 270k words, tagged and chunked\n",
      "46 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 67, 'page_label': '46', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4397}\n",
      "\n",
      "--- Chunk 4398 ---\n",
      "Content:\n",
      "Corpus Compiler Contents\n",
      "CoNLL 2002 Named Entity CoNLL 700k words, POS and named entity tagged (Dutch, Spanish)\n",
      "CoNLL 2007 Dependency Parsed Tree-\n",
      "banks (selections)\n",
      "CoNLL 150k words, dependency parsed (Basque, Catalan)\n",
      "Dependency Treebank Narad Dependency parsed version of Penn Treebank sample\n",
      "Floresta Treebank Diana Santos et al. 9k sentences, tagged and parsed (Portuguese)\n",
      "Gazetteer Lists Various Lists of cities and countries\n",
      "Genesis Corpus Misc web sources 6 texts, 200k words, 6 languages\n",
      "Gutenberg (selections) Hart, Newby, et al. 18 texts, 2M words\n",
      "Inaugural Address Corpus CSpan U.S. Presidential Inaugural Addresses (1789–present)\n",
      "Indian POS Tagged Corpus Kumaran et al. 60k words, tagged (Bangla, Hindi, Marathi, Telugu)\n",
      "MacMorpho Corpus NILC, USP, Brazil 1M words, tagged (Brazilian Portuguese)\n",
      "Movie Reviews Pang, Lee 2k movie reviews with sentiment polarity classification\n",
      "Names Corpus Kantrowitz, Ross 8k male and female names\n",
      "NIST 1999 Info Extr (selections) Garofolo 63k words, newswire and named entity SGML markup\n",
      "NPS Chat Corpus Forsyth, Martell 10k IM chat posts, POS and dialogue-act tagged\n",
      "Penn Treebank (selections) LDC 40k words, tagged and parsed\n",
      "PP Attachment Corpus Ratnaparkhi 28k prepositional phrases, tagged as noun or verb modifiers\n",
      "Proposition Bank Palmer 113k propositions, 3,300 verb frames\n",
      "Question Classification Li, Roth 6k questions, categorized\n",
      "Reuters Corpus Reuters 1.3M words, 10k news documents, categorized\n",
      "Roget’s Thesaurus Project Gutenberg 200k words, formatted text\n",
      "RTE Textual Entailment Dagan et al. 8k sentence pairs, categorized\n",
      "SEMCOR Rus, Mihalcea 880k words, POS and sense tagged\n",
      "Senseval 2 Corpus Pedersen 600k words, POS and sense tagged\n",
      "Shakespeare texts (selections) Bosak 8 books in XML format\n",
      "State of the Union Corpus CSpan 485k words, formatted text\n",
      "Stopwords Corpus Porter et al. 2,400 stopwords for 11 languages\n",
      "Swadesh Corpus Wiktionary Comparative wordlists in 24 languages\n",
      "Switchboard Corpus (selections) LDC 36 phone calls, transcribed, parsed\n",
      "TIMIT Corpus (selections) NIST/LDC Audio files and transcripts for 16 speakers\n",
      "Univ Decl of Human Rights United Nations 480k words, 300+ languages\n",
      "VerbNet 2.1 Palmer et al. 5k verbs, hierarchically organized, linked to WordNet\n",
      "Wordlist Corpus OpenOffice.org et al. 960k words and 20k affixes for 8 languages\n",
      "WordNet 3.0 (English)\n",
      "Miller, Fellbaum 145k synonym sets\n",
      "2.1  Accessing Text Corpora | 47...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 68, 'page_label': '47', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4398}\n",
      "\n",
      "--- Chunk 4399 ---\n",
      "Content:\n",
      "Corpora in Other Languages\n",
      "NLTK \n",
      "comes with corpora for many languages, though in some cases you will need to\n",
      "learn how to manipulate character encodings in Python before using these corpora (see\n",
      "Section 3.3).\n",
      ">>> nltk.corpus.cess_esp.words()\n",
      "['El', 'grupo', 'estatal', 'Electricit\\xe9_de_France', ...]\n",
      ">>> nltk.corpus.floresta.words()\n",
      "['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]\n",
      ">>> nltk.corpus.indian.words('hindi.pos')\n",
      "['\\xe0\\xa4\\xaa\\xe0\\xa5\\x82\\xe0\\xa4\\xb0\\xe0\\xa5\\x8d\\xe0\\xa4\\xa3',\n",
      "'\\xe0\\xa4\\xaa\\xe0\\xa5\\x8d\\xe0\\xa4\\xb0\\xe0\\xa4\\xa4\\xe0\\xa4\\xbf\\xe0\\xa4\\xac\\xe0\\xa4\n",
      "\\x82\\xe0\\xa4\\xa7', ...]\n",
      ">>> nltk.corpus.udhr.fileids()\n",
      "['Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1',\n",
      "'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1',\n",
      "'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', ...]\n",
      ">>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]\n",
      "[u'Saben', u'umat', u'manungsa', u'lair', u'kanthi', ...]\n",
      "The last of these corpora, udhr, contains the Universal Declaration of Human Rights\n",
      "in over 300 languages. The fileids for this corpus include information about the char-\n",
      "acter encoding used in the file, such as UTF8 or Latin1. Let’s use a conditional frequency\n",
      "distribution to examine the differences in word lengths for a selection of languages\n",
      "included in the udhr corpus. The output is shown in Figure 2-2 (run the program your-\n",
      "self to see a color plot). Note that True and False are Python’s built-in Boolean values.\n",
      ">>> from nltk.corpus import udhr\n",
      ">>> languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
      "...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (lang, len(word))\n",
      "...           for lang in languages\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 69, 'page_label': '48', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4399}\n",
      "\n",
      "--- Chunk 4400 ---\n",
      "Content:\n",
      ". Note that True and False are Python’s built-in Boolean values.\n",
      ">>> from nltk.corpus import udhr\n",
      ">>> languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
      "...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (lang, len(word))\n",
      "...           for lang in languages\n",
      "...           for word in udhr.words(lang + '-Latin1'))\n",
      ">>> cfd.plot(cumulative=True)\n",
      "Your Turn: Pick a language of interest in udhr.fileids(), and define a\n",
      "variable raw_text = udhr.raw(Language-Latin1). Now plot a frequency\n",
      "distribution of the letters of the text using\n",
      "nltk.FreqDist(raw_text).plot().\n",
      "Unfortunately, for many languages, substantial corpora are not yet available. Often\n",
      "there is insufficient government or industrial support for developing language resour-\n",
      "ces, and individual efforts are piecemeal and hard to discover or reuse. Some languages\n",
      "have no established writing system, or are endangered. (See Section 2.7 for suggestions\n",
      "on how to locate language resources.)\n",
      "48 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 69, 'page_label': '48', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4400}\n",
      "\n",
      "--- Chunk 4401 ---\n",
      "Content:\n",
      "Text Corpus Structure\n",
      "We \n",
      "have seen a variety of corpus structures so far; these are summarized in Fig-\n",
      "ure 2-3. The simplest kind lacks any structure: it is just a collection of texts. Often,\n",
      "texts are grouped into categories that might correspond to genre, source, author, lan-\n",
      "guage, etc. Sometimes these categories overlap, notably in the case of topical categories,\n",
      "as a text can be relevant to more than one topic. Occasionally, text collections have\n",
      "temporal structure, news collections being the most common example.\n",
      "NLTK’s corpus readers support efficient access to a variety of corpora, and can be used\n",
      "to work with new corpora. Table 2-3 lists functionality provided by the corpus readers.\n",
      "Figure 2-2. Cumulative word length distributions: Six translations of the Universal Declaration of\n",
      "Human \n",
      "Rights are processed; this graph shows that words having five or fewer letters account for\n",
      "about 80% of Ibibio text, 60% of German text, and 25% of Inuktitut text.\n",
      "2.1  Accessing Text Corpora | 49...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 70, 'page_label': '49', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4401}\n",
      "\n",
      "--- Chunk 4402 ---\n",
      "Content:\n",
      "Figure 2-3. Common structures for text corpora: The simplest kind of corpus is a collection of isolated\n",
      "texts \n",
      "with no particular organization; some corpora are structured into categories, such as genre\n",
      "(Brown Corpus); some categorizations overlap, such as topic categories (Reuters Corpus); other\n",
      "corpora represent language use over time (Inaugural Address Corpus).\n",
      "Table 2-3. Basic corpus functionality defined in NLTK: More documentation can be found using\n",
      "help(nltk.corpus.reader) and by reading the online Corpus HOWTO at http://www.nltk.org/howto.\n",
      "Example Description\n",
      "fileids() The files of the corpus\n",
      "fileids([categories]) The files of the corpus corresponding to these categories\n",
      "categories() The categories of the corpus\n",
      "categories([fileids]) The categories of the corpus corresponding to these files\n",
      "raw() The raw content of the corpus\n",
      "raw(fileids=[f1,f2,f3]) The raw content of the specified files\n",
      "raw(categories=[c1,c2]) The raw content of the specified categories\n",
      "words() The words of the whole corpus\n",
      "words(fileids=[f1,f2,f3]) The words of the specified fileids\n",
      "words(categories=[c1,c2]) The words of the specified categories\n",
      "sents() The sentences of the specified categories\n",
      "sents(fileids=[f1,f2,f3]) The sentences of the specified fileids\n",
      "sents(categories=[c1,c2]) The sentences of the specified categories\n",
      "abspath(fileid) The location of the given file on disk\n",
      "encoding(fileid) The encoding of the file (if known)\n",
      "open(fileid) Open a stream for reading the given corpus file\n",
      "root() The path to the root of locally installed corpus\n",
      "readme() The contents of the README file of the corpus\n",
      "We illustrate the difference between some of the corpus access methods here:\n",
      ">>> raw = gutenberg.raw(\"burgess-busterbrown.txt\")\n",
      ">>> raw[1:20]\n",
      "'The Adventures of B'\n",
      ">>> words = gutenberg.words(\"burgess-busterbrown.txt\")\n",
      ">>> words[1:20]\n",
      "50 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 71, 'page_label': '50', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4402}\n",
      "\n",
      "--- Chunk 4403 ---\n",
      "Content:\n",
      "['The', 'Adventures', 'of', 'Buster', 'Bear', 'by', 'Thornton', 'W', '.',\n",
      "'Burgess', '1920', ']', 'I', 'BUSTER', 'BEAR', 'GOES', 'FISHING', 'Buster',\n",
      "'Bear']\n",
      ">>> sents = gutenberg.sents(\"burgess-busterbrown.txt\")\n",
      ">>> sents[1:20]\n",
      "[['I'], ['BUSTER', 'BEAR', 'GOES', 'FISHING'], ['Buster', 'Bear', 'yawned', 'as',\n",
      "'he', 'lay', 'on', 'his', 'comfortable', 'bed', 'of', 'leaves', 'and', 'watched',\n",
      "'the', 'first', 'early', 'morning', 'sunbeams', 'creeping', 'through', ...], ...]\n",
      "Loading Your Own Corpus\n",
      "If \n",
      "you have a your own collection of text files that you would like to access using the\n",
      "methods discussed earlier, you can easily load them with the help of NLTK’s Plain\n",
      "textCorpusReader. Check the location of your files on your file system; in the following\n",
      "example, we have taken this to be the directory /usr/share/dict. Whatever the location,\n",
      "set this to be the value of corpus_root \n",
      " . The second parameter of the PlaintextCor\n",
      "pusReader \n",
      "initializer \n",
      "  can be a list of fileids, like ['a.txt', 'test/b.txt'], or a pattern\n",
      "that matches all fileids, like '[abc]/.*\\.txt' (see Section 3.4 for information about\n",
      "regular expressions).\n",
      ">>> from nltk.corpus import PlaintextCorpusReader\n",
      ">>> corpus_root = '/usr/share/dict' \n",
      ">>> wordlists = PlaintextCorpusReader(corpus_root, '.*') \n",
      ">>> wordlists.fileids()\n",
      "['README', 'connectives', 'propernames', 'web2', 'web2a', 'words']\n",
      ">>> wordlists.words('connectives')\n",
      "['the', 'of', 'and', 'to', 'a', 'in', 'that', 'is', ...]\n",
      "As \n",
      "another example, suppose you have your own local copy of Penn Treebank (release\n",
      "3), in C:\\corpora. We can use the BracketParseCorpusReader to access this corpus...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 72, 'page_label': '51', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4403}\n",
      "\n",
      "--- Chunk 4404 ---\n",
      "Content:\n",
      ". We can use the BracketParseCorpusReader to access this corpus. We\n",
      "specify the corpus_root to be the location of the parsed Wall Street Journal component\n",
      "of the corpus \n",
      " , and give a file_pattern that matches the files contained within its\n",
      "subfolders \n",
      "  (using forward slashes).\n",
      ">>> from nltk.corpus import BracketParseCorpusReader\n",
      ">>> corpus_root = r\"C:\\corpora\\penntreebank\\parsed\\mrg\\wsj\" \n",
      ">>> file_pattern = r\".*/wsj_.*\\.mrg\" \n",
      ">>> ptb = BracketParseCorpusReader(corpus_root, file_pattern)\n",
      ">>> ptb.fileids()\n",
      "['00/wsj_0001.mrg', '00/wsj_0002.mrg', '00/wsj_0003.mrg', '00/wsj_0004.mrg', ...]\n",
      ">>> len(ptb.sents())\n",
      "49208\n",
      ">>> ptb.sents(fileids='20/wsj_2013.mrg')[19]\n",
      "['The', '55-year-old', 'Mr.', 'Noriega', 'is', \"n't\", 'as', 'smooth', 'as', 'the',\n",
      "'shah', 'of', 'Iran', ',', 'as', 'well-born', 'as', 'Nicaragua', \"'s\", 'Anastasio',\n",
      "'Somoza', ',', 'as', 'imperial', 'as', 'Ferdinand', 'Marcos', 'of', 'the', 'Philippines',\n",
      "'or', 'as', 'bloody', 'as', 'Haiti', \"'s\", 'Baby', Doc', 'Duvalier', '.']\n",
      "2.1  Accessing Text Corpora | 51...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 72, 'page_label': '51', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4404}\n",
      "\n",
      "--- Chunk 4405 ---\n",
      "Content:\n",
      "2.2  Conditional Frequency Distributions\n",
      "We \n",
      "introduced frequency distributions in Section 1.3. We saw that given some list\n",
      "mylist of words or other items, FreqDist(mylist) would compute the number of\n",
      "occurrences of each item in the list. Here we will generalize this idea.\n",
      "When the texts of a corpus are divided into several categories (by genre, topic, author,\n",
      "etc.), we can maintain separate frequency distributions for each category. This will\n",
      "allow us to study systematic differences between the categories. In the previous section,\n",
      "we achieved this using NLTK’s ConditionalFreqDist data type. A conditional fre-\n",
      "quency distribution is a collection of frequency distributions, each one for a different\n",
      "“condition.” The condition will often be the category of the text. Figure 2-4 depicts a\n",
      "fragment of a conditional frequency distribution having just two conditions, one for\n",
      "news text and one for romance text.\n",
      "Figure 2-4. Counting words appearing in a text collection (a conditional frequency distribution).\n",
      "Conditions and Events\n",
      "A \n",
      "frequency distribution counts observable events, such as the appearance of words in\n",
      "a text. A conditional frequency distribution needs to pair each event with a condition.\n",
      "So instead of processing a sequence of words \n",
      " , we have to process a sequence of\n",
      "pairs \n",
      " :\n",
      ">>> text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...] \n",
      ">>> pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...] \n",
      "Each pair has the form (condition, event). If we were processing the entire Brown\n",
      "Corpus by genre, there would be 15 conditions (one per genre) and 1,161,192 events\n",
      "(one per word).\n",
      "Counting Words by Genre\n",
      "In Section 2.1, we saw a conditional frequency distribution where the condition was\n",
      "the section of the Brown Corpus, and for each condition we counted words. Whereas\n",
      "FreqDist() takes a simple list as input, ConditionalFreqDist() takes a list of pairs.\n",
      "52 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 73, 'page_label': '52', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4405}\n",
      "\n",
      "--- Chunk 4406 ---\n",
      "Content:\n",
      ">>> from nltk.corpus import brown\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (genre, word)\n",
      "...           for genre in brown.categories()\n",
      "...           for word in brown.words(categories=genre))\n",
      "Let’s \n",
      "break this down, and look at just two genres, news and romance. For each genre \n",
      ",\n",
      "we \n",
      "loop over every word in the genre \n",
      " , producing pairs consisting of the genre and\n",
      "the word \n",
      " :\n",
      ">>> genre_word = [(genre, word) \n",
      "...               for genre in ['news', 'romance'] \n",
      "...               for word in brown.words(categories=genre)] \n",
      ">>> len(genre_word)\n",
      "170576\n",
      "So, \n",
      "as we can see in the following code, pairs at the beginning of the list genre_word will\n",
      "be of the form ( 'news', word) \n",
      " , whereas those at the end will be of the form ( 'roman\n",
      "ce', word) \n",
      " .\n",
      ">>> genre_word[:4]\n",
      "[('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ('news', 'Grand')] \n",
      ">>> genre_word[-4:]\n",
      "[('romance', 'afraid'), ('romance', 'not'), ('romance', \"''\"), ('romance', '.')] \n",
      "We can now use this list of pairs to create a ConditionalFreqDist, and save it in a variable\n",
      "cfd. As usual, we can type the name of the variable to inspect it \n",
      " , and verify it has two\n",
      "conditions \n",
      " :\n",
      ">>> cfd = nltk.ConditionalFreqDist(genre_word)\n",
      ">>> cfd \n",
      "<ConditionalFreqDist with 2 conditions>\n",
      ">>> cfd.conditions()\n",
      "['news', 'romance'] \n",
      "Let’s access the two conditions, and satisfy ourselves that each is just a frequency\n",
      "distribution:\n",
      ">>> cfd['news']\n",
      "<FreqDist with 100554 outcomes>\n",
      ">>> cfd['romance']\n",
      "<FreqDist with 70022 outcomes>\n",
      ">>> list(cfd['romance'])\n",
      "[',', '.', 'the', 'and', 'to', 'a', 'of', '``', \"''\", 'was', 'I', 'in', 'he', 'had',\n",
      "'?', 'her', 'that', 'it', 'his', 'she', 'with', 'you', 'for', 'at', 'He', 'on', 'him',\n",
      "'said', '!', '--', 'be', 'as', ';', 'have', 'but', 'not', 'would', 'She', 'The', ...]\n",
      ">>> cfd['romance']['could']\n",
      "193\n",
      "Plotting and Tabulating Distributions\n",
      "Apart \n",
      "from combining two or more frequency distributions, and being easy to initialize,\n",
      "a ConditionalFreqDist provides some useful methods for tabulation and plotting.\n",
      "2.2  Conditional Frequency Distributions | 53...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 74, 'page_label': '53', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4406}\n",
      "\n",
      "--- Chunk 4407 ---\n",
      "Content:\n",
      "The plot in Figure 2-1 was based on a conditional frequency distribution reproduced\n",
      "in the following code. The condition is either of the words america or citizen \n",
      " , and\n",
      "the \n",
      "counts being plotted are the number of times the word occurred in a particular\n",
      "speech. It exploits the fact that the filename for each speech—for example,\n",
      "1865-Lincoln.txt—contains the year as the first four characters \n",
      " . This code generates\n",
      "the \n",
      "pair ('america', '1865') for every instance of a word whose lowercased form starts\n",
      "with america—such as Americans—in the file 1865-Lincoln.txt.\n",
      ">>> from nltk.corpus import inaugural\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (target, fileid[:4]) \n",
      "...           for fileid in inaugural.fileids()\n",
      "...           for w in inaugural.words(fileid)\n",
      "...           for target in ['america', 'citizen'] \n",
      "...           if w.lower().startswith(target))\n",
      "The \n",
      "plot in Figure 2-2 was also based on a conditional frequency distribution, repro-\n",
      "duced in the following code. This time, the condition is the name of the language, and\n",
      "the counts being plotted are derived from word lengths \n",
      " . It exploits the fact that the\n",
      "filename for each language is the language name followed by '-Latin1' (the character\n",
      "encoding).\n",
      ">>> from nltk.corpus import udhr\n",
      ">>> languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
      "...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (lang, len(word)) \n",
      "...           for lang in languages\n",
      "...           for word in udhr.words(lang + '-Latin1'))\n",
      "In the plot() and tabulate() \n",
      "methods, we can optionally specify which conditions to\n",
      "display with a conditions= parameter. When we omit it, we get all the conditions.\n",
      "Similarly, we can limit the samples to display with a samples= parameter. This makes\n",
      "it possible to load a large quantity of data into a conditional frequency distribution,\n",
      "and then to explore it by plotting or tabulating selected conditions and samples. It also\n",
      "gives us full control over the order of conditions and samples in any displays. For ex-\n",
      "ample, we can tabulate the cumulative frequency data just for two languages, and for\n",
      "words less than 10 characters long, as shown next. We interpret the last cell on the top\n",
      "row to mean that 1,638 words of the English text have nine or fewer letters.\n",
      ">>> cfd.tabulate(conditions=['English', 'German_Deutsch'],\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 75, 'page_label': '54', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4407}\n",
      "\n",
      "--- Chunk 4408 ---\n",
      "Content:\n",
      ". This makes\n",
      "it possible to load a large quantity of data into a conditional frequency distribution,\n",
      "and then to explore it by plotting or tabulating selected conditions and samples. It also\n",
      "gives us full control over the order of conditions and samples in any displays. For ex-\n",
      "ample, we can tabulate the cumulative frequency data just for two languages, and for\n",
      "words less than 10 characters long, as shown next. We interpret the last cell on the top\n",
      "row to mean that 1,638 words of the English text have nine or fewer letters.\n",
      ">>> cfd.tabulate(conditions=['English', 'German_Deutsch'],\n",
      "...              samples=range(10), cumulative=True)\n",
      "                  0    1    2    3    4    5    6    7    8    9\n",
      "       English    0  185  525  883  997 1166 1283 1440 1558 1638\n",
      "German_Deutsch    0  171  263  614  717  894 1013 1110 1213 1275\n",
      "54 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 75, 'page_label': '54', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4408}\n",
      "\n",
      "--- Chunk 4409 ---\n",
      "Content:\n",
      "Your Turn: Working with the news and romance genres from the\n",
      "Brown Corpus, find out which days of the week are most newsworthy,\n",
      "and which are most romantic. Define a variable called days containing\n",
      "a list of days of the week, i.e., ['Monday', ...]. Now tabulate the counts\n",
      "for these words using cfd.tabulate(samples=days). Now try the same\n",
      "thing using plot in place of tabulate. You may control the output order\n",
      "of days with the help of an extra parameter: condi\n",
      "tions=['Monday', ...].\n",
      "You may have noticed that the multiline expressions we have been using with condi-\n",
      "tional frequency distributions look like list comprehensions, but without the brackets.\n",
      "In general, when we use a list comprehension as a parameter to a function, like\n",
      "set([w.lower for w in t]), we are permitted to omit the square brackets and just write\n",
      "set(w.lower() for w in t). (See the discussion of “generator expressions” in Sec-\n",
      "tion 4.2 for more about this.)\n",
      "Generating Random Text with Bigrams\n",
      "We can use a conditional frequency distribution to create a table of bigrams (word\n",
      "pairs, introduced in Section 1.3). The bigrams() function takes a list of words and builds\n",
      "a list of consecutive word pairs:\n",
      ">>> sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven',\n",
      "...   'and', 'the', 'earth', '.']\n",
      ">>> nltk.bigrams(sent)\n",
      "[('In', 'the'), ('the', 'beginning'), ('beginning', 'God'), ('God', 'created'),\n",
      "('created', 'the'), ('the', 'heaven'), ('heaven', 'and'), ('and', 'the'),\n",
      "('the', 'earth'), ('earth', '.')]\n",
      "In Example 2-1, we treat each word as a condition, and for each one we effectively\n",
      "create a frequency distribution over the following words. The function gener\n",
      "ate_model() contains a simple loop to generate text. When we call the function, we\n",
      "choose a word (such as 'living') as our initial context. Then, once inside the loop, we\n",
      "print the current value of the variable word, and reset word to be the most likely token\n",
      "in that context (using max()); next time through the loop, we use that word as our new\n",
      "context. As you can see by inspecting the output, this simple approach to text gener-\n",
      "ation tends to get stuck in loops. Another method would be to randomly choose the\n",
      "next word from among the available words.\n",
      "Example 2-1. Generating random text: This program obtains all bigrams from the text of the book\n",
      "of Genesis, then constructs a conditional frequency distribution to record which words are most likely\n",
      "to follow a given word; e.g., after the word living, the most likely word is creature; the\n",
      "generate_model() function uses this data, and a seed word, to generate random text.\n",
      "def generate_model(cfdist, word, num=15):\n",
      "    for i in range(num):\n",
      "        print word,\n",
      "        word = cfdist[word].max()\n",
      "2.2  Conditional Frequency Distributions | 55...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 76, 'page_label': '55', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4409}\n",
      "\n",
      "--- Chunk 4410 ---\n",
      "Content:\n",
      "text = nltk.corpus.genesis.words('english-kjv.txt')\n",
      "bigrams = nltk.bigrams(text)\n",
      "cfd = nltk.ConditionalFreqDist(bigrams) \n",
      ">>> print cfd['living']\n",
      "<FreqDist: 'creature': 7, 'thing': 4, 'substance': 2, ',': 1, '.': 1, 'soul': 1>\n",
      ">>> generate_model(cfd, 'living')\n",
      "living creature that he said , and the land of the land of the land\n",
      "Conditional \n",
      "frequency distributions are a useful data structure for many NLP tasks.\n",
      "Their commonly used methods are summarized in Table 2-4.\n",
      "Table 2-4. NLTK’s conditional frequency distributions: Commonly used methods and idioms for\n",
      "defining, accessing, and visualizing a conditional frequency distribution of counters\n",
      "Example Description\n",
      "cfdist = ConditionalFreqDist(pairs) Create a conditional frequency distribution from a list of pairs\n",
      "cfdist.conditions() Alphabetically sorted list of conditions\n",
      "cfdist[condition] The frequency distribution for this condition\n",
      "cfdist[condition][sample] Frequency for the given sample for this condition\n",
      "cfdist.tabulate() Tabulate the conditional frequency distribution\n",
      "cfdist.tabulate(samples, conditions) Tabulation limited to the specified samples and conditions\n",
      "cfdist.plot() Graphical plot of the conditional frequency distribution\n",
      "cfdist.plot(samples, conditions) Graphical plot limited to the specified samples and conditions\n",
      "cfdist1 < cfdist2 Test if samples in cfdist1 occur less frequently than in cfdist2\n",
      "2.3  More Python: Reusing Code\n",
      "By \n",
      "this time you’ve probably typed and retyped a lot of code in the Python interactive\n",
      "interpreter. If you mess up when retyping a complex example, you have to enter it again.\n",
      "Using the arrow keys to access and modify previous commands is helpful but only goes\n",
      "so far. In this section, we see two important ways to reuse code: text editors and Python\n",
      "functions.\n",
      "Creating Programs with a Text Editor\n",
      "The Python interactive interpreter performs your instructions as soon as you type them.\n",
      "Often, it is better to compose a multiline program using a text editor, then ask Python\n",
      "to run the whole program at once. Using IDLE, you can do this by going to the File\n",
      "menu and opening a new window. Try this now, and enter the following one-line\n",
      "program:\n",
      "print 'Monty Python'\n",
      "56 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 77, 'page_label': '56', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4410}\n",
      "\n",
      "--- Chunk 4411 ---\n",
      "Content:\n",
      "Save this program in a file called monty.py, then go to the Run menu and select the\n",
      "command Run Module. (We’ll learn what modules are shortly.) The result in the main\n",
      "IDLE window should look like this:\n",
      ">>> ================================ RESTART ================================\n",
      ">>>\n",
      "Monty Python\n",
      ">>>\n",
      "You can also type from monty import * and it will do the same thing.\n",
      "From now on, you have a choice of using the interactive interpreter or a text editor to\n",
      "create your programs. It is often convenient to test your ideas using the interpreter,\n",
      "revising a line of code until it does what you expect. Once you’re ready, you can paste\n",
      "the code (minus any >>> or ... prompts) into the text editor, continue to expand it,\n",
      "and finally save the program in a file so that you don’t have to type it in again later.\n",
      "Give the file a short but descriptive name, using all lowercase letters and separating\n",
      "words with underscore, and using the .py filename extension, e.g., monty_python.py.\n",
      "Important: Our inline code examples include the >>> and ... prompts\n",
      "as if we are interacting directly with the interpreter. As they get more\n",
      "complicated, you should instead type them into the editor, without the\n",
      "prompts, and run them from the editor as shown earlier. When we pro-\n",
      "vide longer programs in this book, we will leave out the prompts to\n",
      "remind you to type them into a file rather than using the interpreter.\n",
      "You can see this already in Example 2-1. Note that the example still\n",
      "includes a couple of lines with the Python prompt; this is the interactive\n",
      "part of the task where you inspect some data and invoke a function.\n",
      "Remember that all code samples like Example 2-1 are downloadable\n",
      "from http://www.nltk.org/.\n",
      "Functions\n",
      "Suppose that you work on analyzing text that involves different forms of the same word,\n",
      "and that part of your program needs to work out the plural form of a given singular\n",
      "noun. Suppose it needs to do this work in two places, once when it is processing some\n",
      "texts and again when it is processing user input.\n",
      "Rather than repeating the same code several times over, it is more efficient and reliable\n",
      "to localize this work inside a function. A function is just a named block of code that\n",
      "performs some well-defined task, as we saw in Section 1.1. A function is usually defined\n",
      "to take some inputs, using special variables known as parameters, and it may produce\n",
      "a result, also known as a return value. We define a function using the keyword def\n",
      "followed by the function name and any input parameters, followed by the body of the\n",
      "function. Here’s the function we saw in Section 1.1 (including the import statement\n",
      "that makes division behave as expected):\n",
      "2.3  More Python: Reusing Code | 57...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 78, 'page_label': '57', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4411}\n",
      "\n",
      "--- Chunk 4412 ---\n",
      "Content:\n",
      ">>> from __future__ import division\n",
      ">>> def lexical_diversity(text):\n",
      "...     return len(text) / len(set(text))\n",
      "We \n",
      "use the keyword return to indicate the value that is produced as output by the\n",
      "function. In this example, all the work of the function is done in the return statement.\n",
      "Here’s an equivalent definition that does the same work using multiple lines of code.\n",
      "We’ll change the parameter name from text to my_text_data to remind you that this is\n",
      "an arbitrary choice:\n",
      ">>> def lexical_diversity(my_text_data):\n",
      "...     word_count = len(my_text_data)\n",
      "...     vocab_size = len(set(my_text_data))\n",
      "...     diversity_score = word_count / vocab_size\n",
      "...     return diversity_score\n",
      "Notice that we’ve created some new variables inside the body of the function. These\n",
      "are local variables and are not accessible outside the function. So now we have defined\n",
      "a function with the name lexical_diversity. But just defining it won’t produce any\n",
      "output! Functions do nothing until they are “called” (or “invoked”).\n",
      "Let’s return to our earlier scenario, and actually define a simple function to work out\n",
      "English plurals. The function plural() in Example 2-2 takes a singular noun and gen-\n",
      "erates a plural form, though it is not always correct. (We’ll discuss functions at greater\n",
      "length in Section 4.4.)\n",
      "Example 2-2. A Python function: This function tries to work out the plural form of any English noun;\n",
      "the keyword def (define) is followed by the function name, then a parameter inside parentheses, and\n",
      "a colon; the body of the function is the indented block of code; it tries to recognize patterns within the\n",
      "word and process the word accordingly; e.g., if the word ends with y, delete the y and add ies.\n",
      "def plural(word):\n",
      "    if word.endswith('y'):\n",
      "        return word[:-1] + 'ies'\n",
      "    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n",
      "        return word + 'es'\n",
      "    elif word.endswith('an'):\n",
      "        return word[:-2] + 'en'\n",
      "    else:\n",
      "        return word + 's'\n",
      ">>> plural('fairy')\n",
      "'fairies'\n",
      ">>> plural('woman')\n",
      "'women'\n",
      "The endswith() function is always associated with a string object (e.g., word in Exam-\n",
      "ple 2-2). To call such functions, we give the name of the object, a period, and then the\n",
      "name of the function. These functions are usually known as methods.\n",
      "58 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 79, 'page_label': '58', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4412}\n",
      "\n",
      "--- Chunk 4413 ---\n",
      "Content:\n",
      "Modules\n",
      "Over \n",
      "time you will find that you create a variety of useful little text-processing functions,\n",
      "and you end up copying them from old programs to new ones. Which file contains the\n",
      "latest version of the function you want to use? It makes life a lot easier if you can collect\n",
      "your work into a single place, and access previously defined functions without making\n",
      "copies.\n",
      "To do this, save your function(s) in a file called (say) textproc.py. Now, you can access\n",
      "your work simply by importing it from the file:\n",
      ">>> from textproc import plural\n",
      ">>> plural('wish')\n",
      "wishes\n",
      ">>> plural('fan')\n",
      "fen\n",
      "Our plural function obviously has an error, since the plural of fan is fans. Instead of\n",
      "typing in a new version of the function, we can simply edit the existing one. Thus, at\n",
      "every stage, there is only one version of our plural function, and no confusion about\n",
      "which one is being used.\n",
      "A collection of variable and function definitions in a file is called a Python module. A\n",
      "collection of related modules is called a package. NLTK’s code for processing the\n",
      "Brown Corpus is an example of a module, and its collection of code for processing all\n",
      "the different corpora is an example of a package. NLTK itself is a set of packages,\n",
      "sometimes called a library.\n",
      "Caution!\n",
      "If \n",
      "you are creating a file to contain some of your Python code, do not\n",
      "name your file nltk.py: it may get imported in place of the “real” NLTK\n",
      "package. When it imports modules, Python first looks in the current\n",
      "directory (folder).\n",
      "2.4  Lexical Resources\n",
      "A lexicon, or lexical resource, is a collection of words and/or phrases along with asso-\n",
      "ciated information, such as part-of-speech and sense definitions. Lexical resources are\n",
      "secondary to texts, and are usually created and enriched with the help of texts. For\n",
      "example, if we have defined a text my_text, then vocab = sorted(set(my_text)) builds\n",
      "the vocabulary of my_text, whereas word_freq = FreqDist(my_text) counts the fre-\n",
      "quency of each word in the text. Both vocab and word_freq are simple lexical resources.\n",
      "Similarly, a concordance like the one we saw in Section 1.1 gives us information about\n",
      "word usage that might help in the preparation of a dictionary. Standard terminology\n",
      "for lexicons is illustrated in Figure 2-5. A lexical entry consists of a headword (also\n",
      "known as a lemma) along with additional information, such as the part-of-speech and\n",
      "2.4  Lexical Resources | 59...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 80, 'page_label': '59', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4413}\n",
      "\n",
      "--- Chunk 4414 ---\n",
      "Content:\n",
      "the sense definition. Two distinct words having the same spelling are called\n",
      "homonyms.\n",
      "The \n",
      "simplest kind of lexicon is nothing more than a sorted list of words. Sophisticated\n",
      "lexicons include complex structure within and across the individual entries. In this\n",
      "section, we’ll look at some lexical resources included with NLTK.\n",
      "Wordlist Corpora\n",
      "NLTK includes some corpora that are nothing more than wordlists. The Words Corpus\n",
      "is the /usr/dict/words file from Unix, used by some spellcheckers. We can use it to find\n",
      "unusual or misspelled words in a text corpus, as shown in Example 2-3.\n",
      "Example 2-3. Filtering a text: This program computes the vocabulary of a text, then removes all items\n",
      "that occur in an existing wordlist, leaving just the uncommon or misspelled words.\n",
      "def unusual_words(text):\n",
      "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
      "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
      "    unusual = text_vocab.difference(english_vocab)\n",
      "    return sorted(unusual)\n",
      ">>> unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))\n",
      "['abbeyland', 'abhorrence', 'abominably', 'abridgement', 'accordant', 'accustomary',\n",
      "'adieus', 'affability', 'affectedly', 'aggrandizement', 'alighted', 'allenham',\n",
      "'amiably', 'annamaria', 'annuities', 'apologising', 'arbour', 'archness', ...]\n",
      ">>> unusual_words(nltk.corpus.nps_chat.words())\n",
      "['aaaaaaaaaaaaaaaaa', 'aaahhhh', 'abou', 'abourted', 'abs', 'ack', 'acros',\n",
      "'actualy', 'adduser', 'addy', 'adoted', 'adreniline', 'ae', 'afe', 'affari', 'afk',\n",
      "'agaibn', 'agurlwithbigguns', 'ahah', 'ahahah', 'ahahh', 'ahahha', 'ahem', 'ahh', ...]\n",
      "There is also a corpus of stopwords, that is, high-frequency words such as the, to, and\n",
      "also that we sometimes want to filter out of a document before further processing.\n",
      "Stopwords usually have little lexical content, and their presence in a text fails to dis-\n",
      "tinguish it from other texts.\n",
      ">>> from nltk.corpus import stopwords\n",
      ">>> stopwords.words('english')\n",
      "['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across',\n",
      "Figure 2-5. Lexicon terminology: Lexical entries for two lemmas having the same spelling\n",
      "(homonyms), providing part-of-speech and gloss information.\n",
      "60 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 81, 'page_label': '60', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4414}\n",
      "\n",
      "--- Chunk 4415 ---\n",
      "Content:\n",
      "'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow',\n",
      "'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', ...]\n",
      "Let’s \n",
      "define a function to compute what fraction of words in a text are not in the stop-\n",
      "words list:\n",
      ">>> def content_fraction(text):\n",
      "...     stopwords = nltk.corpus.stopwords.words('english')\n",
      "...     content = [w for w in text if w.lower() not in stopwords]\n",
      "...     return len(content) / len(text)\n",
      "...\n",
      ">>> content_fraction(nltk.corpus.reuters.words())\n",
      "0.65997695393285261\n",
      "Thus, with the help of stopwords, we filter out a third of the words of the text. Notice\n",
      "that we’ve combined two different kinds of corpus here, using a lexical resource to filter\n",
      "the content of a text corpus.\n",
      "Figure 2-6. A word puzzle: A grid of randomly chosen letters with rules for creating words out of the\n",
      "letters; this puzzle is known as “Target.”\n",
      "A \n",
      "wordlist is useful for solving word puzzles, such as the one in Figure 2-6. Our program\n",
      "iterates through every word and, for each one, checks whether it meets the conditions.\n",
      "It is easy to check obligatory letter \n",
      "  and length \n",
      "  constraints (and we’ll only look for\n",
      "words \n",
      "with six or more letters here). It is trickier to check that candidate solutions only\n",
      "use combinations of the supplied letters, especially since some of the supplied letters\n",
      "appear twice (here, the letter v). The FreqDist comparison method \n",
      " permits us to\n",
      "check \n",
      "that the frequency of each letter in the candidate word is less than or equal to the\n",
      "frequency of the corresponding letter in the puzzle.\n",
      ">>> puzzle_letters = nltk.FreqDist('egivrvonl')\n",
      ">>> obligatory = 'r'\n",
      ">>> wordlist = nltk.corpus.words.words()\n",
      ">>> [w for w in wordlist if len(w) >= 6 \n",
      "...                      and obligatory in w \n",
      "...                      and nltk.FreqDist(w) <= puzzle_letters] \n",
      "['glover', 'gorlin', 'govern', 'grovel', 'ignore', 'involver', 'lienor',\n",
      "'linger', 'longer', 'lovering', 'noiler', 'overling', 'region', 'renvoi',\n",
      "'revolving', 'ringle', 'roving', 'violer', 'virole']\n",
      "One \n",
      "more wordlist corpus is the Names Corpus, containing 8,000 first names catego-\n",
      "rized by gender. The male and female names are stored in separate files. Let’s find names\n",
      "that appear in both files, i.e., names that are ambiguous for gender:\n",
      "2.4  Lexical Resources | 61...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 82, 'page_label': '61', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4415}\n",
      "\n",
      "--- Chunk 4416 ---\n",
      "Content:\n",
      ">>> names = nltk.corpus.names\n",
      ">>> names.fileids()\n",
      "['female.txt', 'male.txt']\n",
      ">>> male_names = names.words('male.txt')\n",
      ">>> female_names = names.words('female.txt')\n",
      ">>> [w for w in male_names if w in female_names]\n",
      "['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',\n",
      "'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',\n",
      "'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ...]\n",
      "It \n",
      "is well known that names ending in the letter a are almost always female. We can see\n",
      "this and some other patterns in the graph in Figure 2-7, produced by the following code.\n",
      "Remember that name[-1] is the last letter of name.\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (fileid, name[-1])\n",
      "...           for fileid in names.fileids()\n",
      "...           for name in names.words(fileid))\n",
      ">>> cfd.plot()\n",
      "Figure 2-7. Conditional frequency distribution: This plot shows the number of female and male names\n",
      "ending \n",
      "with each letter of the alphabet; most names ending with a, e, or i are female; names ending\n",
      "in h and l are equally likely to be male or female; names ending in k, o, r, s, and t are likely to be male.\n",
      "62 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 83, 'page_label': '62', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4416}\n",
      "\n",
      "--- Chunk 4417 ---\n",
      "Content:\n",
      "A Pronouncing Dictionary\n",
      "A slightly richer kind of lexical resource is a table (or spreadsheet), containing a word\n",
      "plus \n",
      "some properties in each row. NLTK includes the CMU Pronouncing Dictionary\n",
      "for U.S. English, which was designed for use by speech synthesizers.\n",
      ">>> entries = nltk.corpus.cmudict.entries()\n",
      ">>> len(entries)\n",
      "127012\n",
      ">>> for entry in entries[39943:39951]:\n",
      "...     print entry\n",
      "...\n",
      "('fir', ['F', 'ER1'])\n",
      "('fire', ['F', 'AY1', 'ER0'])\n",
      "('fire', ['F', 'AY1', 'R'])\n",
      "('firearm', ['F', 'AY1', 'ER0', 'AA2', 'R', 'M'])\n",
      "('firearm', ['F', 'AY1', 'R', 'AA2', 'R', 'M'])\n",
      "('firearms', ['F', 'AY1', 'ER0', 'AA2', 'R', 'M', 'Z'])\n",
      "('firearms', ['F', 'AY1', 'R', 'AA2', 'R', 'M', 'Z'])\n",
      "('fireball', ['F', 'AY1', 'ER0', 'B', 'AO2', 'L'])\n",
      "For each word, this lexicon provides a list of phonetic codes—distinct labels for each\n",
      "contrastive sound—known as phones. Observe that fire has two pronunciations (in\n",
      "U.S. English): the one-syllable F AY1 R, and the two-syllable F AY1 ER0. The symbols\n",
      "in the CMU Pronouncing Dictionary are from the Arpabet, described in more detail at\n",
      "http://en.wikipedia.org/wiki/Arpabet.\n",
      "Each entry consists of two parts, and we can process these individually using a more\n",
      "complex version of the for statement. Instead of writing for entry in entries:, we\n",
      "replace entry with two variable names, word, pron \n",
      ". Now, each time through the loop,\n",
      "word \n",
      "is assigned the first part of the entry, and pron is assigned the second part of the\n",
      "entry:\n",
      ">>> for word, pron in entries: \n",
      "...     if len(pron) == 3: \n",
      "...         ph1, ph2, ph3 = pron \n",
      "...         if ph1 == 'P' and ph3 == 'T':\n",
      "...             print word, ph2,\n",
      "...\n",
      "pait EY1 pat AE1 pate EY1 patt AE1 peart ER1 peat IY1 peet IY1 peete IY1 pert ER1\n",
      "pet EH1 pete IY1 pett EH1 piet IY1 piette IY1 pit IH1 pitt IH1 pot AA1 pote OW1\n",
      "pott AA1 pout AW1 puett UW1 purt ER1 put UH1 putt AH1\n",
      "The \n",
      "program just shown scans the lexicon looking for entries whose pronunciation\n",
      "consists of three phones \n",
      " . If the condition is true, it assigns the contents of pron to\n",
      "three new variables: ph1, ph2, and ph3. Notice the unusual form of the statement that\n",
      "does that work \n",
      " .\n",
      "Here’s \n",
      "another example of the same for statement, this time used inside a list compre-\n",
      "hension...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 84, 'page_label': '63', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4417}\n",
      "\n",
      "--- Chunk 4418 ---\n",
      "Content:\n",
      ". If the condition is true, it assigns the contents of pron to\n",
      "three new variables: ph1, ph2, and ph3. Notice the unusual form of the statement that\n",
      "does that work \n",
      " .\n",
      "Here’s \n",
      "another example of the same for statement, this time used inside a list compre-\n",
      "hension. This program finds all words whose pronunciation ends with a syllable\n",
      "sounding like nicks. You could use this method to find rhyming words.\n",
      "2.4  Lexical Resources | 63...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 84, 'page_label': '63', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4418}\n",
      "\n",
      "--- Chunk 4419 ---\n",
      "Content:\n",
      ">>> syllable = ['N', 'IH0', 'K', 'S']\n",
      ">>> [word for word, pron in entries if pron[-4:] == syllable]\n",
      "[\"atlantic's\", 'audiotronics', 'avionics', 'beatniks', 'calisthenics', 'centronics',\n",
      "'chetniks', \"clinic's\", 'clinics', 'conics', 'cynics', 'diasonics', \"dominic's\",\n",
      "'ebonics', 'electronics', \"electronics'\", 'endotronics', \"endotronics'\", 'enix', ...]\n",
      "Notice \n",
      "that the one pronunciation is spelled in several ways: nics, niks, nix, and even\n",
      "ntic’s with a silent t, for the word atlantic’s. Let’s look for some other mismatches\n",
      "between pronunciation and writing. Can you summarize the purpose of the following\n",
      "examples and explain how they work?\n",
      ">>> [w for w, pron in entries if pron[-1] == 'M' and w[-1] == 'n']\n",
      "['autumn', 'column', 'condemn', 'damn', 'goddamn', 'hymn', 'solemn']\n",
      ">>> sorted(set(w[:2] for w, pron in entries if pron[0] == 'N' and w[0] != 'n'))\n",
      "['gn', 'kn', 'mn', 'pn']\n",
      "The phones contain digits to represent primary stress (1), secondary stress (2), and no\n",
      "stress (0). As our final example, we define a function to extract the stress digits and then\n",
      "scan our lexicon to find words having a particular stress pattern.\n",
      ">>> def stress(pron):\n",
      "...     return [char for phone in pron for char in phone if char.isdigit()]\n",
      ">>> [w for w, pron in entries if stress(pron) == ['0', '1', '0', '2', '0']]\n",
      "['abbreviated', 'abbreviating', 'accelerated', 'accelerating', 'accelerator',\n",
      "'accentuated', 'accentuating', 'accommodated', 'accommodating', 'accommodative',\n",
      "'accumulated', 'accumulating', 'accumulative', 'accumulator', 'accumulators', ...]\n",
      ">>> [w for w, pron in entries if stress(pron) == ['0', '2', '0', '1', '0']]\n",
      "['abbreviation', 'abbreviations', 'abomination', 'abortifacient', 'abortifacients',\n",
      "'academicians', 'accommodation', 'accommodations', 'accreditation', 'accreditations',\n",
      "'accumulation', 'accumulations', 'acetylcholine', 'acetylcholine', 'adjudication', ...]\n",
      "A subtlety of this program is that our user-defined function stress() is\n",
      "invoked \n",
      "inside the condition of a list comprehension. There is also a\n",
      "doubly nested for loop. There’s a lot going on here, and you might want\n",
      "to return to this once you’ve had more experience using list compre-\n",
      "hensions.\n",
      "We can use a conditional frequency distribution to help us find minimally contrasting\n",
      "sets of words. Here we find all the p words consisting of three sounds \n",
      " , and group\n",
      "them according to their first and last sounds \n",
      " .\n",
      ">>> p3 = [(pron[0]+'-'+pron[2], word) \n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 85, 'page_label': '64', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4419}\n",
      "\n",
      "--- Chunk 4420 ---\n",
      "Content:\n",
      ". There is also a\n",
      "doubly nested for loop. There’s a lot going on here, and you might want\n",
      "to return to this once you’ve had more experience using list compre-\n",
      "hensions.\n",
      "We can use a conditional frequency distribution to help us find minimally contrasting\n",
      "sets of words. Here we find all the p words consisting of three sounds \n",
      " , and group\n",
      "them according to their first and last sounds \n",
      " .\n",
      ">>> p3 = [(pron[0]+'-'+pron[2], word) \n",
      "...       for (word, pron) in entries\n",
      "...       if pron[0] == 'P' and len(pron) == 3] \n",
      ">>> cfd = nltk.ConditionalFreqDist(p3)\n",
      ">>> for template in cfd.conditions():\n",
      "...     if len(cfd[template]) > 10:\n",
      "...         words = cfd[template].keys()\n",
      "...         wordlist = ' '.join(words)\n",
      "...         print template, wordlist[:70] + \"...\"\n",
      "...\n",
      "P-CH perch puche poche peach petsche poach pietsch putsch pautsch piche pet...\n",
      "64 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 85, 'page_label': '64', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4420}\n",
      "\n",
      "--- Chunk 4421 ---\n",
      "Content:\n",
      "P-K pik peek pic pique paque polk perc poke perk pac pock poch purk pak pa...\n",
      "P-L pil poehl pille pehl pol pall pohl pahl paul perl pale paille perle po...\n",
      "P-N paine payne pon pain pin pawn pinn pun pine paign pen pyne pane penn p...\n",
      "P-P pap paap pipp paup pape pup pep poop pop pipe paape popp pip peep pope...\n",
      "P-R paar poor par poore pear pare pour peer pore parr por pair porr pier...\n",
      "P-S pearse piece posts pasts peace perce pos pers pace puss pesce pass pur...\n",
      "P-T pot puett pit pete putt pat purt pet peart pott pett pait pert pote pa...\n",
      "P-Z pays p.s pao's pais paws p.'s pas pez paz pei's pose poise peas paiz p...\n",
      "Rather \n",
      "than iterating over the whole dictionary, we can also access it by looking up\n",
      "particular words. We will use Python’s dictionary data structure, which we will study\n",
      "systematically in Section 5.3. We look up a dictionary by specifying its name, followed\n",
      "by a key (such as the word 'fire') inside square brackets \n",
      " .\n",
      ">>> prondict = nltk.corpus.cmudict.dict()\n",
      ">>> prondict['fire'] \n",
      "[['F', 'AY1', 'ER0'], ['F', 'AY1', 'R']]\n",
      ">>> prondict['blog'] \n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "KeyError: 'blog'\n",
      ">>> prondict['blog'] = [['B', 'L', 'AA1', 'G']] \n",
      ">>> prondict['blog']\n",
      "[['B', 'L', 'AA1', 'G']]\n",
      "If \n",
      "we try to look up a non-existent key \n",
      " , we get a KeyError. This is similar to what\n",
      "happens when we index a list with an integer that is too large, producing an IndexEr\n",
      "ror. The word blog is missing from the pronouncing dictionary, so we tweak our version\n",
      "by assigning a value for this key \n",
      "  (this has no effect on the NLTK corpus; next time\n",
      "we access it, blog will still be absent).\n",
      "We \n",
      "can use any lexical resource to process a text, e.g., to filter out words having some\n",
      "lexical property (like nouns), or mapping every word of the text. For example, the\n",
      "following text-to-speech function looks up each word of the text in the pronunciation\n",
      "dictionary:\n",
      ">>> text = ['natural', 'language', 'processing']\n",
      ">>> [ph for w in text for ph in prondict[w][0]]\n",
      "['N', 'AE1', 'CH', 'ER0', 'AH0', 'L', 'L', 'AE1', 'NG', 'G', 'W', 'AH0', 'JH',\n",
      "'P', 'R', 'AA1', 'S', 'EH0', 'S', 'IH0', 'NG']\n",
      "Comparative Wordlists\n",
      "Another example of a tabular lexicon is the comparative wordlist...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 86, 'page_label': '65', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4421}\n",
      "\n",
      "--- Chunk 4422 ---\n",
      "Content:\n",
      ". NLTK includes\n",
      "so-called Swadesh wordlists, lists of about 200 common words in several languages.\n",
      "The languages are identified using an ISO 639 two-letter code.\n",
      ">>> from nltk.corpus import swadesh\n",
      ">>> swadesh.fileids()\n",
      "['be', 'bg', 'bs', 'ca', 'cs', 'cu', 'de', 'en', 'es', 'fr', 'hr', 'it', 'la', 'mk',\n",
      "'nl', 'pl', 'pt', 'ro', 'ru', 'sk', 'sl', 'sr', 'sw', 'uk']\n",
      ">>> swadesh.words('en')\n",
      "['I', 'you (singular), thou', 'he', 'we', 'you (plural)', 'they', 'this', 'that',\n",
      "2.4  Lexical Resources | 65...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 86, 'page_label': '65', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4422}\n",
      "\n",
      "--- Chunk 4423 ---\n",
      "Content:\n",
      "'here', 'there', 'who', 'what', 'where', 'when', 'how', 'not', 'all', 'many', 'some',\n",
      "'few', 'other', 'one', 'two', 'three', 'four', 'five', 'big', 'long', 'wide', ...]\n",
      "We \n",
      "can access cognate words from multiple languages using the entries() method,\n",
      "specifying a list of languages. With one further step we can convert this into a simple\n",
      "dictionary (we’ll learn about dict() in Section 5.3).\n",
      ">>> fr2en = swadesh.entries(['fr', 'en'])\n",
      ">>> fr2en\n",
      "[('je', 'I'), ('tu, vous', 'you (singular), thou'), ('il', 'he'), ...]\n",
      ">>> translate = dict(fr2en)\n",
      ">>> translate['chien']\n",
      "'dog'\n",
      ">>> translate['jeter']\n",
      "'throw'\n",
      "We can make our simple translator more useful by adding other source languages. Let’s\n",
      "get the German-English and Spanish-English pairs, convert each to a dictionary using\n",
      "dict(), then update our original translate dictionary with these additional mappings:\n",
      ">>> de2en = swadesh.entries(['de', 'en'])    # German-English\n",
      ">>> es2en = swadesh.entries(['es', 'en'])    # Spanish-English\n",
      ">>> translate.update(dict(de2en))\n",
      ">>> translate.update(dict(es2en))\n",
      ">>> translate['Hund']\n",
      "'dog'\n",
      ">>> translate['perro']\n",
      "'dog'\n",
      "We can compare words in various Germanic and Romance languages:\n",
      ">>> languages = ['en', 'de', 'nl', 'es', 'fr', 'pt', 'la']\n",
      ">>> for i in [139, 140, 141, 142]:\n",
      "...     print swadesh.entries(languages)[i]\n",
      "...\n",
      "('say', 'sagen', 'zeggen', 'decir', 'dire', 'dizer', 'dicere')\n",
      "('sing', 'singen', 'zingen', 'cantar', 'chanter', 'cantar', 'canere')\n",
      "('play', 'spielen', 'spelen', 'jugar', 'jouer', 'jogar, brincar', 'ludere')\n",
      "('float', 'schweben', 'zweven', 'flotar', 'flotter', 'flutuar, boiar', 'fluctuare')\n",
      "Shoebox and Toolbox Lexicons\n",
      "Perhaps the single most popular tool used by linguists for managing data is Toolbox,\n",
      "previously known as Shoebox since it replaces the field linguist’s traditional shoebox\n",
      "full of file cards. Toolbox is freely downloadable from http://www.sil.org/computing/\n",
      "toolbox/.\n",
      "A Toolbox file consists of a collection of entries, where each entry is made up of one\n",
      "or more fields. Most fields are optional or repeatable, which means that this kind of\n",
      "lexical resource cannot be treated as a table or spreadsheet.\n",
      "Here is a dictionary for the Rotokas language. We see just the first entry, for the word\n",
      "kaa, meaning “to gag”:\n",
      "66 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 87, 'page_label': '66', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4423}\n",
      "\n",
      "--- Chunk 4424 ---\n",
      "Content:\n",
      ">>> from nltk.corpus import toolbox\n",
      ">>> toolbox.entries('rotokas.dic')\n",
      "[('kaa', [('ps', 'V'), ('pt', 'A'), ('ge', 'gag'), ('tkp', 'nek i pas'),\n",
      "('dcsv', 'true'), ('vx', '1'), ('sc', '???'), ('dt', '29/Oct/2005'),\n",
      "('ex', 'Apoka ira kaaroi aioa-ia reoreopaoro.'),\n",
      "('xp', 'Kaikai i pas long nek bilong Apoka bikos em i kaikai na toktok.'),\n",
      "('xe', 'Apoka is gagging from food while talking.')]), ...]\n",
      "Entries consist of a series of attribute-value pairs, such as ('ps', 'V') to indicate that\n",
      "the \n",
      "part-of-speech is 'V' (verb), and ('ge', 'gag') to indicate that the gloss-into-\n",
      "English is 'gag'. The last three pairs contain an example sentence in Rotokas and its\n",
      "translations into Tok Pisin and English.\n",
      "The loose structure of Toolbox files makes it hard for us to do much more with them\n",
      "at this stage. XML provides a powerful way to process this kind of corpus, and we will\n",
      "return to this topic in Chapter 11.\n",
      "The Rotokas language is spoken on the island of Bougainville, Papua\n",
      "New \n",
      "Guinea. This lexicon was contributed to NLTK by Stuart Robin-\n",
      "son. Rotokas is notable for having an inventory of just 12 phonemes\n",
      "(contrastive sounds); see http://en.wikipedia.org/wiki/Rotokas_language\n",
      "2.5  WordNet\n",
      "WordNet is a semantically oriented dictionary of English, similar to a traditional the-\n",
      "saurus but with a richer structure. NLTK includes the English WordNet, with 155,287\n",
      "words and 117,659 synonym sets. We’ll begin by looking at synonyms and how they\n",
      "are accessed in WordNet.\n",
      "Senses and Synonyms\n",
      "Consider the sentence in (1a). If we replace the word motorcar in (1a) with automo-\n",
      "bile, to get (1b), the meaning of the sentence stays pretty much the same:\n",
      "(1) a. Benz is credited with the invention of the motorcar.\n",
      "b. Benz is credited with the invention of the automobile.\n",
      "Since everything else in the sentence has remained unchanged, we can conclude that\n",
      "the words motorcar and automobile have the same meaning, i.e., they are synonyms.\n",
      "We can explore these words with the help of WordNet:\n",
      ">>> from nltk.corpus import wordnet as wn\n",
      ">>> wn.synsets('motorcar')\n",
      "[Synset('car.n.01')]\n",
      "Thus, motorcar has just one possible meaning and it is identified as car.n.01, the first\n",
      "noun sense of car. The entity car.n.01 is called a synset, or “synonym set,” a collection\n",
      "of synonymous words (or “lemmas”):\n",
      "2.5  WordNet | 67...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 88, 'page_label': '67', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4424}\n",
      "\n",
      "--- Chunk 4425 ---\n",
      "Content:\n",
      ">>> wn.synset('car.n.01').lemma_names\n",
      "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
      "Each \n",
      "word of a synset can have several meanings, e.g., car can also signify a train car-\n",
      "riage, a gondola, or an elevator car. However, we are only interested in the single\n",
      "meaning that is common to all words of this synset. Synsets also come with a prose\n",
      "definition and some example sentences:\n",
      ">>> wn.synset('car.n.01').definition\n",
      "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'\n",
      ">>> wn.synset('car.n.01').examples\n",
      "['he needs a car to get to work']\n",
      "Although definitions help humans to understand the intended meaning of a synset, the\n",
      "words of the synset are often more useful for our programs. To eliminate ambiguity,\n",
      "we will identify these words as car.n.01.automobile, car.n.01.motorcar, and so on.\n",
      "This pairing of a synset with a word is called a lemma. We can get all the lemmas for\n",
      "a given synset \n",
      ", look up a particular lemma \n",
      " , get the synset corresponding to a lemma\n",
      ", and get the “name” of a lemma \n",
      " :\n",
      ">>> wn.synset('car.n.01').lemmas \n",
      "[Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'),\n",
      "Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]\n",
      ">>> wn.lemma('car.n.01.automobile') \n",
      "Lemma('car.n.01.automobile')\n",
      ">>> wn.lemma('car.n.01.automobile').synset \n",
      "Synset('car.n.01')\n",
      ">>> wn.lemma('car.n.01.automobile').name \n",
      "'automobile'\n",
      "Unlike \n",
      "the words automobile and motorcar, which are unambiguous and have one syn-\n",
      "set, the word car is ambiguous, having five synsets:\n",
      ">>> wn.synsets('car')\n",
      "[Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'),\n",
      "Synset('cable_car.n.01')]\n",
      ">>> for synset in wn.synsets('car'):\n",
      "...     print synset.lemma_names\n",
      "...\n",
      "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
      "['car', 'railcar', 'railway_car', 'railroad_car']\n",
      "['car', 'gondola']\n",
      "['car', 'elevator_car']\n",
      "['cable_car', 'car']\n",
      "For convenience, we can access all the lemmas involving the word car as follows:\n",
      ">>> wn.lemmas('car')\n",
      "[Lemma('car.n.01.car'), Lemma('car.n.02.car'), Lemma('car.n.03.car'),\n",
      "Lemma('car.n.04.car'), Lemma('cable_car.n.01.car')]\n",
      "68 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 89, 'page_label': '68', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4425}\n",
      "\n",
      "--- Chunk 4426 ---\n",
      "Content:\n",
      "Your Turn: Write down all the senses of the word dish that you can\n",
      "think of. Now, explore this word with the help of WordNet, using the\n",
      "same operations shown earlier.\n",
      "The WordNet Hierarchy\n",
      "WordNet synsets correspond to abstract concepts, and they don’t always have corre-\n",
      "sponding words in English. These concepts are linked together in a hierarchy. Some\n",
      "concepts are very general, such as Entity, State, Event; these are called unique begin-\n",
      "ners or root synsets. Others, such as gas guzzler and hatchback, are much more specific.\n",
      "A small portion of a concept hierarchy is illustrated in Figure 2-8.\n",
      "Figure 2-8. Fragment of WordNet concept hierarchy: Nodes correspond to synsets; edges indicate the\n",
      "hypernym/hyponym relation, i.e., the relation between superordinate and subordinate concepts.\n",
      "WordNet \n",
      "makes it easy to navigate between concepts. For example, given a concept\n",
      "like motorcar, we can look at the concepts that are more specific—the (immediate)\n",
      "hyponyms.\n",
      ">>> motorcar = wn.synset('car.n.01')\n",
      ">>> types_of_motorcar = motorcar.hyponyms()\n",
      ">>> types_of_motorcar[26]\n",
      "Synset('ambulance.n.01')\n",
      ">>> sorted([lemma.name for synset in types_of_motorcar for lemma in synset.lemmas])\n",
      "['Model_T', 'S.U.V.', 'SUV', 'Stanley_Steamer', 'ambulance', 'beach_waggon',\n",
      "'beach_wagon', 'bus', 'cab', 'compact', 'compact_car', 'convertible',\n",
      "'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car',\n",
      "'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'hatchback', 'heap',\n",
      "'horseless_carriage', 'hot-rod', 'hot_rod', 'jalopy', 'jeep', 'landrover',\n",
      "'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car',\n",
      "2.5  WordNet | 69...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 90, 'page_label': '69', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4426}\n",
      "\n",
      "--- Chunk 4427 ---\n",
      "Content:\n",
      "'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer',\n",
      "'racing_car', 'roadster', 'runabout', 'saloon', 'secondhand_car', 'sedan',\n",
      "'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car',\n",
      "'station_waggon', 'station_wagon', 'stock_car', 'subcompact', 'subcompact_car',\n",
      "'taxi', 'taxicab', 'tourer', 'touring_car', 'two-seater', 'used-car', 'waggon',\n",
      "'wagon']\n",
      "We \n",
      "can also navigate up the hierarchy by visiting hypernyms. Some words have multiple\n",
      "paths, because they can be classified in more than one way. There are two paths between\n",
      "car.n.01 and entity.n.01 because wheeled_vehicle.n.01 can be classified as both a\n",
      "vehicle and a container.\n",
      ">>> motorcar.hypernyms()\n",
      "[Synset('motor_vehicle.n.01')]\n",
      ">>> paths = motorcar.hypernym_paths()\n",
      ">>> len(paths)\n",
      "2\n",
      ">>> [synset.name for synset in paths[0]]\n",
      "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01',\n",
      "'instrumentality.n.03', 'container.n.01', 'wheeled_vehicle.n.01',\n",
      "'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01']\n",
      ">>> [synset.name for synset in paths[1]]\n",
      "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01',\n",
      "'instrumentality.n.03', 'conveyance.n.03', 'vehicle.n.01', 'wheeled_vehicle.n.01',\n",
      "'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01']\n",
      "We can get the most general hypernyms (or root hypernyms) of a synset as follows:\n",
      ">>> motorcar.root_hypernyms()\n",
      "[Synset('entity.n.01')]\n",
      "Your Turn: Try out NLTK’s convenient graphical WordNet browser:\n",
      "nltk.app.wordnet(). Explore the WordNet hierarchy by following the\n",
      "hypernym and hyponym links.\n",
      "More Lexical Relations\n",
      "Hypernyms and hyponyms are called lexical relations because they relate one synset\n",
      "to another. These two relations navigate up and down the “is-a” hierarchy. Another\n",
      "important way to navigate the WordNet network is from items to their components\n",
      "(meronyms) or to the things they are contained in (holonyms). For example, the parts\n",
      "of a tree are its trunk, crown, and so on; these are the part_meronyms(). The substance\n",
      "a tree is made of includes heartwood and sapwood, i.e., the substance_meronyms()...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 91, 'page_label': '70', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4427}\n",
      "\n",
      "--- Chunk 4428 ---\n",
      "Content:\n",
      ". Explore the WordNet hierarchy by following the\n",
      "hypernym and hyponym links.\n",
      "More Lexical Relations\n",
      "Hypernyms and hyponyms are called lexical relations because they relate one synset\n",
      "to another. These two relations navigate up and down the “is-a” hierarchy. Another\n",
      "important way to navigate the WordNet network is from items to their components\n",
      "(meronyms) or to the things they are contained in (holonyms). For example, the parts\n",
      "of a tree are its trunk, crown, and so on; these are the part_meronyms(). The substance\n",
      "a tree is made of includes heartwood and sapwood, i.e., the substance_meronyms(). A\n",
      "collection of trees forms a forest, i.e., the member_holonyms():\n",
      ">>> wn.synset('tree.n.01').part_meronyms()\n",
      "[Synset('burl.n.02'), Synset('crown.n.07'), Synset('stump.n.01'),\n",
      "Synset('trunk.n.01'), Synset('limb.n.02')]\n",
      ">>> wn.synset('tree.n.01').substance_meronyms()\n",
      "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]\n",
      "70 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 91, 'page_label': '70', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4428}\n",
      "\n",
      "--- Chunk 4429 ---\n",
      "Content:\n",
      ">>> wn.synset('tree.n.01').member_holonyms()\n",
      "[Synset('forest.n.01')]\n",
      "To \n",
      "see just how intricate things can get, consider the word mint, which has several\n",
      "closely related senses. We can see that mint.n.04 is part of mint.n.02 and the substance\n",
      "from which mint.n.05 is made.\n",
      ">>> for synset in wn.synsets('mint', wn.NOUN):\n",
      "...     print synset.name + ':', synset.definition\n",
      "...\n",
      "batch.n.02: (often followed by `of') a large number or amount or extent\n",
      "mint.n.02: any north temperate plant of the genus Mentha with aromatic leaves and\n",
      "           small mauve flowers\n",
      "mint.n.03: any member of the mint family of plants\n",
      "mint.n.04: the leaves of a mint plant used fresh or candied\n",
      "mint.n.05: a candy that is flavored with a mint oil\n",
      "mint.n.06: a plant where money is coined by authority of the government\n",
      ">>> wn.synset('mint.n.04').part_holonyms()\n",
      "[Synset('mint.n.02')]\n",
      ">>> wn.synset('mint.n.04').substance_holonyms()\n",
      "[Synset('mint.n.05')]\n",
      "There are also relationships between verbs. For example, the act of walking involves\n",
      "the act of stepping, so walking entails stepping. Some verbs have multiple entailments:\n",
      ">>> wn.synset('walk.v.01').entailments()\n",
      "[Synset('step.v.01')]\n",
      ">>> wn.synset('eat.v.01').entailments()\n",
      "[Synset('swallow.v.01'), Synset('chew.v.01')]\n",
      ">>> wn.synset('tease.v.03').entailments()\n",
      "[Synset('arouse.v.07'), Synset('disappoint.v.01')]\n",
      "Some lexical relationships hold between lemmas, e.g., antonymy:\n",
      ">>> wn.lemma('supply.n.02.supply').antonyms()\n",
      "[Lemma('demand.n.02.demand')]\n",
      ">>> wn.lemma('rush.v.01.rush').antonyms()\n",
      "[Lemma('linger.v.04.linger')]\n",
      ">>> wn.lemma('horizontal.a.01.horizontal').antonyms()\n",
      "[Lemma('vertical.a.01.vertical'), Lemma('inclined.a.02.inclined')]\n",
      ">>> wn.lemma('staccato.r.01.staccato').antonyms()\n",
      "[Lemma('legato.r.01.legato')]\n",
      "You can see the lexical relations, and the other methods defined on a synset, using\n",
      "dir(). For example, try dir(wn.synset('harmony.n.02')).\n",
      "Semantic Similarity\n",
      "We have seen that synsets are linked by a complex network of lexical relations. Given\n",
      "a particular synset, we can traverse the WordNet network to find synsets with related\n",
      "meanings. Knowing which words are semantically related is useful for indexing a col-\n",
      "lection of texts, so that a search for a general term such as vehicle will match documents\n",
      "containing specific terms such as limousine.\n",
      "2.5  WordNet | 71...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 92, 'page_label': '71', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4429}\n",
      "\n",
      "--- Chunk 4430 ---\n",
      "Content:\n",
      "Recall that each synset has one or more hypernym paths that link it to a root hypernym\n",
      "such \n",
      "as entity.n.01. Two synsets linked to the same root may have several hypernyms\n",
      "in common (see Figure 2-8). If two synsets share a very specific hypernym—one that\n",
      "is low down in the hypernym hierarchy—they must be closely related.\n",
      ">>> right = wn.synset('right_whale.n.01')\n",
      ">>> orca = wn.synset('orca.n.01')\n",
      ">>> minke = wn.synset('minke_whale.n.01')\n",
      ">>> tortoise = wn.synset('tortoise.n.01')\n",
      ">>> novel = wn.synset('novel.n.01')\n",
      ">>> right.lowest_common_hypernyms(minke)\n",
      "[Synset('baleen_whale.n.01')]\n",
      ">>> right.lowest_common_hypernyms(orca)\n",
      "[Synset('whale.n.02')]\n",
      ">>> right.lowest_common_hypernyms(tortoise)\n",
      "[Synset('vertebrate.n.01')]\n",
      ">>> right.lowest_common_hypernyms(novel)\n",
      "[Synset('entity.n.01')]\n",
      "Of course we know that whale is very specific (and baleen whale even more so), whereas\n",
      "vertebrate is more general and entity is completely general. We can quantify this concept\n",
      "of generality by looking up the depth of each synset:\n",
      ">>> wn.synset('baleen_whale.n.01').min_depth()\n",
      "14\n",
      ">>> wn.synset('whale.n.02').min_depth()\n",
      "13\n",
      ">>> wn.synset('vertebrate.n.01').min_depth()\n",
      "8\n",
      ">>> wn.synset('entity.n.01').min_depth()\n",
      "0\n",
      "Similarity measures have been defined over the collection of WordNet synsets that\n",
      "incorporate this insight. For example, path_similarity assigns a score in the range\n",
      "0–1 based on the shortest path that connects the concepts in the hypernym hierarchy\n",
      "(-1 is returned in those cases where a path cannot be found). Comparing a synset with\n",
      "itself will return 1. Consider the following similarity scores, relating right whale to minke\n",
      "whale, orca, tortoise, and novel. Although the numbers won’t mean much, they decrease\n",
      "as we move away from the semantic space of sea creatures to inanimate objects.\n",
      ">>> right.path_similarity(minke)\n",
      "0.25\n",
      ">>> right.path_similarity(orca)\n",
      "0.16666666666666666\n",
      ">>> right.path_similarity(tortoise)\n",
      "0.076923076923076927\n",
      ">>> right.path_similarity(novel)\n",
      "0.043478260869565216\n",
      "72 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 93, 'page_label': '72', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4430}\n",
      "\n",
      "--- Chunk 4431 ---\n",
      "Content:\n",
      "Several other similarity measures are available; you can type help(wn)\n",
      "for \n",
      "more information. NLTK also includes VerbNet, a hierarchical verb\n",
      "lexicon linked to WordNet. It can be accessed with nltk.corpus.verb\n",
      "net.\n",
      "2.6  Summary\n",
      "• A text corpus is a large, structured collection of texts. NLTK comes with many\n",
      "corpora, e.g., the Brown Corpus, nltk.corpus.brown.\n",
      "• Some text corpora are categorized, e.g., by genre or topic; sometimes the categories\n",
      "of a corpus overlap each other.\n",
      "• A conditional frequency distribution is a collection of frequency distributions, each\n",
      "one for a different condition. They can be used for counting word frequencies,\n",
      "given a context or a genre.\n",
      "• Python programs more than a few lines long should be entered using a text editor,\n",
      "saved to a file with a .py extension, and accessed using an import statement.\n",
      "• Python functions permit you to associate a name with a particular block of code,\n",
      "and reuse that code as often as necessary.\n",
      "• Some functions, known as “methods,” are associated with an object, and we give\n",
      "the object name followed by a period followed by the method name, like this:\n",
      "x.funct(y), e.g., word.isalpha().\n",
      "• To find out about some variable v, type help(v) in the Python interactive interpreter\n",
      "to read the help entry for this kind of object.\n",
      "• WordNet is a semantically oriented dictionary of English, consisting of synonym\n",
      "sets—or synsets—and organized into a network.\n",
      "• Some functions are not available by default, but must be accessed using Python’s\n",
      "import statement.\n",
      "2.7  Further Reading\n",
      "Extra materials for this chapter are posted at http://www.nltk.org/, including links to\n",
      "freely available resources on the Web. The corpus methods are summarized in the\n",
      "Corpus HOWTO, at http://www.nltk.org/howto, and documented extensively in the\n",
      "online API documentation.\n",
      "Significant sources of published corpora are the Linguistic Data Consortium (LDC) and\n",
      "the European Language Resources Agency (ELRA). Hundreds of annotated text and\n",
      "speech corpora are available in dozens of languages. Non-commercial licenses permit\n",
      "the data to be used in teaching and research. For some corpora, commercial licenses\n",
      "are also available (but for a higher fee).\n",
      "2.7  Further Reading | 73...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 94, 'page_label': '73', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4431}\n",
      "\n",
      "--- Chunk 4432 ---\n",
      "Content:\n",
      "These and many other language resources have been documented using OLAC Meta-\n",
      "data, \n",
      "and can be searched via the OLAC home page at http://www.language-archives\n",
      ".org/. Corpora List (see http://gandalf.aksis.uib.no/corpora/sub.html) is a mailing list for\n",
      "discussions about corpora, and you can find resources by searching the list archives or\n",
      "posting to the list. The most complete inventory of the world’s languages is Ethno-\n",
      "logue, http://www.ethnologue.com/. Of 7,000 languages, only a few dozen have sub-\n",
      "stantial digital resources suitable for use in NLP.\n",
      "This chapter has touched on the field of Corpus Linguistics. Other useful books in\n",
      "this area include (Biber, Conrad, & Reppen, 1998), (McEnery, 2006), (Meyer, 2002),\n",
      "(Sampson & McCarthy, 2005), and (Scott & Tribble, 2006). Further readings in quan-\n",
      "titative data analysis in linguistics are: (Baayen, 2008), (Gries, 2009), and (Woods,\n",
      "Fletcher, & Hughes, 1986).\n",
      "The original description of WordNet is (Fellbaum, 1998). Although WordNet was\n",
      "originally developed for research in psycholinguistics, it is now widely used in NLP and\n",
      "Information Retrieval. WordNets are being developed for many other languages, as\n",
      "documented at http://www.globalwordnet.org/. For a study of WordNet similarity\n",
      "measures, see (Budanitsky & Hirst, 2006).\n",
      "Other topics touched on in this chapter were phonetics and lexical semantics, and we\n",
      "refer readers to Chapters 7 and 20 of (Jurafsky & Martin, 2008).\n",
      "2.8  Exercises\n",
      "1. ○ Create a variable phrase containing a list of words. Experiment with the opera-\n",
      "tions described in this chapter, including addition, multiplication, indexing, slic-\n",
      "ing, and sorting.\n",
      "2. ○ Use the corpus module to explore austen-persuasion.txt. How many word\n",
      "tokens does this book have? How many word types?\n",
      "3. ○ Use the Brown Corpus reader nltk.corpus.brown.words() or the Web Text Cor-\n",
      "pus reader nltk.corpus.webtext.words() to access some sample text in two differ-\n",
      "ent genres.\n",
      "4. ○ Read in the texts of the State of the Union addresses, using the state_union corpus\n",
      "reader. Count occurrences of men, women, and people in each document. What has\n",
      "happened to the usage of these words over time?\n",
      "5. ○ Investigate the holonym-meronym relations for some nouns. Remember that\n",
      "there are three kinds of holonym-meronym relation, so you need to use member_mer\n",
      "onyms(), part_meronyms(), substance_meronyms(), member_holonyms(),\n",
      "part_holonyms(), and substance_holonyms().\n",
      "6. ○ In the discussion of comparative wordlists, we created an object called trans\n",
      "late, which you could look up using words in both German and Italian in order\n",
      "74 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 95, 'page_label': '74', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4432}\n",
      "\n",
      "--- Chunk 4433 ---\n",
      "Content:\n",
      "to get corresponding words in English. What problem might arise with this ap-\n",
      "proach? Can you suggest a way to avoid this problem?\n",
      "7. ○ According \n",
      "to Strunk and White’s Elements of Style, the word however, used at\n",
      "the start of a sentence, means “in whatever way” or “to whatever extent,” and not\n",
      "“nevertheless.” They give this example of correct usage: However you advise him,\n",
      "he will probably do as he thinks best.  (http://www.bartleby.com/141/strunk3.html)\n",
      "Use the concordance tool to study actual usage of this word in the various texts we\n",
      "have been considering. See also the LanguageLog posting “Fossilized prejudices\n",
      "about ‘however’” at http://itre.cis.upenn.edu/~myl/languagelog/archives/001913\n",
      ".html.\n",
      "8. ◑ Define a conditional frequency distribution over the Names Corpus that allows\n",
      "you to see which initial letters are more frequent for males versus females (see\n",
      "Figure 2-7).\n",
      "9. ◑ Pick a pair of texts and study the differences between them, in terms of vocabu-\n",
      "lary, vocabulary richness, genre, etc. Can you find pairs of words that have quite\n",
      "different meanings across the two texts, such as monstrous in Moby Dick and in\n",
      "Sense and Sensibility?\n",
      "10. ◑ Read the BBC News article: “UK’s Vicky Pollards ‘left behind’” at http://news\n",
      ".bbc.co.uk/1/hi/education/6173441.stm. The article gives the following statistic\n",
      "about teen language: “the top 20 words used, including yeah, no, but and like,\n",
      "account for around a third of all words.” How many word types account for a third\n",
      "of all word tokens, for a variety of text sources? What do you conclude about this\n",
      "statistic? Read more about this on LanguageLog, at http://itre.cis.upenn.edu/~myl/\n",
      "languagelog/archives/003993.html.\n",
      "11. ◑ Investigate the table of modal distributions and look for other patterns. Try to\n",
      "explain them in terms of your own impressionistic understanding of the different\n",
      "genres. Can you find other closed classes of words that exhibit significant differ-\n",
      "ences across different genres?\n",
      "12. ◑ The CMU Pronouncing Dictionary contains multiple pronunciations for certain\n",
      "words. How many distinct words does it contain? What fraction of words in this\n",
      "dictionary have more than one possible pronunciation?\n",
      "13. ◑ What percentage of noun synsets have no hyponyms? You can get all noun syn-\n",
      "sets using wn.all_synsets('n').\n",
      "14. ◑ Define a function supergloss(s) that takes a synset s as its argument and returns\n",
      "a string consisting of the concatenation of the definition of s, and the definitions\n",
      "of all the hypernyms and hyponyms of s.\n",
      "15. ◑ Write a program to find all words that occur at least three times in the Brown\n",
      "Corpus.\n",
      "16. ◑ Write a program to generate a table of lexical diversity scores (i.e., token/type\n",
      "ratios), as we saw in Table 1-1 . Include the full set of Brown Corpus genres\n",
      "2.8  Exercises | 75...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 96, 'page_label': '75', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4433}\n",
      "\n",
      "--- Chunk 4434 ---\n",
      "Content:\n",
      "(nltk.corpus.brown.categories()). Which genre has the lowest diversity (greatest\n",
      "number of tokens per type)? Is this what you would have expected?\n",
      "17. ◑ Write a function that finds the 50 most frequently occurring words of a text that\n",
      "are not stopwords.\n",
      "18. ◑ Write a program to print the 50 most frequent bigrams (pairs of adjacent words)\n",
      "of a text, omitting bigrams that contain stopwords.\n",
      "19. ◑ Write a program to create a table of word frequencies by genre, like the one given\n",
      "in Section 2.1 for modals. Choose your own words and try to find words whose\n",
      "presence (or absence) is typical of a genre. Discuss your findings.\n",
      "20. ◑ Write a function word_freq() that takes a word and the name of a section of the\n",
      "Brown Corpus as arguments, and computes the frequency of the word in that sec-\n",
      "tion of the corpus.\n",
      "21. ◑ Write a program to guess the number of syllables contained in a text, making\n",
      "use of the CMU Pronouncing Dictionary.\n",
      "22. ◑ Define a function hedge(text) that processes a text and produces a new version\n",
      "with the word 'like' between every third word.\n",
      "23. ● Zipf’s Law: Let f(w) be the frequency of a word w in free text. Suppose that all\n",
      "the words of a text are ranked according to their frequency, with the most frequent\n",
      "word first. Zipf’s Law states that the frequency of a word type is inversely\n",
      "proportional to its rank (i.e., f × r = k, for some constant k). For example, the 50th\n",
      "most common word type should occur three times as frequently as the 150th most\n",
      "common word type.\n",
      "a. Write a function to process a large text and plot word frequency against word\n",
      "rank using pylab.plot. Do you confirm Zipf’s law? (Hint: it helps to use a\n",
      "logarithmic scale.) What is going on at the extreme ends of the plotted line?\n",
      "b. Generate random text, e.g., using random.choice(\"abcdefg \"), taking care to\n",
      "include the space character. You will need to import random first. Use the string\n",
      "concatenation operator to accumulate characters into a (very) long string.\n",
      "Then tokenize this string, generate the Zipf plot as before, and compare the\n",
      "two plots. What do you make of Zipf’s Law in the light of this?\n",
      "24. ● Modify the text generation program in Example 2-1 further, to do the following\n",
      "tasks:\n",
      "76 | Chapter 2:  Accessing Text Corpora and Lexical Resources...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 97, 'page_label': '76', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4434}\n",
      "\n",
      "--- Chunk 4435 ---\n",
      "Content:\n",
      "a. Store the n most likely words in a list words, then randomly choose a word\n",
      "from the list using random.choice(). (You will need to import random first.)\n",
      "b. Select a particular genre, such as a section of the Brown Corpus or a Genesis\n",
      "translation, one of the Gutenberg texts, or one of the Web texts. Train the\n",
      "model on this corpus and get it to generate random text. You may have to\n",
      "experiment with different start words. How intelligible is the text? Discuss the\n",
      "strengths and weaknesses of this method of generating random text.\n",
      "c. Now train your system using two distinct genres and experiment with gener-\n",
      "ating text in the hybrid genre. Discuss your observations.\n",
      "25. ● Define a function find_language() that takes a string as its argument and returns\n",
      "a list of languages that have that string as a word. Use the udhr corpus and limit\n",
      "your searches to files in the Latin-1 encoding.\n",
      "26. ● What is the branching factor of the noun hypernym hierarchy? I.e., for every\n",
      "noun synset that has hyponyms—or children in the hypernym hierarchy—how\n",
      "many do they have on average? You can get all noun synsets using wn.all_syn\n",
      "sets('n').\n",
      "27. ● The polysemy of a word is the number of senses it has. Using WordNet, we can\n",
      "determine that the noun dog has seven senses with len(wn.synsets('dog', 'n')).\n",
      "Compute the average polysemy of nouns, verbs, adjectives, and adverbs according\n",
      "to WordNet.\n",
      "28. ● Use one of the predefined similarity measures to score the similarity of each of\n",
      "the following pairs of words. Rank the pairs in order of decreasing similarity. How\n",
      "close is your ranking to the order given here, an order that was established exper-\n",
      "imentally by (Miller & Charles, 1998): car-automobile, gem-jewel, journey-voyage,\n",
      "boy-lad, coast-shore, asylum-madhouse, magician-wizard, midday-noon, furnace-\n",
      "stove, food-fruit, bird-cock, bird-crane, tool-implement, brother-monk, lad-\n",
      "brother, crane-implement, journey-car, monk-oracle, cemetery-woodland, food-\n",
      "rooster, coast-hill, forest-graveyard, shore-woodland, monk-slave, coast-forest,\n",
      "lad-wizard, chord-smile, glass-magician, rooster-voyage, noon-string.\n",
      "2.8  Exercises | 77...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 98, 'page_label': '77', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4435}\n",
      "\n",
      "--- Chunk 4436 ---\n",
      "Content:\n",
      "CHAPTER 3\n",
      "Processing Raw Text\n",
      "The most important source of texts is undoubtedly the Web. It’s convenient to have\n",
      "existing \n",
      "text collections to explore, such as the corpora we saw in the previous chapters.\n",
      "However, you probably have your own text sources in mind, and need to learn how to\n",
      "access them.\n",
      "The goal of this chapter is to answer the following questions:\n",
      "1. How can we write programs to access text from local files and from the Web, in\n",
      "order to get hold of an unlimited range of language material?\n",
      "2. How can we split documents up into individual words and punctuation symbols,\n",
      "so we can carry out the same kinds of analysis we did with text corpora in earlier\n",
      "chapters?\n",
      "3. How can we write programs to produce formatted output and save it in a file?\n",
      "In order to address these questions, we will be covering key concepts in NLP, including\n",
      "tokenization and stemming. Along the way you will consolidate your Python knowl-\n",
      "edge and learn about strings, files, and regular expressions. Since so much text on the\n",
      "Web is in HTML format, we will also see how to dispense with markup.\n",
      "Important: From this chapter onwards, our program samples will as-\n",
      "sume you begin your interactive session or your program with the fol-\n",
      "lowing import statements:\n",
      ">>> from __future__ import division\n",
      ">>> import nltk, re, pprint\n",
      "79...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 100, 'page_label': '79', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4436}\n",
      "\n",
      "--- Chunk 4437 ---\n",
      "Content:\n",
      "3.1  Accessing Text from the Web and from Disk\n",
      "Electronic Books\n",
      "A \n",
      "small sample of texts from Project Gutenberg appears in the NLTK corpus collection.\n",
      "However, you may be interested in analyzing other texts from Project Gutenberg. You\n",
      "can browse the catalog of 25,000 free online books at http://www.gutenberg.org/cata\n",
      "log/, and obtain a URL to an ASCII text file. Although 90% of the texts in Project\n",
      "Gutenberg are in English, it includes material in over 50 other languages, including\n",
      "Catalan, Chinese, Dutch, Finnish, French, German, Italian, Portuguese, and Spanish\n",
      "(with more than 100 texts each).\n",
      "Text number 2554 is an English translation of Crime and Punishment, and we can access\n",
      "it as follows.\n",
      ">>> from urllib import urlopen\n",
      ">>> url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n",
      ">>> raw = urlopen(url).read()\n",
      ">>> type(raw)\n",
      "<type 'str'>\n",
      ">>> len(raw)\n",
      "1176831\n",
      ">>> raw[:75]\n",
      "'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'\n",
      "The read() process will take a few seconds as it downloads this large\n",
      "book. If you’re using an Internet proxy that is not correctly detected by\n",
      "Python, you may need to specify the proxy manually as follows:\n",
      ">>> proxies = {'http': 'http://www.someproxy.com:3128'}\n",
      ">>> raw = urlopen(url, proxies=proxies).read()\n",
      "The variable raw contains a string with 1,176,831 characters. (We can see that it is a\n",
      "string, using type(raw).) This is the raw content of the book, including many details\n",
      "we are not interested in, such as whitespace, line breaks, and blank lines. Notice the\n",
      "\\r and \\n in the opening line of the file, which is how Python displays the special carriage\n",
      "return and line-feed characters (the file must have been created on a Windows ma-\n",
      "chine). For our language processing, we want to break up the string into words and\n",
      "punctuation, as we saw in Chapter 1. This step is called tokenization, and it produces\n",
      "our familiar structure, a list of words and punctuation.\n",
      ">>> tokens = nltk.word_tokenize(raw)\n",
      ">>> type(tokens)\n",
      "<type 'list'>\n",
      ">>> len(tokens)\n",
      "255809\n",
      ">>> tokens[:10]\n",
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n",
      "80 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 101, 'page_label': '80', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4437}\n",
      "\n",
      "--- Chunk 4438 ---\n",
      "Content:\n",
      "Notice that NLTK was needed for tokenization, but not for any of the earlier tasks of\n",
      "opening a URL and reading it into a string. If we now take the further step of creating\n",
      "an \n",
      "NLTK text from this list, we can carry out all of the other linguistic processing we\n",
      "saw in Chapter 1, along with the regular list operations, such as slicing:\n",
      ">>> text = nltk.Text(tokens)\n",
      ">>> type(text)\n",
      "<type 'nltk.text.Text'>\n",
      ">>> text[1020:1060]\n",
      "['CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in',\n",
      "'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in',\n",
      "'which', 'he', 'lodged', 'in', 'S', '.', 'Place', 'and', 'walked', 'slowly',\n",
      "',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K', '.', 'bridge', '.']\n",
      ">>> text.collocations()\n",
      "Katerina Ivanovna; Pulcheria Alexandrovna; Avdotya Romanovna; Pyotr\n",
      "Petrovitch; Project Gutenberg; Marfa Petrovna; Rodion Romanovitch;\n",
      "Sofya Semyonovna; Nikodim Fomitch; did not; Hay Market; Andrey\n",
      "Semyonovitch; old woman; Literary Archive; Dmitri Prokofitch; great\n",
      "deal; United States; Praskovya Pavlovna; Porfiry Petrovitch; ear rings\n",
      "Notice that Project Gutenberg appears as a collocation. This is because each text down-\n",
      "loaded from Project Gutenberg contains a header with the name of the text, the author,\n",
      "the names of people who scanned and corrected the text, a license, and so on. Some-\n",
      "times this information appears in a footer at the end of the file. We cannot reliably\n",
      "detect where the content begins and ends, and so have to resort to manual inspection\n",
      "of the file, to discover unique strings that mark the beginning and the end, before\n",
      "trimming raw to be just the content and nothing else:\n",
      ">>> raw.find(\"PART I\")\n",
      "5303\n",
      ">>> raw.rfind(\"End of Project Gutenberg's Crime\")\n",
      "1157681\n",
      ">>> raw = raw[5303:1157681] \n",
      ">>> raw.find(\"PART I\")\n",
      "0\n",
      "The find() and rfind() \n",
      "(“reverse find”) methods help us get the right index values to\n",
      "use for slicing the string \n",
      " . We overwrite raw with this slice, so now it begins with\n",
      "“PART I” and goes up to (but not including) the phrase that marks the end of the\n",
      "content.\n",
      "This was our first brush with the reality of the Web: texts found on the Web may contain\n",
      "unwanted material, and there may not be an automatic way to remove it. But with a\n",
      "small amount of extra work we can extract the material we need.\n",
      "Dealing with HTML\n",
      "Much of the text on the Web is in the form of HTML documents. You can use a web\n",
      "browser to save a page as text to a local file, then access this as described in the later\n",
      "section on files. However, if you’re going to do this often, it’s easiest to get Python to\n",
      "do the work directly. The first step is the same as before, using urlopen...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 102, 'page_label': '81', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4438}\n",
      "\n",
      "--- Chunk 4439 ---\n",
      "Content:\n",
      ". But with a\n",
      "small amount of extra work we can extract the material we need.\n",
      "Dealing with HTML\n",
      "Much of the text on the Web is in the form of HTML documents. You can use a web\n",
      "browser to save a page as text to a local file, then access this as described in the later\n",
      "section on files. However, if you’re going to do this often, it’s easiest to get Python to\n",
      "do the work directly. The first step is the same as before, using urlopen. For fun we’ll\n",
      "3.1  Accessing Text from the Web and from Disk | 81...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 102, 'page_label': '81', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4439}\n",
      "\n",
      "--- Chunk 4440 ---\n",
      "Content:\n",
      "pick a BBC News story called “Blondes to die out in 200 years,” an urban legend passed\n",
      "along by the BBC as established scientific fact:\n",
      ">>> url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
      ">>> html = urlopen(url).read()\n",
      ">>> html[:60]\n",
      "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'\n",
      "You \n",
      "can type print html to see the HTML content in all its glory, including meta tags,\n",
      "an image map, JavaScript, forms, and tables.\n",
      "Getting text out of HTML is a sufficiently common task that NLTK provides a helper\n",
      "function nltk.clean_html(), which takes an HTML string and returns raw text. We\n",
      "can then tokenize this to get our familiar text structure:\n",
      ">>> raw = nltk.clean_html(html)\n",
      ">>> tokens = nltk.word_tokenize(raw)\n",
      ">>> tokens\n",
      "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'\", 'to', 'die', 'out', ...]\n",
      "This still contains unwanted material concerning site navigation and related stories.\n",
      "With some trial and error you can find the start and end indexes of the content and\n",
      "select the tokens of interest, and initialize a text as before.\n",
      ">>> tokens = tokens[96:399]\n",
      ">>> text = nltk.Text(tokens)\n",
      ">>> text.concordance('gene')\n",
      " they say too few people now carry the gene for blondes to last beyond the next tw\n",
      "t blonde hair is caused by a recessive gene . In order for a child to have blonde\n",
      "to have blonde hair , it must have the gene on both sides of the family in the gra\n",
      "there is a disadvantage of having that gene or by chance . They don ' t disappear\n",
      "ondes would disappear is if having the gene was a disadvantage and I do not think\n",
      "For more sophisticated processing of HTML, use the Beautiful Soup\n",
      "package, available at http://www.crummy.com/software/BeautifulSoup/.\n",
      "Processing Search Engine Results\n",
      "The Web can be thought of as a huge corpus of unannotated text. Web search engines\n",
      "provide an efficient means of searching this large quantity of text for relevant linguistic\n",
      "examples. The main advantage of search engines is size: since you are searching such\n",
      "a large set of documents, you are more likely to find any linguistic pattern you are\n",
      "interested in. Furthermore, you can make use of very specific patterns, which would\n",
      "match only one or two examples on a smaller example, but which might match tens of\n",
      "thousands of examples when run on the Web. A second advantage of web search en-\n",
      "gines is that they are very easy to use. Thus, they provide a very convenient tool for\n",
      "quickly checking a theory, to see if it is reasonable. See Table 3-1 for an example.\n",
      "82 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 103, 'page_label': '82', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4440}\n",
      "\n",
      "--- Chunk 4441 ---\n",
      "Content:\n",
      "Table 3-1. Google hits for collocations: The number of hits for collocations involving the words\n",
      "absolutely or definitely, \n",
      "followed by one of adore, love, like, or prefer. (Liberman, in LanguageLog,\n",
      "2005)\n",
      "Google hits adore love like prefer\n",
      "absolutely 289,000 905,000 16,200 644\n",
      "definitely 1,460 51,000 158,000 62,600\n",
      "ratio 198:1 18:1 1:10 1:97\n",
      "Unfortunately, search engines have some significant shortcomings. First, the allowable\n",
      "range \n",
      "of search patterns is severely restricted. Unlike local corpora, where you write\n",
      "programs to search for arbitrarily complex patterns, search engines generally only allow\n",
      "you to search for individual words or strings of words, sometimes with wildcards. Sec-\n",
      "ond, search engines give inconsistent results, and can give widely different figures when\n",
      "used at different times or in different geographical regions. When content has been\n",
      "duplicated across multiple sites, search results may be boosted. Finally, the markup in\n",
      "the result returned by a search engine may change unpredictably, breaking any pattern-\n",
      "based method of locating particular content (a problem which is ameliorated by the\n",
      "use of search engine APIs).\n",
      "Your Turn: Search the Web for \"the of\" (inside quotes). Based on the\n",
      "large count, can we conclude that the of is a frequent collocation in\n",
      "English?\n",
      "Processing RSS Feeds\n",
      "The blogosphere is an important source of text, in both formal and informal registers.\n",
      "With the help of a third-party Python library called the Universal Feed Parser, freely\n",
      "downloadable from http://feedparser.org/, we can access the content of a blog, as shown\n",
      "here:\n",
      ">>> import feedparser\n",
      ">>> llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
      ">>> llog['feed']['title']\n",
      "u'Language Log'\n",
      ">>> len(llog.entries)\n",
      "15\n",
      ">>> post = llog.entries[2]\n",
      ">>> post.title\n",
      "u\"He's My BF\"\n",
      ">>> content = post.content[0].value\n",
      ">>> content[:70]\n",
      "u'<p>Today I was chatting with three of our visiting graduate students f'\n",
      ">>> nltk.word_tokenize(nltk.html_clean(content))\n",
      ">>> nltk.word_tokenize(nltk.clean_html(llog.entries[2].content[0].value))\n",
      "[u'Today', u'I', u'was', u'chatting', u'with', u'three', u'of', u'our', u'visiting',\n",
      "u'graduate', u'students', u'from', u'the', u'PRC', u'.', u'Thinking', u'that', u'I',\n",
      "3.1  Accessing Text from the Web and from Disk | 83...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 104, 'page_label': '83', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4441}\n",
      "\n",
      "--- Chunk 4442 ---\n",
      "Content:\n",
      "u'was', u'being', u'au', u'courant', u',', u'I', u'mentioned', u'the', u'expression',\n",
      "u'DUI4XIANG4', u'\\u5c0d\\u8c61', u'(\"', u'boy', u'/', u'girl', u'friend', u'\"', ...]\n",
      "Note \n",
      "that the resulting strings have a u prefix to indicate that they are Unicode strings\n",
      "(see Section 3.3). With some further work, we can write programs to create a small\n",
      "corpus of blog posts, and use this as the basis for our NLP work.\n",
      "Reading Local Files\n",
      "In order to read a local file, we need to use Python’s built-in open() function, followed\n",
      "by the read() method. Supposing you have a file document.txt, you can load its contents\n",
      "like this:\n",
      ">>> f = open('document.txt')\n",
      ">>> raw = f.read()\n",
      "Your Turn: Create a file called document.txt using a text editor, and\n",
      "type in a few lines of text, and save it as plain text. If you are using IDLE,\n",
      "select the New Window command in the File menu, typing the required\n",
      "text into this window, and then saving the file as document.txt inside\n",
      "the directory that IDLE offers in the pop-up dialogue box. Next, in the\n",
      "Python interpreter, open the file using f = open('document.txt'), then\n",
      "inspect its contents using print f.read().\n",
      "Various things might have gone wrong when you tried this. If the interpreter couldn’t\n",
      "find your file, you would have seen an error like this:\n",
      ">>> f = open('document.txt')\n",
      "Traceback (most recent call last):\n",
      "File \"<pyshell#7>\", line 1, in -toplevel-\n",
      "f = open('document.txt')\n",
      "IOError: [Errno 2] No such file or directory: 'document.txt'\n",
      "To check that the file that you are trying to open is really in the right directory, use\n",
      "IDLE’s Open command in the File menu; this will display a list of all the files in the\n",
      "directory where IDLE is running. An alternative is to examine the current directory\n",
      "from within Python:\n",
      ">>> import os\n",
      ">>> os.listdir('.')\n",
      "Another possible problem you might have encountered when accessing a text file is the\n",
      "newline conventions, which are different for different operating systems. The built-in\n",
      "open() function has a second parameter for controlling how the file is opened: open('do\n",
      "cument.txt', 'rU'). 'r' means to open the file for reading (the default), and 'U' stands\n",
      "for “Universal”, which lets us ignore the different conventions used for marking new-\n",
      "lines.\n",
      "Assuming that you can open the file, there are several methods for reading it. The\n",
      "read() method creates a string with the contents of the entire file:\n",
      "84 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 105, 'page_label': '84', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4442}\n",
      "\n",
      "--- Chunk 4443 ---\n",
      "Content:\n",
      ">>> f.read()\n",
      "'Time flies like an arrow.\\nFruit flies like a banana.\\n'\n",
      "Recall \n",
      "that the '\\n' characters are newlines; this is equivalent to pressing Enter on a\n",
      "keyboard and starting a new line.\n",
      "We can also read a file one line at a time using a for loop:\n",
      ">>> f = open('document.txt', 'rU')\n",
      ">>> for line in f:\n",
      "...     print line.strip()\n",
      "Time flies like an arrow.\n",
      "Fruit flies like a banana.\n",
      "Here we use the strip() method to remove the newline character at the end of the input\n",
      "line.\n",
      "NLTK’s corpus files can also be accessed using these methods. We simply have to use\n",
      "nltk.data.find() to get the filename for any corpus item. Then we can open and read\n",
      "it in the way we just demonstrated:\n",
      ">>> path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n",
      ">>> raw = open(path, 'rU').read()\n",
      "Extracting Text from PDF, MSWord, and Other Binary Formats\n",
      "ASCII text and HTML text are human-readable formats. Text often comes in binary\n",
      "formats—such as PDF and MSWord—that can only be opened using specialized soft-\n",
      "ware. Third-party libraries such as pypdf and pywin32 provide access to these formats.\n",
      "Extracting text from multicolumn documents is particularly challenging. For one-off\n",
      "conversion of a few documents, it is simpler to open the document with a suitable\n",
      "application, then save it as text to your local drive, and access it as described below. If\n",
      "the document is already on the Web, you can enter its URL in Google’s search box.\n",
      "The search result often includes a link to an HTML version of the document, which\n",
      "you can save as text.\n",
      "Capturing User Input\n",
      "Sometimes we want to capture the text that a user inputs when she is interacting with\n",
      "our program. To prompt the user to type a line of input, call the Python function\n",
      "raw_input(). After saving the input to a variable, we can manipulate it just as we have\n",
      "done for other strings.\n",
      ">>> s = raw_input(\"Enter some text: \")\n",
      "Enter some text: On an exceptionally hot evening early in July\n",
      ">>> print \"You typed\", len(nltk.word_tokenize(s)), \"words.\"\n",
      "You typed 8 words.\n",
      "3.1  Accessing Text from the Web and from Disk | 85...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 106, 'page_label': '85', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4443}\n",
      "\n",
      "--- Chunk 4444 ---\n",
      "Content:\n",
      "The NLP Pipeline\n",
      "Figure 3-1 \n",
      "summarizes what we have covered in this section, including the process of\n",
      "building a vocabulary that we saw in Chapter 1. (One step, normalization, will be\n",
      "discussed in Section 3.6.)\n",
      "Figure 3-1. The processing pipeline: We open a URL and read its HTML content, remove the markup\n",
      "and \n",
      "select a slice of characters; this is then tokenized and optionally converted into an nltk.Text\n",
      "object; we can also lowercase all the words and extract the vocabulary.\n",
      "There’s a lot going on in this pipeline. To understand it properly, it helps to be clear\n",
      "about the type of each variable that it mentions. We find out the type of any Python\n",
      "object x using type(x); e.g., type(1) is <int> since 1 is an integer.\n",
      "When we load the contents of a URL or file, and when we strip out HTML markup,\n",
      "we are dealing with strings, Python’s <str> data type (we will learn more about strings\n",
      "in Section 3.2):\n",
      ">>> raw = open('document.txt').read()\n",
      ">>> type(raw)\n",
      "<type 'str'>\n",
      "When we tokenize a string we produce a list (of words), and this is Python’s <list>\n",
      "type. Normalizing and sorting lists produces other lists:\n",
      ">>> tokens = nltk.word_tokenize(raw)\n",
      ">>> type(tokens)\n",
      "<type 'list'>\n",
      ">>> words = [w.lower() for w in tokens]\n",
      ">>> type(words)\n",
      "<type 'list'>\n",
      ">>> vocab = sorted(set(words))\n",
      ">>> type(vocab)\n",
      "<type 'list'>\n",
      "The type of an object determines what operations you can perform on it. So, for ex-\n",
      "ample, we can append to a list but not to a string:\n",
      "86 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 107, 'page_label': '86', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4444}\n",
      "\n",
      "--- Chunk 4445 ---\n",
      "Content:\n",
      ">>> vocab.append('blog')\n",
      ">>> raw.append('blog')\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "AttributeError: 'str' object has no attribute 'append'\n",
      "Similarly, \n",
      "we can concatenate strings with strings, and lists with lists, but we cannot\n",
      "concatenate strings with lists:\n",
      ">>> query = 'Who knows?'\n",
      ">>> beatles = ['john', 'paul', 'george', 'ringo']\n",
      ">>> query + beatles\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: cannot concatenate 'str' and 'list' objects\n",
      "In the next section, we examine strings more closely and further explore the relationship\n",
      "between strings and lists.\n",
      "3.2  Strings: Text Processing at the Lowest Level\n",
      "It’s time to study a fundamental data type that we’ve been studiously avoiding so far.\n",
      "In earlier chapters we focused on a text as a list of words. We didn’t look too closely\n",
      "at words and how they are handled in the programming language. By using NLTK’s\n",
      "corpus interface we were able to ignore the files that these texts had come from. The\n",
      "contents of a word, and of a file, are represented by programming languages as a fun-\n",
      "damental data type known as a string. In this section, we explore strings in detail, and\n",
      "show the connection between strings, words, texts, and files.\n",
      "Basic Operations with Strings\n",
      "Strings are specified using single quotes \n",
      " or double quotes \n",
      " , as shown in the fol-\n",
      "lowing \n",
      "code example. If a string contains a single quote, we must backslash-escape the\n",
      "quote \n",
      "  so Python knows a literal quote character is intended, or else put the string in\n",
      "double \n",
      "quotes \n",
      " . Otherwise, the quote inside the string \n",
      "  will be interpreted as a close\n",
      "quote, and the Python interpreter will report a syntax error:\n",
      ">>> monty = 'Monty Python' \n",
      ">>> monty\n",
      "'Monty Python'\n",
      ">>> circus = \"Monty Python's Flying Circus\" \n",
      ">>> circus\n",
      "\"Monty Python's Flying Circus\"\n",
      ">>> circus = 'Monty Python\\'s Flying Circus' \n",
      ">>> circus\n",
      "\"Monty Python's Flying Circus\"\n",
      ">>> circus = 'Monty Python's Flying Circus' \n",
      "  File \"<stdin>\", line 1\n",
      "    circus = 'Monty Python's Flying Circus'\n",
      "                           ^\n",
      "SyntaxError: invalid syntax\n",
      "3.2  Strings: Text Processing at the Lowest Level | 87...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 108, 'page_label': '87', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4445}\n",
      "\n",
      "--- Chunk 4446 ---\n",
      "Content:\n",
      "Sometimes strings go over several lines. Python provides us with various ways of en-\n",
      "tering \n",
      "them. In the next example, a sequence of two strings is joined into a single string.\n",
      "We need to use backslash \n",
      "  or parentheses \n",
      "  so that the interpreter knows that the\n",
      "statement is not complete after the first line.\n",
      ">>> couplet = \"Shall I compare thee to a Summer's day?\"\\\n",
      "...           \"Thou are more lovely and more temperate:\" \n",
      ">>> print couplet\n",
      "Shall I compare thee to a Summer's day?Thou are more lovely and more temperate:\n",
      ">>> couplet = (\"Rough winds do shake the darling buds of May,\"\n",
      "...           \"And Summer's lease hath all too short a date:\") \n",
      ">>> print couplet\n",
      "Rough winds do shake the darling buds of May,And Summer's lease hath all too short a date:\n",
      "Unfortunately \n",
      "these methods do not give us a newline between the two lines of the\n",
      "sonnet. Instead, we can use a triple-quoted string as follows:\n",
      ">>> couplet = \"\"\"Shall I compare thee to a Summer's day?\n",
      "... Thou are more lovely and more temperate:\"\"\"\n",
      ">>> print couplet\n",
      "Shall I compare thee to a Summer's day?\n",
      "Thou are more lovely and more temperate:\n",
      ">>> couplet = '''Rough winds do shake the darling buds of May,\n",
      "... And Summer's lease hath all too short a date:'''\n",
      ">>> print couplet\n",
      "Rough winds do shake the darling buds of May,\n",
      "And Summer's lease hath all too short a date:\n",
      "Now that we can define strings, we can try some simple operations on them. First let’s\n",
      "look at the + operation, known as concatenation \n",
      ". It produces a new string that is a\n",
      "copy \n",
      "of the two original strings pasted together end-to-end. Notice that concatenation\n",
      "doesn’t do anything clever like insert a space between the words. We can even multiply\n",
      "strings \n",
      ":\n",
      ">>> 'very' + 'very' + 'very' \n",
      "'veryveryvery'\n",
      ">>> 'very' * 3 \n",
      "'veryveryvery'\n",
      "Your Turn: Try running the following code, then try to use your un-\n",
      "derstanding of the string + and * operations to figure out how it works.\n",
      "Be careful to distinguish between the string ' ', which is a single white-\n",
      "space character, and '', which is the empty string.\n",
      ">>> a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\n",
      ">>> b = [' ' * 2 * (7 - i) + 'very' * i for i in a]\n",
      ">>> for line in b:\n",
      "...     print b\n",
      "We’ve seen that the addition and multiplication operations apply to strings, not just\n",
      "numbers. However, note that we cannot use subtraction or division with strings:\n",
      "88 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 109, 'page_label': '88', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4446}\n",
      "\n",
      "--- Chunk 4447 ---\n",
      "Content:\n",
      ">>> 'very' - 'y'\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: unsupported operand type(s) for -: 'str' and 'str'\n",
      ">>> 'very' / 2\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: unsupported operand type(s) for /: 'str' and 'int'\n",
      "These \n",
      "error messages are another example of Python telling us that we have got our\n",
      "data types in a muddle. In the first case, we are told that the operation of subtraction\n",
      "(i.e., -) cannot apply to objects of type str (strings), while in the second, we are told\n",
      "that division cannot take str and int as its two operands.\n",
      "Printing Strings\n",
      "So far, when we have wanted to look at the contents of a variable or see the result of a\n",
      "calculation, we have just typed the variable name into the interpreter. We can also see\n",
      "the contents of a variable using the print statement:\n",
      ">>> print monty\n",
      "Monty Python\n",
      "Notice that there are no quotation marks this time. When we inspect a variable by\n",
      "typing its name in the interpreter, the interpreter prints the Python representation of\n",
      "its value. Since it’s a string, the result is quoted. However, when we tell the interpreter\n",
      "to print the contents of the variable, we don’t see quotation characters, since there are\n",
      "none inside the string.\n",
      "The print statement allows us to display more than one item on a line in various ways,\n",
      "as shown here:\n",
      ">>> grail = 'Holy Grail'\n",
      ">>> print monty + grail\n",
      "Monty PythonHoly Grail\n",
      ">>> print monty, grail\n",
      "Monty Python Holy Grail\n",
      ">>> print monty, \"and the\", grail\n",
      "Monty Python and the Holy Grail\n",
      "Accessing Individual Characters\n",
      "As we saw in Section 1.2 for lists, strings are indexed, starting from zero. When we\n",
      "index a string, we get one of its characters (or letters). A single character is nothing\n",
      "special—it’s just a string of length 1.\n",
      ">>> monty[0]\n",
      "'M'\n",
      ">>> monty[3]\n",
      "'t'\n",
      ">>> monty[5]\n",
      "' '\n",
      "3.2  Strings: Text Processing at the Lowest Level | 89...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 110, 'page_label': '89', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4447}\n",
      "\n",
      "--- Chunk 4448 ---\n",
      "Content:\n",
      "As with lists, if we try to access an index that is outside of the string, we get an error:\n",
      ">>> monty[20]\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in ?\n",
      "IndexError: string index out of range\n",
      "Again \n",
      "as with lists, we can use negative indexes for strings, where -1 is the index of the\n",
      "last character \n",
      " . Positive and negative indexes give us two ways to refer to any position\n",
      "in \n",
      "a string. In this case, when the string had a length of 12, indexes 5 and -7 both refer\n",
      "to the same character (a space). (Notice that 5 = len(monty) - 7.)\n",
      ">>> monty[-1] \n",
      "'n'\n",
      ">>> monty[5]\n",
      "' '\n",
      ">>> monty[-7]\n",
      "' '\n",
      "We \n",
      "can write for loops to iterate over the characters in strings. This print statement\n",
      "ends with a trailing comma, which is how we tell Python not to print a newline at the\n",
      "end.\n",
      ">>> sent = 'colorless green ideas sleep furiously'\n",
      ">>> for char in sent:\n",
      "...     print char,\n",
      "...\n",
      "c o l o r l e s s   g r e e n   i d e a s   s l e e p   f u r i o u s l y\n",
      "We can count individual characters as well. We should ignore the case distinction by\n",
      "normalizing everything to lowercase, and filter out non-alphabetic characters:\n",
      ">>> from nltk.corpus import gutenberg\n",
      ">>> raw = gutenberg.raw('melville-moby_dick.txt')\n",
      ">>> fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\n",
      ">>> fdist.keys()\n",
      "['e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'l', 'd', 'u', 'm', 'c', 'w',\n",
      "'f', 'g', 'p', 'b', 'y', 'v', 'k', 'q', 'j', 'x', 'z']\n",
      "This gives us the letters of the alphabet, with the most frequently occurring letters listed\n",
      "first (this is quite complicated and we’ll explain it more carefully later). You might like\n",
      "to visualize the distribution using fdist.plot(). The relative character frequencies of\n",
      "a text can be used in automatically identifying the language of the text.\n",
      "Accessing Substrings\n",
      "A substring is any continuous section of a string that we want to pull out for further\n",
      "processing. We can easily access substrings using the same slice notation we used for\n",
      "lists (see Figure 3-2). For example, the following code accesses the substring starting\n",
      "at index 6, up to (but not including) index 10:\n",
      ">>> monty[6:10]\n",
      "'Pyth'\n",
      "90 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 111, 'page_label': '90', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4448}\n",
      "\n",
      "--- Chunk 4449 ---\n",
      "Content:\n",
      "Here we see the characters are 'P', 'y', 't', and 'h', which correspond to monty[6] ...\n",
      "monty[9] but not monty[10]. This is because a slice starts at the first index but finishes\n",
      "one before the end index.\n",
      "We can also slice with negative indexes—the same basic rule of starting from the start\n",
      "index and stopping one before the end index applies; here we stop before the space\n",
      "character.\n",
      ">>> monty[-12:-7]\n",
      "'Monty'\n",
      "As with list slices, if we omit the first value, the substring begins at the start of the string.\n",
      "If we omit the second value, the substring continues to the end of the string:\n",
      ">>> monty[:5]\n",
      "'Monty'\n",
      ">>> monty[6:]\n",
      "'Python'\n",
      "We test if a string contains a particular substring using the in operator, as follows:\n",
      ">>> phrase = 'And now for something completely different'\n",
      ">>> if 'thing' in phrase:\n",
      "...     print 'found \"thing\"'\n",
      "found \"thing\"\n",
      "We can also find the position of a substring within a string, using find():\n",
      ">>> monty.find('Python')\n",
      "6\n",
      "Your Turn: Make up a sentence and assign it to a variable, e.g., sent =\n",
      "'my sentence...'. Now write slice expressions to pull out individual\n",
      "words. (This is obviously not a convenient way to process the words of\n",
      "a text!)\n",
      "Figure 3-2. String slicing: The string Monty Python is shown along with its positive and negative\n",
      "indexes; two substrings are selected using “slice” notation. The slice [m,n] contains the characters\n",
      "from position m through n-1.\n",
      "3.2  Strings: Text Processing at the Lowest Level | 91...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 112, 'page_label': '91', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4449}\n",
      "\n",
      "--- Chunk 4450 ---\n",
      "Content:\n",
      "More Operations on Strings\n",
      "Python \n",
      "has comprehensive support for processing strings. A summary, including some\n",
      "operations we haven’t seen yet, is shown in Table 3-2. For more information on strings,\n",
      "type help(str) at the Python prompt.\n",
      "Table 3-2. Useful string methods: Operations on strings in addition to the string tests shown in\n",
      "Table 1-4; all methods produce a new string or list\n",
      "Method Functionality\n",
      "s.find(t) Index of first instance of string t inside s (-1 if not found)\n",
      "s.rfind(t) Index of last instance of string t inside s (-1 if not found)\n",
      "s.index(t) Like s.find(t), except it raises ValueError if not found\n",
      "s.rindex(t) Like s.rfind(t), except it raises ValueError if not found\n",
      "s.join(text) Combine the words of the text into a string using s as the glue\n",
      "s.split(t) Split s into a list wherever a t is found (whitespace by default)\n",
      "s.splitlines() Split s into a list of strings, one per line\n",
      "s.lower() A lowercased version of the string s\n",
      "s.upper() An uppercased version of the string s\n",
      "s.titlecase() A titlecased version of the string s\n",
      "s.strip() A copy of s without leading or trailing whitespace\n",
      "s.replace(t, u) Replace instances of t with u inside s\n",
      "The Difference Between Lists and Strings\n",
      "Strings and lists are both kinds of sequence. We can pull them apart by indexing and\n",
      "slicing \n",
      "them, and we can join them together by concatenating them. However, we can-\n",
      "not join strings and lists:\n",
      ">>> query = 'Who knows?'\n",
      ">>> beatles = ['John', 'Paul', 'George', 'Ringo']\n",
      ">>> query[2]\n",
      "'o'\n",
      ">>> beatles[2]\n",
      "'George'\n",
      ">>> query[:2]\n",
      "'Wh'\n",
      ">>> beatles[:2]\n",
      "['John', 'Paul']\n",
      ">>> query + \" I don't\"\n",
      "\"Who knows? I don't\"\n",
      ">>> beatles + 'Brian'\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: can only concatenate list (not \"str\") to list\n",
      ">>> beatles + ['Brian']\n",
      "['John', 'Paul', 'George', 'Ringo', 'Brian']\n",
      "92 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 113, 'page_label': '92', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4450}\n",
      "\n",
      "--- Chunk 4451 ---\n",
      "Content:\n",
      "When we open a file for reading into a Python program, we get a string corresponding\n",
      "to \n",
      "the contents of the whole file. If we use a for loop to process the elements of this\n",
      "string, all we can pick out are the individual characters—we don’t get to choose the\n",
      "granularity. By contrast, the elements of a list can be as big or small as we like: for\n",
      "example, they could be paragraphs, sentences, phrases, words, characters. So lists have\n",
      "the advantage that we can be flexible about the elements they contain, and corre-\n",
      "spondingly flexible about any downstream processing. Consequently, one of the first\n",
      "things we are likely to do in a piece of NLP code is tokenize a string into a list of strings\n",
      "(Section 3.7). Conversely, when we want to write our results to a file, or to a terminal,\n",
      "we will usually format them as a string (Section 3.9).\n",
      "Lists and strings do not have exactly the same functionality. Lists have the added power\n",
      "that you can change their elements:\n",
      ">>> beatles[0] = \"John Lennon\"\n",
      ">>> del beatles[-1]\n",
      ">>> beatles\n",
      "['John Lennon', 'Paul', 'George']\n",
      "On the other hand, if we try to do that with a string—changing the 0th character in\n",
      "query to 'F'—we get:\n",
      ">>> query[0] = 'F'\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in ?\n",
      "TypeError: object does not support item assignment\n",
      "This is because strings are immutable: you can’t change a string once you have created\n",
      "it. However, lists are mutable, and their contents can be modified at any time. As a\n",
      "result, lists support operations that modify the original value rather than producing a\n",
      "new value.\n",
      "Your Turn: Consolidate your knowledge of strings by trying some of\n",
      "the exercises on strings at the end of this chapter.\n",
      "3.3  Text Processing with Unicode\n",
      "Our programs will often need to deal with different languages, and different character\n",
      "sets. The concept of “plain text” is a fiction. If you live in the English-speaking world\n",
      "you probably use ASCII, possibly without realizing it. If you live in Europe you might\n",
      "use one of the extended Latin character sets, containing such characters as “ø” for\n",
      "Danish and Norwegian, “ ő” for Hungarian, “ñ” for Spanish and Breton, and “ ň” for\n",
      "Czech and Slovak. In this section, we will give an overview of how to use Unicode for\n",
      "processing texts that use non-ASCII character sets.\n",
      "3.3  Text Processing with Unicode | 93...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 114, 'page_label': '93', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4451}\n",
      "\n",
      "--- Chunk 4452 ---\n",
      "Content:\n",
      "What Is Unicode?\n",
      "Unicode \n",
      "supports over a million characters. Each character is assigned a number, called\n",
      "a code point. In Python, code points are written in the form \\uXXXX, where XXXX\n",
      "is the number in four-digit hexadecimal form.\n",
      "Within a program, we can manipulate Unicode strings just like normal strings. How-\n",
      "ever, when Unicode characters are stored in files or displayed on a terminal, they must\n",
      "be encoded as a stream of bytes. Some encodings (such as ASCII and Latin-2) use a\n",
      "single byte per code point, so they can support only a small subset of Unicode, enough\n",
      "for a single language. Other encodings (such as UTF-8) use multiple bytes and can\n",
      "represent the full range of Unicode characters.\n",
      "Text in files will be in a particular encoding, so we need some mechanism for translating\n",
      "it into Unicode—translation into Unicode is called decoding. Conversely, to write out\n",
      "Unicode to a file or a terminal, we first need to translate it into a suitable encoding—\n",
      "this translation out of Unicode is called encoding, and is illustrated in Figure 3-3.\n",
      "Figure 3-3. Unicode decoding and encoding.\n",
      "From \n",
      "a Unicode perspective, characters are abstract entities that can be realized as one\n",
      "or more glyphs. Only glyphs can appear on a screen or be printed on paper. A font is\n",
      "a mapping from characters to glyphs.\n",
      "Extracting Encoded Text from Files\n",
      "Let’s assume that we have a small text file, and that we know how it is encoded. For\n",
      "example, polish-lat2.txt, as the name suggests, is a snippet of Polish text (from the Polish\n",
      "Wikipedia; see http://pl.wikipedia.org/wiki/Biblioteka_Pruska). This file is encoded as\n",
      "Latin-2, also known as ISO-8859-2. The function nltk.data.find() locates the file for\n",
      "us.\n",
      "94 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 115, 'page_label': '94', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4452}\n",
      "\n",
      "--- Chunk 4453 ---\n",
      "Content:\n",
      ">>> path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
      "The \n",
      "Python codecs module provides functions to read encoded data into Unicode\n",
      "strings, and to write out Unicode strings in encoded form. The codecs.open() function\n",
      "takes an encoding parameter to specify the encoding of the file being read or written.\n",
      "So let’s import the codecs module, and call it with the encoding 'latin2' to open our\n",
      "Polish file as Unicode:\n",
      ">>> import codecs\n",
      ">>> f = codecs.open(path, encoding='latin2')\n",
      "For a list of encoding parameters allowed by codecs, see http://docs.python.org/lib/\n",
      "standard-encodings.html. Note that we can write Unicode-encoded data to a file using\n",
      "f = codecs.open(path, 'w', encoding='utf-8').\n",
      "Text read from the file object f will be returned in Unicode. As we pointed out earlier,\n",
      "in order to view this text on a terminal, we need to encode it, using a suitable encoding.\n",
      "The Python-specific encoding unicode_escape is a dummy encoding that converts all\n",
      "non-ASCII characters into their \\uXXXX representations. Code points above the ASCII\n",
      "0–127 range but below 256 are represented in the two-digit form \\xXX.\n",
      ">>> for line in f:\n",
      "...     line = line.strip()\n",
      "...     print line.encode('unicode_escape')\n",
      "Pruska Biblioteka Pa\\u0144stwowa. Jej dawne zbiory znane pod nazw\\u0105\n",
      "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
      "Niemc\\xf3w pod koniec II wojny \\u015bwiatowej na Dolny \\u015al\\u0105sk, zosta\\u0142y\n",
      "odnalezione po 1945 r. na terytorium Polski. Trafi\\u0142y do Biblioteki\n",
      "Jagiello\\u0144skiej w Krakowie, obejmuj\\u0105 ponad 500 tys. zabytkowych\n",
      "archiwali\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n",
      "The first line in this output illustrates a Unicode escape string preceded by the \\u escape\n",
      "string, namely \\u0144. The relevant Unicode character will be displayed on the screen\n",
      "as the glyph ń. In the third line of the preceding example, we see \\xf3, which corre-\n",
      "sponds to the glyph ó, and is within the 128–255 range.\n",
      "In Python, a Unicode string literal can be specified by preceding an ordinary string\n",
      "literal with a u, as in u'hello'. Arbitrary Unicode characters are defined using the\n",
      "\\uXXXX escape sequence inside a Unicode string literal. We find the integer ordinal\n",
      "of a character using ord(). For example:\n",
      ">>> ord('a')\n",
      "97\n",
      "The hexadecimal four-digit notation for 97 is 0061, so we can define a Unicode string\n",
      "literal with the appropriate escape sequence:\n",
      ">>> a = u'\\u0061'\n",
      ">>> a\n",
      "u'a'\n",
      ">>> print a\n",
      "a\n",
      "3.3  Text Processing with Unicode | 95...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 116, 'page_label': '95', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4453}\n",
      "\n",
      "--- Chunk 4454 ---\n",
      "Content:\n",
      "Notice that the Python print statement is assuming a default encoding of the Unicode\n",
      "character, namely ASCII. However, ń is outside the ASCII range, so cannot be printed\n",
      "unless we specify an encoding. In the following example, we have specified that\n",
      "print should use the repr() of the string, which outputs the UTF-8 escape sequences\n",
      "(of the form \\xXX) rather than trying to render the glyphs.\n",
      ">>> nacute = u'\\u0144'\n",
      ">>> nacute\n",
      "u'\\u0144'\n",
      ">>> nacute_utf = nacute.encode('utf8')\n",
      ">>> print repr(nacute_utf)\n",
      "'\\xc5\\x84'\n",
      "If your operating system and locale are set up to render UTF-8 encoded characters, you\n",
      "ought to be able to give the Python command print nacute_utf and see ń on your\n",
      "screen.\n",
      "There are many factors determining what glyphs are rendered on your\n",
      "screen. \n",
      "If you are sure that you have the correct encoding, but your\n",
      "Python code is still failing to produce the glyphs you expected, you\n",
      "should also check that you have the necessary fonts installed on your\n",
      "system.\n",
      "The module unicodedata lets us inspect the properties of Unicode characters. In the\n",
      "following example, we select all characters in the third line of our Polish text outside\n",
      "the ASCII range and print their UTF-8 escaped value, followed by their code point\n",
      "integer using the standard Unicode convention (i.e., prefixing the hex digits with U+),\n",
      "followed by their Unicode name.\n",
      ">>> import unicodedata\n",
      ">>> lines = codecs.open(path, encoding='latin2').readlines()\n",
      ">>> line = lines[2]\n",
      ">>> print line.encode('unicode_escape')\n",
      "Niemc\\xf3w pod koniec II wojny \\u015bwiatowej na Dolny \\u015al\\u0105sk, zosta\\u0142y\\n\n",
      ">>> for c in line:\n",
      "...     if ord(c) > 127:\n",
      "...         print '%r U+%04x %s' % (c.encode('utf8'), ord(c), unicodedata.name(c))\n",
      "'\\xc3\\xb3' U+00f3 LATIN SMALL LETTER O WITH ACUTE\n",
      "'\\xc5\\x9b' U+015b LATIN SMALL LETTER S WITH ACUTE\n",
      "'\\xc5\\x9a' U+015a LATIN CAPITAL LETTER S WITH ACUTE\n",
      "'\\xc4\\x85' U+0105 LATIN SMALL LETTER A WITH OGONEK\n",
      "'\\xc5\\x82' U+0142 LATIN SMALL LETTER L WITH STROKE\n",
      "If you replace the %r (which yields the repr() value) by %s in the format string of the\n",
      "preceding code sample, and if your system supports UTF-8, you should see an output\n",
      "like the following:\n",
      "ó U+00f3 LATIN SMALL LETTER O WITH ACUTE\n",
      "ś U+015b LATIN SMALL LETTER S WITH ACUTE\n",
      "Ś U+015a LATIN CAPITAL LETTER S WITH ACUTE\n",
      "96 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 117, 'page_label': '96', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4454}\n",
      "\n",
      "--- Chunk 4455 ---\n",
      "Content:\n",
      "ą U+0105 LATIN SMALL LETTER A WITH OGONEK\n",
      "ł U+0142 LATIN SMALL LETTER L WITH STROKE\n",
      "Alternatively, \n",
      "you may need to replace the encoding 'utf8' in the example by\n",
      "'latin2', again depending on the details of your system.\n",
      "The next examples illustrate how Python string methods and the re module accept\n",
      "Unicode strings.\n",
      ">>> line.find(u'zosta\\u0142y')\n",
      "54\n",
      ">>> line = line.lower()\n",
      ">>> print line.encode('unicode_escape')\n",
      "niemc\\xf3w pod koniec ii wojny \\u015bwiatowej na dolny \\u015bl\\u0105sk, zosta\\u0142y\\n\n",
      ">>> import re\n",
      ">>> m = re.search(u'\\u015b\\w*', line)\n",
      ">>> m.group()\n",
      "u'\\u015bwiatowej'\n",
      "NLTK tokenizers allow Unicode strings as input, and correspondingly yield Unicode\n",
      "strings as output.\n",
      ">>> nltk.word_tokenize(line)  \n",
      "[u'niemc\\xf3w', u'pod', u'koniec', u'ii', u'wojny', u'\\u015bwiatowej',\n",
      "u'na', u'dolny', u'\\u015bl\\u0105sk', u'zosta\\u0142y']\n",
      "Using Your Local Encoding in Python\n",
      "If you are used to working with characters in a particular local encoding, you probably\n",
      "want to be able to use your standard methods for inputting and editing strings in a\n",
      "Python file. In order to do this, you need to include the string '# -*- coding: <coding>\n",
      "-*-' as the first or second line of your file. Note that <coding> has to be a string like\n",
      "'latin-1', 'big5', or 'utf-8' (see Figure 3-4).\n",
      "Figure 3-4 also illustrates how regular expressions can use encoded strings.\n",
      "3.4  Regular Expressions for Detecting Word Patterns\n",
      "Many linguistic processing tasks involve pattern matching. For example, we can find\n",
      "words ending with ed using endswith('ed'). We saw a variety of such “word tests” in\n",
      "Table 1-4. Regular expressions give us a more powerful and flexible method for de-\n",
      "scribing the character patterns we are interested in.\n",
      "There are many other published introductions to regular expressions,\n",
      "organized \n",
      "around the syntax of regular expressions and applied to\n",
      "searching text files. Instead of doing this again, we focus on the use of\n",
      "regular expressions at different stages of linguistic processing. As usual,\n",
      "we’ll adopt a problem-based approach and present new features only as\n",
      "they are needed to solve practical problems. In our discussion we will\n",
      "mark regular expressions using chevrons like this: «patt».\n",
      "3.4  Regular Expressions for Detecting Word Patterns | 97...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 118, 'page_label': '97', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4455}\n",
      "\n",
      "--- Chunk 4456 ---\n",
      "Content:\n",
      "To use regular expressions in Python, we need to import the re library using: import\n",
      "re. We also need a list of words to search; we’ll use the Words Corpus again ( Sec-\n",
      "tion 2.4). We will preprocess it to remove any proper names.\n",
      ">>> import re\n",
      ">>> wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
      "Using Basic Metacharacters\n",
      "Let’s find words ending with ed using the regular expression « ed$». We will use the\n",
      "re.search(p, s) function to check whether the pattern p can be found somewhere\n",
      "inside the string s. We need to specify the characters of interest, and use the dollar sign,\n",
      "which has a special behavior in the context of regular expressions in that it matches the\n",
      "end of the word:\n",
      ">>> [w for w in wordlist if re.search('ed$', w)]\n",
      "['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', ...]\n",
      "The . wildcard symbol matches any single character. Suppose we have room in a\n",
      "crossword puzzle for an eight-letter word, with j as its third letter and t as its sixth letter.\n",
      "In place of each blank cell we use a period:\n",
      ">>> [w for w in wordlist if re.search('^..j..t..$', w)]\n",
      "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', ...]\n",
      "Figure 3-4. Unicode and IDLE: UTF-8 encoded string literals in the IDLE editor; this requires that\n",
      "an appropriate font is set in IDLE’s preferences; here we have chosen Courier CE.\n",
      "98 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 119, 'page_label': '98', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4456}\n",
      "\n",
      "--- Chunk 4457 ---\n",
      "Content:\n",
      "Your Turn: The caret symbol ^ matches the start of a string, just like\n",
      "the $ matches the end. What results do we get with the example just\n",
      "shown if we leave out both of these, and search for «..j..t..»?\n",
      "Finally, the ? symbol specifies that the previous character is optional. Thus «^e-?mail\n",
      "$» will match both email and e-mail. We could count the total number of occurrences\n",
      "of this word (in either spelling) in a text using sum(1 for w in text if re.search('^e-?\n",
      "mail$', w)).\n",
      "Ranges and Closures\n",
      "The T9 system is used for entering text on mobile phones (see Figure 3-5). Two or more\n",
      "words that are entered with the same sequence of keystrokes are known as\n",
      "textonyms. For example, both hole and golf are entered by pressing the sequence 4653.\n",
      "What other words could be produced with the same sequence? Here we use the regular\n",
      "expression «^[ghi][mno][jlk][def]$»:\n",
      ">>> [w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]\n",
      "['gold', 'golf', 'hold', 'hole']\n",
      "The first part of the expression, «^[ghi]», matches the start of a word followed by g,\n",
      "h, or i. The next part of the expression, «[mno]», constrains the second character to be m,\n",
      "n, or o. The third and fourth characters are also constrained. Only four words satisfy\n",
      "all these constraints. Note that the order of characters inside the square brackets is not\n",
      "significant, so we could have written «^[hig][nom][ljk][fed]$» and matched the same\n",
      "words.\n",
      "Figure 3-5. T9: Text on 9 keys.\n",
      "Your Turn: Look for some “finger-twisters,” by searching for words\n",
      "that use only part of the number-pad. For example « ^[ghijklmno]+$»,\n",
      "or more concisely, «^[g-o]+$», will match words that only use keys 4,\n",
      "5, 6 in the center row, and «^[a-fj-o]+$» will match words that use keys\n",
      "2, 3, 5, 6 in the top-right corner. What do - and + mean?\n",
      "3.4  Regular Expressions for Detecting Word Patterns | 99...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 120, 'page_label': '99', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4457}\n",
      "\n",
      "--- Chunk 4458 ---\n",
      "Content:\n",
      "Let’s explore the + symbol a bit further. Notice that it can be applied to individual\n",
      "letters, or to bracketed sets of letters:\n",
      ">>> chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
      ">>> [w for w in chat_words if re.search('^m+i+n+e+$', w)]\n",
      "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine',\n",
      "'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\n",
      ">>> [w for w in chat_words if re.search('^[ha]+$', w)]\n",
      "['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh',\n",
      "'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'h', 'ha', 'haaa',\n",
      "'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', ...]\n",
      "It should be clear that + simply means “one or more instances of the preceding item,”\n",
      "which could be an individual character like m, a set like [fed], or a range like [d-f].\n",
      "Now let’s replace + with *, which means “zero or more instances of the preceding item.”\n",
      "The regular expression «^m*i*n*e*$» will match everything that we found using «^m+i\n",
      "+n+e+$», but also words where some of the letters don’t appear at all, e.g., me, min, and\n",
      "mmmmm. Note that the + and * symbols are sometimes referred to as Kleene clo-\n",
      "sures, or simply closures.\n",
      "The ^ operator has another function when it appears as the first character inside square\n",
      "brackets. For example, «[^aeiouAEIOU]» matches any character other than a vowel. We\n",
      "can search the NPS Chat Corpus for words that are made up entirely of non-vowel\n",
      "characters using «^[^aeiouAEIOU]+$» to find items like these: :):):), grrr, cyb3r, and\n",
      "zzzzzzzz...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 121, 'page_label': '100', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4458}\n",
      "\n",
      "--- Chunk 4459 ---\n",
      "Content:\n",
      ". Note that the + and * symbols are sometimes referred to as Kleene clo-\n",
      "sures, or simply closures.\n",
      "The ^ operator has another function when it appears as the first character inside square\n",
      "brackets. For example, «[^aeiouAEIOU]» matches any character other than a vowel. We\n",
      "can search the NPS Chat Corpus for words that are made up entirely of non-vowel\n",
      "characters using «^[^aeiouAEIOU]+$» to find items like these: :):):), grrr, cyb3r, and\n",
      "zzzzzzzz. Notice this includes non-alphabetic characters.\n",
      "Here are some more examples of regular expressions being used to find tokens that\n",
      "match a particular pattern, illustrating the use of some new symbols: \\, {}, (), and |.\n",
      ">>> wsj = sorted(set(nltk.corpus.treebank.words()))\n",
      ">>> [w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]\n",
      "['0.0085', '0.05', '0.1', '0.16', '0.2', '0.25', '0.28', '0.3', '0.4', '0.5',\n",
      "'0.50', '0.54', '0.56', '0.60', '0.7', '0.82', '0.84', '0.9', '0.95', '0.99',\n",
      "'1.01', '1.1', '1.125', '1.14', '1.1650', '1.17', '1.18', '1.19', '1.2', ...]\n",
      ">>> [w for w in wsj if re.search('^[A-Z]+\\$$', w)]\n",
      "['C$', 'US$']\n",
      ">>> [w for w in wsj if re.search('^[0-9]{4}$', w)]\n",
      "['1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', ...]\n",
      ">>> [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]\n",
      "['10-day', '10-lap', '10-year', '100-share', '12-point', '12-year', ...]\n",
      ">>> [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]\n",
      "['black-and-white', 'bread-and-butter', 'father-in-law', 'machine-gun-toting',\n",
      "'savings-and-loan']\n",
      ">>> [w for w in wsj if re.search('(ed|ing)$', w)]\n",
      "['62%-owned', 'Absorbed', 'According', 'Adopting', 'Advanced', 'Advancing', ...]\n",
      "Your Turn: Study the previous examples and try to work out what the \\,\n",
      "{}, (), and | notations mean before you read on.\n",
      "100 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 121, 'page_label': '100', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4459}\n",
      "\n",
      "--- Chunk 4460 ---\n",
      "Content:\n",
      "You probably worked out that a backslash means that the following character is de-\n",
      "prived \n",
      "of its special powers and must literally match a specific character in the word.\n",
      "Thus, while . is special, \\. only matches a period. The braced expressions, like {3,5},\n",
      "specify the number of repeats of the previous item. The pipe character indicates a choice\n",
      "between the material on its left or its right. Parentheses indicate the scope of an oper-\n",
      "ator, and they can be used together with the pipe (or disjunction) symbol like this:\n",
      "«w(i|e|ai|oo)t», matching wit, wet, wait, and woot. It is instructive to see what happens\n",
      "when you omit the parentheses from the last expression in the example, and search for\n",
      "«ed|ing$».\n",
      "The metacharacters we have seen are summarized in Table 3-3.\n",
      "Table 3-3. Basic regular expression metacharacters, including wildcards, ranges, and closures\n",
      "Operator Behavior\n",
      ". Wildcard, matches any character\n",
      "^abc Matches some pattern abc at the start of a string\n",
      "abc$ Matches some pattern abc at the end of a string\n",
      "[abc] Matches one of a set of characters\n",
      "[A-Z0-9] Matches one of a range of characters\n",
      "ed|ing|s Matches one of the specified strings (disjunction)\n",
      "* Zero or more of previous item, e.g., a*, [a-z]* (also known as Kleene Closure)\n",
      "+ One or more of previous item, e.g., a+, [a-z]+\n",
      "? Zero or one of the previous item (i.e., optional), e.g., a?, [a-z]?\n",
      "{n} Exactly n repeats where n is a non-negative integer\n",
      "{n,} At least n repeats\n",
      "{,n} No more than n repeats\n",
      "{m,n} At least m and no more than n repeats\n",
      "a(b|c)+ Parentheses that indicate the scope of the operators\n",
      "To the Python interpreter, a regular expression is just like any other string. If the string\n",
      "contains \n",
      "a backslash followed by particular characters, it will interpret these specially.\n",
      "For example, \\b would be interpreted as the backspace character. In general, when\n",
      "using regular expressions containing backslash, we should instruct the interpreter not\n",
      "to look inside the string at all, but simply to pass it directly to the re library for pro-\n",
      "cessing. We do this by prefixing the string with the letter r, to indicate that it is a raw\n",
      "string. For example, the raw string r'\\band\\b' contains two \\b symbols that are\n",
      "interpreted by the re library as matching word boundaries instead of backspace char-\n",
      "acters. If you get into the habit of using r'...' for regular expressions—as we will do\n",
      "from now on—you will avoid having to think about these complications.\n",
      "3.4  Regular Expressions for Detecting Word Patterns | 101...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 122, 'page_label': '101', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4460}\n",
      "\n",
      "--- Chunk 4461 ---\n",
      "Content:\n",
      "3.5  Useful Applications of Regular Expressions\n",
      "The \n",
      "previous examples all involved searching for words w that match some regular\n",
      "expression regexp using re.search(regexp, w). Apart from checking whether a regular\n",
      "expression matches a word, we can use regular expressions to extract material from\n",
      "words, or to modify words in specific ways.\n",
      "Extracting Word Pieces\n",
      "The re.findall() (“find all”) method finds all (non-overlapping) matches of the given\n",
      "regular expression. Let’s find all the vowels in a word, then count them:\n",
      ">>> word = 'supercalifragilisticexpialidocious'\n",
      ">>> re.findall(r'[aeiou]', word)\n",
      "['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']\n",
      ">>> len(re.findall(r'[aeiou]', word))\n",
      "16\n",
      "Let’s look for all sequences of two or more vowels in some text, and determine their\n",
      "relative frequency:\n",
      ">>> wsj = sorted(set(nltk.corpus.treebank.words()))\n",
      ">>> fd = nltk.FreqDist(vs for word in wsj\n",
      "...                       for vs in re.findall(r'[aeiou]{2,}', word))\n",
      ">>> fd.items()\n",
      "[('io', 549), ('ea', 476), ('ie', 331), ('ou', 329), ('ai', 261), ('ia', 253),\n",
      "('ee', 217), ('oo', 174), ('ua', 109), ('au', 106), ('ue', 105), ('ui', 95),\n",
      "('ei', 86), ('oi', 65), ('oa', 59), ('eo', 39), ('iou', 27), ('eu', 18), ...]\n",
      "Your Turn: In the W3C Date Time Format, dates are represented like\n",
      "this: \n",
      "2009-12-31. Replace the ? in the following Python code with a\n",
      "regular expression, in order to convert the string '2009-12-31' to a list\n",
      "of integers [2009, 12, 31]:\n",
      "[int(n) for n in re.findall(?, '2009-12-31')]\n",
      "Doing More with Word Pieces\n",
      "Once we can use re.findall() to extract material from words, there are interesting\n",
      "things to do with the pieces, such as glue them back together or plot them.\n",
      "It is sometimes noted that English text is highly redundant, and it is still easy to read\n",
      "when word-internal vowels are left out. For example, declaration becomes dclrtn, and\n",
      "inalienable becomes inlnble, retaining any initial or final vowel sequences. The regular\n",
      "expression in our next example matches initial vowel sequences, final vowel sequences,\n",
      "and all consonants; everything else is ignored. This three-way disjunction is processed\n",
      "left-to-right, and if one of the three parts matches the word, any later parts of the regular\n",
      "expression are ignored. We use re.findall() to extract all the matching pieces, and\n",
      "''.join() to join them together (see Section 3.9 for more about the join operation).\n",
      "102 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 123, 'page_label': '102', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4461}\n",
      "\n",
      "--- Chunk 4462 ---\n",
      "Content:\n",
      ">>> regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\n",
      ">>> def compress(word):\n",
      "...     pieces = re.findall(regexp, word)\n",
      "...     return ''.join(pieces)\n",
      "...\n",
      ">>> english_udhr = nltk.corpus.udhr.words('English-Latin1')\n",
      ">>> print nltk.tokenwrap(compress(w) for w in english_udhr[:75])\n",
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
      "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
      "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n",
      "Next, \n",
      "let’s combine regular expressions with conditional frequency distributions. Here\n",
      "we will extract all consonant-vowel sequences from the words of Rotokas, such as ka\n",
      "and si. Since each of these is a pair, it can be used to initialize a conditional frequency\n",
      "distribution. We then tabulate the frequency of each pair:\n",
      ">>> rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
      ">>> cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
      ">>> cfd = nltk.ConditionalFreqDist(cvs)\n",
      ">>> cfd.tabulate()\n",
      "     a    e    i    o    u\n",
      "k  418  148   94  420  173\n",
      "p   83   31  105   34   51\n",
      "r  187   63   84   89   79\n",
      "s    0    0  100    2    1\n",
      "t   47    8    0  148   37\n",
      "v   93   27  105   48   49\n",
      "Examining the rows for s and t, we see they are in partial “complementary distribution,”\n",
      "which is evidence that they are not distinct phonemes in the language. Thus, we could\n",
      "conceivably drop s from the Rotokas alphabet and simply have a pronunciation rule\n",
      "that the letter t is pronounced s when followed by i. (Note that the single entry having\n",
      "su, namely kasuari, ‘cassowary’ is borrowed from English).\n",
      "If we want to be able to inspect the words behind the numbers in that table, it would\n",
      "be helpful to have an index, allowing us to quickly find the list of words that contains\n",
      "a given consonant-vowel pair. For example, cv_index['su'] should give us all words\n",
      "containing su. Here’s how we can do this:\n",
      ">>> cv_word_pairs = [(cv, w) for w in rotokas_words\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 124, 'page_label': '103', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4462}\n",
      "\n",
      "--- Chunk 4463 ---\n",
      "Content:\n",
      ". (Note that the single entry having\n",
      "su, namely kasuari, ‘cassowary’ is borrowed from English).\n",
      "If we want to be able to inspect the words behind the numbers in that table, it would\n",
      "be helpful to have an index, allowing us to quickly find the list of words that contains\n",
      "a given consonant-vowel pair. For example, cv_index['su'] should give us all words\n",
      "containing su. Here’s how we can do this:\n",
      ">>> cv_word_pairs = [(cv, w) for w in rotokas_words\n",
      "...                          for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
      ">>> cv_index = nltk.Index(cv_word_pairs)\n",
      ">>> cv_index['su']\n",
      "['kasuari']\n",
      ">>> cv_index['po']\n",
      "['kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa',\n",
      "'kapokao', 'kapokapo', 'kapokapo', 'kapokapoa', 'kapokapoa', 'kapokapora', ...]\n",
      "This program processes each word w in turn, and for each one, finds every substring\n",
      "that matches the regular expression « [ptksvr][aeiou]». In the case of the word ka-\n",
      "suari, it finds ka, su, and ri. Therefore, the cv_word_pairs list will contain ('ka', 'ka\n",
      "3.5  Useful Applications of Regular Expressions | 103...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 124, 'page_label': '103', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4463}\n",
      "\n",
      "--- Chunk 4464 ---\n",
      "Content:\n",
      "suari'), ('su', 'kasuari'), and ('ri', 'kasuari'). One further step, using\n",
      "nltk.Index(), converts this into a useful index.\n",
      "Finding Word Stems\n",
      "When we use a web search engine, we usually don’t mind (or even notice) if the words\n",
      "in the document differ from our search terms in having different endings. A query for\n",
      "laptops finds documents containing laptop and vice versa. Indeed, laptop and laptops\n",
      "are just two forms of the same dictionary word (or lemma). For some language pro-\n",
      "cessing tasks we want to ignore word endings, and just deal with word stems.\n",
      "There are various ways we can pull out the stem of a word. Here’s a simple-minded\n",
      "approach that just strips off anything that looks like a suffix:\n",
      ">>> def stem(word):\n",
      "...     for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
      "...         if word.endswith(suffix):\n",
      "...             return word[:-len(suffix)]\n",
      "...     return word\n",
      "Although we will ultimately use NLTK’s built-in stemmers, it’s interesting to see how\n",
      "we can use regular expressions for this task. Our first step is to build up a disjunction\n",
      "of all the suffixes. We need to enclose it in parentheses in order to limit the scope of\n",
      "the disjunction.\n",
      ">>> re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')\n",
      "['ing']\n",
      "Here, re.findall() just gave us the suffix even though the regular expression matched\n",
      "the entire word. This is because the parentheses have a second function, to select sub-\n",
      "strings to be extracted. If we want to use the parentheses to specify the scope of the\n",
      "disjunction, but not to select the material to be output, we have to add ?:, which is just\n",
      "one of many arcane subtleties of regular expressions. Here’s the revised version.\n",
      ">>> re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')\n",
      "['processing']\n",
      "However, we’d actually like to split the word into stem and suffix. So we should just\n",
      "parenthesize both parts of the regular expression:\n",
      ">>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')\n",
      "[('process', 'ing')]\n",
      "This looks promising, but still has a problem. Let’s look at a different word, processes:\n",
      ">>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')\n",
      "[('processe', 's')]\n",
      "The regular expression incorrectly found an -s suffix instead of an -es suffix. This dem-\n",
      "onstrates another subtlety: the star operator is “greedy” and so the .* part of the ex-\n",
      "pression tries to consume as much of the input as possible. If we use the “non-greedy”\n",
      "version of the star operator, written *?, we get what we want:\n",
      "104 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 125, 'page_label': '104', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4464}\n",
      "\n",
      "--- Chunk 4465 ---\n",
      "Content:\n",
      ">>> re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')\n",
      "[('process', 'es')]\n",
      "This \n",
      "works even when we allow an empty suffix, by making the content of the second\n",
      "parentheses optional:\n",
      ">>> re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')\n",
      "[('language', '')]\n",
      "This approach still has many problems (can you spot them?), but we will move on to\n",
      "define a function to perform stemming, and apply it to a whole text:\n",
      ">>> def stem(word):\n",
      "...     regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
      "...     stem, suffix = re.findall(regexp, word)[0]\n",
      "...     return stem\n",
      "...\n",
      ">>> raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
      "... is no basis for a system of government.  Supreme executive power derives from\n",
      "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
      ">>> tokens = nltk.word_tokenize(raw)\n",
      ">>> [stem(t) for t in tokens]\n",
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'ly', 'in', 'pond',\n",
      "'distribut', 'sword', 'i', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern',\n",
      "'.', 'Supreme', 'execut', 'power', 'deriv', 'from', 'a', 'mandate', 'from',\n",
      "'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "Notice that our regular expression removed the s from ponds but also from is and\n",
      "basis. It produced some non-words, such as distribut and deriv, but these are acceptable\n",
      "stems in some applications.\n",
      "Searching Tokenized Text\n",
      "You can use a special kind of regular expression for searching across multiple words in\n",
      "a text (where a text is a list of tokens). For example, \"<a> <man>\" finds all instances of\n",
      "a man in the text. The angle brackets are used to mark token boundaries, and any\n",
      "whitespace between the angle brackets is ignored (behaviors that are unique to NLTK’s\n",
      "findall() method for texts). In the following example, we include <.*> \n",
      ", which will\n",
      "match any single token, and enclose it in parentheses so only the matched word (e.g.,\n",
      "monied) \n",
      "and not the matched phrase (e.g., a monied man) is produced. The second\n",
      "example finds three-word phrases ending with the word bro...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 126, 'page_label': '105', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4465}\n",
      "\n",
      "--- Chunk 4466 ---\n",
      "Content:\n",
      ". For example, \"<a> <man>\" finds all instances of\n",
      "a man in the text. The angle brackets are used to mark token boundaries, and any\n",
      "whitespace between the angle brackets is ignored (behaviors that are unique to NLTK’s\n",
      "findall() method for texts). In the following example, we include <.*> \n",
      ", which will\n",
      "match any single token, and enclose it in parentheses so only the matched word (e.g.,\n",
      "monied) \n",
      "and not the matched phrase (e.g., a monied man) is produced. The second\n",
      "example finds three-word phrases ending with the word bro \n",
      " . The last example finds\n",
      "sequences of three or more words starting with the letter l \n",
      " .\n",
      ">>> from nltk.corpus import gutenberg, nps_chat\n",
      ">>> moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
      ">>> moby.findall(r\"<a> (<.*>) <man>\") \n",
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n",
      ">>> chat = nltk.Text(nps_chat.words())\n",
      ">>> chat.findall(r\"<.*> <.*> <bro>\") \n",
      "you rule bro; telling you bro; u twizted bro\n",
      "3.5  Useful Applications of Regular Expressions | 105...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 126, 'page_label': '105', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4466}\n",
      "\n",
      "--- Chunk 4467 ---\n",
      "Content:\n",
      ">>> chat.findall(r\"<l.*>{3,}\") \n",
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n",
      "Your Turn: Consolidate your understanding of regular expression pat-\n",
      "terns and substitutions using nltk.re_show(p, s), which annotates the\n",
      "string s to show every place where pattern p was matched, and\n",
      "nltk.app.nemo(), which provides a graphical interface for exploring reg-\n",
      "ular expressions. For more practice, try some of the exercises on regular\n",
      "expressions at the end of this chapter.\n",
      "It is easy to build search patterns when the linguistic phenomenon we’re studying is\n",
      "tied to particular words. In some cases, a little creativity will go a long way. For instance,\n",
      "searching a large text corpus for expressions of the form x and other ys  allows us to\n",
      "discover hypernyms (see Section 2.5):\n",
      ">>> from nltk.corpus import brown\n",
      ">>> hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
      ">>> hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")\n",
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n",
      "With enough text, this approach would give us a useful store of information about the\n",
      "taxonomy of objects, without the need for any manual labor. However, our search\n",
      "results will usually contain false positives, i.e., cases that we would want to exclude.\n",
      "For example, the result demands and other factors suggests that demand is an instance\n",
      "of the type factor, but this sentence is actually about wage demands. Nevertheless, we\n",
      "could construct our own ontology of English concepts by manually correcting the out-\n",
      "put of such searches.\n",
      "This combination of automatic and manual processing is the most com-\n",
      "mon \n",
      "way for new corpora to be constructed. We will return to this in\n",
      "Chapter 11.\n",
      "Searching corpora also suffers from the problem of false negatives, i.e., omitting cases\n",
      "that we would want to include. It is risky to conclude that some linguistic phenomenon\n",
      "doesn’t exist in a corpus just because we couldn’t find any instances of a search pattern.\n",
      "Perhaps we just didn’t think carefully enough about suitable patterns.\n",
      "Your Turn: Look for instances of the pattern as x as y to discover in-\n",
      "formation about entities and their properties.\n",
      "106 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 127, 'page_label': '106', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4467}\n",
      "\n",
      "--- Chunk 4468 ---\n",
      "Content:\n",
      "3.6  Normalizing Text\n",
      "In \n",
      "earlier program examples we have often converted text to lowercase before doing\n",
      "anything with its words, e.g., set(w.lower() for w in text). By using lower(), we have\n",
      "normalized the text to lowercase so that the distinction between The and the is ignored.\n",
      "Often we want to go further than this and strip off any affixes, a task known as stem-\n",
      "ming. A further step is to make sure that the resulting form is a known word in a\n",
      "dictionary, a task known as lemmatization. We discuss each of these in turn. First, we\n",
      "need to define the data we will use in this section:\n",
      ">>> raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
      "... is no basis for a system of government.  Supreme executive power derives from\n",
      "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
      ">>> tokens = nltk.word_tokenize(raw)\n",
      "Stemmers\n",
      "NLTK includes several off-the-shelf stemmers, and if you ever need a stemmer, you\n",
      "should use one of these in preference to crafting your own using regular expressions,\n",
      "since NLTK’s stemmers handle a wide range of irregular cases. The Porter and Lan-\n",
      "caster stemmers follow their own rules for stripping affixes. Observe that the Porter\n",
      "stemmer correctly handles the word lying (mapping it to lie), whereas the Lancaster\n",
      "stemmer does not.\n",
      ">>> porter = nltk.PorterStemmer()\n",
      ">>> lancaster = nltk.LancasterStemmer()\n",
      ">>> [porter.stem(t) for t in tokens]\n",
      "['DENNI', ':', 'Listen', ',', 'strang', 'women', 'lie', 'in', 'pond',\n",
      "'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern',\n",
      "'.', 'Suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from',\n",
      "'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      ">>> [lancaster.stem(t) for t in tokens]\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut',\n",
      "'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem',\n",
      "'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not',\n",
      "'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n",
      "Stemming is not a well-defined process, and we typically pick the stemmer that best\n",
      "suits the application we have in mind. The Porter Stemmer is a good choice if you are\n",
      "indexing some texts and want to support search using alternative forms of words (il-\n",
      "lustrated in Example 3-1, which uses object-oriented programming techniques that are\n",
      "outside the scope of this book, string formatting techniques to be covered in Sec-\n",
      "tion 3.9, and the enumerate() function to be explained in Section 4.2).\n",
      "Example 3-1...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 128, 'page_label': '107', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4468}\n",
      "\n",
      "--- Chunk 4469 ---\n",
      "Content:\n",
      ". The Porter Stemmer is a good choice if you are\n",
      "indexing some texts and want to support search using alternative forms of words (il-\n",
      "lustrated in Example 3-1, which uses object-oriented programming techniques that are\n",
      "outside the scope of this book, string formatting techniques to be covered in Sec-\n",
      "tion 3.9, and the enumerate() function to be explained in Section 4.2).\n",
      "Example 3-1. Indexing a text using a stemmer.\n",
      "class IndexedText(object):\n",
      "    def __init__(self, stemmer, text):\n",
      "        self._text = text\n",
      "        self._stemmer = stemmer\n",
      "3.6  Normalizing Text | 107...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 128, 'page_label': '107', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4469}\n",
      "\n",
      "--- Chunk 4470 ---\n",
      "Content:\n",
      "self._index = nltk.Index((self._stem(word), i)\n",
      "                                 for (i, word) in enumerate(text))\n",
      "    def concordance(self, word, width=40):\n",
      "        key = self._stem(word)\n",
      "        wc = width/4                # words of context\n",
      "        for i in self._index[key]:\n",
      "            lcontext = ' '.join(self._text[i-wc:i])\n",
      "            rcontext = ' '.join(self._text[i:i+wc])\n",
      "            ldisplay = '%*s'  % (width, lcontext[-width:])\n",
      "            rdisplay = '%-*s' % (width, rcontext[:width])\n",
      "            print ldisplay, rdisplay\n",
      "    def _stem(self, word):\n",
      "        return self._stemmer.stem(word).lower()\n",
      ">>> porter = nltk.PorterStemmer()\n",
      ">>> grail = nltk.corpus.webtext.words('grail.txt')\n",
      ">>> text = IndexedText(porter, grail)\n",
      ">>> text.concordance('lie')\n",
      "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n",
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !\n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well\n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which\n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n",
      "Lemmatization\n",
      "The \n",
      "WordNet lemmatizer removes affixes only if the resulting word is in its dictionary.\n",
      "This additional checking process makes the lemmatizer slower than the stemmers just\n",
      "mentioned...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 129, 'page_label': '108', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4470}\n",
      "\n",
      "--- Chunk 4471 ---\n",
      "Content:\n",
      ". [ clap clap ] PIGLET : Well\n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which\n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n",
      "Lemmatization\n",
      "The \n",
      "WordNet lemmatizer removes affixes only if the resulting word is in its dictionary.\n",
      "This additional checking process makes the lemmatizer slower than the stemmers just\n",
      "mentioned. Notice that it doesn’t handle lying, but it converts women to woman.\n",
      ">>> wnl = nltk.WordNetLemmatizer()\n",
      ">>> [wnl.lemmatize(t) for t in tokens]\n",
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond',\n",
      "'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of',\n",
      "'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a',\n",
      "'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical',\n",
      "'aquatic', 'ceremony', '.']\n",
      "The WordNet lemmatizer is a good choice if you want to compile the vocabulary of\n",
      "some texts and want a list of valid lemmas (or lexicon headwords).\n",
      "Another normalization task involves identifying non-standard\n",
      "words, \n",
      "including numbers, abbreviations, and dates, and mapping any\n",
      "such tokens to a special vocabulary. For example, every decimal number\n",
      "could be mapped to a single token 0.0, and every acronym could be\n",
      "mapped to AAA. This keeps the vocabulary small and improves the ac-\n",
      "curacy of many language modeling tasks.\n",
      "108 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 129, 'page_label': '108', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4471}\n",
      "\n",
      "--- Chunk 4472 ---\n",
      "Content:\n",
      "3.7  Regular Expressions for Tokenizing Text\n",
      "Tokenization \n",
      "is the task of cutting a string into identifiable linguistic units that consti-\n",
      "tute a piece of language data. Although it is a fundamental task, we have been able to\n",
      "delay it until now because many corpora are already tokenized, and because NLTK\n",
      "includes some tokenizers. Now that you are familiar with regular expressions, you can\n",
      "learn how to use them to tokenize text, and to have much more control over the process.\n",
      "Simple Approaches to Tokenization\n",
      "The very simplest method for tokenizing text is to split on whitespace. Consider the\n",
      "following text from Alice’s Adventures in Wonderland:\n",
      ">>> raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
      "... though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
      "... well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\n",
      "We could split this raw text on whitespace using raw.split(). To do the same using a\n",
      "regular expression, it is not enough to match any space characters in the string \n",
      ", since\n",
      "this \n",
      "results in tokens that contain a \\n newline character; instead, we need to match\n",
      "any number of spaces, tabs, or newlines \n",
      " :\n",
      ">>> re.split(r' ', raw) \n",
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in',\n",
      "'a', 'very', 'hopeful', 'tone\\nthough),', \"'I\", \"won't\", 'have', 'any', 'pepper',\n",
      "'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\\nwell', 'without--Maybe',\n",
      "\"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n",
      ">>> re.split(r'[ \\t\\n]+', raw) \n",
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in',\n",
      "'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper',\n",
      "'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe',\n",
      "\"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n",
      "The regular expression «[ \\t\\n]+» \n",
      "matches one or more spaces, tabs (\\t), or newlines\n",
      "(\\n). Other whitespace characters, such as carriage return and form feed, should really\n",
      "be included too. Instead, we will use a built-in re abbreviation, \\s, which means any\n",
      "whitespace character. The second statement in the preceding example can be rewritten\n",
      "as re.split(r'\\s+', raw).\n",
      "Important: Remember to prefix regular expressions with the letter r\n",
      "(meaning “raw”), which instructs the Python interpreter to treat the\n",
      "string literally, rather than processing any backslashed characters it\n",
      "contains.\n",
      "Splitting on whitespace gives us tokens like '(not' and 'herself,'...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 130, 'page_label': '109', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4472}\n",
      "\n",
      "--- Chunk 4473 ---\n",
      "Content:\n",
      ". Other whitespace characters, such as carriage return and form feed, should really\n",
      "be included too. Instead, we will use a built-in re abbreviation, \\s, which means any\n",
      "whitespace character. The second statement in the preceding example can be rewritten\n",
      "as re.split(r'\\s+', raw).\n",
      "Important: Remember to prefix regular expressions with the letter r\n",
      "(meaning “raw”), which instructs the Python interpreter to treat the\n",
      "string literally, rather than processing any backslashed characters it\n",
      "contains.\n",
      "Splitting on whitespace gives us tokens like '(not' and 'herself,'. An alternative is to\n",
      "use the fact that Python provides us with a character class \\w for word characters,\n",
      "equivalent to [a-zA-Z0-9_]. It also defines the complement of this class, \\W, i.e., all\n",
      "3.7  Regular Expressions for Tokenizing Text | 109...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 130, 'page_label': '109', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4473}\n",
      "\n",
      "--- Chunk 4474 ---\n",
      "Content:\n",
      "characters other than letters, digits, or underscore. We can use \\W in a simple regular\n",
      "expression to split the input on anything other than a word character:\n",
      ">>> re.split(r'\\W+', raw)\n",
      "['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in',\n",
      "'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper',\n",
      "'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without',\n",
      "'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered',\n",
      "'']\n",
      "Observe that this gives us empty strings at the start and the end (to understand why,\n",
      "try doing 'xx'.split('x')). With re.findall(r'\\w+', raw), we get the same tokens,\n",
      "but without the empty strings, using a pattern that matches the words instead of the\n",
      "spaces. Now that we’re matching the words, we’re in a position to extend the regular\n",
      "expression to cover a wider range of cases. The regular expression «\\w+|\\S\\w*» will first\n",
      "try to match any sequence of word characters. If no match is found, it will try to match\n",
      "any non-whitespace character (\\S is the complement of \\s) followed by further word\n",
      "characters. This means that punctuation is grouped with any following letters\n",
      "(e.g., ’s) but that sequences of two or more punctuation characters are separated.\n",
      ">>> re.findall(r'\\w+|\\S\\w*', raw)\n",
      "[\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',',\n",
      "'(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\",\n",
      "'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does',\n",
      "'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that',\n",
      "'makes', 'people', 'hot', '-tempered', ',', \"'\", '.', '.', '.']\n",
      "Let’s generalize the \\w+ in the preceding expression to permit word-internal hyphens\n",
      "and apostrophes: «\\w+([-']\\w+)*». This expression means \\w+ followed by zero or more\n",
      "instances of [-']\\w+; it would match hot-tempered and it’s...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 131, 'page_label': '110', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4474}\n",
      "\n",
      "--- Chunk 4475 ---\n",
      "Content:\n",
      ". This expression means \\w+ followed by zero or more\n",
      "instances of [-']\\w+; it would match hot-tempered and it’s. (We need to include ?: in\n",
      "this expression for reasons discussed earlier.) We’ll also add a pattern to match quote\n",
      "characters so these are kept separate from the text they enclose.\n",
      ">>> print re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw)\n",
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',',\n",
      "'(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I',\n",
      "\"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup',\n",
      "'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper',\n",
      "'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n",
      "The expression in this example also included « [-.(]+», which causes the double hy-\n",
      "phen, ellipsis, and open parenthesis to be tokenized separately.\n",
      "Table 3-4 lists the regular expression character class symbols we have seen in this sec-\n",
      "tion, in addition to some other useful symbols.\n",
      "Table 3-4. Regular expression symbols\n",
      "Symbol Function\n",
      "\\b Word boundary (zero width)\n",
      "\\d Any decimal digit (equivalent to [0-9])\n",
      "110 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 131, 'page_label': '110', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4475}\n",
      "\n",
      "--- Chunk 4476 ---\n",
      "Content:\n",
      "Symbol Function\n",
      "\\D Any non-digit character (equivalent to [^0-9])\n",
      "\\s Any whitespace character (equivalent to [ \\t\\n\\r\\f\\v]\n",
      "\\S Any non-whitespace character (equivalent to [^ \\t\\n\\r\\f\\v])\n",
      "\\w Any alphanumeric character (equivalent to [a-zA-Z0-9_])\n",
      "\\W Any non-alphanumeric character (equivalent to [^a-zA-Z0-9_])\n",
      "\\t The tab character\n",
      "\\n The newline character\n",
      "NLTK’s Regular Expression Tokenizer\n",
      "The \n",
      "function nltk.regexp_tokenize() is similar to re.findall() (as we’ve been using\n",
      "it for tokenization). However, nltk.regexp_tokenize() is more efficient for this task,\n",
      "and avoids the need for special treatment of parentheses. For readability we break up\n",
      "the regular expression over several lines and add a comment about each line. The special\n",
      "(?x) “verbose flag” tells Python to strip out the embedded whitespace and comments.\n",
      ">>> text = 'That U.S.A. poster-print costs $12.40...'\n",
      ">>> pattern = r'''(?x)    # set flag to allow verbose regexps\n",
      "...     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
      "...   | \\w+(-\\w+)*        # words with optional internal hyphens\n",
      "...   | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
      "...   | \\.\\.\\.            # ellipsis\n",
      "...   | [][.,;\"'?():-_`]  # these are separate tokens\n",
      "... '''\n",
      ">>> nltk.regexp_tokenize(text, pattern)\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
      "When using the verbose flag, you can no longer use ' ' to match a space character; use\n",
      "\\s instead. The regexp_tokenize() function has an optional gaps parameter. When set\n",
      "to True, the regular expression specifies the gaps between tokens, as with re.split().\n",
      "We can evaluate a tokenizer by comparing the resulting tokens with a\n",
      "wordlist, and then report any tokens that don’t appear in the wordlist,\n",
      "using set(tokens).difference(wordlist). \n",
      "You’ll probably want to\n",
      "lowercase all the tokens first.\n",
      "Further Issues with Tokenization\n",
      "Tokenization turns out to be a far more difficult task than you might have expected.\n",
      "No single solution works well across the board, and we must decide what counts as a\n",
      "token depending on the application domain.\n",
      "When developing a tokenizer it helps to have access to raw text which has been man-\n",
      "ually tokenized, in order to compare the output of your tokenizer with high-quality (or\n",
      "3.7  Regular Expressions for Tokenizing Text | 111...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 132, 'page_label': '111', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4476}\n",
      "\n",
      "--- Chunk 4477 ---\n",
      "Content:\n",
      "“gold-standard”) tokens. The NLTK corpus collection includes a sample of Penn Tree-\n",
      "bank \n",
      "data, including the raw Wall Street Journal  text ( nltk.corpus.tree\n",
      "bank_raw.raw()) and the tokenized version (nltk.corpus.treebank.words()).\n",
      "A final issue for tokenization is the presence of contractions, such as didn’t. If we are\n",
      "analyzing the meaning of a sentence, it would probably be more useful to normalize\n",
      "this form to two separate forms: did and n’t (or not). We can do this work with the help\n",
      "of a lookup table.\n",
      "3.8  Segmentation\n",
      "This section discusses more advanced concepts, which you may prefer to skip on the\n",
      "first time through this chapter.\n",
      "Tokenization is an instance of a more general problem of segmentation. In this section,\n",
      "we will look at two other instances of this problem, which use radically different tech-\n",
      "niques to the ones we have seen so far in this chapter.\n",
      "Sentence Segmentation\n",
      "Manipulating texts at the level of individual words often presupposes the ability to\n",
      "divide a text into individual sentences. As we have seen, some corpora already provide\n",
      "access at the sentence level. In the following example, we compute the average number\n",
      "of words per sentence in the Brown Corpus:\n",
      ">>> len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())\n",
      "20.250994070456922\n",
      "In other cases, the text is available only as a stream of characters. Before tokenizing the\n",
      "text into words, we need to segment it into sentences. NLTK facilitates this by including\n",
      "the Punkt sentence segmenter (Kiss & Strunk, 2006). Here is an example of its use in\n",
      "segmenting the text of a novel. (Note that if the segmenter’s internal data has been\n",
      "updated by the time you read this, you will see different output.)\n",
      ">>> sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      ">>> text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
      ">>> sents = sent_tokenizer.tokenize(text)\n",
      ">>> pprint.pprint(sents[171:181])\n",
      "['\"Nonsense!',\n",
      " '\" said Gregory, who was very rational when anyone else\\nattempted paradox.',\n",
      " '\"Why do all the clerks and navvies in the\\nrailway trains look so sad and tired,...',\n",
      " 'I will\\ntell you.',\n",
      " 'It is because they know that the train is going right.',\n",
      " 'It\\nis because they know that whatever place they have taken a ticket\\nfor that ...',\n",
      " 'It is because after they have\\npassed Sloane Square they know that the next stat...',\n",
      " 'Oh, their wild rapture!',\n",
      " 'oh,\\ntheir eyes like stars and their souls again in Eden, if the next\\nstation w...'\n",
      " '\"\\n\\n\"It is you who are unpoetical,\" replied the poet Syme.']\n",
      "112 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 133, 'page_label': '112', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4477}\n",
      "\n",
      "--- Chunk 4478 ---\n",
      "Content:\n",
      "Notice that this example is really a single sentence, reporting the speech of Mr. Lucian\n",
      "Gregory. \n",
      "However, the quoted speech contains several sentences, and these have been\n",
      "split into individual strings. This is reasonable behavior for most applications.\n",
      "Sentence segmentation is difficult because a period is used to mark abbreviations, and\n",
      "some periods simultaneously mark an abbreviation and terminate a sentence, as often\n",
      "happens with acronyms like U.S.A.\n",
      "For another approach to sentence segmentation, see Section 6.2.\n",
      "Word Segmentation\n",
      "For some writing systems, tokenizing text is made more difficult by the fact that there\n",
      "is no visual representation of word boundaries. For example, in Chinese, the three-\n",
      "character string: 爱国人 (ai4 “love” [verb], guo3 “country”, ren2 “person”) could be\n",
      "tokenized as 爱国 / 人, “country-loving person,” or as 爱 / 国人, “love country-person.”\n",
      "A similar problem arises in the processing of spoken language, where the hearer must\n",
      "segment a continuous speech stream into individual words. A particularly challenging\n",
      "version of this problem arises when we don’t know the words in advance. This is the\n",
      "problem faced by a language learner, such as a child hearing utterances from a parent.\n",
      "Consider the following artificial example, where word boundaries have been removed:\n",
      "(1) a. doyouseethekitty\n",
      "b. seethedoggy\n",
      "c. doyoulikethekitty\n",
      "d. likethedoggy\n",
      "Our first challenge is simply to represent the problem: we need to find a way to separate\n",
      "text content from the segmentation. We can do this by annotating each character with\n",
      "a boolean value to indicate whether or not a word-break appears after the character (an\n",
      "idea that will be used heavily for “chunking” in Chapter 7). Let’s assume that the learner\n",
      "is given the utterance breaks, since these often correspond to extended pauses. Here is\n",
      "a possible representation, including the initial and target segmentations:\n",
      ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
      ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
      ">>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
      "Observe that the segmentation strings consist of zeros and ones. They are one character\n",
      "shorter than the source text, since a text of length n can be broken up in only n–1 places.\n",
      "The segment() function in Example 3-2 demonstrates that we can get back to the orig-\n",
      "inal segmented text from its representation.\n",
      "3.8  Segmentation | 113...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 134, 'page_label': '113', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4478}\n",
      "\n",
      "--- Chunk 4479 ---\n",
      "Content:\n",
      "Example 3-2. Reconstruct segmented text from string representation: s eg1 and seg2 represent the\n",
      "initial and final segmentations of some hypothetical child-directed speech; the segment() function can\n",
      "use them to reproduce the segmented text.\n",
      "def segment(text, segs):\n",
      "    words = []\n",
      "    last = 0\n",
      "    for i in range(len(segs)):\n",
      "        if segs[i] == '1':\n",
      "            words.append(text[last:i+1])\n",
      "            last = i+1\n",
      "    words.append(text[last:])\n",
      "    return words\n",
      ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
      ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
      ">>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
      ">>> segment(text, seg1)\n",
      "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      ">>> segment(text, seg2)\n",
      "['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you',\n",
      " 'like', 'the', kitty', 'like', 'the', 'doggy']\n",
      "Now the segmentation task becomes a search problem: find the bit string that causes\n",
      "the text string to be correctly segmented into words. We assume the learner is acquiring\n",
      "words and storing them in an internal lexicon. Given a suitable lexicon, it is possible\n",
      "to reconstruct the source text as a sequence of lexical items. Following (Brent & Cart-\n",
      "wright, 1995), we can define an objective function, a scoring function whose value\n",
      "we will try to optimize, based on the size of the lexicon and the amount of information\n",
      "needed to reconstruct the source text from the lexicon. We illustrate this in Figure 3-6.\n",
      "Figure 3-6. Calculation of objective function: Given a hypothetical segmentation of the source text\n",
      "(on \n",
      "the left), derive a lexicon and a derivation table that permit the source text to be reconstructed,\n",
      "then total up the number of characters used by each lexical item (including a boundary marker) and\n",
      "each derivation, to serve as a score of the quality of the segmentation; smaller values of the score\n",
      "indicate a better segmentation.\n",
      "It is a simple matter to implement this objective function, as shown in Example 3-3.\n",
      "114 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 135, 'page_label': '114', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4479}\n",
      "\n",
      "--- Chunk 4480 ---\n",
      "Content:\n",
      "Example 3-3. Computing the cost of storing the lexicon and reconstructing the source text.\n",
      "def evaluate(text, segs):\n",
      "    words = segment(text, segs)\n",
      "    text_size = len(words)\n",
      "    lexicon_size = len(' '.join(list(set(words))))\n",
      "    return text_size + lexicon_size\n",
      ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
      ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
      ">>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
      ">>> seg3 = \"0000100100000011001000000110000100010000001100010000001\"\n",
      ">>> segment(text, seg3)\n",
      "['doyou', 'see', 'thekitt', 'y', 'see', 'thedogg', 'y', 'doyou', 'like',\n",
      " 'thekitt', 'y', 'like', 'thedogg', 'y']\n",
      ">>> evaluate(text, seg3)\n",
      "46\n",
      ">>> evaluate(text, seg2)\n",
      "47\n",
      ">>> evaluate(text, seg1)\n",
      "63\n",
      "The \n",
      "final step is to search for the pattern of zeros and ones that maximizes this objective\n",
      "function, shown in Example 3-4. Notice that the best segmentation includes “words”\n",
      "like thekitty, since there’s not enough evidence in the data to split this any further.\n",
      "Example 3-4. Non-deterministic search using simulated annealing: Begin searching with phrase\n",
      "segmentations only; randomly perturb the zeros and ones proportional to the “temperature”; with\n",
      "each iteration the temperature is lowered and the perturbation of boundaries is reduced.\n",
      "from random import randint\n",
      "def flip(segs, pos):\n",
      "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:] \n",
      "def flip_n(segs, n):\n",
      "    for i in range(n):\n",
      "        segs = flip(segs, randint(0,len(segs)-1))\n",
      "    return segs\n",
      "def anneal(text, segs, iterations, cooling_rate):\n",
      "    temperature = float(len(segs))\n",
      "    while temperature > 0.5:\n",
      "        best_segs, best = segs, evaluate(text, segs)\n",
      "        for i in range(iterations):\n",
      "            guess = flip_n(segs, int(round(temperature)))\n",
      "            score = evaluate(text, guess)\n",
      "            if score < best:\n",
      "                best, best_segs = score, guess\n",
      "        score, segs = best, best_segs\n",
      "        temperature = temperature / cooling_rate\n",
      "        print evaluate(text, segs), segment(text, segs)\n",
      "3.8  Segmentation | 115...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 136, 'page_label': '115', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4480}\n",
      "\n",
      "--- Chunk 4481 ---\n",
      "Content:\n",
      "print\n",
      "    return segs\n",
      ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
      ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
      ">>> anneal(text, seg1, 5000, 1.2)\n",
      "60 ['doyouseetheki', 'tty', 'see', 'thedoggy', 'doyouliketh', 'ekittylike', 'thedoggy']\n",
      "58 ['doy', 'ouseetheki', 'ttysee', 'thedoggy', 'doy', 'o', 'ulikethekittylike', 'thedoggy']\n",
      "56 ['doyou', 'seetheki', 'ttysee', 'thedoggy', 'doyou', 'liketh', 'ekittylike', 'thedoggy']\n",
      "54 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'likethekittylike', 'thedoggy']\n",
      "53 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "51 ['doyou', 'seethekittysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "42 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "'0000100100000001001000000010000100010000000100010000000'\n",
      "With \n",
      "enough data, it is possible to automatically segment text into words with a rea-\n",
      "sonable degree of accuracy. Such methods can be applied to tokenization for writing\n",
      "systems that don’t have any visual representation of word boundaries.\n",
      "3.9  Formatting: From Lists to Strings\n",
      "Often we write a program to report a single data item, such as a particular element in\n",
      "a corpus that meets some complicated criterion, or a single summary statistic such as\n",
      "a word-count or the performance of a tagger. More often, we write a program to produce\n",
      "a structured result; for example, a tabulation of numbers or linguistic forms, or a re-\n",
      "formatting of the original data. When the results to be presented are linguistic, textual\n",
      "output is usually the most natural choice. However, when the results are numerical, it\n",
      "may be preferable to produce graphical output. In this section, you will learn about a\n",
      "variety of ways to present program output.\n",
      "From Lists to Strings\n",
      "The simplest kind of structured object we use for text processing is lists of words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 137, 'page_label': '116', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4481}\n",
      "\n",
      "--- Chunk 4482 ---\n",
      "Content:\n",
      ". More often, we write a program to produce\n",
      "a structured result; for example, a tabulation of numbers or linguistic forms, or a re-\n",
      "formatting of the original data. When the results to be presented are linguistic, textual\n",
      "output is usually the most natural choice. However, when the results are numerical, it\n",
      "may be preferable to produce graphical output. In this section, you will learn about a\n",
      "variety of ways to present program output.\n",
      "From Lists to Strings\n",
      "The simplest kind of structured object we use for text processing is lists of words. When\n",
      "we want to output these to a display or a file, we must convert these lists into strings.\n",
      "To do this in Python we use the join() method, and specify the string to be used as the\n",
      "“glue”:\n",
      ">>> silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n",
      ">>> ' '.join(silly)\n",
      "'We called him Tortoise because he taught us .'\n",
      ">>> ';'.join(silly)\n",
      "'We;called;him;Tortoise;because;he;taught;us;.'\n",
      ">>> ''.join(silly)\n",
      "'WecalledhimTortoisebecausehetaughtus.'\n",
      "So ' '.join(silly) means: take all the items in silly and concatenate them as one big\n",
      "string, using ' ' as a spacer between the items. I.e., join() is a method of the string\n",
      "that you want to use as the glue. (Many people find this notation for join() counter-\n",
      "intuitive.) The join() method only works on a list of strings—what we have been calling\n",
      "a text—a complex type that enjoys some privileges in Python.\n",
      "116 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 137, 'page_label': '116', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4482}\n",
      "\n",
      "--- Chunk 4483 ---\n",
      "Content:\n",
      "Strings and Formats\n",
      "We have seen that there are two ways to display the contents of an object:\n",
      ">>> word = 'cat'\n",
      ">>> sentence = \"\"\"hello\n",
      "... world\"\"\"\n",
      ">>> print word\n",
      "cat\n",
      ">>> print sentence\n",
      "hello\n",
      "world\n",
      ">>> word\n",
      "'cat'\n",
      ">>> sentence\n",
      "'hello\\nworld'\n",
      "The print \n",
      "command yields Python’s attempt to produce the most human-readable form\n",
      "of an object. The second method—naming the variable at a prompt—shows us a string\n",
      "that can be used to recreate this object. It is important to keep in mind that both of\n",
      "these are just strings, displayed for the benefit of you, the user. They do not give us any\n",
      "clue as to the actual internal representation of the object.\n",
      "There are many other useful ways to display an object as a string of characters. This\n",
      "may be for the benefit of a human reader, or because we want to export our data to a\n",
      "particular file format for use in an external program.\n",
      "Formatted output typically contains a combination of variables and pre-specified\n",
      "strings. For example, given a frequency distribution fdist, we could do:\n",
      ">>> fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
      ">>> for word in fdist:\n",
      "...     print word, '->', fdist[word], ';',\n",
      "dog -> 4 ; cat -> 3 ; snake -> 1 ;\n",
      "Apart from the problem of unwanted whitespace, print statements that contain alter-\n",
      "nating variables and constants can be difficult to read and maintain. A better solution\n",
      "is to use string formatting expressions.\n",
      ">>> for word in fdist:\n",
      "...    print '%s->%d;' % (word, fdist[word]),\n",
      "dog->4; cat->3; snake->1;\n",
      "To understand what is going on here, let’s test out the string formatting expression on\n",
      "its own. (By now this will be your usual method of exploring new syntax.)\n",
      ">>> '%s->%d;' % ('cat', 3)\n",
      "'cat->3;'\n",
      ">>> '%s->%d;' % 'cat'\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: not enough arguments for format string\n",
      "3.9  Formatting: From Lists to Strings | 117...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 138, 'page_label': '117', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4483}\n",
      "\n",
      "--- Chunk 4484 ---\n",
      "Content:\n",
      "The special symbols %s and %d are placeholders for strings and (decimal) integers. We\n",
      "can \n",
      "embed these inside a string, then use the % operator to combine them. Let’s unpack\n",
      "this code further, in order to see this behavior up close:\n",
      ">>> '%s->' % 'cat'\n",
      "'cat->'\n",
      ">>> '%d' % 3\n",
      "'3'\n",
      ">>> 'I want a %s right now' % 'coffee'\n",
      "'I want a coffee right now'\n",
      "We can have a number of placeholders, but following the % operator we need to specify\n",
      "a tuple with exactly the same number of values:\n",
      ">>> \"%s wants a %s %s\" % (\"Lee\", \"sandwich\", \"for lunch\")\n",
      "'Lee wants a sandwich for lunch'\n",
      "We can also provide the values for the placeholders indirectly. Here’s an example using\n",
      "a for loop:\n",
      ">>> template = 'Lee wants a %s right now'\n",
      ">>> menu = ['sandwich', 'spam fritter', 'pancake']\n",
      ">>> for snack in menu:\n",
      "...     print template % snack\n",
      "...\n",
      "Lee wants a sandwich right now\n",
      "Lee wants a spam fritter right now\n",
      "Lee wants a pancake right now\n",
      "The %s and %d symbols are called conversion specifiers. They start with the % character\n",
      "and end with a conversion character such as s (for string) or d (for decimal integer) The\n",
      "string containing conversion specifiers is called a format string. We combine a format\n",
      "string with the % operator and a tuple of values to create a complete string formatting\n",
      "expression.\n",
      "Lining Things Up\n",
      "So far our formatting strings generated output of arbitrary width on the page (or screen),\n",
      "such as %s and %d. We can specify a width as well, such as %6s, producing a string that\n",
      "is padded to width 6. It is right-justified by default \n",
      ", but we can include a minus sign\n",
      "to \n",
      "make it left-justified \n",
      " . In case we don’t know in advance how wide a displayed\n",
      "value \n",
      "should be, the width value can be replaced with a star in the formatting string,\n",
      "then specified using a variable \n",
      " .\n",
      ">>> '%6s' % 'dog' \n",
      "'   dog'\n",
      ">>> '%-6s' % 'dog' \n",
      "'dog   '\n",
      ">>> width = 6\n",
      ">>> '%-*s' % (width, 'dog') \n",
      "'dog   '\n",
      "118 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 139, 'page_label': '118', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4484}\n",
      "\n",
      "--- Chunk 4485 ---\n",
      "Content:\n",
      "Other control characters are used for decimal integers and floating-point numbers.\n",
      "Since \n",
      "the percent character % has a special interpretation in formatting strings, we have\n",
      "to precede it with another % to get it in the output.\n",
      ">>> count, total = 3205, 9375\n",
      ">>> \"accuracy for %d words: %2.4f%%\" % (total, 100 * count / total)\n",
      "'accuracy for 9375 words: 34.1867%'\n",
      "An important use of formatting strings is for tabulating data. Recall that in Sec-\n",
      "tion 2.1 we saw data being tabulated from a conditional frequency distribution. Let’s\n",
      "perform the tabulation ourselves, exercising full control of headings and column\n",
      "widths, as shown in Example 3-5. Note the clear separation between the language\n",
      "processing work, and the tabulation of results.\n",
      "Example 3-5. Frequency of modals in different sections of the Brown Corpus.\n",
      "def tabulate(cfdist, words, categories):\n",
      "    print '%-16s' % 'Category',\n",
      "    for word in words:                                  # column headings\n",
      "        print '%6s' % word,\n",
      "    print\n",
      "    for category in categories:\n",
      "        print '%-16s' % category,                       # row heading\n",
      "        for word in words:                              # for each word\n",
      "            print '%6d' % cfdist[category][word],       # print table cell\n",
      "        print                                           # end the row\n",
      ">>> from nltk.corpus import brown\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (genre, word)\n",
      "...           for genre in brown.categories()\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 140, 'page_label': '119', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4485}\n",
      "\n",
      "--- Chunk 4486 ---\n",
      "Content:\n",
      ".           (genre, word)\n",
      "...           for genre in brown.categories()\n",
      "...           for word in brown.words(categories=genre))\n",
      ">>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
      ">>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
      ">>> tabulate(cfd, modals, genres)\n",
      "Category            can  could    may  might   must   will\n",
      "news                 93     86     66     38     50    389\n",
      "religion             82     59     78     12     54     71\n",
      "hobbies             268     58    131     22     83    264\n",
      "science_fiction      16     49      4     12      8     16\n",
      "romance              74    193     11     51     45     43\n",
      "humor                16     30      8      8      9     13\n",
      "Recall from the listing in Example 3-1 that we used a formatting string \"%*s\". This\n",
      "allows us to specify the width of a field using a variable.\n",
      ">>> '%*s' % (15, \"Monty Python\")\n",
      "'   Monty Python'\n",
      "We could use this to automatically customize the column to be just wide enough to\n",
      "accommodate all the words, using width = max(len(w) for w in words). Remember\n",
      "that the comma at the end of print statements adds an extra space, and this is sufficient\n",
      "to prevent the column headings from running into each other.\n",
      "3.9  Formatting: From Lists to Strings | 119...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 140, 'page_label': '119', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4486}\n",
      "\n",
      "--- Chunk 4487 ---\n",
      "Content:\n",
      "Writing Results to a File\n",
      "We \n",
      "have seen how to read text from files (Section 3.1). It is often useful to write output\n",
      "to files as well. The following code opens a file output.txt for writing, and saves the\n",
      "program output to the file.\n",
      ">>> output_file = open('output.txt', 'w')\n",
      ">>> words = set(nltk.corpus.genesis.words('english-kjv.txt'))\n",
      ">>> for word in sorted(words):\n",
      "...     output_file.write(word + \"\\n\")\n",
      "Your Turn: What is the effect of appending \\n to each string before we\n",
      "write it to the file? If you’re using a Windows machine, you may want\n",
      "to use word + \"\\r\\n\" instead. What happens if we do\n",
      "output_file.write(word)\n",
      "When we write non-text data to a file, we must convert it to a string first. We can do\n",
      "this conversion using formatting strings, as we saw earlier. Let’s write the total number\n",
      "of words to our file, before closing it.\n",
      ">>> len(words)\n",
      "2789\n",
      ">>> str(len(words))\n",
      "'2789'\n",
      ">>> output_file.write(str(len(words)) + \"\\n\")\n",
      ">>> output_file.close()\n",
      "Caution!\n",
      "You \n",
      "should avoid filenames that contain space characters, such as\n",
      "output file.txt , or that are identical except for case distinctions, e.g.,\n",
      "Output.txt and output.TXT.\n",
      "Text Wrapping\n",
      "When the output of our program is text-like, instead of tabular, it will usually be nec-\n",
      "essary to wrap it so that it can be displayed conveniently. Consider the following output,\n",
      "which overflows its line, and which uses a complicated print statement:\n",
      ">>> saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',\n",
      "...           'more', 'is', 'said', 'than', 'done', '.']\n",
      ">>> for word in saying:\n",
      "...     print word, '(' + str(len(word)) + '),',\n",
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), \n",
      "We can take care of line wrapping with the help of Python’s textwrap module. For\n",
      "maximum clarity we will separate each step onto its own line:\n",
      ">>> from textwrap import fill\n",
      ">>> format = '%s (%d),'\n",
      "120 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 141, 'page_label': '120', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4487}\n",
      "\n",
      "--- Chunk 4488 ---\n",
      "Content:\n",
      ">>> pieces = [format % (word, len(word)) for word in saying]\n",
      ">>> output = ' '.join(pieces)\n",
      ">>> wrapped = fill(output)\n",
      ">>> print wrapped\n",
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more\n",
      "(4), is (2), said (4), than (4), done (4), . (1),\n",
      "Notice \n",
      "that there is a linebreak between more and its following number. If we wanted\n",
      "to avoid this, we could redefine the formatting string so that it contained no spaces\n",
      "(e.g., '%s_(%d),'), then instead of printing the value of wrapped, we could print wrap\n",
      "ped.replace('_', ' ').\n",
      "3.10  Summary\n",
      "• In this book we view a text as a list of words. A “raw text” is a potentially long\n",
      "string containing words and whitespace formatting, and is how we typically store\n",
      "and visualize a text.\n",
      "• A string is specified in Python using single or double quotes: 'Monty Python',\n",
      "\"Monty Python\".\n",
      "• The characters of a string are accessed using indexes, counting from zero: 'Monty\n",
      "Python'[0] gives the value M. The length of a string is found using len().\n",
      "• Substrings are accessed using slice notation: 'Monty Python'[1:5] gives the value\n",
      "onty. If the start index is omitted, the substring begins at the start of the string; if\n",
      "the end index is omitted, the slice continues to the end of the string.\n",
      "• Strings can be split into lists: 'Monty Python'.split() gives ['Monty', 'Python'].\n",
      "Lists can be joined into strings: '/'.join(['Monty', 'Python']) gives 'Monty/\n",
      "Python'.\n",
      "• We can read text from a file f using text = open(f).read(). We can read text from\n",
      "a URL u using text = urlopen(u).read(). We can iterate over the lines of a text file\n",
      "using for line in open(f).\n",
      "• Texts found on the Web may contain unwanted material (such as headers, footers,\n",
      "and markup), that need to be removed before we do any linguistic processing.\n",
      "• Tokenization is the segmentation of a text into basic units—or tokens—such as\n",
      "words and punctuation. Tokenization based on whitespace is inadequate for many\n",
      "applications because it bundles punctuation together with words. NLTK provides\n",
      "an off-the-shelf tokenizer nltk.word_tokenize().\n",
      "• Lemmatization is a process that maps the various forms of a word (such as ap-\n",
      "peared, appears) to the canonical or citation form of the word, also known as the\n",
      "lexeme or lemma (e.g., appear).\n",
      "• Regular expressions are a powerful and flexible method of specifying patterns.\n",
      "Once we have imported the re module, we can use re.findall() to find all sub-\n",
      "strings in a string that match a pattern.\n",
      "3.10  Summary | 121...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 142, 'page_label': '121', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4488}\n",
      "\n",
      "--- Chunk 4489 ---\n",
      "Content:\n",
      "• If a regular expression string includes a backslash, you should tell Python not to\n",
      "preprocess the string, by using a raw string with an r prefix: r'regexp'.\n",
      "• When backslash is used before certain characters, e.g., \\n, this takes on a special\n",
      "meaning (newline character); however, when backslash is used before regular ex-\n",
      "pression wildcards and operators, e.g., \\., \\|, \\$, these characters lose their special\n",
      "meaning and are matched literally.\n",
      "• A string formatting expression template % arg_tuple consists of a format string\n",
      "template that contains conversion specifiers like %-6s and %0.2d.\n",
      "3.11  Further Reading\n",
      "Extra materials for this chapter are posted at http://www.nltk.org/, including links to\n",
      "freely available resources on the Web. Remember to consult the Python reference ma-\n",
      "terials at http://docs.python.org/. (For example, this documentation covers “universal\n",
      "newline support,” explaining how to work with the different newline conventions used\n",
      "by various operating systems.)\n",
      "For more examples of processing words with NLTK, see the tokenization, stemming,\n",
      "and corpus HOWTOs at http://www.nltk.org/howto. Chapters 2 and 3 of (Jurafsky &\n",
      "Martin, 2008) contain more advanced material on regular expressions and morphology.\n",
      "For more extensive discussion of text processing with Python, see (Mertz, 2003). For\n",
      "information about normalizing non-standard words, see (Sproat et al., 2001).\n",
      "There are many references for regular expressions, both practical and theoretical. For\n",
      "an introductory tutorial to using regular expressions in Python, see Kuchling’s Regular\n",
      "Expression HOWTO, http://www.amk.ca/python/howto/regex/. For a comprehensive\n",
      "and detailed manual in using regular expressions, covering their syntax in most major\n",
      "programming languages, including Python, see (Friedl, 2002). Other presentations in-\n",
      "clude Section 2.1 of (Jurafsky & Martin, 2008), and Chapter 3 of (Mertz, 2003).\n",
      "There are many online resources for Unicode. Useful discussions of Python’s facilities\n",
      "for handling Unicode are:\n",
      "• PEP-100 http://www.python.org/dev/peps/pep-0100/\n",
      "• Jason Orendorff, Unicode for Programmers, http://www.jorendorff.com/articles/uni\n",
      "code/\n",
      "• A. M. Kuchling, Unicode HOWTO, http://www.amk.ca/python/howto/unicode\n",
      "• Frederik Lundh, Python Unicode Objects , http://effbot.org/zone/unicode-objects\n",
      ".htm\n",
      "• Joel Spolsky, The Absolute Minimum Every Software Developer Absolutely, Posi-\n",
      "tively Must Know About Unicode and Character Sets (No Excuses!), http://www.joe\n",
      "lonsoftware.com/articles/Unicode.html\n",
      "122 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 143, 'page_label': '122', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4489}\n",
      "\n",
      "--- Chunk 4490 ---\n",
      "Content:\n",
      "The problem of tokenizing Chinese text is a major focus of SIGHAN, the ACL Special\n",
      "Interest \n",
      "Group on Chinese Language Processing ( http://sighan.org/). Our method for\n",
      "segmenting English text follows (Brent & Cartwright, 1995); this work falls in the area\n",
      "of language acquisition (Niyogi, 2006).\n",
      "Collocations are a special case of multiword expressions. A multiword expression is\n",
      "a small phrase whose meaning and other properties cannot be predicted from its words\n",
      "alone, e.g., part-of-speech (Baldwin & Kim, 2010).\n",
      "Simulated annealing is a heuristic for finding a good approximation to the optimum\n",
      "value of a function in a large, discrete search space, based on an analogy with annealing\n",
      "in metallurgy. The technique is described in many Artificial Intelligence texts.\n",
      "The approach to discovering hyponyms in text using search patterns like x and other\n",
      "ys is described by (Hearst, 1992).\n",
      "3.12  Exercises\n",
      "1. ○ Define a string s = 'colorless'. Write a Python statement that changes this to\n",
      "“colourless” using only the slice and concatenation operations.\n",
      "2. ○ We can use the slice notation to remove morphological endings on words. For\n",
      "example, 'dogs'[:-1] removes the last character of dogs, leaving dog. Use slice\n",
      "notation to remove the affixes from these words (we’ve inserted a hyphen to indi-\n",
      "cate the affix boundary, but omit this from your strings): dish-es, run-ning, nation-\n",
      "ality, un-do, pre-heat.\n",
      "3. ○ We saw how we can generate an IndexError by indexing beyond the end of a\n",
      "string. Is it possible to construct an index that goes too far to the left, before the\n",
      "start of the string?\n",
      "4. ○ We can specify a “step” size for the slice. The following returns every second\n",
      "character within the slice: monty[6:11:2]. It also works in the reverse direction:\n",
      "monty[10:5:-2]. Try these for yourself, and then experiment with different step\n",
      "values.\n",
      "5. ○ What happens if you ask the interpreter to evaluate monty[::-1]? Explain why\n",
      "this is a reasonable result.\n",
      "6. ○ Describe the class of strings matched by the following regular expressions:\n",
      "a. [a-zA-Z]+\n",
      "b. [A-Z][a-z]*\n",
      "c. p[aeiou]{,2}t\n",
      "d. \\d+(\\.\\d+)?\n",
      "e. ([^aeiou][aeiou][^aeiou])*\n",
      "f. \\w+|[^\\w\\s]+\n",
      "Test your answers using nltk.re_show().\n",
      "3.12  Exercises | 123...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 144, 'page_label': '123', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4490}\n",
      "\n",
      "--- Chunk 4491 ---\n",
      "Content:\n",
      "7. ○ Write regular expressions to match the following classes of strings:\n",
      "a.\n",
      "A single determiner (assume that a, an, and the are the only determiners)\n",
      "b. An arithmetic expression using integers, addition, and multiplication, such as\n",
      "2*3+8\n",
      "8. ○ Write a utility function that takes a URL as its argument, and returns the contents\n",
      "of the URL, with all HTML markup removed. Use urllib.urlopen to access the\n",
      "contents of the URL, e.g.:\n",
      "raw_contents = urllib.urlopen('http://www.nltk.org/').read()\n",
      "9. ○ Save some text into a file corpus.txt. Define a function load(f) that reads from\n",
      "the file named in its sole argument, and returns a string containing the text of the\n",
      "file.\n",
      "a. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the various\n",
      "kinds of punctuation in this text. Use one multiline regular expression inline\n",
      "comments, using the verbose flag (?x).\n",
      "b. Use nltk.regexp_tokenize() to create a tokenizer that tokenizes the following\n",
      "kinds of expressions: monetary amounts; dates; names of people and\n",
      "organizations.\n",
      "10. ○ Rewrite the following loop as a list comprehension:\n",
      ">>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
      ">>> result = []\n",
      ">>> for word in sent:\n",
      "...     word_len = (word, len(word))\n",
      "...     result.append(word_len)\n",
      ">>> result\n",
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n",
      "11. ○ Define a string raw containing a sentence of your own choosing. Now, split raw\n",
      "on some character other than space, such as 's'.\n",
      "12. ○ Write a for loop to print out the characters of a string, one per line.\n",
      "13. ○ What is the difference between calling split on a string with no argument and\n",
      "one with ' ' as the argument, e.g., sent.split() versus sent.split(' ')? What\n",
      "happens when the string being split contains tab characters, consecutive space\n",
      "characters, or a sequence of tabs and spaces? (In IDLE you will need to use '\\t' to\n",
      "enter a tab character.)\n",
      "14. ○ Create a variable words containing a list of words. Experiment with\n",
      "words.sort() and sorted(words). What is the difference?\n",
      "15. ○ Explore the difference between strings and integers by typing the following at a\n",
      "Python prompt: \"3\" * 7 and 3 * 7. Try converting between strings and integers\n",
      "using int(\"3\") and str(3).\n",
      "16. ○ Earlier, we asked you to use a text editor to create a file called test.py, containing\n",
      "the single line monty = 'Monty Python'. If you haven’t already done this (or can’t\n",
      "find the file), go ahead and do it now. Next, start up a new session with the Python\n",
      "124 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 145, 'page_label': '124', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4491}\n",
      "\n",
      "--- Chunk 4492 ---\n",
      "Content:\n",
      "interpreter, and enter the expression monty at the prompt. You will get an error\n",
      "from the interpreter. Now, try the following (note that you have to leave off\n",
      "the .py part of the filename):\n",
      ">>> from test import msg\n",
      ">>> msg\n",
      "This time, Python should return with a value. You can also try import test, in\n",
      "which case Python should be able to evaluate the expression test.monty at the\n",
      "prompt.\n",
      "17. ○ What happens when the formatting strings %6s and %-6s are used to display\n",
      "strings that are longer than six characters?\n",
      "18. ◑ Read in some text from a corpus, tokenize it, and print the list of all wh-word\n",
      "types that occur. (wh-words in English are used in questions, relative clauses, and\n",
      "exclamations: who, which, what, and so on.) Print them in order. Are any words\n",
      "duplicated in this list, because of the presence of case distinctions or punctuation?\n",
      "19. ◑ Create a file consisting of words and (made up) frequencies, where each line\n",
      "consists of a word, the space character, and a positive integer, e.g., fuzzy 53. Read\n",
      "the file into a Python list using open(filename).readlines(). Next, break each line\n",
      "into its two fields using split(), and convert the number into an integer using\n",
      "int(). The result should be a list of the form: [['fuzzy', 53], ...].\n",
      "20. ◑ Write code to access a favorite web page and extract some text from it. For\n",
      "example, access a weather site and extract the forecast top temperature for your\n",
      "town or city today.\n",
      "21. ◑ Write a function unknown() that takes a URL as its argument, and returns a list\n",
      "of unknown words that occur on that web page. In order to do this, extract all\n",
      "substrings consisting of lowercase letters (using re.findall()) and remove any\n",
      "items from this set that occur in the Words Corpus ( nltk.corpus.words). Try to\n",
      "categorize these words manually and discuss your findings.\n",
      "22. ◑ Examine the results of processing the URL http://news.bbc.co.uk/ using the reg-\n",
      "ular expressions suggested above. You will see that there is still a fair amount of\n",
      "non-textual data there, particularly JavaScript commands. You may also find that\n",
      "sentence breaks have not been properly preserved. Define further regular expres-\n",
      "sions that improve the extraction of text from this web page.\n",
      "23. ◑ Are you able to write a regular expression to tokenize text in such a way that the\n",
      "word don’t is tokenized into do and n’t? Explain why this regular expression won’t\n",
      "work: «n't|\\w+».\n",
      "24. ◑ Try to write code to convert text into hAck3r, using regular expressions and\n",
      "substitution, where e → 3, i → 1, o → 0, l → |, s → 5, . → 5w33t!, ate → 8. Normalize\n",
      "the text to lowercase before converting it. Add more substitutions of your own.\n",
      "Now try to map s to two different values: $ for word-initial s, and 5 for word-\n",
      "internal s.\n",
      "3.12  Exercises | 125...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 146, 'page_label': '125', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4492}\n",
      "\n",
      "--- Chunk 4493 ---\n",
      "Content:\n",
      "25. ◑ Pig Latin is a simple transformation of English text. Each word of the text is\n",
      "converted as follows: move any consonant (or consonant cluster) that appears at\n",
      "the start of the word to the end, then append ay, e.g., string → ingstray, idle →\n",
      "idleay (see http://en.wikipedia.org/wiki/Pig_Latin).\n",
      "a. Write a function to convert a word to Pig Latin.\n",
      "b. Write code that converts text, instead of individual words.\n",
      "c. Extend it further to preserve capitalization, to keep qu together (so that\n",
      "quiet becomes ietquay, for example), and to detect when y is used as a con-\n",
      "sonant (e.g., yellow) versus a vowel (e.g., style).\n",
      "26. ◑ Download some text from a language that has vowel harmony (e.g., Hungarian),\n",
      "extract the vowel sequences of words, and create a vowel bigram table.\n",
      "27. ◑ Python’s random module includes a function choice() which randomly chooses\n",
      "an item from a sequence; e.g., choice(\"aehh \") will produce one of four possible\n",
      "characters, with the letter h being twice as frequent as the others. Write a generator\n",
      "expression that produces a sequence of 500 randomly chosen letters drawn from\n",
      "the string \"aehh \", and put this expression inside a call to the ''.join() function,\n",
      "to concatenate them into one long string. You should get a result that looks like\n",
      "uncontrolled sneezing or maniacal laughter: he haha ee heheeh eha. Use split()\n",
      "and join() again to normalize the whitespace in this string.\n",
      "28. ◑ Consider the numeric expressions in the following sentence from the MedLine\n",
      "Corpus: The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15%\n",
      "and 8.16 +/- 0.23%, respectively. Should we say that the numeric expression 4.53\n",
      "+/- 0.15% is three words? Or should we say that it’s a single compound word? Or\n",
      "should we say that it is actually nine words, since it’s read “four point five three,\n",
      "plus or minus fifteen percent”? Or should we say that it’s not a “real” word at all,\n",
      "since it wouldn’t appear in any dictionary? Discuss these different possibilities. Can\n",
      "you think of application domains that motivate at least two of these answers?\n",
      "29. ◑ Readability measures are used to score the reading difficulty of a text, for the\n",
      "purposes of selecting texts of appropriate difficulty for language learners. Let us\n",
      "define μw to be the average number of letters per word, and μs to be the average\n",
      "number of words per sentence, in a given text. The Automated Readability Index\n",
      "(ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score\n",
      "for various sections of the Brown Corpus, including section f (popular lore) and\n",
      "j (learned). Make use of the fact that nltk.corpus.brown.words() produces a se-\n",
      "quence of words, whereas nltk.corpus.brown.sents() produces a sequence of\n",
      "sentences.\n",
      "30. ◑ Use the Porter Stemmer to normalize some tokenized text, calling the stemmer\n",
      "on each word. Do the same thing with the Lancaster Stemmer, and see if you ob-\n",
      "serve any differences.\n",
      "31...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 147, 'page_label': '126', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4493}\n",
      "\n",
      "--- Chunk 4494 ---\n",
      "Content:\n",
      ". Compute the ARI score\n",
      "for various sections of the Brown Corpus, including section f (popular lore) and\n",
      "j (learned). Make use of the fact that nltk.corpus.brown.words() produces a se-\n",
      "quence of words, whereas nltk.corpus.brown.sents() produces a sequence of\n",
      "sentences.\n",
      "30. ◑ Use the Porter Stemmer to normalize some tokenized text, calling the stemmer\n",
      "on each word. Do the same thing with the Lancaster Stemmer, and see if you ob-\n",
      "serve any differences.\n",
      "31. ◑ Define the variable saying to contain the list [ 'After', 'all', 'is', 'said',\n",
      "'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']. Process the list\n",
      "126 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 147, 'page_label': '126', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4494}\n",
      "\n",
      "--- Chunk 4495 ---\n",
      "Content:\n",
      "using a for loop, and store the result in a new list lengths. Hint: begin by assigning\n",
      "the empty list to lengths, using lengths = []. Then each time through the loop,\n",
      "use append() to add another length value to the list.\n",
      "32. ◑ Define a variable silly to contain the string: 'newly formed bland ideas are\n",
      "inexpressible in an infuriating way'. (This happens to be the legitimate inter-\n",
      "pretation that bilingual English-Spanish speakers can assign to Chomsky’s famous\n",
      "nonsense phrase colorless green ideas sleep furiously, according to Wikipedia). Now\n",
      "write code to perform the following tasks:\n",
      "a. Split silly into a list of strings, one per word, using Python’s split() opera-\n",
      "tion, and save this to a variable called bland.\n",
      "b. Extract the second letter of each word in silly and join them into a string, to\n",
      "get 'eoldrnnnna'.\n",
      "c. Combine the words in bland back into a single string, using join(). Make sure\n",
      "the words in the resulting string are separated with whitespace.\n",
      "d. Print the words of silly in alphabetical order, one per line.\n",
      "33. ◑ The index() function can be used to look up items in sequences. For example,\n",
      "'inexpressible'.index('e') tells us the index of the first position of the letter e.\n",
      "a. What happens when you look up a substring, e.g., 'inexpressi\n",
      "ble'.index('re')?\n",
      "b. Define a variable words containing a list of words. Now use words.index() to\n",
      "look up the position of an individual word.\n",
      "c. Define a variable silly as in Exercise 32. Use the index() function in combi-\n",
      "nation with list slicing to build a list phrase consisting of all the words up to\n",
      "(but not including) in in silly.\n",
      "34. ◑ Write code to convert nationality adjectives such as Canadian and Australian to\n",
      "their corresponding nouns Canada and Australia (see http://en.wikipedia.org/wiki/\n",
      "List_of_adjectival_forms_of_place_names).\n",
      "35. ◑ Read the LanguageLog post on phrases of the form as best as p can and as best p\n",
      "can, where p is a pronoun. Investigate this phenomenon with the help of a corpus\n",
      "and the findall() method for searching tokenized text described in Section 3.5.\n",
      "The post is at http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html.\n",
      "36. ◑ Study the lolcat version of the book of Genesis, accessible as nltk.corpus.gene\n",
      "sis.words('lolcat.txt'), and the rules for converting text into lolspeak at http://\n",
      "www.lolcatbible.com/index.php?title=How_to_speak_lolcat. Define regular expres-\n",
      "sions to convert English words into corresponding lolspeak words.\n",
      "37. ◑ Read about the re.sub() function for string substitution using regular expres-\n",
      "sions, using help(re.sub) and by consulting the further readings for this chapter.\n",
      "Use re.sub in writing code to remove HTML tags from an HTML file, and to\n",
      "normalize whitespace.\n",
      "3.12  Exercises | 127...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 148, 'page_label': '127', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4495}\n",
      "\n",
      "--- Chunk 4496 ---\n",
      "Content:\n",
      "38. ● An interesting challenge for tokenization is words that have been split across a\n",
      "linebreak. E.g., if long-term is split, then we have the string long-\\nterm.\n",
      "a. Write a regular expression that identifies words that are hyphenated at a line-\n",
      "break. The expression will need to include the \\n character.\n",
      "b. Use re.sub() to remove the \\n character from these words.\n",
      "c. How might you identify words that should not remain hyphenated once the\n",
      "newline is removed, e.g., 'encyclo-\\npedia'?\n",
      "39. ● Read the Wikipedia entry on Soundex. Implement this algorithm in Python.\n",
      "40. ● Obtain raw texts from two or more genres and compute their respective reading\n",
      "difficulty scores as in the earlier exercise on reading difficulty. E.g., compare ABC\n",
      "Rural News and ABC Science News (nltk.corpus.abc). Use Punkt to perform sen-\n",
      "tence segmentation.\n",
      "41. ● Rewrite the following nested loop as a nested list comprehension:\n",
      ">>> words = ['attribution', 'confabulation', 'elocution',\n",
      "...          'sequoia', 'tenacious', 'unidirectional']\n",
      ">>> vsequences = set()\n",
      ">>> for word in words:\n",
      "...     vowels = []\n",
      "...     for char in word:\n",
      "...         if char in 'aeiou':\n",
      "...             vowels.append(char)\n",
      "...     vsequences.add(''.join(vowels))\n",
      ">>> sorted(vsequences)\n",
      "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']\n",
      "42. ● Use WordNet to create a semantic index for a text collection. Extend the con-\n",
      "cordance search program in Example 3-1, indexing each word using the offset of\n",
      "its first synset, e.g., wn.synsets('dog')[0].offset (and optionally the offset of some\n",
      "of its ancestors in the hypernym hierarchy).\n",
      "43. ● With the help of a multilingual corpus such as the Universal Declaration of\n",
      "Human Rights Corpus ( nltk.corpus.udhr), along with NLTK’s frequency distri-\n",
      "bution and rank correlation functionality (nltk.FreqDist, nltk.spearman_correla\n",
      "tion), develop a system that guesses the language of a previously unseen text. For\n",
      "simplicity, work with a single character encoding and just a few languages.\n",
      "44. ● Write a program that processes a text and discovers cases where a word has been\n",
      "used with a novel sense. For each word, compute the WordNet similarity between\n",
      "all synsets of the word and all synsets of the words in its context. (Note that this\n",
      "is a crude approach; doing it well is a difficult, open research problem.)\n",
      "45. ● Read the article on normalization of non-standard words (Sproat et al., 2001),\n",
      "and implement a similar system for text normalization.\n",
      "128 | Chapter 3:  Processing Raw Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 149, 'page_label': '128', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4496}\n",
      "\n",
      "--- Chunk 4497 ---\n",
      "Content:\n",
      "CHAPTER 4\n",
      "Writing Structured Programs\n",
      "By now you will have a sense of the capabilities of the Python programming language\n",
      "for \n",
      "processing natural language. However, if you’re new to Python or to programming,\n",
      "you may still be wrestling with Python and not feel like you are in full control yet. In\n",
      "this chapter we’ll address the following questions:\n",
      "1. How can you write well-structured, readable programs that you and others will be\n",
      "able to reuse easily?\n",
      "2. How do the fundamental building blocks work, such as loops, functions, and\n",
      "assignment?\n",
      "3. What are some of the pitfalls with Python programming, and how can you avoid\n",
      "them?\n",
      "Along the way, you will consolidate your knowledge of fundamental programming\n",
      "constructs, learn more about using features of the Python language in a natural and\n",
      "concise way, and learn some useful techniques in visualizing natural language data. As\n",
      "before, this chapter contains many examples and exercises (and as before, some exer-\n",
      "cises introduce new material). Readers new to programming should work through them\n",
      "carefully and consult other introductions to programming if necessary; experienced\n",
      "programmers can quickly skim this chapter.\n",
      "In the other chapters of this book, we have organized the programming concepts as\n",
      "dictated by the needs of NLP. Here we revert to a more conventional approach, where\n",
      "the material is more closely tied to the structure of the programming language. There’s\n",
      "not room for a complete presentation of the language, so we’ll just focus on the language\n",
      "constructs and idioms that are most important for NLP.\n",
      "129...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 150, 'page_label': '129', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4497}\n",
      "\n",
      "--- Chunk 4498 ---\n",
      "Content:\n",
      "4.1  Back to the Basics\n",
      "Assignment\n",
      "Assignment \n",
      "would seem to be the most elementary programming concept, not deserv-\n",
      "ing a separate discussion. However, there are some surprising subtleties here. Consider\n",
      "the following code fragment:\n",
      ">>> foo = 'Monty'\n",
      ">>> bar = foo \n",
      ">>> foo = 'Python' \n",
      ">>> bar\n",
      "'Monty'\n",
      "This \n",
      "behaves exactly as expected. When we write bar = foo in the code \n",
      " , the value\n",
      "of foo (the string 'Monty') is assigned to bar. That is, bar is a copy of foo, so when we\n",
      "overwrite foo with a new string 'Python' on line \n",
      " , the value of bar is not affected.\n",
      "However, \n",
      "assignment statements do not always involve making copies in this way.\n",
      "Assignment always copies the value of an expression, but a value is not always what\n",
      "you might expect it to be. In particular, the “value” of a structured object such as a list\n",
      "is actually just a reference to the object. In the following example, \n",
      " assigns the refer-\n",
      "ence \n",
      "of foo to the new variable bar. Now when we modify something inside foo on line\n",
      ", we can see that the contents of bar have also been changed.\n",
      ">>> foo = ['Monty', 'Python']\n",
      ">>> bar = foo \n",
      ">>> foo[1] = 'Bodkin' \n",
      ">>> bar\n",
      "['Monty', 'Bodkin']\n",
      "The line bar = foo \n",
      " does not copy the contents of the variable, only its “object refer-\n",
      "ence.” \n",
      "To understand what is going on here, we need to know how lists are stored in\n",
      "the computer’s memory. In Figure 4-1, we see that a list foo is a reference to an object\n",
      "stored at location 3133 (which is itself a series of pointers to other locations holding\n",
      "strings). When we assign bar = foo, it is just the object reference 3133 that gets copied.\n",
      "This behavior extends to other aspects of the language, such as parameter passing\n",
      "(Section 4.4).\n",
      "130 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 151, 'page_label': '130', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4498}\n",
      "\n",
      "--- Chunk 4499 ---\n",
      "Content:\n",
      "Let’s experiment some more, by creating a variable empty holding the empty list, then\n",
      "using it three times on the next line.\n",
      ">>> empty = []\n",
      ">>> nested = [empty, empty, empty]\n",
      ">>> nested\n",
      "[[], [], []]\n",
      ">>> nested[1].append('Python')\n",
      ">>> nested\n",
      "[['Python'], ['Python'], ['Python']]\n",
      "Observe \n",
      "that changing one of the items inside our nested list of lists changed them all.\n",
      "This is because each of the three elements is actually just a reference to one and the\n",
      "same list in memory.\n",
      "Your Turn: Use multiplication to create a list of lists: nested = [[]] *\n",
      "3. Now modify one of the elements of the list, and observe that all the\n",
      "elements are changed. Use Python’s id() function to find out the nu-\n",
      "merical identifier for any object, and verify that id(nested[0]),\n",
      "id(nested[1]), and id(nested[2]) are all the same.\n",
      "Now, notice that when we assign a new value to one of the elements of the list, it does\n",
      "not propagate to the others:\n",
      ">>> nested = [[]] * 3\n",
      ">>> nested[1].append('Python')\n",
      ">>> nested[1] = ['Monty']\n",
      ">>> nested\n",
      "[['Python'], ['Monty'], ['Python']]\n",
      "We began with a list containing three references to a single empty list object. Then we\n",
      "modified that object by appending 'Python' to it, resulting in a list containing three\n",
      "references to a single list object ['Python']. Next, we overwrote one of those references\n",
      "with a reference to a new object ['Monty']. This last step modified one of the three\n",
      "object references inside the nested list. However, the ['Python'] object wasn’t changed,\n",
      "Figure 4-1. List assignment and computer memory: Two list objects f oo and bar reference the same\n",
      "location in the computer’s memory; updating foo will also modify bar, and vice versa.\n",
      "4.1  Back to the Basics | 131...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 152, 'page_label': '131', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4499}\n",
      "\n",
      "--- Chunk 4500 ---\n",
      "Content:\n",
      "and is still referenced from two places in our nested list of lists. It is crucial to appreciate\n",
      "this \n",
      "difference between modifying an object via an object reference and overwriting an\n",
      "object reference.\n",
      "Important: To copy the items from a list foo to a new list bar, you can\n",
      "write bar = foo[:]. This copies the object references inside the list. To\n",
      "copy a structure without copying any object references, use copy.deep\n",
      "copy().\n",
      "Equality\n",
      "Python provides two ways to check that a pair of items are the same. The is operator\n",
      "tests for object identity. We can use it to verify our earlier observations about objects.\n",
      "First, we create a list containing several copies of the same object, and demonstrate that\n",
      "they are not only identical according to ==, but also that they are one and the same\n",
      "object:\n",
      ">>> size = 5\n",
      ">>> python = ['Python']\n",
      ">>> snake_nest = [python] * size\n",
      ">>> snake_nest[0] == snake_nest[1] == snake_nest[2] == snake_nest[3] == snake_nest[4]\n",
      "True\n",
      ">>> snake_nest[0] is snake_nest[1] is snake_nest[2] is snake_nest[3] is snake_nest[4]\n",
      "True\n",
      "Now let’s put a new python in this nest. We can easily show that the objects are not\n",
      "all identical:\n",
      ">>> import random\n",
      ">>> position = random.choice(range(size))\n",
      ">>> snake_nest[position] = ['Python']\n",
      ">>> snake_nest\n",
      "[['Python'], ['Python'], ['Python'], ['Python'], ['Python']]\n",
      ">>> snake_nest[0] == snake_nest[1] == snake_nest[2] == snake_nest[3] == snake_nest[4]\n",
      "True\n",
      ">>> snake_nest[0] is snake_nest[1] is snake_nest[2] is snake_nest[3] is snake_nest[4]\n",
      "False\n",
      "You can do several pairwise tests to discover which position contains the interloper,\n",
      "but the id() function makes detection is easier:\n",
      ">>> [id(snake) for snake in snake_nest]\n",
      "[513528, 533168, 513528, 513528, 513528]\n",
      "This reveals that the second item of the list has a distinct identifier. If you try running\n",
      "this code snippet yourself, expect to see different numbers in the resulting list, and\n",
      "don’t be surprised if the interloper is in a different position.\n",
      "Having two kinds of equality might seem strange. However, it’s really just the type-\n",
      "token distinction, familiar from natural language, here showing up in a programming\n",
      "language.\n",
      "132 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 153, 'page_label': '132', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4500}\n",
      "\n",
      "--- Chunk 4501 ---\n",
      "Content:\n",
      "Conditionals\n",
      "In \n",
      "the condition part of an if statement, a non-empty string or list is evaluated as true,\n",
      "while an empty string or list evaluates as false.\n",
      ">>> mixed = ['cat', '', ['dog'], []]\n",
      ">>> for element in mixed:\n",
      "...     if element:\n",
      "...         print element\n",
      "...\n",
      "cat\n",
      "['dog']\n",
      "That is, we don’t need to say if len(element) > 0: in the condition.\n",
      "What’s \n",
      "the difference between using if...elif as opposed to using a couple of if\n",
      "statements in a row? Well, consider the following situation:\n",
      ">>> animals = ['cat', 'dog']\n",
      ">>> if 'cat' in animals:\n",
      "...     print 1\n",
      "... elif 'dog' in animals:\n",
      "...     print 2\n",
      "...\n",
      "1\n",
      "Since the if clause of the statement is satisfied, Python never tries to evaluate the\n",
      "elif clause, so we never get to print out 2. By contrast, if we replaced the elif by an\n",
      "if, then we would print out both 1 and 2. So an elif clause potentially gives us more\n",
      "information than a bare if clause; when it evaluates to true, it tells us not only that the\n",
      "condition is satisfied, but also that the condition of the main if clause was not satisfied.\n",
      "The functions all() and any() can be applied to a list (or other sequence) to check\n",
      "whether all or any items meet some condition:\n",
      ">>> sent = ['No', 'good', 'fish', 'goes', 'anywhere', 'without', 'a', 'porpoise', '.']\n",
      ">>> all(len(w) > 4 for w in sent)\n",
      "False\n",
      ">>> any(len(w) > 4 for w in sent)\n",
      "True\n",
      "4.2  Sequences\n",
      "So far, we have seen two kinds of sequence object: strings and lists. Another kind of\n",
      "sequence is called a tuple. Tuples are formed with the comma operator \n",
      ", and typically\n",
      "enclosed \n",
      "using parentheses. We’ve actually seen them in the previous chapters, and\n",
      "sometimes referred to them as “pairs,” since there were always two members. However,\n",
      "tuples can have any number of members. Like lists and strings, tuples can be indexed\n",
      " and sliced \n",
      " , and have a length \n",
      " .\n",
      ">>> t = 'walk', 'fem', 3 \n",
      ">>> t\n",
      "('walk', 'fem', 3)\n",
      "4.2  Sequences | 133...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 154, 'page_label': '133', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4501}\n",
      "\n",
      "--- Chunk 4502 ---\n",
      "Content:\n",
      ">>> t[0] \n",
      "'walk'\n",
      ">>> t[1:] \n",
      "('fem', 3)\n",
      ">>> len(t) \n",
      "Caution!\n",
      "Tuples \n",
      "are constructed using the comma operator. Parentheses are a\n",
      "more general feature of Python syntax, designed for grouping. A tuple\n",
      "containing the single element 'snark' is defined by adding a trailing\n",
      "comma, like this: 'snark',. The empty tuple is a special case, and is\n",
      "defined using empty parentheses ().\n",
      "Let’s compare strings, lists, and tuples directly, and do the indexing, slice, and length\n",
      "operation on each type:\n",
      ">>> raw = 'I turned off the spectroroute'\n",
      ">>> text = ['I', 'turned', 'off', 'the', 'spectroroute']\n",
      ">>> pair = (6, 'turned')\n",
      ">>> raw[2], text[3], pair[1]\n",
      "('t', 'the', 'turned')\n",
      ">>> raw[-3:], text[-3:], pair[-3:]\n",
      "('ute', ['off', 'the', 'spectroroute'], (6, 'turned'))\n",
      ">>> len(raw), len(text), len(pair)\n",
      "(29, 5, 2)\n",
      "Notice in this code sample that we computed multiple values on a single line, separated\n",
      "by commas. These comma-separated expressions are actually just tuples—Python al-\n",
      "lows us to omit the parentheses around tuples if there is no ambiguity. When we print\n",
      "a tuple, the parentheses are always displayed. By using tuples in this way, we are im-\n",
      "plicitly aggregating items together.\n",
      "Your Turn: Define a set, e.g., using set(text), and see what happens\n",
      "when you convert it to a list or iterate over its members.\n",
      "Operating on Sequence Types\n",
      "We can iterate over the items in a sequence s in a variety of useful ways, as shown in\n",
      "Table 4-1.\n",
      "Table 4-1. Various ways to iterate over sequences\n",
      "Python expression Comment\n",
      "for item in s Iterate over the items of s\n",
      "for item in sorted(s) Iterate over the items of s in order\n",
      "for item in set(s) Iterate over unique elements of s\n",
      "134 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 155, 'page_label': '134', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4502}\n",
      "\n",
      "--- Chunk 4503 ---\n",
      "Content:\n",
      "Python expression Comment\n",
      "for item in reversed(s) Iterate over elements of s in reverse\n",
      "for item in set(s).difference(t) Iterate over elements of s not in t\n",
      "for item in random.shuffle(s) Iterate over elements of s in random order\n",
      "The sequence functions illustrated in Table 4-1 can be combined in various ways; for\n",
      "example, to get unique elements of s sorted in reverse, use reversed(sorted(set(s))).\n",
      "We can convert between these sequence types. For example, tuple(s) converts any\n",
      "kind of sequence into a tuple, and list(s) converts any kind of sequence into a list.\n",
      "We can convert a list of strings to a single string using the join() function, e.g.,\n",
      "':'.join(words).\n",
      "Some other objects, such as a FreqDist, can be converted into a sequence (using\n",
      "list()) and support iteration:\n",
      ">>> raw = 'Red lorry, yellow lorry, red lorry, yellow lorry.'\n",
      ">>> text = nltk.word_tokenize(raw)\n",
      ">>> fdist = nltk.FreqDist(text)\n",
      ">>> list(fdist)\n",
      "['lorry', ',', 'yellow', '.', 'Red', 'red']\n",
      ">>> for key in fdist:\n",
      "...     print fdist[key],\n",
      "...\n",
      "4 3 2 1 1 1\n",
      "In the next example, we use tuples to re-arrange the contents of our list. (We can omit\n",
      "the parentheses because the comma has higher precedence than assignment.)\n",
      ">>> words = ['I', 'turned', 'off', 'the', 'spectroroute']\n",
      ">>> words[2], words[3], words[4] = words[3], words[4], words[2]\n",
      ">>> words\n",
      "['I', 'turned', 'the', 'spectroroute', 'off']\n",
      "This is an idiomatic and readable way to move items inside a list. It is equivalent to the\n",
      "following traditional way of doing such tasks that does not use tuples (notice that this\n",
      "method needs a temporary variable tmp).\n",
      ">>> tmp = words[2]\n",
      ">>> words[2] = words[3]\n",
      ">>> words[3] = words[4]\n",
      ">>> words[4] = tmp\n",
      "As we have seen, Python has sequence functions such as sorted() and reversed() that\n",
      "rearrange the items of a sequence. There are also functions that modify the structure of\n",
      "a sequence, which can be handy for language processing. Thus, zip() takes the items\n",
      "of two or more sequences and “zips” them together into a single list of pairs. Given a\n",
      "sequence s, enumerate(s) returns pairs consisting of an index and the item at that index.\n",
      ">>> words = ['I', 'turned', 'off', 'the', 'spectroroute']\n",
      ">>> tags = ['noun', 'verb', 'prep', 'det', 'noun']\n",
      ">>> zip(words, tags)\n",
      "4.2  Sequences | 135...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 156, 'page_label': '135', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4503}\n",
      "\n",
      "--- Chunk 4504 ---\n",
      "Content:\n",
      "[('I', 'noun'), ('turned', 'verb'), ('off', 'prep'),\n",
      "('the', 'det'), ('spectroroute', 'noun')]\n",
      ">>> list(enumerate(words))\n",
      "[(0, 'I'), (1, 'turned'), (2, 'off'), (3, 'the'), (4, 'spectroroute')]\n",
      "For \n",
      "some NLP tasks it is necessary to cut up a sequence into two or more parts. For\n",
      "instance, we might want to “train” a system on 90% of the data and test it on the\n",
      "remaining 10%. To do this we decide the location where we want to cut the data \n",
      " ,\n",
      "then cut the sequence at that location \n",
      " .\n",
      ">>> text = nltk.corpus.nps_chat.words()\n",
      ">>> cut = int(0.9 * len(text)) \n",
      ">>> training_data, test_data = text[:cut], text[cut:] \n",
      ">>> text == training_data + test_data \n",
      "True\n",
      ">>> len(training_data) / len(test_data) \n",
      "9\n",
      "We \n",
      "can verify that none of the original data is lost during this process, nor is it dupli-\n",
      "cated \n",
      " . We can also verify that the ratio of the sizes of the two pieces is what we\n",
      "intended \n",
      " .\n",
      "Combining Different Sequence Types\n",
      "Let’s \n",
      "combine our knowledge of these three sequence types, together with list com-\n",
      "prehensions, to perform the task of sorting the words in a string by their length.\n",
      ">>> words = 'I turned off the spectroroute'.split() \n",
      ">>> wordlens = [(len(word), word) for word in words] \n",
      ">>> wordlens.sort() \n",
      ">>> ' '.join(w for (_, w) in wordlens) \n",
      "'I off the turned spectroroute'\n",
      "Each \n",
      "of the preceding lines of code contains a significant feature. A simple string is\n",
      "actually an object with methods defined on it, such as split() \n",
      " . We use a list com-\n",
      "prehension \n",
      "to build a list of tuples \n",
      " , where each tuple consists of a number (the word\n",
      "length) and the word, e.g., (3, 'the'). We use the sort() method \n",
      "  to sort the list in\n",
      "place. \n",
      "Finally, we discard the length information and join the words back into a single\n",
      "string \n",
      " . (The underscore \n",
      "  is just a regular Python variable, but we can use underscore\n",
      "by convention to indicate that we will not use its value.)\n",
      "We \n",
      "began by talking about the commonalities in these sequence types, but the previous\n",
      "code illustrates important differences in their roles. First, strings appear at the beginning\n",
      "and the end: this is typical in the context where our program is reading in some text\n",
      "and producing output for us to read. Lists and tuples are used in the middle, but for\n",
      "different purposes. A list is typically a sequence of objects all having the same type, of\n",
      "arbitrary length. We often use lists to hold sequences of words. In contrast, a tuple is\n",
      "typically a collection of objects of different types, of fixed length. We often use a tuple\n",
      "to hold a record, a collection of different fields relating to some entity. This distinction\n",
      "between the use of lists and tuples takes some getting used to, so here is another\n",
      "example:\n",
      "136 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 157, 'page_label': '136', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4504}\n",
      "\n",
      "--- Chunk 4505 ---\n",
      "Content:\n",
      ">>> lexicon = [\n",
      "...     ('the', 'det', ['Di:', 'D@']),\n",
      "...     ('off', 'prep', ['Qf', 'O:f'])\n",
      "... ]\n",
      "Here, \n",
      "a lexicon is represented as a list because it is a collection of objects of a single\n",
      "type—lexical entries—of no predetermined length. An individual entry is represented\n",
      "as a tuple because it is a collection of objects with different interpretations, such as the\n",
      "orthographic form, the part-of-speech, and the pronunciations (represented in the\n",
      "SAMPA computer-readable phonetic alphabet; see http://www.phon.ucl.ac.uk/home/\n",
      "sampa/). Note that these pronunciations are stored using a list. (Why?)\n",
      "A good way to decide when to use tuples versus lists is to ask whether\n",
      "the \n",
      "interpretation of an item depends on its position. For example, a\n",
      "tagged token combines two strings having different interpretations, and\n",
      "we choose to interpret the first item as the token and the second item\n",
      "as the tag. Thus we use tuples like this: ('grail', 'noun'). A tuple of\n",
      "the form ('noun', 'grail') would be non-sensical since it would be a\n",
      "word noun tagged grail. In contrast, the elements of a text are all tokens,\n",
      "and position is not significant. Thus we use lists like this: ['venetian',\n",
      "'blind']. A list of the form ['blind', 'venetian'] would be equally\n",
      "valid. The linguistic meaning of the words might be different, but the\n",
      "interpretation of list items as tokens is unchanged.\n",
      "The distinction between lists and tuples has been described in terms of usage. However,\n",
      "there is a more fundamental difference: in Python, lists are mutable, whereas tuples\n",
      "are immutable. In other words, lists can be modified, whereas tuples cannot. Here are\n",
      "some of the operations on lists that do in-place modification of the list:\n",
      ">>> lexicon.sort()\n",
      ">>> lexicon[1] = ('turned', 'VBD', ['t3:nd', 't3`nd'])\n",
      ">>> del lexicon[0]\n",
      "Your Turn:  Convert lexicon to a tuple, using lexicon =\n",
      "tuple(lexicon), then try each of the operations, to confirm that none of\n",
      "them is permitted on tuples.\n",
      "Generator Expressions\n",
      "We’ve been making heavy use of list comprehensions, for compact and readable pro-\n",
      "cessing of texts. Here’s an example where we tokenize and normalize a text:\n",
      ">>> text = '''\"When I use a word,\" Humpty Dumpty said in rather a scornful tone,\n",
      "... \"it means just what I choose it to mean - neither more nor less.\"'''\n",
      ">>> [w.lower() for w in nltk.word_tokenize(text)]\n",
      "['\"', 'when', 'i', 'use', 'a', 'word', ',', '\"', 'humpty', 'dumpty', 'said', ...]\n",
      "4.2  Sequences | 137...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 158, 'page_label': '137', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4505}\n",
      "\n",
      "--- Chunk 4506 ---\n",
      "Content:\n",
      "Suppose we now want to process these words further. We can do this by inserting the\n",
      "preceding \n",
      "expression inside a call to some other function \n",
      " , but Python allows us to\n",
      "omit the brackets \n",
      " .\n",
      ">>> max([w.lower() for w in nltk.word_tokenize(text)]) \n",
      "'word'\n",
      ">>> max(w.lower() for w in nltk.word_tokenize(text)) \n",
      "'word'\n",
      "The \n",
      "second line uses a generator expression. This is more than a notational conven-\n",
      "ience: in many language processing situations, generator expressions will be more ef-\n",
      "ficient. In \n",
      " , storage for the list object must be allocated before the value of max() is\n",
      "computed. If the text is very large, this could be slow. In \n",
      " , the data is streamed to the\n",
      "calling \n",
      "function. Since the calling function simply has to find the maximum value—the\n",
      "word that comes latest in lexicographic sort order—it can process the stream of data\n",
      "without having to store anything more than the maximum value seen so far.\n",
      "4.3  Questions of Style\n",
      "Programming is as much an art as a science. The undisputed “bible” of programming,\n",
      "a 2,500 page multivolume work by Donald Knuth, is called The Art of Computer Pro-\n",
      "gramming. Many books have been written on Literate Programming, recognizing that\n",
      "humans, not just computers, must read and understand programs. Here we pick up on\n",
      "some issues of programming style that have important ramifications for the readability\n",
      "of your code, including code layout, procedural versus declarative style, and the use of\n",
      "loop variables.\n",
      "Python Coding Style\n",
      "When writing programs you make many subtle choices about names, spacing, com-\n",
      "ments, and so on. When you look at code written by other people, needless differences\n",
      "in style make it harder to interpret the code. Therefore, the designers of the Python\n",
      "language have published a style guide for Python code, available at http://www.python\n",
      ".org/dev/peps/pep-0008/. The underlying value presented in the style guide is consis-\n",
      "tency, for the purpose of maximizing the readability of code. We briefly review some\n",
      "of its key recommendations here, and refer readers to the full guide for detailed dis-\n",
      "cussion with examples.\n",
      "Code layout should use four spaces per indentation level. You should make sure that\n",
      "when you write Python code in a file, you avoid tabs for indentation, since these can\n",
      "be misinterpreted by different text editors and the indentation can be messed up. Lines\n",
      "should be less than 80 characters long; if necessary, you can break a line inside paren-\n",
      "theses, brackets, or braces, because Python is able to detect that the line continues over\n",
      "to the next line, as in the following examples:\n",
      ">>> cv_word_pairs = [(cv, w) for w in rotokas_words\n",
      "...                          for cv in re.findall('[ptksvr][aeiou]', w)]\n",
      "138 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 159, 'page_label': '138', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4506}\n",
      "\n",
      "--- Chunk 4507 ---\n",
      "Content:\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (genre, word)\n",
      "...           for genre in brown.categories()\n",
      "...           for word in brown.words(categories=genre))\n",
      " \n",
      ">>> ha_words = ['aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh', 'ahhahahaha',\n",
      "...             'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'ha',\n",
      "...             'haaa', 'hah', 'haha', 'hahaaa', 'hahah', 'hahaha']\n",
      "If \n",
      "you need to break a line outside parentheses, brackets, or braces, you can often add\n",
      "extra parentheses, and you can always add a backslash at the end of the line that is\n",
      "broken:\n",
      ">>> if (len(syllables) > 4 and len(syllables[2]) == 3 and\n",
      "...    syllables[2][2] in [aeiou] and syllables[2][3] == syllables[1][3]):\n",
      "...     process(syllables)\n",
      ">>> if len(syllables) > 4 and len(syllables[2]) == 3 and \\\n",
      "...    syllables[2][2] in [aeiou] and syllables[2][3] == syllables[1][3]:\n",
      "...     process(syllables)\n",
      "Typing spaces instead of tabs soon becomes a chore. Many program-\n",
      "ming \n",
      "editors have built-in support for Python, and can automatically\n",
      "indent code and highlight any syntax errors (including indentation er-\n",
      "rors). For a list of Python-aware editors, please see http://wiki.python\n",
      ".org/moin/PythonEditors.\n",
      "Procedural Versus Declarative Style\n",
      "We have just seen how the same task can be performed in different ways, with impli-\n",
      "cations for efficiency. Another factor influencing program development is programming\n",
      "style. Consider the following program to compute the average length of words in the\n",
      "Brown Corpus:\n",
      ">>> tokens = nltk.corpus.brown.words(categories='news')\n",
      ">>> count = 0\n",
      ">>> total = 0\n",
      ">>> for token in tokens:\n",
      "...     count += 1\n",
      "...     total += len(token)\n",
      ">>> print total / count\n",
      "4.2765382469\n",
      "In this program we use the variable count to keep track of the number of tokens seen,\n",
      "and total to store the combined length of all words. This is a low-level style, not far\n",
      "removed from machine code, the primitive operations performed by the computer’s\n",
      "CPU. The two variables are just like a CPU’s registers, accumulating values at many\n",
      "intermediate stages, values that are meaningless until the end. We say that this program\n",
      "is written in a procedural style, dictating the machine operations step by step. Now\n",
      "consider the following program that computes the same thing:\n",
      "4.3  Questions of Style | 139...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 160, 'page_label': '139', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4507}\n",
      "\n",
      "--- Chunk 4508 ---\n",
      "Content:\n",
      ">>> total = sum(len(t) for t in tokens)\n",
      ">>> print total / len(tokens)\n",
      "4.2765382469\n",
      "The \n",
      "first line uses a generator expression to sum the token lengths, while the second\n",
      "line computes the average as before. Each line of code performs a complete, meaningful\n",
      "task, which can be understood in terms of high-level properties like: “total is the sum\n",
      "of the lengths of the tokens.” Implementation details are left to the Python interpreter.\n",
      "The second program uses a built-in function, and constitutes programming at a more\n",
      "abstract level; the resulting code is more declarative. Let’s look at an extreme example:\n",
      ">>> word_list = []\n",
      ">>> len_word_list = len(word_list)\n",
      ">>> i = 0\n",
      ">>> while i < len(tokens):\n",
      "...     j = 0\n",
      "...     while j < len_word_list and word_list[j] < tokens[i]:\n",
      "...         j += 1\n",
      "...     if j == 0 or tokens[i] != word_list[j]:\n",
      "...         word_list.insert(j, tokens[i])\n",
      "...         len_word_list += 1\n",
      "...     i += 1\n",
      "The equivalent declarative version uses familiar built-in functions, and its purpose is\n",
      "instantly recognizable:\n",
      ">>> word_list = sorted(set(tokens))\n",
      "Another case where a loop counter seems to be necessary is for printing a counter with\n",
      "each line of output. Instead, we can use enumerate(), which processes a sequence s and\n",
      "produces a tuple of the form (i, s[i]) for each item in s, starting with (0, s[0]). Here\n",
      "we enumerate the keys of the frequency distribution, and capture the integer-string pair\n",
      "in the variables rank and word. We print rank+1 so that the counting appears to start\n",
      "from 1, as required when producing a list of ranked items.\n",
      ">>> fd = nltk.FreqDist(nltk.corpus.brown.words())\n",
      ">>> cumulative = 0.0\n",
      ">>> for rank, word in enumerate(fd):\n",
      "...     cumulative += fd[word] * 100 / fd.N()\n",
      "...     print \"%3d %6.2f%% %s\" % (rank+1, cumulative, word)\n",
      "...     if cumulative > 25:\n",
      "...         break\n",
      "...\n",
      "  1   5.40% the\n",
      "  2  10.42% ,\n",
      "  3  14.67% .\n",
      "  4  17.78% of\n",
      "  5  20.19% and\n",
      "  6  22.40% to\n",
      "  7  24.29% a\n",
      "  8  25.97% in\n",
      "It’s sometimes tempting to use loop variables to store a maximum or minimum value\n",
      "seen so far. Let’s use this method to find the longest word in a text.\n",
      "140 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 161, 'page_label': '140', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4508}\n",
      "\n",
      "--- Chunk 4509 ---\n",
      "Content:\n",
      ">>> text = nltk.corpus.gutenberg.words('milton-paradise.txt')\n",
      ">>> longest = ''\n",
      ">>> for word in text:\n",
      "...     if len(word) > len(longest):\n",
      "...         longest = word\n",
      ">>> longest\n",
      "'unextinguishable'\n",
      "However, \n",
      "a more transparent solution uses two list comprehensions, both having forms\n",
      "that should be familiar by now:\n",
      ">>> maxlen = max(len(word) for word in text)\n",
      ">>> [word for word in text if len(word) == maxlen]\n",
      "['unextinguishable', 'transubstantiate', 'inextinguishable', 'incomprehensible']\n",
      "Note that our first solution found the first word having the longest length, while the\n",
      "second solution found all of the longest words (which is usually what we would want).\n",
      "Although there’s a theoretical efficiency difference between the two solutions, the main\n",
      "overhead is reading the data into main memory; once it’s there, a second pass through\n",
      "the data is effectively instantaneous. We also need to balance our concerns about pro-\n",
      "gram efficiency with programmer efficiency. A fast but cryptic solution will be harder\n",
      "to understand and maintain.\n",
      "Some Legitimate Uses for Counters\n",
      "There are cases where we still want to use loop variables in a list comprehension. For\n",
      "example, we need to use a loop variable to extract successive overlapping n-grams from\n",
      "a list:\n",
      ">>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
      ">>> n = 3\n",
      ">>> [sent[i:i+n] for i in range(len(sent)-n+1)]\n",
      "[['The', 'dog', 'gave'],\n",
      " ['dog', 'gave', 'John'],\n",
      " ['gave', 'John', 'the'],\n",
      " ['John', 'the', 'newspaper']]\n",
      "It is quite tricky to get the range of the loop variable right. Since this is a common\n",
      "operation in NLP, NLTK supports it with functions bigrams(text) and\n",
      "trigrams(text), and a general-purpose ngrams(text, n).\n",
      "Here’s an example of how we can use loop variables in building multidimensional\n",
      "structures. For example, to build an array with m rows and n columns, where each cell\n",
      "is a set, we could use a nested list comprehension:\n",
      ">>> m, n = 3, 7\n",
      ">>> array = [[set() for i in range(n)] for j in range(m)]\n",
      ">>> array[2][5].add('Alice')\n",
      ">>> pprint.pprint(array)\n",
      "[[set([]), set([]), set([]), set([]), set([]), set([]), set([])],\n",
      " [set([]), set([]), set([]), set([]), set([]), set([]), set([])],\n",
      " [set([]), set([]), set([]), set([]), set([]), set(['Alice']), set([])]]\n",
      "4.3  Questions of Style | 141...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 162, 'page_label': '141', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4509}\n",
      "\n",
      "--- Chunk 4510 ---\n",
      "Content:\n",
      "Observe that the loop variables i and j are not used anywhere in the resulting object;\n",
      "they are just needed for a syntactically correct for statement. As another example of\n",
      "this usage, observe that the expression ['very' for i in range(3)] produces a list\n",
      "containing three instances of 'very', with no integers in sight.\n",
      "Note that it would be incorrect to do this work using multiplication, for reasons con-\n",
      "cerning object copying that were discussed earlier in this section.\n",
      ">>> array = [[set()] * n] * m\n",
      ">>> array[2][5].add(7)\n",
      ">>> pprint.pprint(array)\n",
      "[[set([7]), set([7]), set([7]), set([7]), set([7]), set([7]), set([7])],\n",
      " [set([7]), set([7]), set([7]), set([7]), set([7]), set([7]), set([7])],\n",
      " [set([7]), set([7]), set([7]), set([7]), set([7]), set([7]), set([7])]]\n",
      "Iteration is an important programming device. It is tempting to adopt idioms from other\n",
      "languages. However, Python offers some elegant and highly readable alternatives, as\n",
      "we have seen.\n",
      "4.4  Functions: The Foundation of Structured Programming\n",
      "Functions provide an effective way to package and reuse program code, as already\n",
      "explained in Section 2.3. For example, suppose we find that we often want to read text\n",
      "from an HTML file. This involves several steps: opening the file, reading it in, normal-\n",
      "izing whitespace, and stripping HTML markup. We can collect these steps into a func-\n",
      "tion, and give it a name such as get_text(), as shown in Example 4-1.\n",
      "Example 4-1. Read text from a file.\n",
      "import re\n",
      "def get_text(file):\n",
      "    \"\"\"Read text from a file, normalizing whitespace and stripping HTML markup.\"\"\"\n",
      "    text = open(file).read()\n",
      "    text = re.sub('\\s+', ' ', text)\n",
      "    text = re.sub(r'<.*?>', ' ', text)\n",
      "    return text\n",
      "Now, any time we want to get cleaned-up text from an HTML file, we can just call\n",
      "get_text() with the name of the file as its only argument. It will return a string, and we\n",
      "can assign this to a variable, e.g., contents = get_text(\"test.html\"). Each time we\n",
      "want to use this series of steps, we only have to call the function.\n",
      "Using functions has the benefit of saving space in our program. More importantly, our\n",
      "choice of name for the function helps make the program readable. In the case of the\n",
      "preceding example, whenever our program needs to read cleaned-up text from a file\n",
      "we don’t have to clutter the program with four lines of code; we simply need to call\n",
      "get_text(). This naming helps to provide some “semantic interpretation”—it helps a\n",
      "reader of our program to see what the program “means.”\n",
      "142 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 163, 'page_label': '142', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4510}\n",
      "\n",
      "--- Chunk 4511 ---\n",
      "Content:\n",
      "Notice that this example function definition contains a string. The first string inside a\n",
      "function \n",
      "definition is called a docstring. Not only does it document the purpose of the\n",
      "function to someone reading the code, it is accessible to a programmer who has loaded\n",
      "the code from a file:\n",
      ">>> help(get_text)\n",
      "Help on function get_text:\n",
      "get_text(file)\n",
      "    Read text from a file, normalizing whitespace\n",
      "    and stripping HTML markup.\n",
      "We have seen that functions help to make our work reusable and readable. They also\n",
      "help make it reliable. When we reuse code that has already been developed and tested,\n",
      "we can be more confident that it handles a variety of cases correctly. We also remove\n",
      "the risk of forgetting some important step or introducing a bug. The program that calls\n",
      "our function also has increased reliability. The author of that program is dealing with\n",
      "a shorter program, and its components behave transparently.\n",
      "To summarize, as its name suggests, a function captures functionality. It is a segment\n",
      "of code that can be given a meaningful name and which performs a well-defined task.\n",
      "Functions allow us to abstract away from the details, to see a bigger picture, and to\n",
      "program more effectively.\n",
      "The rest of this section takes a closer look at functions, exploring the mechanics and\n",
      "discussing ways to make your programs easier to read.\n",
      "Function Inputs and Outputs\n",
      "We pass information to functions using a function’s parameters, the parenthesized list\n",
      "of variables and constants following the function’s name in the function definition.\n",
      "Here’s a complete example:\n",
      ">>> def repeat(msg, num):  \n",
      "...     return ' '.join([msg] * num)\n",
      ">>> monty = 'Monty Python'\n",
      ">>> repeat(monty, 3) \n",
      "'Monty Python Monty Python Monty Python'\n",
      "We \n",
      "first define the function to take two parameters, msg and num \n",
      " . Then, we call the\n",
      "function \n",
      "and pass it two arguments, monty and 3 \n",
      " ; these arguments fill the “place-\n",
      "holders” \n",
      "provided by the parameters and provide values for the occurrences of msg and\n",
      "num in the function body.\n",
      "It is not necessary to have any parameters, as we see in the following example:\n",
      ">>> def monty():\n",
      "...     return \"Monty Python\"\n",
      ">>> monty()\n",
      "'Monty Python'\n",
      "4.4  Functions: The Foundation of Structured Programming | 143...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 164, 'page_label': '143', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4511}\n",
      "\n",
      "--- Chunk 4512 ---\n",
      "Content:\n",
      "A function usually communicates its results back to the calling program via the\n",
      "return \n",
      "statement, as we have just seen. To the calling program, it looks as if the function\n",
      "call had been replaced with the function’s result:\n",
      ">>> repeat(monty(), 3)\n",
      "'Monty Python Monty Python Monty Python'\n",
      ">>> repeat('Monty Python', 3)\n",
      "'Monty Python Monty Python Monty Python'\n",
      "A Python function is not required to have a return statement. Some functions do their\n",
      "work as a side effect, printing a result, modifying a file, or updating the contents of a\n",
      "parameter to the function (such functions are called “procedures” in some other\n",
      "programming languages).\n",
      "Consider the following three sort functions. The third one is dangerous because a pro-\n",
      "grammer could use it without realizing that it had modified its input. In general, func-\n",
      "tions should modify the contents of a parameter ( my_sort1()), or return a value\n",
      "(my_sort2()), but not both (my_sort3()).\n",
      ">>> def my_sort1(mylist):      # good: modifies its argument, no return value\n",
      "...     mylist.sort()\n",
      ">>> def my_sort2(mylist):      # good: doesn't touch its argument, returns value\n",
      "...     return sorted(mylist)\n",
      ">>> def my_sort3(mylist):      # bad: modifies its argument and also returns it\n",
      "...     mylist.sort()\n",
      "...     return mylist\n",
      "Parameter Passing\n",
      "Back in Section 4.1, you saw that assignment works on values, but that the value of a\n",
      "structured object is a reference to that object. The same is true for functions. Python\n",
      "interprets function parameters as values (this is known as call-by-value). In the fol-\n",
      "lowing code, set_up() has two parameters, both of which are modified inside the func-\n",
      "tion. We begin by assigning an empty string to w and an empty dictionary to p. After\n",
      "calling the function, w is unchanged, while p is changed:\n",
      ">>> def set_up(word, properties):\n",
      "...     word = 'lolcat'\n",
      "...     properties.append('noun')\n",
      "...     properties = 5\n",
      "...\n",
      ">>> w = ''\n",
      ">>> p = []\n",
      ">>> set_up(w, p)\n",
      ">>> w\n",
      "''\n",
      ">>> p\n",
      "['noun']\n",
      "Notice that w was not changed by the function. When we called set_up(w, p), the value\n",
      "of w (an empty string) was assigned to a new variable word. Inside the function, the value\n",
      "144 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 165, 'page_label': '144', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4512}\n",
      "\n",
      "--- Chunk 4513 ---\n",
      "Content:\n",
      "of word was modified. However, that change did not propagate to w. This parameter\n",
      "passing is identical to the following sequence of assignments:\n",
      ">>> w = ''\n",
      ">>> word = w\n",
      ">>> word = 'lolcat'\n",
      ">>> w\n",
      "''\n",
      "Let’s look at what happened with the list p. When we called set_up(w, p), the value of\n",
      "p (a reference to an empty list) was assigned to a new local variable properties, so both\n",
      "variables now reference the same memory location. The function modifies\n",
      "properties, and this change is also reflected in the value of p, as we saw. The function\n",
      "also assigned a new value to properties (the number 5); this did not modify the contents\n",
      "at that memory location, but created a new local variable. This behavior is just as if we\n",
      "had done the following sequence of assignments:\n",
      ">>> p = []\n",
      ">>> properties = p\n",
      ">>> properties.append['noun']\n",
      ">>> properties = 5\n",
      ">>> p\n",
      "['noun']\n",
      "Thus, to understand Python’s call-by-value parameter passing, it is enough to under-\n",
      "stand how assignment works. Remember that you can use the id() function and is\n",
      "operator to check your understanding of object identity after each statement.\n",
      "Variable Scope\n",
      "Function definitions create a new local scope for variables. When you assign to a new\n",
      "variable inside the body of a function, the name is defined only within that function.\n",
      "The name is not visible outside the function, or in other functions. This behavior means\n",
      "you can choose variable names without being concerned about collisions with names\n",
      "used in your other function definitions.\n",
      "When you refer to an existing name from within the body of a function, the Python\n",
      "interpreter first tries to resolve the name with respect to the names that are local to the\n",
      "function. If nothing is found, the interpreter checks whether it is a global name within\n",
      "the module. Finally, if that does not succeed, the interpreter checks whether the name\n",
      "is a Python built-in. This is the so-called LGB rule of name resolution: local, then\n",
      "global, then built-in.\n",
      "Caution!\n",
      "A \n",
      "function can create a new global variable, using the global declaration.\n",
      "However, this practice should be avoided as much as possible. Defining\n",
      "global variables inside a function introduces dependencies on context\n",
      "and limits the portability (or reusability) of the function. In general you\n",
      "should use parameters for function inputs and return values for function\n",
      "outputs.\n",
      "4.4  Functions: The Foundation of Structured Programming | 145...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 166, 'page_label': '145', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4513}\n",
      "\n",
      "--- Chunk 4514 ---\n",
      "Content:\n",
      "Checking Parameter Types\n",
      "Python \n",
      "does not force us to declare the type of a variable when we write a program,\n",
      "and this permits us to define functions that are flexible about the type of their argu-\n",
      "ments. For example, a tagger might expect a sequence of words, but it wouldn’t care\n",
      "whether this sequence is expressed as a list, a tuple, or an iterator (a new sequence type\n",
      "that we’ll discuss later).\n",
      "However, often we want to write programs for later use by others, and want to program\n",
      "in a defensive style, providing useful warnings when functions have not been invoked\n",
      "correctly. The author of the following tag() function assumed that its argument would\n",
      "always be a string.\n",
      ">>> def tag(word):\n",
      "...     if word in ['a', 'the', 'all']:\n",
      "...         return 'det'\n",
      "...     else:\n",
      "...         return 'noun'\n",
      "...\n",
      ">>> tag('the')\n",
      "'det'\n",
      ">>> tag('knight')\n",
      "'noun'\n",
      ">>> tag([\"'Tis\", 'but', 'a', 'scratch']) \n",
      "'noun'\n",
      "The \n",
      "function returns sensible values for the arguments 'the' and 'knight', but look\n",
      "what happens when it is passed a list \n",
      " —it fails to complain, even though the result\n",
      "which it returns is clearly incorrect. The author of this function could take some extra\n",
      "steps to ensure that the word parameter of the tag() function is a string. A naive ap-\n",
      "proach would be to check the type of the argument using if not type(word) is str,\n",
      "and if word is not a string, to simply return Python’s special empty value, None. This is\n",
      "a slight improvement, because the function is checking the type of the argument, and\n",
      "trying to return a “special” diagnostic value for the wrong input. However, it is also\n",
      "dangerous because the calling program may not detect that None is intended as a “spe-\n",
      "cial” value, and this diagnostic return value may then be propagated to other parts of\n",
      "the program with unpredictable consequences. This approach also fails if the word is\n",
      "a Unicode string, which has type unicode, not str. Here’s a better solution, using an\n",
      "assert statement together with Python’s basestring type that generalizes over both\n",
      "unicode and str.\n",
      ">>> def tag(word):\n",
      "...     assert isinstance(word, basestring), \"argument to tag() must be a string\"\n",
      "...     if word in ['a', 'the', 'all']:\n",
      "...         return 'det'\n",
      "...     else:\n",
      "...         return 'noun'\n",
      "If the assert statement fails, it will produce an error that cannot be ignored, since it\n",
      "halts program execution. Additionally, the error message is easy to interpret. Adding\n",
      "146 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 167, 'page_label': '146', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4514}\n",
      "\n",
      "--- Chunk 4515 ---\n",
      "Content:\n",
      "assertions to a program helps you find logical errors, and is a kind of defensive pro-\n",
      "gramming. \n",
      "A more fundamental approach is to document the parameters to each\n",
      "function using docstrings, as described later in this section.\n",
      "Functional Decomposition\n",
      "Well-structured programs usually make extensive use of functions. When a block of\n",
      "program code grows longer than 10–20 lines, it is a great help to readability if the code\n",
      "is broken up into one or more functions, each one having a clear purpose. This is\n",
      "analogous to the way a good essay is divided into paragraphs, each expressing one main\n",
      "idea.\n",
      "Functions provide an important kind of abstraction. They allow us to group multiple\n",
      "actions into a single, complex action, and associate a name with it. (Compare this with\n",
      "the way we combine the actions of go and bring back into a single more complex action\n",
      "fetch.) When we use functions, the main program can be written at a higher level of\n",
      "abstraction, making its structure transparent, as in the following:\n",
      ">>> data = load_corpus()\n",
      ">>> results = analyze(data)\n",
      ">>> present(results)\n",
      "Appropriate use of functions makes programs more readable and maintainable. Addi-\n",
      "tionally, it becomes possible to reimplement a function—replacing the function’s body\n",
      "with more efficient code—without having to be concerned with the rest of the program.\n",
      "Consider the freq_words function in Example 4-2. It updates the contents of a frequency\n",
      "distribution that is passed in as a parameter, and it also prints a list of the n most\n",
      "frequent words.\n",
      "Example 4-2. Poorly designed function to compute frequent words.\n",
      "def freq_words(url, freqdist, n):\n",
      "    text = nltk.clean_url(url)\n",
      "    for word in nltk.word_tokenize(text):\n",
      "        freqdist.inc(word.lower())\n",
      "    print freqdist.keys()[:n]\n",
      ">>> constitution = \"http://www.archives.gov/national-archives-experience\" \\\n",
      "...                \"/charters/constitution_transcript.html\"\n",
      ">>> fd = nltk.FreqDist()\n",
      ">>> freq_words(constitution, fd, 20)\n",
      "['the', 'of', 'charters', 'bill', 'constitution', 'rights', ',',\n",
      "'declaration', 'impact', 'freedom', '-', 'making', 'independence']\n",
      "This function has a number of problems. The function has two side effects: it modifies\n",
      "the contents of its second parameter, and it prints a selection of the results it has com-\n",
      "puted. The function would be easier to understand and to reuse elsewhere if we initialize\n",
      "the FreqDist() object inside the function (in the same place it is populated), and if we\n",
      "moved the selection and display of results to the calling program. In Example 4-3 we\n",
      "refactor this function, and simplify its interface by providing a single url parameter.\n",
      "4.4  Functions: The Foundation of Structured Programming | 147...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 168, 'page_label': '147', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4515}\n",
      "\n",
      "--- Chunk 4516 ---\n",
      "Content:\n",
      "Example 4-3. Well-designed function to compute frequent words.\n",
      "def freq_words(url):\n",
      "    freqdist = nltk.FreqDist()\n",
      "    text = nltk.clean_url(url)\n",
      "    for word in nltk.word_tokenize(text):\n",
      "        freqdist.inc(word.lower())\n",
      "    return freqdist\n",
      ">>> fd = freq_words(constitution)\n",
      ">>> print fd.keys()[:20]\n",
      "['the', 'of', 'charters', 'bill', 'constitution', 'rights', ',',\n",
      "'declaration', 'impact', 'freedom', '-', 'making', 'independence']\n",
      "Note \n",
      "that we have now simplified the work of freq_words to the point that we can do\n",
      "its work with three lines of code:\n",
      ">>> words = nltk.word_tokenize(nltk.clean_url(constitution))\n",
      ">>> fd = nltk.FreqDist(word.lower() for word in words)\n",
      ">>> fd.keys()[:20]\n",
      "['the', 'of', 'charters', 'bill', 'constitution', 'rights', ',',\n",
      "'declaration', 'impact', 'freedom', '-', 'making', 'independence']\n",
      "Documenting Functions\n",
      "If we have done a good job at decomposing our program into functions, then it should\n",
      "be easy to describe the purpose of each function in plain language, and provide this in\n",
      "the docstring at the top of the function definition. This statement should not explain\n",
      "how the functionality is implemented; in fact, it should be possible to reimplement the\n",
      "function using a different method without changing this statement.\n",
      "For the simplest functions, a one-line docstring is usually adequate (see Example 4-1).\n",
      "You should provide a triple-quoted string containing a complete sentence on a single\n",
      "line. For non-trivial functions, you should still provide a one-sentence summary on the\n",
      "first line, since many docstring processing tools index this string. This should be fol-\n",
      "lowed by a blank line, then a more detailed description of the functionality (see http://\n",
      "www.python.org/dev/peps/pep-0257/ for more information on docstring conventions).\n",
      "Docstrings can include a doctest block, illustrating the use of the function and the\n",
      "expected output. These can be tested automatically using Python’s docutils module.\n",
      "Docstrings should document the type of each parameter to the function, and the return\n",
      "type. At a minimum, that can be done in plain text. However, note that NLTK uses the\n",
      "“epytext” markup language to document parameters. This format can be automatically\n",
      "converted into richly structured API documentation (see http://www.nltk.org/), and in-\n",
      "cludes special handling of certain “fields,” such as @param, which allow the inputs and\n",
      "outputs of functions to be clearly documented. Example 4-4  illustrates a complete\n",
      "docstring.\n",
      "148 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 169, 'page_label': '148', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4516}\n",
      "\n",
      "--- Chunk 4517 ---\n",
      "Content:\n",
      "Example 4-4. Illustration of a complete docstring, consisting of a one-line summary, a more detailed\n",
      "explanation, \n",
      "a doctest example, and epytext markup specifying the parameters, types, return type,\n",
      "and exceptions.\n",
      "def accuracy(reference, test):\n",
      "    \"\"\"\n",
      "    Calculate the fraction of test items that equal the corresponding reference items.\n",
      "    Given a list of reference values and a corresponding list of test values,\n",
      "    return the fraction of corresponding values that are equal.\n",
      "    In particular, return the fraction of indexes\n",
      "    {0<i<=len(test)} such that C{test[i] == reference[i]}.\n",
      "    >>> accuracy(['ADJ', 'N', 'V', 'N'], ['N', 'N', 'V', 'ADJ'])\n",
      "    0.5\n",
      "@param reference: An ordered list of reference values.\n",
      "@type reference: C{list}\n",
      "@param test: A list of values to compare against the corresponding\n",
      "    reference values.\n",
      "@type test: C{list}\n",
      "@rtype: C{float}\n",
      "@raise ValueError: If C{reference} and C{length} do not have the\n",
      "    same length.\n",
      "\"\"\"\n",
      "if len(reference) != len(test):\n",
      "    raise ValueError(\"Lists must have the same length.\")\n",
      "num_correct = 0\n",
      "for x, y in izip(reference, test):\n",
      "    if x == y:\n",
      "        num_correct += 1\n",
      "return float(num_correct) / len(reference)\n",
      "4.5  Doing More with Functions\n",
      "This section discusses more advanced features, which you may prefer to skip on the\n",
      "first time through this chapter.\n",
      "Functions As Arguments\n",
      "So far the arguments we have passed into functions have been simple objects, such as\n",
      "strings, or structured objects, such as lists. Python also lets us pass a function as an\n",
      "argument to another function. Now we can abstract out the operation, and apply a\n",
      "different operation on the same data. As the following examples show, we can pass the\n",
      "built-in function len() or a user-defined function last_letter() as arguments to an-\n",
      "other function:\n",
      ">>> sent = ['Take', 'care', 'of', 'the', 'sense', ',', 'and', 'the',\n",
      "...         'sounds', 'will', 'take', 'care', 'of', 'themselves', '.']\n",
      ">>> def extract_property(prop):\n",
      "...     return [prop(word) for word in sent]\n",
      "...\n",
      "4.5  Doing More with Functions | 149...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 170, 'page_label': '149', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4517}\n",
      "\n",
      "--- Chunk 4518 ---\n",
      "Content:\n",
      ">>> extract_property(len)\n",
      "[4, 4, 2, 3, 5, 1, 3, 3, 6, 4, 4, 4, 2, 10, 1]\n",
      ">>> def last_letter(word):\n",
      "...     return word[-1]\n",
      ">>> extract_property(last_letter)\n",
      "['e', 'e', 'f', 'e', 'e', ',', 'd', 'e', 's', 'l', 'e', 'e', 'f', 's', '.']\n",
      "The \n",
      "objects len and last_letter can be passed around like lists and dictionaries. Notice\n",
      "that parentheses are used after a function name only if we are invoking the function;\n",
      "when we are simply treating the function as an object, these are omitted.\n",
      "Python provides us with one more way to define functions as arguments to other func-\n",
      "tions, so-called lambda expressions. Supposing there was no need to use the last_let\n",
      "ter() function in multiple places, and thus no need to give it a name. Let’s suppose we\n",
      "can equivalently write the following:\n",
      ">>> extract_property(lambda w: w[-1])\n",
      "['e', 'e', 'f', 'e', 'e', ',', 'd', 'e', 's', 'l', 'e', 'e', 'f', 's', '.']\n",
      "Our next example illustrates passing a function to the sorted() function. When we call\n",
      "the latter with a single argument (the list to be sorted), it uses the built-in comparison\n",
      "function cmp(). However, we can supply our own sort function, e.g., to sort by de-\n",
      "creasing length.\n",
      ">>> sorted(sent)\n",
      "[',', '.', 'Take', 'and', 'care', 'care', 'of', 'of', 'sense', 'sounds',\n",
      "'take', 'the', 'the', 'themselves', 'will']\n",
      ">>> sorted(sent, cmp)\n",
      "[',', '.', 'Take', 'and', 'care', 'care', 'of', 'of', 'sense', 'sounds',\n",
      "'take', 'the', 'the', 'themselves', 'will']\n",
      ">>> sorted(sent, lambda x, y: cmp(len(y), len(x)))\n",
      "['themselves', 'sounds', 'sense', 'Take', 'care', 'will', 'take', 'care',\n",
      "'the', 'and', 'the', 'of', 'of', ',', '.']\n",
      "Accumulative Functions\n",
      "These functions start by initializing some storage, and iterate over input to build it up,\n",
      "before returning some final object (a large structure or aggregated result). A standard\n",
      "way to do this is to initialize an empty list, accumulate the material, then return the\n",
      "list, as shown in function search1() in Example 4-5.\n",
      "Example 4-5. Accumulating output into a list.\n",
      "def search1(substring, words):\n",
      "    result = []\n",
      "    for word in words:\n",
      "        if substring in word:\n",
      "            result.append(word)\n",
      "    return result\n",
      "def search2(substring, words):\n",
      "    for word in words:\n",
      "        if substring in word:\n",
      "            yield word\n",
      "150 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 171, 'page_label': '150', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4518}\n",
      "\n",
      "--- Chunk 4519 ---\n",
      "Content:\n",
      "print \"search1:\"\n",
      "for item in search1('zz', nltk.corpus.brown.words()):\n",
      "    print item\n",
      "print \"search2:\"\n",
      "for item in search2('zz', nltk.corpus.brown.words()):\n",
      "    print item\n",
      "The \n",
      "function search2() is a generator. The first time this function is called, it gets as\n",
      "far as the yield statement and pauses. The calling program gets the first word and does\n",
      "any necessary processing. Once the calling program is ready for another word, execu-\n",
      "tion of the function is continued from where it stopped, until the next time it encounters\n",
      "a yield statement. This approach is typically more efficient, as the function only gen-\n",
      "erates the data as it is required by the calling program, and does not need to allocate\n",
      "additional memory to store the output (see the earlier discussion of generator expres-\n",
      "sions).\n",
      "Here’s a more sophisticated example of a generator which produces all permutations\n",
      "of a list of words. In order to force the permutations() function to generate all its output,\n",
      "we wrap it with a call to list() \n",
      ".\n",
      ">>> def permutations(seq):\n",
      "...     if len(seq) <= 1:\n",
      "...         yield seq\n",
      "...     else:\n",
      "...         for perm in permutations(seq[1:]):\n",
      "...             for i in range(len(perm)+1):\n",
      "...                 yield perm[:i] + seq[0:1] + perm[i:]\n",
      "...\n",
      ">>> list(permutations(['police', 'fish', 'buffalo'])) \n",
      "[['police', 'fish', 'buffalo'], ['fish', 'police', 'buffalo'],\n",
      " ['fish', 'buffalo', 'police'], ['police', 'buffalo', 'fish'],\n",
      " ['buffalo', 'police', 'fish'], ['buffalo', 'fish', 'police']]\n",
      "The permutations function uses a technique called recursion, discussed\n",
      "later in Section 4.7. The ability to generate permutations of a set of words\n",
      "is useful for creating data to test a grammar (Chapter 8).\n",
      "Higher-Order Functions\n",
      "Python provides some higher-order functions that are standard features of functional\n",
      "programming languages such as Haskell. We illustrate them here, alongside the equiv-\n",
      "alent expression using list comprehensions.\n",
      "Let’s start by defining a function is_content_word() which checks whether a word is\n",
      "from the open class of content words. We use this function as the first parameter of\n",
      "filter(), which applies the function to each item in the sequence contained in its\n",
      "second parameter, and retains only the items for which the function returns True.\n",
      "4.5  Doing More with Functions | 151...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 172, 'page_label': '151', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4519}\n",
      "\n",
      "--- Chunk 4520 ---\n",
      "Content:\n",
      ">>> def is_content_word(word):\n",
      "...     return word.lower() not in ['a', 'of', 'the', 'and', 'will', ',', '.']\n",
      ">>> sent = ['Take', 'care', 'of', 'the', 'sense', ',', 'and', 'the',\n",
      "...         'sounds', 'will', 'take', 'care', 'of', 'themselves', '.']\n",
      ">>> filter(is_content_word, sent)\n",
      "['Take', 'care', 'sense', 'sounds', 'take', 'care', 'themselves']\n",
      ">>> [w for w in sent if is_content_word(w)]\n",
      "['Take', 'care', 'sense', 'sounds', 'take', 'care', 'themselves']\n",
      "Another \n",
      "higher-order function is map(), which applies a function to every item in a\n",
      "sequence. It is a general version of the extract_property() function we saw earlier in\n",
      "this section. Here is a simple way to find the average length of a sentence in the news\n",
      "section of the Brown Corpus, followed by an equivalent version with list comprehen-\n",
      "sion calculation:\n",
      ">>> lengths = map(len, nltk.corpus.brown.sents(categories='news'))\n",
      ">>> sum(lengths) / len(lengths)\n",
      "21.7508111616\n",
      ">>> lengths = [len(w) for w in nltk.corpus.brown.sents(categories='news'))]\n",
      ">>> sum(lengths) / len(lengths)\n",
      "21.7508111616\n",
      "In the previous examples, we specified a user-defined function is_content_word() and\n",
      "a built-in function len(). We can also provide a lambda expression. Here’s a pair of\n",
      "equivalent examples that count the number of vowels in each word.\n",
      ">>> map(lambda w: len(filter(lambda c: c.lower() in \"aeiou\", w)), sent)\n",
      "[2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 2, 1, 3, 0]\n",
      ">>> [len([c for c in w if c.lower() in \"aeiou\"]) for w in sent]\n",
      "[2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 2, 1, 3, 0]\n",
      "The solutions based on list comprehensions are usually more readable than the solu-\n",
      "tions based on higher-order functions, and we have favored the former approach\n",
      "throughout this book.\n",
      "Named Arguments\n",
      "When there are a lot of parameters it is easy to get confused about the correct order.\n",
      "Instead we can refer to parameters by name, and even assign them a default value just\n",
      "in case one was not provided by the calling program. Now the parameters can be speci-\n",
      "fied in any order, and can be omitted.\n",
      ">>> def repeat(msg='<empty>', num=1):\n",
      "...     return msg * num\n",
      ">>> repeat(num=3)\n",
      "'<empty><empty><empty>'\n",
      ">>> repeat(msg='Alice')\n",
      "'Alice'\n",
      ">>> repeat(num=5, msg='Alice')\n",
      "'AliceAliceAliceAliceAlice'\n",
      "These are called keyword arguments. If we mix these two kinds of parameters, then\n",
      "we must ensure that the unnamed parameters precede the named ones. It has to be this\n",
      "152 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 173, 'page_label': '152', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4520}\n",
      "\n",
      "--- Chunk 4521 ---\n",
      "Content:\n",
      "way, since unnamed parameters are defined by position. We can define a function that\n",
      "takes \n",
      "an arbitrary number of unnamed and named parameters, and access them via an\n",
      "in-place list of arguments *args and an in-place dictionary of keyword arguments\n",
      "**kwargs.\n",
      ">>> def generic(*args, **kwargs):\n",
      "...     print args\n",
      "...     print kwargs\n",
      "...\n",
      ">>> generic(1, \"African swallow\", monty=\"python\")\n",
      "(1, 'African swallow')\n",
      "{'monty': 'python'}\n",
      "When *args appears as a function parameter, it actually corresponds to all the unnamed\n",
      "parameters of the function. As another illustration of this aspect of Python syntax,\n",
      "consider the zip() function, which operates on a variable number of arguments. We’ll\n",
      "use the variable name *song to demonstrate that there’s nothing special about the name\n",
      "*args.\n",
      ">>> song = [['four', 'calling', 'birds'],\n",
      "...         ['three', 'French', 'hens'],\n",
      "...         ['two', 'turtle', 'doves']]\n",
      ">>> zip(song[0], song[1], song[2])\n",
      "[('four', 'three', 'two'), ('calling', 'French', 'turtle'), ('birds', 'hens', 'doves')]\n",
      ">>> zip(*song)\n",
      "[('four', 'three', 'two'), ('calling', 'French', 'turtle'), ('birds', 'hens', 'doves')]\n",
      "It should be clear from this example that typing *song is just a convenient shorthand,\n",
      "and equivalent to typing out song[0], song[1], song[2].\n",
      "Here’s another example of the use of keyword arguments in a function definition, along\n",
      "with three equivalent ways to call the function:\n",
      ">>> def freq_words(file, min=1, num=10):\n",
      "...     text = open(file).read()\n",
      "...     tokens = nltk.word_tokenize(text)\n",
      "...     freqdist = nltk.FreqDist(t for t in tokens if len(t) >= min)\n",
      "...     return freqdist.keys()[:num]\n",
      ">>> fw = freq_words('ch01.rst', 4, 10)\n",
      ">>> fw = freq_words('ch01.rst', min=4, num=10)\n",
      ">>> fw = freq_words('ch01.rst', num=10, min=4)\n",
      "A side effect of having named arguments is that they permit optionality. Thus we can\n",
      "leave out any arguments where we are happy with the default value:\n",
      "freq_words('ch01.rst', min=4), freq_words('ch01.rst', 4). Another common use of\n",
      "optional arguments is to permit a flag. Here’s a revised version of the same function\n",
      "that reports its progress if a verbose flag is set:\n",
      ">>> def freq_words(file, min=1, num=10, verbose=False):\n",
      "...     freqdist = FreqDist()\n",
      "...     if trace: print \"Opening\", file\n",
      "...     text = open(file).read()\n",
      "...     if trace: print \"Read in %d characters\" % len(file)\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 174, 'page_label': '153', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4521}\n",
      "\n",
      "--- Chunk 4522 ---\n",
      "Content:\n",
      ". Another common use of\n",
      "optional arguments is to permit a flag. Here’s a revised version of the same function\n",
      "that reports its progress if a verbose flag is set:\n",
      ">>> def freq_words(file, min=1, num=10, verbose=False):\n",
      "...     freqdist = FreqDist()\n",
      "...     if trace: print \"Opening\", file\n",
      "...     text = open(file).read()\n",
      "...     if trace: print \"Read in %d characters\" % len(file)\n",
      "...     for word in nltk.word_tokenize(text):\n",
      "4.5  Doing More with Functions | 153...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 174, 'page_label': '153', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4522}\n",
      "\n",
      "--- Chunk 4523 ---\n",
      "Content:\n",
      "...         if len(word) >= min:\n",
      "...             freqdist.inc(word)\n",
      "...             if trace and freqdist.N() % 100 == 0: print \".\"\n",
      "...     if trace: print\n",
      "...     return freqdist.keys()[:num]\n",
      "Caution!\n",
      "Take \n",
      "care not to use a mutable object as the default value of a parameter.\n",
      "A series of calls to the function will use the same object, sometimes with\n",
      "bizarre results, as we will see in the discussion of debugging later.\n",
      "4.6  Program Development\n",
      "Programming is a skill that is acquired over several years of experience with a variety\n",
      "of programming languages and tasks. Key high-level abilities are algorithm design and\n",
      "its manifestation in structured programming. Key low-level abilities include familiarity\n",
      "with the syntactic constructs of the language, and knowledge of a variety of diagnostic\n",
      "methods for trouble-shooting a program which does not exhibit the expected behavior.\n",
      "This section describes the internal structure of a program module and how to organize\n",
      "a multi-module program. Then it describes various kinds of error that arise during\n",
      "program development, what you can do to fix them and, better still, to avoid them in\n",
      "the first place.\n",
      "Structure of a Python Module\n",
      "The purpose of a program module is to bring logically related definitions and functions\n",
      "together in order to facilitate reuse and abstraction. Python modules are nothing more\n",
      "than individual .py files. For example, if you were working with a particular corpus\n",
      "format, the functions to read and write the format could be kept together. Constants\n",
      "used by both formats, such as field separators, or a EXTN = \".inf\" filename extension,\n",
      "could be shared. If the format was updated, you would know that only one file needed\n",
      "to be changed. Similarly, a module could contain code for creating and manipulating\n",
      "a particular data structure such as syntax trees, or code for performing a particular\n",
      "processing task such as plotting corpus statistics.\n",
      "When you start writing Python modules, it helps to have some examples to emulate.\n",
      "You can locate the code for any NLTK module on your system using the __file__\n",
      "variable:\n",
      ">>> nltk.metrics.distance.__file__\n",
      "'/usr/lib/python2.5/site-packages/nltk/metrics/distance.pyc'\n",
      "This returns the location of the compiled .pyc file for the module, and you’ll probably\n",
      "see a different location on your machine. The file that you will need to open is the\n",
      "corresponding .py source file, and this will be in the same directory as the .pyc file.\n",
      "154 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 175, 'page_label': '154', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4523}\n",
      "\n",
      "--- Chunk 4524 ---\n",
      "Content:\n",
      "Alternatively, you can view the latest version of this module on the Web at http://code\n",
      ".google.com/p/nltk/source/browse/trunk/nltk/nltk/metrics/distance.py.\n",
      "Like \n",
      "every other NLTK module, distance.py begins with a group of comment lines giving\n",
      "a one-line title of the module and identifying the authors. (Since the code is distributed,\n",
      "it also includes the URL where the code is available, a copyright statement, and license\n",
      "information.) Next is the module-level docstring, a triple-quoted multiline string con-\n",
      "taining information about the module that will be printed when someone types\n",
      "help(nltk.metrics.distance).\n",
      "# Natural Language Toolkit: Distance Metrics\n",
      "#\n",
      "# Copyright (C) 2001-2009 NLTK Project\n",
      "# Author: Edward Loper <edloper@gradient.cis.upenn.edu>\n",
      "#         Steven Bird <sb@csse.unimelb.edu.au>\n",
      "#         Tom Lippincott <tom@cs.columbia.edu>\n",
      "# URL: <http://www.nltk.org/>\n",
      "# For license information, see LICENSE.TXT\n",
      "#\n",
      "\"\"\"\n",
      "Distance Metrics.\n",
      "Compute the distance between two items (usually strings).\n",
      "As metrics, they must satisfy the following three requirements:\n",
      "1. d(a, a) = 0\n",
      "2. d(a, b) >= 0\n",
      "3. d(a, c) <= d(a, b) + d(b, c)\n",
      "\"\"\"\n",
      "After this comes all the import statements required for the module, then any global\n",
      "variables, followed by a series of function definitions that make up most of the module.\n",
      "Other modules define “classes,” the main building blocks of object-oriented program-\n",
      "ming, which falls outside the scope of this book. (Most NLTK modules also include a\n",
      "demo() function, which can be used to see examples of the module in use.)\n",
      "Some module variables and functions are only used within the module.\n",
      "These \n",
      "should have names beginning with an underscore, e.g.,\n",
      "_helper(), since this will hide the name. If another module imports this\n",
      "one, using the idiom: from module import *, these names will not be\n",
      "imported. You can optionally list the externally accessible names of a\n",
      "module using a special built-in variable like this: __all__ = ['edit_dis\n",
      "tance', 'jaccard_distance'].\n",
      "Multimodule Programs\n",
      "Some programs bring together a diverse range of tasks, such as loading data from a\n",
      "corpus, performing some analysis tasks on the data, then visualizing it. We may already\n",
      "4.6  Program Development | 155...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 176, 'page_label': '155', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4524}\n",
      "\n",
      "--- Chunk 4525 ---\n",
      "Content:\n",
      "have stable modules that take care of loading data and producing visualizations. Our\n",
      "work \n",
      "might involve coding up the analysis task, and just invoking functions from the\n",
      "existing modules. This scenario is depicted in Figure 4-2.\n",
      "Figure 4-2. Structure of a multimodule program: The main program my_program.py imports\n",
      "functions from two other modules; unique analysis tasks are localized to the main program, while\n",
      "common loading and visualization tasks are kept apart to facilitate reuse and abstraction.\n",
      "By dividing our work into several modules and using import statements to access func-\n",
      "tions defined elsewhere, we can keep the individual modules simple and easy to main-\n",
      "tain. This approach will also result in a growing collection of modules, and make it\n",
      "possible for us to build sophisticated systems involving a hierarchy of modules. De-\n",
      "signing such systems well is a complex software engineering task, and beyond the scope\n",
      "of this book.\n",
      "Sources of Error\n",
      "Mastery of programming depends on having a variety of problem-solving skills to draw\n",
      "upon when the program doesn’t work as expected. Something as trivial as a misplaced\n",
      "symbol might cause the program to behave very differently. We call these “bugs” be-\n",
      "cause they are tiny in comparison to the damage they can cause. They creep into our\n",
      "code unnoticed, and it’s only much later when we’re running the program on some\n",
      "new data that their presence is detected. Sometimes, fixing one bug only reveals an-\n",
      "other, and we get the distinct impression that the bug is on the move. The only reas-\n",
      "surance we have is that bugs are spontaneous and not the fault of the programmer.\n",
      "156 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 177, 'page_label': '156', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4525}\n",
      "\n",
      "--- Chunk 4526 ---\n",
      "Content:\n",
      "Flippancy aside, debugging code is hard because there are so many ways for it to be\n",
      "faulty. \n",
      "Our understanding of the input data, the algorithm, or even the programming\n",
      "language, may be at fault. Let’s look at examples of each of these.\n",
      "First, the input data may contain some unexpected characters. For example, WordNet\n",
      "synset names have the form tree.n.01, with three components separated using periods.\n",
      "The NLTK WordNet module initially decomposed these names using split('.').\n",
      "However, this method broke when someone tried to look up the word PhD, which has\n",
      "the synset name ph.d..n.01, containing four periods instead of the expected two. The\n",
      "solution was to use rsplit('.', 2) to do at most two splits, using the rightmost in-\n",
      "stances of the period, and leaving the ph.d. string intact. Although several people had\n",
      "tested the module before it was released, it was some weeks before someone detected\n",
      "the problem (see http://code.google.com/p/nltk/issues/detail?id=297).\n",
      "Second, a supplied function might not behave as expected. For example, while testing\n",
      "NLTK’s interface to WordNet, one of the authors noticed that no synsets had any\n",
      "antonyms defined, even though the underlying database provided a large quantity of\n",
      "antonym information. What looked like a bug in the WordNet interface turned out to\n",
      "be a misunderstanding about WordNet itself: antonyms are defined for lemmas, not\n",
      "for synsets. The only “bug” was a misunderstanding of the interface (see http://code\n",
      ".google.com/p/nltk/issues/detail?id=98).\n",
      "Third, our understanding of Python’s semantics may be at fault. It is easy to make the\n",
      "wrong assumption about the relative scope of two operators. For example, \"%s.%s.\n",
      "%02d\" % \"ph.d.\", \"n\", 1 produces a runtime error TypeError: not enough arguments\n",
      "for format string. This is because the percent operator has higher precedence than\n",
      "the comma operator. The fix is to add parentheses in order to force the required scope.\n",
      "As another example, suppose we are defining a function to collect all tokens of a text\n",
      "having a given length. The function has parameters for the text and the word length,\n",
      "and an extra parameter that allows the initial value of the result to be given as a\n",
      "parameter:\n",
      ">>> def find_words(text, wordlength, result=[]):\n",
      "...     for word in text:\n",
      "...         if len(word) == wordlength:\n",
      "...             result.append(word)\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 178, 'page_label': '157', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4526}\n",
      "\n",
      "--- Chunk 4527 ---\n",
      "Content:\n",
      ". The fix is to add parentheses in order to force the required scope.\n",
      "As another example, suppose we are defining a function to collect all tokens of a text\n",
      "having a given length. The function has parameters for the text and the word length,\n",
      "and an extra parameter that allows the initial value of the result to be given as a\n",
      "parameter:\n",
      ">>> def find_words(text, wordlength, result=[]):\n",
      "...     for word in text:\n",
      "...         if len(word) == wordlength:\n",
      "...             result.append(word)\n",
      "...     return result\n",
      ">>> find_words(['omg', 'teh', 'lolcat', 'sitted', 'on', 'teh', 'mat'], 3) \n",
      "['omg', 'teh', 'teh', 'mat']\n",
      ">>> find_words(['omg', 'teh', 'lolcat', 'sitted', 'on', 'teh', 'mat'], 2, ['ur']) \n",
      "['ur', 'on']\n",
      ">>> find_words(['omg', 'teh', 'lolcat', 'sitted', 'on', 'teh', 'mat'], 3) \n",
      "['omg', 'teh', 'teh', 'mat', 'omg', 'teh', 'teh', 'mat']\n",
      "The \n",
      "first time we call find_words() \n",
      " , we get all three-letter words as expected. The\n",
      "second time we specify an initial value for the result, a one-element list ['ur'], and as\n",
      "expected, \n",
      "the result has this word along with the other two-letter word in our text.\n",
      "Now, the next time we call find_words() \n",
      "  we use the same parameters as in \n",
      " , but\n",
      "we \n",
      "get a different result! Each time we call find_words() with no third parameter, the\n",
      "4.6  Program Development | 157...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 178, 'page_label': '157', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4527}\n",
      "\n",
      "--- Chunk 4528 ---\n",
      "Content:\n",
      "result will simply extend the result of the previous call, rather than start with the empty\n",
      "result \n",
      "list as specified in the function definition. The program’s behavior is not as ex-\n",
      "pected because we incorrectly assumed that the default value was created at the time\n",
      "the function was invoked. However, it is created just once, at the time the Python\n",
      "interpreter loads the function. This one list object is used whenever no explicit value\n",
      "is provided to the function.\n",
      "Debugging Techniques\n",
      "Since most code errors result from the programmer making incorrect assumptions, the\n",
      "first thing to do when you detect a bug is to check your assumptions. Localize the prob-\n",
      "lem by adding print statements to the program, showing the value of important vari-\n",
      "ables, and showing how far the program has progressed.\n",
      "If the program produced an “exception”—a runtime error—the interpreter will print\n",
      "a stack trace, pinpointing the location of program execution at the time of the error.\n",
      "If the program depends on input data, try to reduce this to the smallest size while still\n",
      "producing the error.\n",
      "Once you have localized the problem to a particular function or to a line of code, you\n",
      "need to work out what is going wrong. It is often helpful to recreate the situation using\n",
      "the interactive command line. Define some variables, and then copy-paste the offending\n",
      "line of code into the session and see what happens. Check your understanding of the\n",
      "code by reading some documentation and examining other code samples that purport\n",
      "to do the same thing that you are trying to do. Try explaining your code to someone\n",
      "else, in case she can see where things are going wrong.\n",
      "Python provides a debugger which allows you to monitor the execution of your pro-\n",
      "gram, specify line numbers where execution will stop (i.e., breakpoints), and step\n",
      "through sections of code and inspect the value of variables. You can invoke the debug-\n",
      "ger on your code as follows:\n",
      ">>> import pdb\n",
      ">>> import mymodule\n",
      ">>> pdb.run('mymodule.myfunction()')\n",
      "It will present you with a prompt (Pdb) where you can type instructions to the debugger.\n",
      "Type help to see the full list of commands. Typing step (or just s) will execute the\n",
      "current line and stop. If the current line calls a function, it will enter the function and\n",
      "stop at the first line. Typing next (or just n) is similar, but it stops execution at the next\n",
      "line in the current function. The break (or b) command can be used to create or list\n",
      "breakpoints. Type continue (or c) to continue execution as far as the next breakpoint.\n",
      "Type the name of any variable to inspect its value.\n",
      "We can use the Python debugger to locate the problem in our find_words() function.\n",
      "Remember that the problem arose the second time the function was called. We’ll start\n",
      "by calling the function without using the debugger \n",
      ", using the smallest possible input.\n",
      "The second time, we’ll call it with the debugger \n",
      " .\n",
      "158 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 179, 'page_label': '158', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4528}\n",
      "\n",
      "--- Chunk 4529 ---\n",
      "Content:\n",
      ">>> import pdb\n",
      ">>> find_words(['cat'], 3) \n",
      "['cat']\n",
      ">>> pdb.run(\"find_words(['dog'], 3)\") \n",
      "> <string>(1)<module>()\n",
      "(Pdb) step\n",
      "--Call--\n",
      "> <stdin>(1)find_words()\n",
      "(Pdb) args\n",
      "text = ['dog']\n",
      "wordlength = 3\n",
      "result = ['cat']\n",
      "Here \n",
      "we typed just two commands into the debugger: step took us inside the function,\n",
      "and args showed the values of its arguments (or parameters). We see immediately that\n",
      "result has an initial value of ['cat'], and not the empty list as expected. The debugger\n",
      "has helped us to localize the problem, prompting us to check our understanding of\n",
      "Python functions.\n",
      "Defensive Programming\n",
      "In order to avoid some of the pain of debugging, it helps to adopt some defensive\n",
      "programming habits. Instead of writing a 20-line program and then testing it, build the\n",
      "program bottom-up out of small pieces that are known to work. Each time you combine\n",
      "these pieces to make a larger unit, test it carefully to see that it works as expected.\n",
      "Consider adding assert statements to your code, specifying properties of a variable,\n",
      "e.g., assert(isinstance(text, list)). If the value of the text variable later becomes a\n",
      "string when your code is used in some larger context, this will raise an\n",
      "AssertionError and you will get immediate notification of the problem.\n",
      "Once you think you’ve found the bug, view your solution as a hypothesis. Try to predict\n",
      "the effect of your bugfix before re-running the program. If the bug isn’t fixed, don’t fall\n",
      "into the trap of blindly changing the code in the hope that it will magically start working\n",
      "again. Instead, for each change, try to articulate a hypothesis about what is wrong and\n",
      "why the change will fix the problem. Then undo the change if the problem was not\n",
      "resolved.\n",
      "As you develop your program, extend its functionality, and fix any bugs, it helps to\n",
      "maintain a suite of test cases. This is called regression testing, since it is meant to\n",
      "detect situations where the code “regresses”—where a change to the code has an un-\n",
      "intended side effect of breaking something that used to work. Python provides a simple\n",
      "regression-testing framework in the form of the doctest module. This module searches\n",
      "a file of code or documentation for blocks of text that look like an interactive Python\n",
      "session, of the form you have already seen many times in this book. It executes the\n",
      "Python commands it finds, and tests that their output matches the output supplied in\n",
      "the original file. Whenever there is a mismatch, it reports the expected and actual val-\n",
      "ues. For details, please consult the doctest documentation at\n",
      "4.6  Program Development | 159...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 180, 'page_label': '159', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4529}\n",
      "\n",
      "--- Chunk 4530 ---\n",
      "Content:\n",
      "http://docs.python.org/library/doctest.html. Apart from its value for regression testing,\n",
      "the doctest module is useful for ensuring that your software documentation stays in\n",
      "sync with your code.\n",
      "Perhaps the most important defensive programming strategy is to set out your code\n",
      "clearly, choose meaningful variable and function names, and simplify the code wher-\n",
      "ever possible by decomposing it into functions and modules with well-documented\n",
      "interfaces.\n",
      "4.7  Algorithm Design\n",
      "This section discusses more advanced concepts, which you may prefer to skip on the\n",
      "first time through this chapter.\n",
      "A major part of algorithmic problem solving is selecting or adapting an appropriate\n",
      "algorithm for the problem at hand. Sometimes there are several alternatives, and choos-\n",
      "ing the best one depends on knowledge about how each alternative performs as the size\n",
      "of the data grows. Whole books are written on this topic, and we only have space to\n",
      "introduce some key concepts and elaborate on the approaches that are most prevalent\n",
      "in natural language processing.\n",
      "The best-known strategy is known as divide-and-conquer. We attack a problem of\n",
      "size n by dividing it into two problems of size n/2, solve these problems, and combine\n",
      "their results into a solution of the original problem. For example, suppose that we had\n",
      "a pile of cards with a single word written on each card. We could sort this pile by\n",
      "splitting it in half and giving it to two other people to sort (they could do the same in\n",
      "turn). Then, when two sorted piles come back, it is an easy task to merge them into a\n",
      "single sorted pile. See Figure 4-3 for an illustration of this process.\n",
      "Another example is the process of looking up a word in a dictionary. We open the book\n",
      "somewhere around the middle and compare our word with the current page. If it’s\n",
      "earlier in the dictionary, we repeat the process on the first half; if it’s later, we use the\n",
      "second half. This search method is called binary search since it splits the problem in\n",
      "half at every step.\n",
      "In another approach to algorithm design, we attack a problem by transforming it into\n",
      "an instance of a problem we already know how to solve. For example, in order to detect\n",
      "duplicate entries in a list, we can pre-sort the list, then scan through it once to check\n",
      "whether any adjacent pairs of elements are identical.\n",
      "Recursion\n",
      "The earlier examples of sorting and searching have a striking property: to solve a prob-\n",
      "lem of size n, we have to break it in half and then work on one or more problems of\n",
      "size n/2. A common way to implement such methods uses recursion. We define a\n",
      "function f, which simplifies the problem, and calls itself to solve one or more easier\n",
      "160 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 181, 'page_label': '160', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4530}\n",
      "\n",
      "--- Chunk 4531 ---\n",
      "Content:\n",
      "instances of the same problem. It then combines the results into a solution for the\n",
      "original problem.\n",
      "For \n",
      "example, suppose we have a set of n words, and want to calculate how many dif-\n",
      "ferent ways they can be combined to make a sequence of words. If we have only one\n",
      "word (n=1), there is just one way to make it into a sequence. If we have a set of two\n",
      "words, there are two ways to put them into a sequence. For three words there are six\n",
      "possibilities. In general, for n words, there are n × n-1 × … × 2 × 1 ways (i.e., the factorial\n",
      "of n). We can code this up as follows:\n",
      ">>> def factorial1(n):\n",
      "...     result = 1\n",
      "...     for i in range(n):\n",
      "...         result *= (i+1)\n",
      "...     return result\n",
      "However, there is also a recursive algorithm for solving this problem, based on the\n",
      "following observation. Suppose we have a way to construct all orderings for n-1 distinct\n",
      "words. Then for each such ordering, there are n places where we can insert a new word:\n",
      "at the start, the end, or any of the n-2 boundaries between the words. Thus we simply\n",
      "multiply the number of solutions found for n-1 by the value of n. We also need the\n",
      "base case, to say that if we have a single word, there’s just one ordering. We can code\n",
      "this up as follows:\n",
      ">>> def factorial2(n):\n",
      "...     if n == 1:\n",
      "...         return 1\n",
      "...     else:\n",
      "...         return n * factorial2(n-1)\n",
      "Figure 4-3. Sorting by divide-and-conquer: To sort an array, we split it in half and sort each half\n",
      "(recursively); \n",
      "we merge each sorted half back into a whole list (again recursively); this algorithm is\n",
      "known as “Merge Sort.”\n",
      "4.7  Algorithm Design | 161...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 182, 'page_label': '161', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4531}\n",
      "\n",
      "--- Chunk 4532 ---\n",
      "Content:\n",
      "These two algorithms solve the same problem. One uses iteration while the other uses\n",
      "recursion. \n",
      "We can use recursion to navigate a deeply nested object, such as the Word-\n",
      "Net hypernym hierarchy. Let’s count the size of the hypernym hierarchy rooted at a\n",
      "given synset s. We’ll do this by finding the size of each hyponym of s, then adding these\n",
      "together (we will also add 1 for the synset itself). The following function size1() does\n",
      "this work; notice that the body of the function includes a recursive call to size1():\n",
      ">>> def size1(s):\n",
      "...     return 1 + sum(size1(child) for child in s.hyponyms())\n",
      "We can also design an iterative solution to this problem which processes the hierarchy\n",
      "in layers. The first layer is the synset itself \n",
      ", then all the hyponyms of the synset, then\n",
      "all \n",
      "the hyponyms of the hyponyms. Each time through the loop it computes the next\n",
      "layer by finding the hyponyms of everything in the last layer \n",
      " . It also maintains a total\n",
      "of the number of synsets encountered so far \n",
      " .\n",
      ">>> def size2(s):\n",
      "...     layer = [s] \n",
      "...     total = 0\n",
      "...     while layer:\n",
      "...         total += len(layer) \n",
      "...         layer = [h for c in layer for h in c.hyponyms()] \n",
      "...     return total\n",
      "Not \n",
      "only is the iterative solution much longer, it is harder to interpret. It forces us to\n",
      "think procedurally, and keep track of what is happening with the layer and total\n",
      "variables through time. Let’s satisfy ourselves that both solutions give the same result.\n",
      "We’ll use a new form of the import statement, allowing us to abbreviate the name\n",
      "wordnet to wn:\n",
      ">>> from nltk.corpus import wordnet as wn\n",
      ">>> dog = wn.synset('dog.n.01')\n",
      ">>> size1(dog)\n",
      "190\n",
      ">>> size2(dog)\n",
      "190\n",
      "As a final example of recursion, let’s use it to construct a deeply nested object. A letter\n",
      "trie is a data structure that can be used for indexing a lexicon, one letter at a time. (The\n",
      "name is based on the word retrieval.) For example, if trie contained a letter trie, then\n",
      "trie['c'] would be a smaller trie which held all words starting with c. Example 4-6\n",
      "demonstrates the recursive process of building a trie, using Python dictionaries ( Sec-\n",
      "tion 5.3). To insert the word chien (French for dog), we split off the c and recursively\n",
      "insert hien into the sub-trie trie['c']. The recursion continues until there are no letters\n",
      "remaining in the word, when we store the intended value (in this case, the word dog).\n",
      "162 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 183, 'page_label': '162', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4532}\n",
      "\n",
      "--- Chunk 4533 ---\n",
      "Content:\n",
      "Example 4-6. Building a letter trie: A recursive function that builds a nested dictionary structure; each\n",
      "level \n",
      "of nesting contains all words with a given prefix, and a sub-trie containing all possible\n",
      "continuations.\n",
      "def insert(trie, key, value):\n",
      "    if key:\n",
      "        first, rest = key[0], key[1:]\n",
      "        if first not in trie:\n",
      "            trie[first] = {}\n",
      "        insert(trie[first], rest, value)\n",
      "    else:\n",
      "        trie['value'] = value\n",
      ">>> trie = nltk.defaultdict(dict)\n",
      ">>> insert(trie, 'chat', 'cat')\n",
      ">>> insert(trie, 'chien', 'dog')\n",
      ">>> insert(trie, 'chair', 'flesh')\n",
      ">>> insert(trie, 'chic', 'stylish')\n",
      ">>> trie = dict(trie)               # for nicer printing\n",
      ">>> trie['c']['h']['a']['t']['value']\n",
      "'cat'\n",
      ">>> pprint.pprint(trie)\n",
      "{'c': {'h': {'a': {'t': {'value': 'cat'}},\n",
      "                  {'i': {'r': {'value': 'flesh'}}},\n",
      "             'i': {'e': {'n': {'value': 'dog'}}}\n",
      "                  {'c': {'value': 'stylish'}}}}}\n",
      "Caution!\n",
      "Despite \n",
      "the simplicity of recursive programming, it comes with a cost.\n",
      "Each time a function is called, some state information needs to be push-\n",
      "ed on a stack, so that once the function has completed, execution can\n",
      "continue from where it left off. For this reason, iterative solutions are\n",
      "often more efficient than recursive solutions.\n",
      "Space-Time Trade-offs\n",
      "We can sometimes significantly speed up the execution of a program by building an\n",
      "auxiliary data structure, such as an index. The listing in Example 4-7 implements a\n",
      "simple text retrieval system for the Movie Reviews Corpus. By indexing the document\n",
      "collection, it provides much faster lookup.\n",
      "Example 4-7. A simple text retrieval system.\n",
      "def raw(file):\n",
      "    contents = open(file).read()\n",
      "    contents = re.sub(r'<.*?>', ' ', contents)\n",
      "    contents = re.sub('\\s+', ' ', contents)\n",
      "    return contents\n",
      "def snippet(doc, term): # buggy\n",
      "    text = ' '*30 + raw(doc) + ' '*30\n",
      "    pos = text.index(term)\n",
      "    return text[pos-30:pos+30]\n",
      "4.7  Algorithm Design | 163...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 184, 'page_label': '163', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4533}\n",
      "\n",
      "--- Chunk 4534 ---\n",
      "Content:\n",
      "print \"Building Index...\"\n",
      "files = nltk.corpus.movie_reviews.abspaths()\n",
      "idx = nltk.Index((w, f) for f in files for w in raw(f).split())\n",
      "query = ''\n",
      "while query != \"quit\":\n",
      "    query = raw_input(\"query> \")\n",
      "    if query in idx:\n",
      "        for doc in idx[query]:\n",
      "            print snippet(doc, query)\n",
      "    else:\n",
      "        print \"Not found\"\n",
      "A \n",
      "more subtle example of a space-time trade-off involves replacing the tokens of a\n",
      "corpus with integer identifiers. We create a vocabulary for the corpus, a list in which\n",
      "each word is stored once, then invert this list so that we can look up any word to find\n",
      "its identifier. Each document is preprocessed, so that a list of words becomes a list of\n",
      "integers. Any language models can now work with integers. See the listing in Exam-\n",
      "ple 4-8 for an example of how to do this for a tagged corpus.\n",
      "Example 4-8. Preprocess tagged corpus data, converting all words and tags to integers.\n",
      "def preprocess(tagged_corpus):\n",
      "    words = set()\n",
      "    tags = set()\n",
      "    for sent in tagged_corpus:\n",
      "        for word, tag in sent:\n",
      "            words.add(word)\n",
      "            tags.add(tag)\n",
      "    wm = dict((w,i) for (i,w) in enumerate(words))\n",
      "    tm = dict((t,i) for (i,t) in enumerate(tags))\n",
      "    return [[(wm[w], tm[t]) for (w,t) in sent] for sent in tagged_corpus]\n",
      "Another example of a space-time trade-off is maintaining a vocabulary list. If you need\n",
      "to process an input text to check that all words are in an existing vocabulary, the vo-\n",
      "cabulary should be stored as a set, not a list. The elements of a set are automatically\n",
      "indexed, so testing membership of a large set will be much faster than testing mem-\n",
      "bership of the corresponding list.\n",
      "We can test this claim using the timeit module. The Timer class has two parameters: a\n",
      "statement that is executed multiple times, and setup code that is executed once at the\n",
      "beginning. We will simulate a vocabulary of 100,000 items using a list \n",
      " or set \n",
      "  of\n",
      "integers. \n",
      "The test statement will generate a random item that has a 50% chance of being\n",
      "in the vocabulary \n",
      " .\n",
      "164 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 185, 'page_label': '164', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4534}\n",
      "\n",
      "--- Chunk 4535 ---\n",
      "Content:\n",
      ">>> from timeit import Timer\n",
      ">>> vocab_size = 100000\n",
      ">>> setup_list = \"import random; vocab = range(%d)\" % vocab_size \n",
      ">>> setup_set = \"import random; vocab = set(range(%d))\" % vocab_size \n",
      ">>> statement = \"random.randint(0, %d) in vocab\" % vocab_size * 2 \n",
      ">>> print Timer(statement, setup_list).timeit(1000)\n",
      "2.78092288971\n",
      ">>> print Timer(statement, setup_set).timeit(1000)\n",
      "0.0037260055542\n",
      "Performing \n",
      "1,000 list membership tests takes a total of 2.8 seconds, whereas the equiv-\n",
      "alent tests on a set take a mere 0.0037 seconds, or three orders of magnitude faster!\n",
      "Dynamic Programming\n",
      "Dynamic programming is a general technique for designing algorithms which is widely\n",
      "used in natural language processing. The term “programming” is used in a different\n",
      "sense to what you might expect, to mean planning or scheduling. Dynamic program-\n",
      "ming is used when a problem contains overlapping subproblems. Instead of computing\n",
      "solutions to these subproblems repeatedly, we simply store them in a lookup table. In\n",
      "the remainder of this section, we will introduce dynamic programming, but in a rather\n",
      "different context to syntactic parsing.\n",
      "Pingala was an Indian author who lived around the 5th century B.C., and wrote a\n",
      "treatise on Sanskrit prosody called the Chandas Shastra. Virahanka extended this work\n",
      "around the 6th century A.D., studying the number of ways of combining short and long\n",
      "syllables to create a meter of length n. Short syllables, marked S, take up one unit of\n",
      "length, while long syllables, marked L, take two. Pingala found, for example, that there\n",
      "are five ways to construct a meter of length 4: V4 = {LL, SSL, SLS, LSS, SSSS}. Observe\n",
      "that we can split V4 into two subsets, those starting with L and those starting with S,\n",
      "as shown in (1).\n",
      "(1) V4 =\n",
      "  LL, LSS\n",
      "    i.e. L prefixed to each item of V2 = {L, SS}\n",
      "  SSL, SLS, SSSS\n",
      "    i.e. S prefixed to each item of V3 = {SL, LS, SSS}\n",
      "With this observation, we can write a little recursive function called virahanka1() to\n",
      "compute these meters, shown in Example 4-9. Notice that, in order to compute V4 we\n",
      "first compute V3 and V2. But to compute V3, we need to first compute V2 and V1. This\n",
      "call structure is depicted in (2).\n",
      "4.7  Algorithm Design | 165...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 186, 'page_label': '165', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4535}\n",
      "\n",
      "--- Chunk 4536 ---\n",
      "Content:\n",
      "Example 4-9. Four ways to compute Sanskrit meter: (i) iterative, (ii) bottom-up dynamic\n",
      "programming, (iii) top-down dynamic programming, and (iv) built-in memoization.\n",
      "def virahanka1(n):\n",
      "    if n == 0:\n",
      "        return [\"\"]\n",
      "    elif n == 1:\n",
      "        return [\"S\"]\n",
      "    else:\n",
      "        s = [\"S\" + prosody for prosody in virahanka1(n-1)]\n",
      "        l = [\"L\" + prosody for prosody in virahanka1(n-2)]\n",
      "        return s + l\n",
      "def virahanka2(n):\n",
      "    lookup = [[\"\"], [\"S\"]]\n",
      "    for i in range(n-1):\n",
      "        s = [\"S\" + prosody for prosody in lookup[i+1]]\n",
      "        l = [\"L\" + prosody for prosody in lookup[i]]\n",
      "        lookup.append(s + l)\n",
      "    return lookup[n]\n",
      "def virahanka3(n, lookup={0:[\"\"], 1:[\"S\"]}):\n",
      "    if n not in lookup:\n",
      "        s = [\"S\" + prosody for prosody in virahanka3(n-1)]\n",
      "        l = [\"L\" + prosody for prosody in virahanka3(n-2)]\n",
      "        lookup[n] = s + l\n",
      "    return lookup[n]\n",
      "from nltk import memoize\n",
      "@memoize\n",
      "def virahanka4(n):\n",
      "    if n == 0:\n",
      "        return [\"\"]\n",
      "    elif n == 1:\n",
      "        return [\"S\"]\n",
      "    else:\n",
      "        s = [\"S\" + prosody for prosody in virahanka4(n-1)]\n",
      "        l = [\"L\" + prosody for prosody in virahanka4(n-2)]\n",
      "        return s + l\n",
      ">>> virahanka1(4)\n",
      "['SSSS', 'SSL', 'SLS', 'LSS', 'LL']\n",
      ">>> virahanka2(4)\n",
      "['SSSS', 'SSL', 'SLS', 'LSS', 'LL']\n",
      ">>> virahanka3(4)\n",
      "['SSSS', 'SSL', 'SLS', 'LSS', 'LL']\n",
      ">>> virahanka4(4)\n",
      "['SSSS', 'SSL', 'SLS', 'LSS', 'LL']\n",
      "166 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 187, 'page_label': '166', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4536}\n",
      "\n",
      "--- Chunk 4537 ---\n",
      "Content:\n",
      "(2)\n",
      "As you can see, V2 is computed twice. This might not seem like a significant problem,\n",
      "but it turns out to be rather wasteful as n gets large: to compute V20 using this recursive\n",
      "technique, we would compute V2 4,181 times; and for V40 we would compute V2\n",
      "63,245,986 times! A much better alternative is to store the value of V2 in a table and\n",
      "look it up whenever we need it. The same goes for other values, such as V3 and so on.\n",
      "Function virahanka2() implements a dynamic programming approach to the problem.\n",
      "It works by filling up a table (called lookup) with solutions to all smaller instances of\n",
      "the problem, stopping as soon as we reach the value we’re interested in. At this point\n",
      "we read off the value and return it. Crucially, each subproblem is only ever solved once.\n",
      "Notice that the approach taken in virahanka2() is to solve smaller problems on the way\n",
      "to solving larger problems. Accordingly, this is known as the bottom-up approach to\n",
      "dynamic programming. Unfortunately it turns out to be quite wasteful for some ap-\n",
      "plications, since it may compute solutions to sub-problems that are never required for\n",
      "solving the main problem. This wasted computation can be avoided using the top-\n",
      "down approach to dynamic programming, which is illustrated in the function vira\n",
      "hanka3() in Example 4-9. Unlike the bottom-up approach, this approach is recursive.\n",
      "It avoids the huge wastage of virahanka1() by checking whether it has previously stored\n",
      "the result. If not, it computes the result recursively and stores it in the table. The last\n",
      "step is to return the stored result. The final method, in virahanka4(), is to use a Python\n",
      "“decorator” called memoize, which takes care of the housekeeping work done by\n",
      "virahanka3() without cluttering up the program. This “memoization” process stores\n",
      "the result of each previous call to the function along with the parameters that were\n",
      "used. If the function is subsequently called with the same parameters, it returns the\n",
      "stored result instead of recalculating it. (This aspect of Python syntax is beyond the\n",
      "scope of this book.)\n",
      "This concludes our brief introduction to dynamic programming. We will encounter it\n",
      "again in Section 8.4.\n",
      "4.8  A Sample of Python Libraries\n",
      "Python has hundreds of third-party libraries, specialized software packages that extend\n",
      "the functionality of Python. NLTK is one such library. To realize the full power of\n",
      "Python programming, you should become familiar with several other libraries. Most\n",
      "of these will need to be manually installed on your computer.\n",
      "4.8  A Sample of Python Libraries | 167...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 188, 'page_label': '167', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4537}\n",
      "\n",
      "--- Chunk 4538 ---\n",
      "Content:\n",
      "Matplotlib\n",
      "Python \n",
      "has some libraries that are useful for visualizing language data. The Matplotlib\n",
      "package supports sophisticated plotting functions with a MATLAB-style interface, and\n",
      "is available from http://matplotlib.sourceforge.net/.\n",
      "So far we have focused on textual presentation and the use of formatted print statements\n",
      "to get output lined up in columns. It is often very useful to display numerical data in\n",
      "graphical form, since this often makes it easier to detect patterns. For example, in\n",
      "Example 3-5, we saw a table of numbers showing the frequency of particular modal\n",
      "verbs in the Brown Corpus, classified by genre. The program in Example 4-10 presents\n",
      "the same information in graphical format. The output is shown in Figure 4-4 (a color\n",
      "figure in the graphical display).\n",
      "Example 4-10. Frequency of modals in different sections of the Brown Corpus.\n",
      "colors = 'rgbcmyk' # red, green, blue, cyan, magenta, yellow, black\n",
      "def bar_chart(categories, words, counts):\n",
      "    \"Plot a bar chart showing counts for each word by category\"\n",
      "    import pylab\n",
      "    ind = pylab.arange(len(words))\n",
      "    width = 1 / (len(categories) + 1)\n",
      "    bar_groups = []\n",
      "    for c in range(len(categories)):\n",
      "        bars = pylab.bar(ind+c*width, counts[categories[c]], width,\n",
      "                         color=colors[c % len(colors)])\n",
      "        bar_groups.append(bars)\n",
      "    pylab.xticks(ind+width, words)\n",
      "    pylab.legend([b[0] for b in bar_groups], categories, loc='upper left')\n",
      "    pylab.ylabel('Frequency')\n",
      "    pylab.title('Frequency of Six Modal Verbs by Genre')\n",
      "    pylab.show()\n",
      ">>> genres = ['news', 'religion', 'hobbies', 'government', 'adventure']\n",
      ">>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
      ">>> cfdist = nltk.ConditionalFreqDist(\n",
      "...              (genre, word)\n",
      "...              for genre in genres\n",
      "...              for word in nltk.corpus.brown.words(categories=genre)\n",
      "...              if word in modals)\n",
      "...\n",
      ">>> counts = {}\n",
      ">>> for genre in genres:\n",
      "...     counts[genre] = [cfdist[genre][word] for word in modals]\n",
      ">>> bar_chart(genres, modals, counts)\n",
      "From the bar chart it is immediately obvious that may and must have almost identical\n",
      "relative frequencies. The same goes for could and might.\n",
      "It is also possible to generate such data visualizations on the fly. For example, a web\n",
      "page with form input could permit visitors to specify search parameters, submit the\n",
      "form, and see a dynamically generated visualization. To do this we have to specify the\n",
      "168 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 189, 'page_label': '168', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4538}\n",
      "\n",
      "--- Chunk 4539 ---\n",
      "Content:\n",
      "Agg backend for matplotlib, which is a library for producing raster (pixel) images \n",
      " .\n",
      "Next, \n",
      "we use all the same PyLab methods as before, but instead of displaying the result\n",
      "on a graphical terminal using pylab.show(), we save it to a file using pylab.savefig()\n",
      ". We specify the filename and dpi, then print HTML markup that directs the web\n",
      "browser to load the file.\n",
      ">>> import matplotlib\n",
      ">>> matplotlib.use('Agg') \n",
      ">>> pylab.savefig('modals.png') \n",
      ">>> print 'Content-Type: text/html'\n",
      ">>> print\n",
      ">>> print '<html><body>'\n",
      ">>> print '<img src=\"modals.png\"/>'\n",
      ">>> print '</body></html>'\n",
      "Figure 4-4. Bar chart showing frequency of modals in different sections of Brown Corpus: This\n",
      "visualization was produced by the program in Example 4-10.\n",
      "NetworkX\n",
      "The \n",
      "NetworkX package is for defining and manipulating structures consisting of nodes\n",
      "and edges, known as graphs. It is available from https://networkx.lanl.gov/. NetworkX\n",
      "4.8  A Sample of Python Libraries | 169...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 190, 'page_label': '169', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4539}\n",
      "\n",
      "--- Chunk 4540 ---\n",
      "Content:\n",
      "can be used in conjunction with Matplotlib to visualize networks, such as WordNet\n",
      "(the \n",
      "semantic network we introduced in Section 2.5). The program in Example 4-11\n",
      "initializes an empty graph \n",
      "  and then traverses the WordNet hypernym hierarchy\n",
      "adding \n",
      "edges to the graph \n",
      " . Notice that the traversal is recursive \n",
      " , applying the\n",
      "programming \n",
      "technique discussed in Section 4.7. The resulting display is shown in\n",
      "Figure 4-5.\n",
      "Example 4-11. Using the NetworkX and Matplotlib libraries.\n",
      "import networkx as nx\n",
      "import matplotlib\n",
      "from nltk.corpus import wordnet as wn\n",
      "def traverse(graph, start, node):\n",
      "    graph.depth[node.name] = node.shortest_path_distance(start)\n",
      "    for child in node.hyponyms():\n",
      "        graph.add_edge(node.name, child.name) \n",
      "        traverse(graph, start, child) \n",
      "def hyponym_graph(start):\n",
      "    G = nx.Graph() \n",
      "    G.depth = {}\n",
      "    traverse(G, start, start)\n",
      "    return G\n",
      "def graph_draw(graph):\n",
      "    nx.draw_graphviz(graph,\n",
      "         node_size = [16 * graph.degree(n) for n in graph],\n",
      "         node_color = [graph.depth[n] for n in graph],\n",
      "         with_labels = False)\n",
      "    matplotlib.pyplot.show()\n",
      ">>> dog = wn.synset('dog.n.01')\n",
      ">>> graph = hyponym_graph(dog)\n",
      ">>> graph_draw(graph)\n",
      "csv\n",
      "Language \n",
      "analysis work often involves data tabulations, containing information about\n",
      "lexical items, the participants in an empirical study, or the linguistic features extracted\n",
      "from a corpus. Here’s a fragment of a simple lexicon, in CSV format:\n",
      "sleep, sli:p, v.i, a condition of body and mind ...\n",
      "walk, wo:k, v.intr, progress by lifting and setting down each foot ...\n",
      "wake, weik, intrans, cease to sleep\n",
      "We can use Python’s CSV library to read and write files stored in this format. For\n",
      "example, we can open a CSV file called lexicon.csv \n",
      " and iterate over its rows \n",
      " :\n",
      ">>> import csv\n",
      ">>> input_file = open(\"lexicon.csv\", \"rb\") \n",
      ">>> for row in csv.reader(input_file): \n",
      "...     print row\n",
      "['sleep', 'sli:p', 'v.i', 'a condition of body and mind ...']\n",
      "170 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 191, 'page_label': '170', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4540}\n",
      "\n",
      "--- Chunk 4541 ---\n",
      "Content:\n",
      "['walk', 'wo:k', 'v.intr', 'progress by lifting and setting down each foot ...']\n",
      "['wake', 'weik', 'intrans', 'cease to sleep']\n",
      "Each \n",
      "row is just a list of strings. If any fields contain numerical data, they will appear\n",
      "as strings, and will have to be converted using int() or float().\n",
      "Figure 4-5. Visualization with NetworkX and Matplotlib: Part of the WordNet hypernym hierarchy\n",
      "is \n",
      "displayed, starting with dog.n.01 (the darkest node in the middle); node size is based on the number\n",
      "of children of the node, and color is based on the distance of the node from dog.n.01; this visualization\n",
      "was produced by the program in Example 4-11.\n",
      "NumPy\n",
      "The NumPy package provides substantial support for numerical processing in Python.\n",
      "NumPy has a multidimensional array object, which is easy to initialize and access:\n",
      ">>> from numpy import array\n",
      ">>> cube = array([ [[0,0,0], [1,1,1], [2,2,2]],\n",
      "...                [[3,3,3], [4,4,4], [5,5,5]],\n",
      "...                [[6,6,6], [7,7,7], [8,8,8]] ])\n",
      ">>> cube[1,1,1]\n",
      "4\n",
      ">>> cube[2].transpose()\n",
      "array([[6, 7, 8],\n",
      "       [6, 7, 8],\n",
      "       [6, 7, 8]])\n",
      ">>> cube[2,1:]\n",
      "array([[7, 7, 7],\n",
      "       [8, 8, 8]])\n",
      "NumPy includes linear algebra functions. Here we perform singular value decomposi-\n",
      "tion on a matrix, an operation used in latent semantic analysis to help identify implicit\n",
      "concepts in a document collection:\n",
      "4.8  A Sample of Python Libraries | 171...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 192, 'page_label': '171', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4541}\n",
      "\n",
      "--- Chunk 4542 ---\n",
      "Content:\n",
      ">>> from numpy import linalg\n",
      ">>> a=array([[4,0], [3,-5]])\n",
      ">>> u,s,vt = linalg.svd(a)\n",
      ">>> u\n",
      "array([[-0.4472136 , -0.89442719],\n",
      "       [-0.89442719,  0.4472136 ]])\n",
      ">>> s\n",
      "array([ 6.32455532,  3.16227766])\n",
      ">>> vt\n",
      "array([[-0.70710678,  0.70710678],\n",
      "       [-0.70710678, -0.70710678]])\n",
      "NLTK’s \n",
      "clustering package nltk.cluster makes extensive use of NumPy arrays, and\n",
      "includes support for k-means clustering, Gaussian EM clustering, group average\n",
      "agglomerative clustering, and dendogram plots. For details, type help(nltk.cluster).\n",
      "Other Python Libraries\n",
      "There are many other Python libraries, and you can search for them with the help of\n",
      "the Python Package Index at http://pypi.python.org/. Many libraries provide an interface\n",
      "to external software, such as relational databases (e.g., mysql-python) and large docu-\n",
      "ment collections (e.g., PyLucene). Many other libraries give access to file formats such\n",
      "as PDF, MSWord, and XML (pypdf, pywin32, xml.etree), RSS feeds (e.g., feedparser),\n",
      "and electronic mail (e.g., imaplib, email).\n",
      "4.9  Summary\n",
      "• Python’s assignment and parameter passing use object references; e.g., if a is a list\n",
      "and we assign b = a, then any operation on a will modify b, and vice versa.\n",
      "• The is operation tests whether two objects are identical internal objects, whereas\n",
      "== tests whether two objects are equivalent. This distinction parallels the type-\n",
      "token distinction.\n",
      "• Strings, lists, and tuples are different kinds of sequence object, supporting common\n",
      "operations such as indexing, slicing, len(), sorted(), and membership testing using\n",
      "in.\n",
      "• We can write text to a file by opening the file for writing\n",
      "ofile = open('output.txt', 'w'\n",
      "then adding content to the file ofile.write(\"Monty Python\"), and finally closing\n",
      "the file ofile.close().\n",
      "• A declarative programming style usually produces more compact, readable code;\n",
      "manually incremented loop variables are usually unnecessary. When a sequence\n",
      "must be enumerated, use enumerate().\n",
      "• Functions are an essential programming abstraction: key concepts to understand\n",
      "are parameter passing, variable scope, and docstrings.\n",
      "172 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 193, 'page_label': '172', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4542}\n",
      "\n",
      "--- Chunk 4543 ---\n",
      "Content:\n",
      "• A function serves as a namespace: names defined inside a function are not visible\n",
      "outside that function, unless those names are declared to be global.\n",
      "• Modules permit logically related material to be localized in a file. A module serves\n",
      "as a namespace: names defined in a module—such as variables and functions—\n",
      "are not visible to other modules, unless those names are imported.\n",
      "• Dynamic programming is an algorithm design technique used widely in NLP that\n",
      "stores the results of previous computations in order to avoid unnecessary\n",
      "recomputation.\n",
      "4.10  Further Reading\n",
      "This chapter has touched on many topics in programming, some specific to Python,\n",
      "and some quite general. We’ve just scratched the surface, and you may want to read\n",
      "more about these topics, starting with the further materials for this chapter available\n",
      "at http://www.nltk.org/.\n",
      "The Python website provides extensive documentation. It is important to understand\n",
      "the built-in functions and standard types, described at http://docs.python.org/library/\n",
      "functions.html and http://docs.python.org/library/stdtypes.html. We have learned about\n",
      "generators and their importance for efficiency; for information about iterators, a closely\n",
      "related topic, see http://docs.python.org/library/itertools.html. Consult your favorite Py-\n",
      "thon book for more information on such topics. An excellent resource for using Python\n",
      "for multimedia processing, including working with sound files, is (Guzdial, 2005).\n",
      "When using the online Python documentation, be aware that your installed version\n",
      "might be different from the version of the documentation you are reading. You can\n",
      "easily check what version you have, with import sys; sys.version. Version-specific\n",
      "documentation is available at http://www.python.org/doc/versions/.\n",
      "Algorithm design is a rich field within computer science. Some good starting points are\n",
      "(Harel, 2004), (Levitin, 2004), and (Knuth, 2006). Useful guidance on the practice of\n",
      "software development is provided in (Hunt & Thomas, 2000) and (McConnell, 2004).\n",
      "4.11  Exercises\n",
      "1. ○ Find out more about sequence objects using Python’s help facility. In the inter-\n",
      "preter, type help(str), help(list), and help(tuple). This will give you a full list of\n",
      "the functions supported by each type. Some functions have special names flanked\n",
      "with underscores; as the help documentation shows, each such function corre-\n",
      "sponds to something more familiar. For example x.__getitem__(y) is just a long-\n",
      "winded way of saying x[y].\n",
      "2. ○ Identify three operations that can be performed on both tuples and lists. Identify\n",
      "three list operations that cannot be performed on tuples. Name a context where\n",
      "using a list instead of a tuple generates a Python error.\n",
      "4.11  Exercises | 173...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 194, 'page_label': '173', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4543}\n",
      "\n",
      "--- Chunk 4544 ---\n",
      "Content:\n",
      "3. ○ Find out how to create a tuple consisting of a single item. There are at least two\n",
      "ways to do this.\n",
      "4. ○ Create a list words = ['is', 'NLP', 'fun', '?']. Use a series of assignment\n",
      "statements (e.g., words[1] = words[2]) and a temporary variable tmp to transform\n",
      "this list into the list ['NLP', 'is', 'fun', '!']. Now do the same transformation\n",
      "using tuple assignment.\n",
      "5. ○ Read about the built-in comparison function cmp, by typing help(cmp). How does\n",
      "it differ in behavior from the comparison operators?\n",
      "6. ○ Does the method for creating a sliding window of n-grams behave correctly for\n",
      "the two limiting cases: n = 1 and n = len(sent)?\n",
      "7. ○ We pointed out that when empty strings and empty lists occur in the condition\n",
      "part of an if clause, they evaluate to False. In this case, they are said to be occurring\n",
      "in a Boolean context. Experiment with different kinds of non-Boolean expressions\n",
      "in Boolean contexts, and see whether they evaluate as True or False.\n",
      "8. ○ Use the inequality operators to compare strings, e.g., 'Monty' < 'Python'. What\n",
      "happens when you do 'Z' < 'a'? Try pairs of strings that have a common prefix,\n",
      "e.g., 'Monty' < 'Montague'. Read up on “lexicographical sort” in order to under-\n",
      "stand what is going on here. Try comparing structured objects, e.g., ('Monty', 1)\n",
      "< ('Monty', 2). Does this behave as expected?\n",
      "9. ○ Write code that removes whitespace at the beginning and end of a string, and\n",
      "normalizes whitespace between words to be a single-space character.\n",
      "a. Do this task using split() and join().\n",
      "b. Do this task using regular expression substitutions.\n",
      "10. ○ Write a program to sort words by length. Define a helper function cmp_len which\n",
      "uses the cmp comparison function on word lengths.\n",
      "11. ◑ Create a list of words and store it in a variable sent1. Now assign sent2 =\n",
      "sent1. Modify one of the items in sent1 and verify that sent2 has changed.\n",
      "a. Now try the same exercise, but instead assign sent2 = sent1[:]. Modify\n",
      "sent1 again and see what happens to sent2. Explain.\n",
      "b. Now define text1 to be a list of lists of strings (e.g., to represent a text consisting\n",
      "of multiple sentences). Now assign text2 = text1[:], assign a new value to\n",
      "one of the words, e.g., text1[1][1] = 'Monty'. Check what this did to text2.\n",
      "Explain.\n",
      "c. Load Python’s deepcopy() function (i.e., from copy import deepcopy), consult\n",
      "its documentation, and test that it makes a fresh copy of any object.\n",
      "12. ◑ Initialize an n-by-m list of lists of empty strings using list multiplication, e.g.,\n",
      "word_table = [[''] * n] * m. What happens when you set one of its values, e.g.,\n",
      "word_table[1][2] = \"hello\"? Explain why this happens. Now write an expression\n",
      "using range() to construct a list of lists, and show that it does not have this problem.\n",
      "174 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 195, 'page_label': '174', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4544}\n",
      "\n",
      "--- Chunk 4545 ---\n",
      "Content:\n",
      "13. ◑ Write code to initialize a two-dimensional array of sets called word_vowels and\n",
      "process a list of words, adding each word to word_vowels[l][v] where l is the length\n",
      "of the word and v is the number of vowels it contains.\n",
      "14. ◑ Write a function novel10(text) that prints any word that appeared in the last\n",
      "10% of a text that had not been encountered earlier.\n",
      "15. ◑ Write a program that takes a sentence expressed as a single string, splits it, and\n",
      "counts up the words. Get it to print out each word and the word’s frequency, one\n",
      "per line, in alphabetical order.\n",
      "16. ◑ Read up on Gematria, a method for assigning numbers to words, and for mapping\n",
      "between words having the same number to discover the hidden meaning of texts\n",
      "(http://en.wikipedia.org/wiki/Gematria, http://essenes.net/gemcal.htm).\n",
      "a. Write a function gematria() that sums the numerical values of the letters of a\n",
      "word, according to the letter values in letter_vals:\n",
      ">>> letter_vals = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':80, 'g':3, 'h':8,\n",
      "... 'i':10, 'j':10, 'k':20, 'l':30, 'm':40, 'n':50, 'o':70, 'p':80, 'q':100,\n",
      "... 'r':200, 's':300, 't':400, 'u':6, 'v':6, 'w':800, 'x':60, 'y':10, 'z':7}\n",
      "b. Process a corpus (e.g., nltk.corpus.state_union) and for each document,\n",
      "count how many of its words have the number 666.\n",
      "c. Write a function decode() to process a text, randomly replacing words with\n",
      "their Gematria equivalents, in order to discover the “hidden meaning” of the\n",
      "text.\n",
      "17. ◑ Write a function shorten(text, n) to process a text, omitting the n most fre-\n",
      "quently occurring words of the text. How readable is it?\n",
      "18. ◑ Write code to print out an index for a lexicon, allowing someone to look up\n",
      "words according to their meanings (or their pronunciations; whatever properties\n",
      "are contained in the lexical entries).\n",
      "19. ◑ Write a list comprehension that sorts a list of WordNet synsets for proximity to\n",
      "a given synset. For example, given the synsets minke_whale.n.01, orca.n.01,\n",
      "novel.n.01, and tortoise.n.01, sort them according to their path_distance() from\n",
      "right_whale.n.01.\n",
      "20. ◑ Write a function that takes a list of words (containing duplicates) and returns a\n",
      "list of words (with no duplicates) sorted by decreasing frequency. E.g., if the input\n",
      "list contained 10 instances of the word table and 9 instances of the word chair,\n",
      "then table would appear before chair in the output list.\n",
      "21. ◑ Write a function that takes a text and a vocabulary as its arguments and returns\n",
      "the set of words that appear in the text but not in the vocabulary. Both arguments\n",
      "can be represented as lists of strings. Can you do this in a single line, using set.dif\n",
      "ference()?\n",
      "22...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 196, 'page_label': '175', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4545}\n",
      "\n",
      "--- Chunk 4546 ---\n",
      "Content:\n",
      ". ◑ Write a function that takes a list of words (containing duplicates) and returns a\n",
      "list of words (with no duplicates) sorted by decreasing frequency. E.g., if the input\n",
      "list contained 10 instances of the word table and 9 instances of the word chair,\n",
      "then table would appear before chair in the output list.\n",
      "21. ◑ Write a function that takes a text and a vocabulary as its arguments and returns\n",
      "the set of words that appear in the text but not in the vocabulary. Both arguments\n",
      "can be represented as lists of strings. Can you do this in a single line, using set.dif\n",
      "ference()?\n",
      "22. ◑ Import the itemgetter() function from the operator module in Python’s standard\n",
      "library (i.e., from operator import itemgetter). Create a list words containing sev-\n",
      "4.11  Exercises | 175...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 196, 'page_label': '175', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4546}\n",
      "\n",
      "--- Chunk 4547 ---\n",
      "Content:\n",
      "eral words. Now try calling: sorted(words, key=itemgetter(1)), and sor\n",
      "ted(words, key=itemgetter(-1)). Explain what itemgetter() is doing.\n",
      "23. ◑ Write a recursive function lookup(trie, key) that looks up a key in a trie, and\n",
      "returns the value it finds. Extend the function to return a word when it is uniquely\n",
      "determined by its prefix (e.g., vanguard is the only word that starts with vang-, so\n",
      "lookup(trie, 'vang') should return the same thing as lookup(trie, 'vanguard')).\n",
      "24. ◑ Read up on “keyword linkage” (Chapter 5 of (Scott & Tribble, 2006)). Extract\n",
      "keywords from NLTK’s Shakespeare Corpus and using the NetworkX package,\n",
      "plot keyword linkage networks.\n",
      "25. ◑ Read about string edit distance and the Levenshtein Algorithm. Try the imple-\n",
      "mentation provided in nltk.edit_dist(). In what way is this using dynamic pro-\n",
      "gramming? Does it use the bottom-up or top-down approach? (See also http://\n",
      "norvig.com/spell-correct.html.)\n",
      "26. ◑ The Catalan numbers arise in many applications of combinatorial mathematics,\n",
      "including the counting of parse trees ( Section 8.6). The series can be defined as\n",
      "follows: C0 = 1, and Cn+1 = Σ0..n (CiCn-i).\n",
      "a. Write a recursive function to compute nth Catalan number Cn.\n",
      "b. Now write another function that does this computation using dynamic pro-\n",
      "gramming.\n",
      "c. Use the timeit module to compare the performance of these functions as n\n",
      "increases.\n",
      "27. ● Reproduce some of the results of (Zhao & Zobel, 2007) concerning authorship\n",
      "identification.\n",
      "28. ● Study gender-specific lexical choice, and see if you can reproduce some of the\n",
      "results of http://www.clintoneast.com/articles/words.php.\n",
      "29. ● Write a recursive function that pretty prints a trie in alphabetically sorted order,\n",
      "for example:\n",
      "chair: 'flesh'\n",
      "---t: 'cat'\n",
      "--ic: 'stylish'\n",
      "---en: 'dog'\n",
      "30. ● With the help of the trie data structure, write a recursive function that processes\n",
      "text, locating the uniqueness point in each word, and discarding the remainder of\n",
      "each word. How much compression does this give? How readable is the resulting\n",
      "text?\n",
      "31. ● Obtain some raw text, in the form of a single, long string. Use Python’s text\n",
      "wrap module to break it up into multiple lines. Now write code to add extra spaces\n",
      "between words, in order to justify the output. Each line must have the same width,\n",
      "and spaces must be approximately evenly distributed across each line. No line can\n",
      "begin or end with a space.\n",
      "176 | Chapter 4:  Writing Structured Programs...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 197, 'page_label': '176', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4547}\n",
      "\n",
      "--- Chunk 4548 ---\n",
      "Content:\n",
      "32. ● Develop a simple extractive summarization tool, that prints the sentences of a\n",
      "document which contain the highest total word frequency. Use FreqDist() to count\n",
      "word frequencies, and use sum to sum the frequencies of the words in each sentence.\n",
      "Rank the sentences according to their score. Finally, print the n highest-scoring\n",
      "sentences in document order. Carefully review the design of your program,\n",
      "especially your approach to this double sorting. Make sure the program is written\n",
      "as clearly as possible.\n",
      "33. ● Develop your own NgramTagger class that inherits from NLTK’s class, and which\n",
      "encapsulates the method of collapsing the vocabulary of the tagged training and\n",
      "testing data that was described in Chapter 5. Make sure that the unigram and\n",
      "default backoff taggers have access to the full vocabulary.\n",
      "34. ● Read the following article on semantic orientation of adjectives. Use the Net-\n",
      "workX package to visualize a network of adjectives with edges to indicate same\n",
      "versus different semantic orientation (see http://www.aclweb.org/anthology/P97\n",
      "-1023).\n",
      "35. ● Design an algorithm to find the “statistically improbable phrases” of a document\n",
      "collection (see http://www.amazon.com/gp/search-inside/sipshelp.html).\n",
      "36. ● Write a program to implement a brute-force algorithm for discovering word\n",
      "squares, a kind of n × n: crossword in which the entry in the nth row is the same\n",
      "as the entry in the nth column. For discussion, see http://itre.cis.upenn.edu/~myl/\n",
      "languagelog/archives/002679.html.\n",
      "4.11  Exercises | 177...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 198, 'page_label': '177', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4548}\n",
      "\n",
      "--- Chunk 4549 ---\n",
      "Content:\n",
      "CHAPTER 5\n",
      "Categorizing and Tagging Words\n",
      "Back in elementary school you learned the difference between nouns, verbs, adjectives,\n",
      "and \n",
      "adverbs. These “word classes” are not just the idle invention of grammarians, but\n",
      "are useful categories for many language processing tasks. As we will see, they arise from\n",
      "simple analysis of the distribution of words in text. The goal of this chapter is to answer\n",
      "the following questions:\n",
      "1. What are lexical categories, and how are they used in natural language processing?\n",
      "2. What is a good Python data structure for storing words and their categories?\n",
      "3. How can we automatically tag each word of a text with its word class?\n",
      "Along the way, we’ll cover some fundamental techniques in NLP, including sequence\n",
      "labeling, n-gram models, backoff, and evaluation. These techniques are useful in many\n",
      "areas, and tagging gives us a simple context in which to present them. We will also see\n",
      "how tagging is the second step in the typical NLP pipeline, following tokenization.\n",
      "The process of classifying words into their parts-of-speech and labeling them accord-\n",
      "ingly is known as part-of-speech tagging, POS tagging, or simply tagging. Parts-\n",
      "of-speech are also known as word classes or lexical categories. The collection of tags\n",
      "used for a particular task is known as a tagset. Our emphasis in this chapter is on\n",
      "exploiting tags, and tagging text automatically.\n",
      "5.1  Using a Tagger\n",
      "A part-of-speech tagger, or POS tagger, processes a sequence of words, and attaches\n",
      "a part of speech tag to each word (don’t forget to import nltk):\n",
      ">>> text = nltk.word_tokenize(\"And now for something completely different\")\n",
      ">>> nltk.pos_tag(text)\n",
      "[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'),\n",
      "('completely', 'RB'), ('different', 'JJ')]\n",
      "Here we see that and is CC, a coordinating conjunction; now and completely are RB, or\n",
      "adverbs; for is IN, a preposition; something is NN, a noun; and different is JJ, an adjective.\n",
      "179...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 200, 'page_label': '179', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4549}\n",
      "\n",
      "--- Chunk 4550 ---\n",
      "Content:\n",
      "NLTK provides documentation for each tag, which can be queried using\n",
      "the \n",
      "tag, e.g., nltk.help.upenn_tagset('RB'), or a regular expression,\n",
      "e.g., nltk.help.upenn_brown_tagset('NN.*'). Some corpora have RE-\n",
      "ADME files with tagset documentation; see nltk.name.readme(), sub-\n",
      "stituting in the name of the corpus.\n",
      "Let’s look at another example, this time including some homonyms:\n",
      ">>> text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
      ">>> nltk.pos_tag(text)\n",
      "[('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'),\n",
      "('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]\n",
      "Notice that refuse and permit both appear as a present tense verb ( VBP) and a noun\n",
      "(NN). E.g., refUSE is a verb meaning “deny,” while REFuse is a noun meaning “trash”\n",
      "(i.e., they are not homophones). Thus, we need to know which word is being used in\n",
      "order to pronounce the text correctly. (For this reason, text-to-speech systems usually\n",
      "perform POS tagging.)\n",
      "Your Turn: Many words, like ski and race, can be used as nouns or\n",
      "verbs with no difference in pronunciation. Can you think of others?\n",
      "Hint: think of a commonplace object and try to put the word to before\n",
      "it to see if it can also be a verb, or think of an action and try to put the\n",
      "before it to see if it can also be a noun. Now make up a sentence with\n",
      "both uses of this word, and run the POS tagger on this sentence.\n",
      "Lexical categories like “noun” and part-of-speech tags like NN seem to have their uses,\n",
      "but the details will be obscure to many readers. You might wonder what justification\n",
      "there is for introducing this extra level of information. Many of these categories arise\n",
      "from superficial analysis of the distribution of words in text. Consider the following\n",
      "analysis involving woman (a noun), bought (a verb), over (a preposition), and the (a\n",
      "determiner). The text.similar() method takes a word w, finds all contexts w1w w2,\n",
      "then finds all words w' that appear in the same context, i.e. w1w'w2.\n",
      ">>> text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
      ">>> text.similar('woman')\n",
      "Building word-context index...\n",
      "man time day year car moment world family house country child boy\n",
      "state job way war girl place room word\n",
      ">>> text.similar('bought')\n",
      "made said put done seen had found left given heard brought got been\n",
      "was set told took in felt that\n",
      ">>> text.similar('over')\n",
      "in on to of and for with from at by that into as up out down through\n",
      "is all about\n",
      ">>> text.similar('the')\n",
      "a his this their its her an that our any all one these my in your no\n",
      "some other and\n",
      "180 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 201, 'page_label': '180', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4550}\n",
      "\n",
      "--- Chunk 4551 ---\n",
      "Content:\n",
      "Observe that searching for woman finds nouns; searching for bought mostly finds verbs;\n",
      "searching for over generally finds prepositions; searching for the finds several deter-\n",
      "miners. A tagger can correctly identify the tags on these words in the context of a\n",
      "sentence, e.g., The woman bought over $150,000 worth of clothes.\n",
      "A tagger can also model our knowledge of unknown words; for example, we can guess\n",
      "that scrobbling is probably a verb, with the root scrobble, and likely to occur in contexts\n",
      "like he was scrobbling.\n",
      "5.2  Tagged Corpora\n",
      "Representing Tagged Tokens\n",
      "By convention in NLTK, a tagged token is represented using a tuple consisting of the\n",
      "token and the tag. We can create one of these special tuples from the standard string\n",
      "representation of a tagged token, using the function str2tuple():\n",
      ">>> tagged_token = nltk.tag.str2tuple('fly/NN')\n",
      ">>> tagged_token\n",
      "('fly', 'NN')\n",
      ">>> tagged_token[0]\n",
      "'fly'\n",
      ">>> tagged_token[1]\n",
      "'NN'\n",
      "We can construct a list of tagged tokens directly from a string. The first step is to\n",
      "tokenize the string to access the individual word/tag strings, and then to convert each\n",
      "of these into a tuple (using str2tuple()).\n",
      ">>> sent = '''\n",
      "... The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN\n",
      "... other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC\n",
      "... Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS\n",
      "... said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB\n",
      "... accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT\n",
      "... interest/NN of/IN both/ABX governments/NNS ''/'' ./.\n",
      "... '''\n",
      ">>> [nltk.tag.str2tuple(t) for t in sent.split()]\n",
      "[('The', 'AT'), ('grand', 'JJ'), ('jury', 'NN'), ('commented', 'VBD'),\n",
      "('on', 'IN'), ('a', 'AT'), ('number', 'NN'), ... ('.', '.')]\n",
      "Reading Tagged Corpora\n",
      "Several of the corpora included with NLTK have been tagged for their part-of-speech.\n",
      "Here’s an example of what you might see if you opened a file from the Brown Corpus\n",
      "with a text editor:\n",
      "The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at inves-\n",
      "tigation/nn of/in Atlanta’s/np$ recent/jj primary/nn election/nn produced/vbd / no/at\n",
      "evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.\n",
      "5.2  Tagged Corpora | 181...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 202, 'page_label': '181', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4551}\n",
      "\n",
      "--- Chunk 4552 ---\n",
      "Content:\n",
      "Other corpora use a variety of formats for storing part-of-speech tags. NLTK’s corpus\n",
      "readers \n",
      "provide a uniform interface so that you don’t have to be concerned with the\n",
      "different file formats. In contrast with the file extract just shown, the corpus reader for\n",
      "the Brown Corpus represents the data as shown next. Note that part-of-speech tags\n",
      "have been converted to uppercase; this has become standard practice since the Brown\n",
      "Corpus was published.\n",
      ">>> nltk.corpus.brown.tagged_words()\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ...]\n",
      ">>> nltk.corpus.brown.tagged_words(simplify_tags=True)\n",
      "[('The', 'DET'), ('Fulton', 'N'), ('County', 'N'), ...]\n",
      "Whenever a corpus contains tagged text, the NLTK corpus interface will have a\n",
      "tagged_words() method. Here are some more examples, again using the output format\n",
      "illustrated for the Brown Corpus:\n",
      ">>> print nltk.corpus.nps_chat.tagged_words()\n",
      "[('now', 'RB'), ('im', 'PRP'), ('left', 'VBD'), ...]\n",
      ">>> nltk.corpus.conll2000.tagged_words()\n",
      "[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ...]\n",
      ">>> nltk.corpus.treebank.tagged_words()\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]\n",
      "Not all corpora employ the same set of tags; see the tagset help functionality and the\n",
      "readme() methods mentioned earlier for documentation. Initially we want to avoid the\n",
      "complications of these tagsets, so we use a built-in mapping to a simplified tagset:\n",
      ">>> nltk.corpus.brown.tagged_words(simplify_tags=True)\n",
      "[('The', 'DET'), ('Fulton', 'NP'), ('County', 'N'), ...]\n",
      ">>> nltk.corpus.treebank.tagged_words(simplify_tags=True)\n",
      "[('Pierre', 'NP'), ('Vinken', 'NP'), (',', ','), ...]\n",
      "Tagged corpora for several other languages are distributed with NLTK, including Chi-\n",
      "nese, Hindi, Portuguese, Spanish, Dutch, and Catalan...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 203, 'page_label': '182', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4552}\n",
      "\n",
      "--- Chunk 4553 ---\n",
      "Content:\n",
      ". These usually contain non-\n",
      "ASCII text, and Python always displays this in hexadecimal when printing a larger\n",
      "structure such as a list.\n",
      ">>> nltk.corpus.sinica_treebank.tagged_words()\n",
      "[('\\xe4\\xb8\\x80', 'Neu'), ('\\xe5\\x8f\\x8b\\xe6\\x83\\x85', 'Nad'), ...]\n",
      ">>> nltk.corpus.indian.tagged_words()\n",
      "[('\\xe0\\xa6\\xae\\xe0\\xa6\\xb9\\xe0\\xa6\\xbf\\xe0\\xa6\\xb7\\xe0\\xa7\\x87\\xe0\\xa6\\xb0', 'NN'),\n",
      "('\\xe0\\xa6\\xb8\\xe0\\xa6\\xa8\\xe0\\xa7\\x8d\\xe0\\xa6\\xa4\\xe0\\xa6\\xbe\\xe0\\xa6\\xa8', 'NN'),\n",
      "...]\n",
      ">>> nltk.corpus.mac_morpho.tagged_words()\n",
      "[('Jersei', 'N'), ('atinge', 'V'), ('m\\xe9dia', 'N'), ...]\n",
      ">>> nltk.corpus.conll2002.tagged_words()\n",
      "[('Sao', 'NC'), ('Paulo', 'VMI'), ('(', 'Fpa'), ...]\n",
      ">>> nltk.corpus.cess_cat.tagged_words()\n",
      "[('El', 'da0ms0'), ('Tribunal_Suprem', 'np0000o'), ...]\n",
      "If your environment is set up correctly, with appropriate editors and fonts, you should\n",
      "be able to display individual strings in a human-readable way. For example, Fig-\n",
      "ure 5-1 shows data accessed using nltk.corpus.indian.\n",
      "182 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 203, 'page_label': '182', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4553}\n",
      "\n",
      "--- Chunk 4554 ---\n",
      "Content:\n",
      "If the corpus is also segmented into sentences, it will have a tagged_sents() method\n",
      "that divides up the tagged words into sentences rather than presenting them as one big\n",
      "list. This will be useful when we come to developing automatic taggers, as they are\n",
      "trained and tested on lists of sentences, not words.\n",
      "A Simplified Part-of-Speech Tagset\n",
      "Tagged corpora use many different conventions for tagging words. To help us get star-\n",
      "ted, we will be looking at a simplified tagset (shown in Table 5-1).\n",
      "Table 5-1. Simplified part-of-speech tagset\n",
      "Tag Meaning Examples\n",
      "ADJ adjective new, good, high, special, big, local\n",
      "ADV adverb really, already, still, early, now\n",
      "CNJ conjunction and, or, but, if, while, although\n",
      "DET determiner the, a, some, most, every, no\n",
      "EX existential there, there’s\n",
      "FW foreign word dolce, ersatz, esprit, quo, maitre\n",
      "MOD modal verb will, can, would, may, must, should\n",
      "N noun year, home, costs, time, education\n",
      "NP proper noun Alison, Africa, April, Washington\n",
      "NUM number twenty-four, fourth, 1991, 14:24\n",
      "PRO pronoun he, their, her, its, my, I, us\n",
      "P preposition on, of, at, with, by, into, under\n",
      "TO the word to to\n",
      "UH interjection ah, bang, ha, whee, hmpf, oops\n",
      "V verb is, has, get, do, make, see, run\n",
      "VD past tense said, took, told, made, asked\n",
      "VG present participle making, going, playing, working\n",
      "VN past participle given, taken, begun, sung\n",
      "WH wh determiner who, which, when, what, where, how\n",
      "5.2  Tagged Corpora | 183...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 204, 'page_label': '183', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4554}\n",
      "\n",
      "--- Chunk 4555 ---\n",
      "Content:\n",
      "Figure 5-1. POS tagged data from four Indian languages: Bangla, Hindi, Marathi, and Telugu.\n",
      "Let’s see which of these tags are the most common in the news category of the Brown\n",
      "Corpus:\n",
      ">>> from nltk.corpus import brown\n",
      ">>> brown_news_tagged = brown.tagged_words(categories='news', simplify_tags=True)\n",
      ">>> tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\n",
      ">>> tag_fd.keys()\n",
      "['N', 'P', 'DET', 'NP', 'V', 'ADJ', ',', '.', 'CNJ', 'PRO', 'ADV', 'VD', ...]\n",
      "Your Turn: Plot the frequency distribution just shown using\n",
      "tag_fd.plot(cumulative=True). What percentage of words are tagged\n",
      "using the first five tags of the above list?\n",
      "We can use these tags to do powerful searches using a graphical POS-concordance tool\n",
      "nltk.app.concordance(). Use it to search for any combination of words and POS tags,\n",
      "e.g., N N N N, hit/VD, hit/VN, or the ADJ man.\n",
      "Nouns\n",
      "Nouns generally refer to people, places, things, or concepts, e.g., woman, Scotland,\n",
      "book, intelligence. Nouns can appear after determiners and adjectives, and can be the\n",
      "subject or object of the verb, as shown in Table 5-2.\n",
      "Table 5-2. Syntactic patterns involving some nouns\n",
      "Word After a determiner Subject of the verb\n",
      "woman the woman who I saw yesterday ... the woman sat down\n",
      "Scotland the Scotland I remember as a child ... Scotland has five million people\n",
      "book the book I bought yesterday ... this book recounts the colonization of Australia\n",
      "intelligence the intelligence displayed by the child ... Mary’s intelligence impressed her teachers\n",
      "The simplified noun tags are N for common nouns like book, and NP for proper nouns\n",
      "like Scotland.\n",
      "184 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 205, 'page_label': '184', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4555}\n",
      "\n",
      "--- Chunk 4556 ---\n",
      "Content:\n",
      "Let’s inspect some tagged text to see what parts-of-speech occur before a noun, with\n",
      "the \n",
      "most frequent ones first. To begin with, we construct a list of bigrams whose mem-\n",
      "bers are themselves word-tag pairs, such as (('The', 'DET'), ('Fulton', 'NP')) and\n",
      "(('Fulton', 'NP'), ('County', 'N')). Then we construct a FreqDist from the tag parts\n",
      "of the bigrams.\n",
      ">>> word_tag_pairs = nltk.bigrams(brown_news_tagged)\n",
      ">>> list(nltk.FreqDist(a[1] for (a, b) in word_tag_pairs if b[1] == 'N'))\n",
      "['DET', 'ADJ', 'N', 'P', 'NP', 'NUM', 'V', 'PRO', 'CNJ', '.', ',', 'VG', 'VN', ...]\n",
      "This confirms our assertion that nouns occur after determiners and adjectives, includ-\n",
      "ing numeral adjectives (tagged as NUM).\n",
      "Verbs\n",
      "Verbs are words that describe events and actions, e.g., fall and eat, as shown in Ta-\n",
      "ble 5-3. In the context of a sentence, verbs typically express a relation involving the\n",
      "referents of one or more noun phrases.\n",
      "Table 5-3. Syntactic patterns involving some verbs\n",
      "Word Simple With modifiers and adjuncts (italicized)\n",
      "fall Rome fell Dot com stocks suddenly fell like a stone\n",
      "eat Mice eat cheese John ate the pizza with gusto\n",
      "What are the most common verbs in news text? Let’s sort all the verbs by frequency:\n",
      ">>> wsj = nltk.corpus.treebank.tagged_words(simplify_tags=True)\n",
      ">>> word_tag_fd = nltk.FreqDist(wsj)\n",
      ">>> [word + \"/\" + tag for (word, tag) in word_tag_fd if tag.startswith('V')]\n",
      "['is/V', 'said/VD', 'was/VD', 'are/V', 'be/V', 'has/V', 'have/V', 'says/V',\n",
      "'were/VD', 'had/VD', 'been/VN', \"'s/V\", 'do/V', 'say/V', 'make/V', 'did/VD',\n",
      "'rose/VD', 'does/V', 'expected/VN', 'buy/V', 'take/V', 'get/V', 'sell/V',\n",
      "'help/V', 'added/VD', 'including/VG', 'according/VG', 'made/VN', 'pay/V', ...]\n",
      "Note \n",
      "that the items being counted in the frequency distribution are word-tag pairs.\n",
      "Since words and tags are paired, we can treat the word as a condition and the tag as an\n",
      "event, and initialize a conditional frequency distribution with a list of condition-event\n",
      "pairs. This lets us see a frequency-ordered list of tags given a word:\n",
      ">>> cfd1 = nltk.ConditionalFreqDist(wsj)\n",
      ">>> cfd1['yield'].keys()\n",
      "['V', 'N']\n",
      ">>> cfd1['cut'].keys()\n",
      "['V', 'VD', 'N', 'VN']\n",
      "We can reverse the order of the pairs, so that the tags are the conditions, and the words\n",
      "are the events. Now we can see likely words for a given tag:\n",
      "5.2  Tagged Corpora | 185...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 206, 'page_label': '185', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4556}\n",
      "\n",
      "--- Chunk 4557 ---\n",
      "Content:\n",
      ">>> cfd2 = nltk.ConditionalFreqDist((tag, word) for (word, tag) in wsj)\n",
      ">>> cfd2['VN'].keys()\n",
      "['been', 'expected', 'made', 'compared', 'based', 'priced', 'used', 'sold',\n",
      "'named', 'designed', 'held', 'fined', 'taken', 'paid', 'traded', 'said', ...]\n",
      "To \n",
      "clarify the distinction between VD (past tense) and VN (past participle), let’s find\n",
      "words that can be both VD and VN, and see some surrounding text:\n",
      ">>> [w for w in cfd1.conditions() if 'VD' in cfd1[w] and 'VN' in cfd1[w]]\n",
      "['Asked', 'accelerated', 'accepted', 'accused', 'acquired', 'added', 'adopted', ...]\n",
      ">>> idx1 = wsj.index(('kicked', 'VD'))\n",
      ">>> wsj[idx1-4:idx1+1]\n",
      "[('While', 'P'), ('program', 'N'), ('trades', 'N'), ('swiftly', 'ADV'),\n",
      "('kicked', 'VD')]\n",
      ">>> idx2 = wsj.index(('kicked', 'VN'))\n",
      ">>> wsj[idx2-4:idx2+1]\n",
      "[('head', 'N'), ('of', 'P'), ('state', 'N'), ('has', 'V'), ('kicked', 'VN')]\n",
      "In this case, we see that the past participle of kicked is preceded by a form of the auxiliary\n",
      "verb have. Is this generally true?\n",
      "Your Turn: Given the list of past participles specified by\n",
      "cfd2['VN'].keys(), try to collect a list of all the word-tag pairs that im-\n",
      "mediately precede items in that list.\n",
      "Adjectives and Adverbs\n",
      "Two other important word classes are adjectives and adverbs. Adjectives describe\n",
      "nouns, and can be used as modifiers (e.g., large in the large pizza), or as predicates (e.g.,\n",
      "the pizza is large ). English adjectives can have internal structure (e.g., fall+ing in the\n",
      "falling stocks). Adverbs modify verbs to specify the time, manner, place, or direction of\n",
      "the event described by the verb (e.g., quickly in the stocks fell quickly). Adverbs may\n",
      "also modify adjectives (e.g., really in Mary’s teacher was really nice).\n",
      "English has several categories of closed class words in addition to prepositions, such\n",
      "as articles (also often called determiners) (e.g., the, a), modals (e.g., should, may),\n",
      "and personal pronouns (e.g., she, they). Each dictionary and grammar classifies these\n",
      "words differently.\n",
      "Your Turn: If you are uncertain about some of these parts-of-speech,\n",
      "study them using nltk.app.concordance(), or watch some of the School-\n",
      "house Rock! grammar videos available at YouTube, or consult Sec-\n",
      "tion 5.9.\n",
      "186 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 207, 'page_label': '186', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4557}\n",
      "\n",
      "--- Chunk 4558 ---\n",
      "Content:\n",
      "Unsimplified Tags\n",
      "Let’s \n",
      "find the most frequent nouns of each noun part-of-speech type. The program in\n",
      "Example 5-1 finds all tags starting with NN, and provides a few example words for each\n",
      "one. You will see that there are many variants of NN; the most important contain $ for\n",
      "possessive nouns, S for plural nouns (since plural nouns typically end in s), and P for\n",
      "proper nouns. In addition, most of the tags have suffix modifiers: -NC for citations,\n",
      "-HL for words in headlines, and -TL for titles (a feature of Brown tags).\n",
      "Example 5-1. Program to find the most frequent noun tags.\n",
      "def findtags(tag_prefix, tagged_text):\n",
      "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
      "                                  if tag.startswith(tag_prefix))\n",
      "    return dict((tag, cfd[tag].keys()[:5]) for tag in cfd.conditions())\n",
      ">>> tagdict = findtags('NN', nltk.corpus.brown.tagged_words(categories='news'))\n",
      ">>> for tag in sorted(tagdict):\n",
      "...     print tag, tagdict[tag]\n",
      "...\n",
      "NN ['year', 'time', 'state', 'week', 'man']\n",
      "NN$ [\"year's\", \"world's\", \"state's\", \"nation's\", \"company's\"]\n",
      "NN$-HL [\"Golf's\", \"Navy's\"]\n",
      "NN$-TL [\"President's\", \"University's\", \"League's\", \"Gallery's\", \"Army's\"]\n",
      "NN-HL ['cut', 'Salary', 'condition', 'Question', 'business']\n",
      "NN-NC ['eva', 'ova', 'aya']\n",
      "NN-TL ['President', 'House', 'State', 'University', 'City']\n",
      "NN-TL-HL ['Fort', 'City', 'Commissioner', 'Grove', 'House']\n",
      "NNS ['years', 'members', 'people', 'sales', 'men']\n",
      "NNS$ [\"children's\", \"women's\", \"men's\", \"janitors'\", \"taxpayers'\"]\n",
      "NNS$-HL [\"Dealers'\", \"Idols'\"]\n",
      "NNS$-TL [\"Women's\", \"States'\", \"Giants'\", \"Officers'\", \"Bombers'\"]\n",
      "NNS-HL ['years', 'idols', 'Creations', 'thanks', 'centers']\n",
      "NNS-TL ['States', 'Nations', 'Masters', 'Rules', 'Communists']\n",
      "NNS-TL-HL ['Nations']\n",
      "When \n",
      "we come to constructing part-of-speech taggers later in this chapter, we will use\n",
      "the unsimplified tags.\n",
      "Exploring Tagged Corpora\n",
      "Let’s briefly return to the kinds of exploration of corpora we saw in previous chapters,\n",
      "this time exploiting POS tags.\n",
      "Suppose we’re studying the word often and want to see how it is used in text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 208, 'page_label': '187', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4558}\n",
      "\n",
      "--- Chunk 4559 ---\n",
      "Content:\n",
      ". We could\n",
      "ask to see the words that follow often:\n",
      ">>> brown_learned_text = brown.words(categories='learned')\n",
      ">>> sorted(set(b for (a, b) in nltk.ibigrams(brown_learned_text) if a == 'often'))\n",
      "[',', '.', 'accomplished', 'analytically', 'appear', 'apt', 'associated', 'assuming',\n",
      "'became', 'become', 'been', 'began', 'call', 'called', 'carefully', 'chose', ...]\n",
      "However, it’s probably more instructive use the tagged_words() method to look at the\n",
      "part-of-speech tag of the following words:\n",
      "5.2  Tagged Corpora | 187...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 208, 'page_label': '187', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4559}\n",
      "\n",
      "--- Chunk 4560 ---\n",
      "Content:\n",
      ">>> brown_lrnd_tagged = brown.tagged_words(categories='learned', simplify_tags=True)\n",
      ">>> tags = [b[1] for (a, b) in nltk.ibigrams(brown_lrnd_tagged) if a[0] == 'often']\n",
      ">>> fd = nltk.FreqDist(tags)\n",
      ">>> fd.tabulate()\n",
      "  VN    V   VD  DET  ADJ  ADV    P  CNJ    ,   TO   VG   WH  VBZ    .\n",
      "  15   12    8    5    5    4    4    3    3    1    1    1    1    1\n",
      "Notice that the most high-frequency parts-of-speech following often are verbs. Nouns\n",
      "never appear in this position (in this particular corpus).\n",
      "Next, \n",
      "let’s look at some larger context, and find words involving particular sequences\n",
      "of tags and words (in this case \"<Verb> to <Verb>\"). In Example 5-2, we consider each\n",
      "three-word window in the sentence \n",
      " , and check whether they meet our criterion \n",
      " .\n",
      "If the tags match, we print the corresponding words \n",
      " .\n",
      "Example 5-2. Searching for three-word phrases using POS tags.\n",
      "from nltk.corpus import brown\n",
      "def process(sentence):\n",
      "    for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence): \n",
      "        if (t1.startswith('V') and t2 == 'TO' and t3.startswith('V')): \n",
      "            print w1, w2, w3 \n",
      ">>> for tagged_sent in brown.tagged_sents():\n",
      "...     process(tagged_sent)\n",
      "...\n",
      "combined to achieve\n",
      "continue to place\n",
      "serve to protect\n",
      "wanted to wait\n",
      "allowed to place\n",
      "expected to become\n",
      "...\n",
      "Finally, \n",
      "let’s look for words that are highly ambiguous as to their part-of-speech tag.\n",
      "Understanding why such words are tagged as they are in each context can help us clarify\n",
      "the distinctions between the tags.\n",
      ">>> brown_news_tagged = brown.tagged_words(categories='news', simplify_tags=True)\n",
      ">>> data = nltk.ConditionalFreqDist((word.lower(), tag)\n",
      "...                                 for (word, tag) in brown_news_tagged)\n",
      ">>> for word in data.conditions():\n",
      "...     if len(data[word]) > 3:\n",
      "...         tags = data[word].keys()\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 209, 'page_label': '188', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4560}\n",
      "\n",
      "--- Chunk 4561 ---\n",
      "Content:\n",
      ".                                 for (word, tag) in brown_news_tagged)\n",
      ">>> for word in data.conditions():\n",
      "...     if len(data[word]) > 3:\n",
      "...         tags = data[word].keys()\n",
      "...         print word, ' '.join(tags)\n",
      "...\n",
      "best ADJ ADV NP V\n",
      "better ADJ ADV V DET\n",
      "close ADV ADJ V N\n",
      "cut V N VN VD\n",
      "even ADV DET ADJ V\n",
      "grant NP N V -\n",
      "hit V VD VN N\n",
      "lay ADJ V NP VD\n",
      "left VD ADJ N VN\n",
      "188 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 209, 'page_label': '188', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4561}\n",
      "\n",
      "--- Chunk 4562 ---\n",
      "Content:\n",
      "like CNJ V ADJ P -\n",
      "near P ADV ADJ DET\n",
      "open ADJ V N ADV\n",
      "past N ADJ DET P\n",
      "present ADJ ADV V N\n",
      "read V VN VD NP\n",
      "right ADJ N DET ADV\n",
      "second NUM ADV DET N\n",
      "set VN V VD N -\n",
      "that CNJ V WH DET\n",
      "Your Turn: Open the POS concordance tool nltk.app.concordance()\n",
      "and load the complete Brown Corpus (simplified tagset). Now pick\n",
      "some of the words listed at the end of the previous code example and\n",
      "see how the tag of the word correlates with the context of the word. E.g.,\n",
      "search for near to see all forms mixed together, near/ADJ to see it used\n",
      "as an adjective, near N to see just those cases where a noun follows, and\n",
      "so forth.\n",
      "5.3  Mapping Words to Properties Using Python Dictionaries\n",
      "As we have seen, a tagged word of the form (word, tag) is an association between a\n",
      "word and a part-of-speech tag. Once we start doing part-of-speech tagging, we will be\n",
      "creating programs that assign a tag to a word, the tag which is most likely in a given\n",
      "context. We can think of this process as mapping from words to tags. The most natural\n",
      "way to store mappings in Python uses the so-called dictionary data type (also known\n",
      "as an associative array or hash array in other programming languages). In this sec-\n",
      "tion, we look at dictionaries and see how they can represent a variety of language in-\n",
      "formation, including parts-of-speech.\n",
      "Indexing Lists Versus Dictionaries\n",
      "A text, as we have seen, is treated in Python as a list of words. An important property\n",
      "of lists is that we can “look up” a particular item by giving its index, e.g., text1[100].\n",
      "Notice how we specify a number and get back a word. We can think of a list as a simple\n",
      "kind of table, as shown in Figure 5-2.\n",
      "Figure 5-2. List lookup: We access the contents of a Python list with the help of an integer index.\n",
      "5.3  Mapping Words to Properties Using Python Dictionaries | 189...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 210, 'page_label': '189', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4562}\n",
      "\n",
      "--- Chunk 4563 ---\n",
      "Content:\n",
      "Contrast this situation with frequency distributions ( Section 1.3), where we specify a\n",
      "word and get back a number, e.g., fdist['monstrous'], which tells us the number of\n",
      "times a given word has occurred in a text. Lookup using words is familiar to anyone\n",
      "who has used a dictionary. Some more examples are shown in Figure 5-3.\n",
      "Figure 5-3. Dictionary lookup: we access the entry of a dictionary using a key such as someone’s name,\n",
      "a \n",
      "web domain, or an English word; other names for dictionary are map, hashmap, hash, and\n",
      "associative array.\n",
      "In the case of a phonebook, we look up an entry using a name and get back a number.\n",
      "When we type a domain name in a web browser, the computer looks this up to get\n",
      "back an IP address. A word frequency table allows us to look up a word and find its\n",
      "frequency in a text collection. In all these cases, we are mapping from names to num-\n",
      "bers, rather than the other way around as with a list. In general, we would like to be\n",
      "able to map between arbitrary types of information. Table 5-4 lists a variety of linguistic\n",
      "objects, along with what they map.\n",
      "Table 5-4. Linguistic objects as mappings from keys to values\n",
      "Linguistic object Maps from Maps to\n",
      "Document Index Word List of pages (where word is found)\n",
      "Thesaurus Word sense List of synonyms\n",
      "Dictionary Headword Entry (part-of-speech, sense definitions, etymology)\n",
      "Comparative Wordlist Gloss term Cognates (list of words, one per language)\n",
      "Morph Analyzer\n",
      "Surface form Morphological analysis (list of component morphemes)\n",
      "Most often, we are mapping from a “word” to some structured object. For example, a\n",
      "document \n",
      "index maps from a word (which we can represent as a string) to a list of pages\n",
      "(represented as a list of integers). In this section, we will see how to represent such\n",
      "mappings in Python.\n",
      "Dictionaries in Python\n",
      "Python provides a dictionary data type that can be used for mapping between arbitrary\n",
      "types. It is like a conventional dictionary, in that it gives you an efficient way to look\n",
      "things up. However, as we see from Table 5-4, it has a much wider range of uses.\n",
      "190 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 211, 'page_label': '190', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4563}\n",
      "\n",
      "--- Chunk 4564 ---\n",
      "Content:\n",
      "To illustrate, we define pos to be an empty dictionary and then add four entries to it,\n",
      "specifying the part-of-speech of some words. We add entries to a dictionary using the\n",
      "familiar square bracket notation:\n",
      ">>> pos = {}\n",
      ">>> pos\n",
      "{}\n",
      ">>> pos['colorless'] = 'ADJ' \n",
      ">>> pos\n",
      "{'colorless': 'ADJ'}\n",
      ">>> pos['ideas'] = 'N'\n",
      ">>> pos['sleep'] = 'V'\n",
      ">>> pos['furiously'] = 'ADV'\n",
      ">>> pos \n",
      "{'furiously': 'ADV', 'ideas': 'N', 'colorless': 'ADJ', 'sleep': 'V'}\n",
      "So, \n",
      "for example, \n",
      "  says that the part-of-speech of colorless is adjective, or more spe-\n",
      "cifically, that the key 'colorless' is assigned the value 'ADJ' in dictionary pos. When\n",
      "we inspect the value of pos \n",
      "  we see a set of key-value pairs. Once we have populated\n",
      "the dictionary in this way, we can employ the keys to retrieve values:\n",
      ">>> pos['ideas']\n",
      "'N'\n",
      ">>> pos['colorless']\n",
      "'ADJ'\n",
      "Of course, we might accidentally use a key that hasn’t been assigned a value.\n",
      ">>> pos['green']\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in ?\n",
      "KeyError: 'green'\n",
      "This \n",
      "raises an important question. Unlike lists and strings, where we can use len() to\n",
      "work out which integers will be legal indexes, how do we work out the legal keys for a\n",
      "dictionary? If the dictionary is not too big, we can simply inspect its contents by eval-\n",
      "uating the variable pos. As we saw earlier in line \n",
      ", this gives us the key-value pairs.\n",
      "Notice \n",
      "that they are not in the same order they were originally entered; this is because\n",
      "dictionaries are not sequences but mappings (see Figure 5-3), and the keys are not\n",
      "inherently ordered.\n",
      "Alternatively, to just find the keys, we can either convert the dictionary to a list \n",
      "  or\n",
      "use \n",
      "the dictionary in a context where a list is expected, as the parameter of sorted()\n",
      " or in a for loop \n",
      " .\n",
      ">>> list(pos) \n",
      "['ideas', 'furiously', 'colorless', 'sleep']\n",
      ">>> sorted(pos) \n",
      "['colorless', 'furiously', 'ideas', 'sleep']\n",
      ">>> [w for w in pos if w.endswith('s')] \n",
      "['colorless', 'ideas']\n",
      "5.3  Mapping Words to Properties Using Python Dictionaries | 191...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 212, 'page_label': '191', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4564}\n",
      "\n",
      "--- Chunk 4565 ---\n",
      "Content:\n",
      "When you type list(pos), you might see a different order to the one\n",
      "shown here. If you want to see the keys in order, just sort them.\n",
      "As well as iterating over all keys in the dictionary with a for loop, we can use the for\n",
      "loop as we did for printing lists:\n",
      ">>> for word in sorted(pos):\n",
      "...     print word + \":\", pos[word]\n",
      "...\n",
      "colorless: ADJ\n",
      "furiously: ADV\n",
      "sleep: V\n",
      "ideas: N\n",
      "Finally, the dictionary methods keys(), values(), and items() allow us to access the\n",
      "keys, values, and key-value pairs as separate lists. We can even sort tuples \n",
      ", which\n",
      "orders \n",
      "them according to their first element (and if the first elements are the same, it\n",
      "uses their second elements).\n",
      ">>> pos.keys()\n",
      "['colorless', 'furiously', 'sleep', 'ideas']\n",
      ">>> pos.values()\n",
      "['ADJ', 'ADV', 'V', 'N']\n",
      ">>> pos.items()\n",
      "[('colorless', 'ADJ'), ('furiously', 'ADV'), ('sleep', 'V'), ('ideas', 'N')]\n",
      ">>> for key, val in sorted(pos.items()): \n",
      "...     print key + \":\", val\n",
      "...\n",
      "colorless: ADJ\n",
      "furiously: ADV\n",
      "ideas: N\n",
      "sleep: V\n",
      "We \n",
      "want to be sure that when we look something up in a dictionary, we get only one\n",
      "value for each key. Now suppose we try to use a dictionary to store the fact that the\n",
      "word sleep can be used as both a verb and a noun:\n",
      ">>> pos['sleep'] = 'V'\n",
      ">>> pos['sleep']\n",
      "'V'\n",
      ">>> pos['sleep'] = 'N'\n",
      ">>> pos['sleep']\n",
      "'N'\n",
      "Initially, pos['sleep'] is given the value 'V'. But this is immediately overwritten with\n",
      "the new value, 'N'. In other words, there can be only one entry in the dictionary for\n",
      "'sleep'. However, there is a way of storing multiple values in that entry: we use a list\n",
      "value, e.g., pos['sleep'] = ['N', 'V']. In fact, this is what we saw in Section 2.4 for\n",
      "the CMU Pronouncing Dictionary, which stores multiple pronunciations for a single\n",
      "word.\n",
      "192 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 213, 'page_label': '192', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4565}\n",
      "\n",
      "--- Chunk 4566 ---\n",
      "Content:\n",
      "Defining Dictionaries\n",
      "We \n",
      "can use the same key-value pair format to create a dictionary. There are a couple\n",
      "of ways to do this, and we will normally use the first:\n",
      ">>> pos = {'colorless': 'ADJ', 'ideas': 'N', 'sleep': 'V', 'furiously': 'ADV'}\n",
      ">>> pos = dict(colorless='ADJ', ideas='N', sleep='V', furiously='ADV')\n",
      "Note that dictionary keys must be immutable types, such as strings and tuples. If we\n",
      "try to define a dictionary using a mutable key, we get a TypeError:\n",
      ">>> pos = {['ideas', 'blogs', 'adventures']: 'N'}\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: list objects are unhashable\n",
      "Default Dictionaries\n",
      "If we try to access a key that is not in a dictionary, we get an error. However, it’s often\n",
      "useful if a dictionary can automatically create an entry for this new key and give it a\n",
      "default value, such as zero or the empty list. Since Python 2.5, a special kind of dic-\n",
      "tionary called a defaultdict has been available. (It is provided as nltk.defaultdict for\n",
      "the benefit of readers who are using Python 2.4.) In order to use it, we have to supply\n",
      "a parameter which can be used to create the default value, e.g., int, float, str, list,\n",
      "dict, tuple.\n",
      ">>> frequency = nltk.defaultdict(int)\n",
      ">>> frequency['colorless'] = 4\n",
      ">>> frequency['ideas']\n",
      "0\n",
      ">>> pos = nltk.defaultdict(list)\n",
      ">>> pos['sleep'] = ['N', 'V']\n",
      ">>> pos['ideas']\n",
      "[]\n",
      "These default values are actually functions that convert other objects to\n",
      "the \n",
      "specified type (e.g., int(\"2\"), list(\"2\")). When they are called with\n",
      "no parameter—say, int(), list()—they return 0 and [] respectively.\n",
      "The preceding examples specified the default value of a dictionary entry to be the default\n",
      "value of a particular data type. However, we can specify any default value we like, simply\n",
      "by providing the name of a function that can be called with no arguments to create the\n",
      "required value. Let’s return to our part-of-speech example, and create a dictionary\n",
      "whose default value for any entry is 'N' \n",
      ". When we access a non-existent entry \n",
      " , it\n",
      "is automatically added to the dictionary \n",
      " .\n",
      ">>> pos = nltk.defaultdict(lambda: 'N') \n",
      ">>> pos['colorless'] = 'ADJ'\n",
      ">>> pos['blog'] \n",
      "'N'\n",
      "5.3  Mapping Words to Properties Using Python Dictionaries | 193...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 214, 'page_label': '193', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4566}\n",
      "\n",
      "--- Chunk 4567 ---\n",
      "Content:\n",
      ">>> pos.items()\n",
      "[('blog', 'N'), ('colorless', 'ADJ')] \n",
      "This example used a lambda expression, introduced in Section 4.4. This\n",
      "lambda expression specifies no parameters, so we call it using paren-\n",
      "theses with no arguments. Thus, the following definitions of f and g are\n",
      "equivalent:\n",
      ">>> f = lambda: 'N'\n",
      ">>> f()\n",
      "'N'\n",
      ">>> def g():\n",
      "...     return 'N'\n",
      ">>> g()\n",
      "'N'\n",
      "Let’s \n",
      "see how default dictionaries could be used in a more substantial language pro-\n",
      "cessing task. Many language processing tasks—including tagging—struggle to cor-\n",
      "rectly process the hapaxes of a text. They can perform better with a fixed vocabulary\n",
      "and a guarantee that no new words will appear. We can preprocess a text to replace\n",
      "low-frequency words with a special “out of vocabulary” token, UNK, with the help of a\n",
      "default dictionary. (Can you work out how to do this without reading on?)\n",
      "We need to create a default dictionary that maps each word to its replacement. The\n",
      "most frequent n words will be mapped to themselves. Everything else will be mapped\n",
      "to UNK.\n",
      ">>> alice = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
      ">>> vocab = nltk.FreqDist(alice)\n",
      ">>> v1000 = list(vocab)[:1000]\n",
      ">>> mapping = nltk.defaultdict(lambda: 'UNK')\n",
      ">>> for v in v1000:\n",
      "...     mapping[v] = v\n",
      "...\n",
      ">>> alice2 = [mapping[v] for v in alice]\n",
      ">>> alice2[:100]\n",
      "['UNK', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'UNK', 'UNK',\n",
      "'UNK', 'UNK', 'CHAPTER', 'I', '.', 'UNK', 'the', 'Rabbit', '-', 'UNK', 'Alice',\n",
      "'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her',\n",
      "'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do',\n",
      "':', 'once', 'or', 'twice', 'she', 'had', 'UNK', 'into', 'the', 'book', 'her',\n",
      "'sister', 'was', 'UNK', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'UNK',\n",
      "'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\",\n",
      "'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', \"?'\", ...]\n",
      ">>> len(set(alice2))\n",
      "1001\n",
      "Incrementally Updating a Dictionary\n",
      "We can employ dictionaries to count occurrences, emulating the method for tallying\n",
      "words shown in Figure 1-3. We begin by initializing an empty defaultdict, then process\n",
      "each part-of-speech tag in the text. If the tag hasn’t been seen before, it will have a zero\n",
      "194 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 215, 'page_label': '194', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4567}\n",
      "\n",
      "--- Chunk 4568 ---\n",
      "Content:\n",
      "count by default. Each time we encounter a tag, we increment its count using the +=\n",
      "operator (see Example 5-3).\n",
      "Example 5-3. Incrementally updating a dictionary, and sorting by value.\n",
      ">>> counts = nltk.defaultdict(int)\n",
      ">>> from nltk.corpus import brown\n",
      ">>> for (word, tag) in brown.tagged_words(categories='news'):\n",
      "...     counts[tag] += 1\n",
      "...\n",
      ">>> counts['N']\n",
      "22226\n",
      ">>> list(counts)\n",
      "['FW', 'DET', 'WH', \"''\", 'VBZ', 'VB+PPO', \"'\", ')', 'ADJ', 'PRO', '*', '-', ...]\n",
      ">>> from operator import itemgetter\n",
      ">>> sorted(counts.items(), key=itemgetter(1), reverse=True)\n",
      "[('N', 22226), ('P', 10845), ('DET', 10648), ('NP', 8336), ('V', 7313), ...]\n",
      ">>> [t for t, c in sorted(counts.items(), key=itemgetter(1), reverse=True)]\n",
      "['N', 'P', 'DET', 'NP', 'V', 'ADJ', ',', '.', 'CNJ', 'PRO', 'ADV', 'VD', ...]\n",
      "The \n",
      "listing in Example 5-3 illustrates an important idiom for sorting a dictionary by its\n",
      "values, to show words in decreasing order of frequency. The first parameter of\n",
      "sorted() is the items to sort, which is a list of tuples consisting of a POS tag and a\n",
      "frequency. The second parameter specifies the sort key using a function itemget\n",
      "ter(). In general, itemgetter(n) returns a function that can be called on some other\n",
      "sequence object to obtain the nth element:\n",
      ">>> pair = ('NP', 8336)\n",
      ">>> pair[1]\n",
      "8336\n",
      ">>> itemgetter(1)(pair)\n",
      "8336\n",
      "The last parameter of sorted() specifies that the items should be returned in reverse\n",
      "order, i.e., decreasing values of frequency.\n",
      "There’s a second useful programming idiom at the beginning of Example 5-3, where\n",
      "we initialize a defaultdict and then use a for loop to update its values. Here’s a sche-\n",
      "matic version:\n",
      ">>> my_dictionary = nltk.defaultdict(function to create default value)\n",
      ">>> for item in sequence:\n",
      "...      my_dictionary[item_key] is updated with information about item\n",
      "Here’s another instance of this pattern, where we index words according to their last\n",
      "two letters:\n",
      ">>> last_letters = nltk.defaultdict(list)\n",
      ">>> words = nltk.corpus.words.words('en')\n",
      ">>> for word in words:\n",
      "...     key = word[-2:]\n",
      "...     last_letters[key].append(word)\n",
      "...\n",
      "5.3  Mapping Words to Properties Using Python Dictionaries | 195...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 216, 'page_label': '195', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4568}\n",
      "\n",
      "--- Chunk 4569 ---\n",
      "Content:\n",
      ">>> last_letters['ly']\n",
      "['abactinally', 'abandonedly', 'abasedly', 'abashedly', 'abashlessly', 'abbreviately',\n",
      "'abdominally', 'abhorrently', 'abidingly', 'abiogenetically', 'abiologically', ...]\n",
      ">>> last_letters['zy']\n",
      "['blazy', 'bleezy', 'blowzy', 'boozy', 'breezy', 'bronzy', 'buzzy', 'Chazy', ...]\n",
      "The \n",
      "following example uses the same pattern to create an anagram dictionary. (You\n",
      "might experiment with the third line to get an idea of why this program works.)\n",
      ">>> anagrams = nltk.defaultdict(list)\n",
      ">>> for word in words:\n",
      "...     key = ''.join(sorted(word))\n",
      "...     anagrams[key].append(word)\n",
      "...\n",
      ">>> anagrams['aeilnrt']\n",
      "['entrail', 'latrine', 'ratline', 'reliant', 'retinal', 'trenail']\n",
      "Since accumulating words like this is such a common task, NLTK provides a more\n",
      "convenient way of creating a defaultdict(list), in the form of nltk.Index():\n",
      ">>> anagrams = nltk.Index((''.join(sorted(w)), w) for w in words)\n",
      ">>> anagrams['aeilnrt']\n",
      "['entrail', 'latrine', 'ratline', 'reliant', 'retinal', 'trenail']\n",
      "nltk.Index is a defaultdict(list) with extra support for initialization.\n",
      "Similarly, nltk.FreqDist \n",
      "is essentially a defaultdict(int) with extra\n",
      "support for initialization (along with sorting and plotting methods).\n",
      "Complex Keys and Values\n",
      "We can use default dictionaries with complex keys and values. Let’s study the range of\n",
      "possible tags for a word, given the word itself and the tag of the previous word. We will\n",
      "see how this information can be used by a POS tagger.\n",
      ">>> pos = nltk.defaultdict(lambda: nltk.defaultdict(int))\n",
      ">>> brown_news_tagged = brown.tagged_words(categories='news', simplify_tags=True)\n",
      ">>> for ((w1, t1), (w2, t2)) in nltk.ibigrams(brown_news_tagged): \n",
      "...     pos[(t1, w2)][t2] += 1 \n",
      "...\n",
      ">>> pos[('DET', 'right')] \n",
      "defaultdict(<type 'int'>, {'ADV': 3, 'ADJ': 9, 'N': 3})\n",
      "This \n",
      "example uses a dictionary whose default value for an entry is a dictionary (whose\n",
      "default value is int(), i.e., zero). Notice how we iterated over the bigrams of the tagged\n",
      "corpus, processing a pair of word-tag pairs for each iteration \n",
      " . Each time through the\n",
      "loop \n",
      "we updated our pos dictionary’s entry for (t1, w2), a tag and its following word\n",
      ". When we look up an item in pos we must specify a compound key \n",
      " , and we get\n",
      "back \n",
      "a dictionary object. A POS tagger could use such information to decide that the\n",
      "word right, when preceded by a determiner, should be tagged as ADJ.\n",
      "196 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 217, 'page_label': '196', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4569}\n",
      "\n",
      "--- Chunk 4570 ---\n",
      "Content:\n",
      "Inverting a Dictionary\n",
      "Dictionaries \n",
      "support efficient lookup, so long as you want to get the value for any key.\n",
      "If d is a dictionary and k is a key, we type d[k] and immediately obtain the value. Finding\n",
      "a key given a value is slower and more cumbersome:\n",
      ">>> counts = nltk.defaultdict(int)\n",
      ">>> for word in nltk.corpus.gutenberg.words('milton-paradise.txt'):\n",
      "...     counts[word] += 1\n",
      "...\n",
      ">>> [key for (key, value) in counts.items() if value == 32]\n",
      "['brought', 'Him', 'virtue', 'Against', 'There', 'thine', 'King', 'mortal',\n",
      "'every', 'been']\n",
      "If we expect to do this kind of “reverse lookup” often, it helps to construct a dictionary\n",
      "that maps values to keys. In the case that no two keys have the same value, this is an\n",
      "easy thing to do. We just get all the key-value pairs in the dictionary, and create a new\n",
      "dictionary of value-key pairs. The next example also illustrates another way of initial-\n",
      "izing a dictionary pos with key-value pairs.\n",
      ">>> pos = {'colorless': 'ADJ', 'ideas': 'N', 'sleep': 'V', 'furiously': 'ADV'}\n",
      ">>> pos2 = dict((value, key) for (key, value) in pos.items())\n",
      ">>> pos2['N']\n",
      "'ideas'\n",
      "Let’s first make our part-of-speech dictionary a bit more realistic and add some more\n",
      "words to pos using the dictionary update() method, to create the situation where mul-\n",
      "tiple keys have the same value. Then the technique just shown for reverse lookup will\n",
      "no longer work (why not?). Instead, we have to use append() to accumulate the words\n",
      "for each part-of-speech, as follows:\n",
      ">>> pos.update({'cats': 'N', 'scratch': 'V', 'peacefully': 'ADV', 'old': 'ADJ'})\n",
      ">>> pos2 = nltk.defaultdict(list)\n",
      ">>> for key, value in pos.items():\n",
      "...     pos2[value].append(key)\n",
      "...\n",
      ">>> pos2['ADV']\n",
      "['peacefully', 'furiously']\n",
      "Now we have inverted the pos dictionary, and can look up any part-of-speech and find\n",
      "all words having that part-of-speech. We can do the same thing even more simply using\n",
      "NLTK’s support for indexing, as follows:\n",
      ">>> pos2 = nltk.Index((value, key) for (key, value) in pos.items())\n",
      ">>> pos2['ADV']\n",
      "['peacefully', 'furiously']\n",
      "A summary of Python’s dictionary methods is given in Table 5-5.\n",
      "5.3  Mapping Words to Properties Using Python Dictionaries | 197...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 218, 'page_label': '197', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4570}\n",
      "\n",
      "--- Chunk 4571 ---\n",
      "Content:\n",
      "Table 5-5. Python’s dictionary methods: A summary of commonly used methods and idioms involving\n",
      "dictionaries\n",
      "Example Description\n",
      "d = {} Create an empty dictionary and assign it to d\n",
      "d[key] = value Assign a value to a given dictionary key\n",
      "d.keys() The list of keys of the dictionary\n",
      "list(d) The list of keys of the dictionary\n",
      "sorted(d) The keys of the dictionary, sorted\n",
      "key in d Test whether a particular key is in the dictionary\n",
      "for key in d Iterate over the keys of the dictionary\n",
      "d.values() The list of values in the dictionary\n",
      "dict([(k1,v1), (k2,v2), ...]) Create a dictionary from a list of key-value pairs\n",
      "d1.update(d2) Add all items from d2 to d1\n",
      "defaultdict(int) A dictionary whose default value is zero\n",
      "5.4  Automatic Tagging\n",
      "In \n",
      "the rest of this chapter we will explore various ways to automatically add part-of-\n",
      "speech tags to text. We will see that the tag of a word depends on the word and its\n",
      "context within a sentence. For this reason, we will be working with data at the level of\n",
      "(tagged) sentences rather than words. We’ll begin by loading the data we will be using.\n",
      ">>> from nltk.corpus import brown\n",
      ">>> brown_tagged_sents = brown.tagged_sents(categories='news')\n",
      ">>> brown_sents = brown.sents(categories='news')\n",
      "The Default Tagger\n",
      "The simplest possible tagger assigns the same tag to each token. This may seem to be\n",
      "a rather banal step, but it establishes an important baseline for tagger performance. In\n",
      "order to get the best result, we tag each word with the most likely tag. Let’s find out\n",
      "which tag is most likely (now using the unsimplified tagset):\n",
      ">>> tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
      ">>> nltk.FreqDist(tags).max()\n",
      "'NN'\n",
      "Now we can create a tagger that tags everything as NN.\n",
      ">>> raw = 'I do not like green eggs and ham, I do not like them Sam I am!'\n",
      ">>> tokens = nltk.word_tokenize(raw)\n",
      ">>> default_tagger = nltk.DefaultTagger('NN')\n",
      ">>> default_tagger.tag(tokens)\n",
      "[('I', 'NN'), ('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('green', 'NN'),\n",
      "('eggs', 'NN'), ('and', 'NN'), ('ham', 'NN'), (',', 'NN'), ('I', 'NN'),\n",
      "198 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 219, 'page_label': '198', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4571}\n",
      "\n",
      "--- Chunk 4572 ---\n",
      "Content:\n",
      "('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('them', 'NN'), ('Sam', 'NN'),\n",
      "('I', 'NN'), ('am', 'NN'), ('!', 'NN')]\n",
      "Unsurprisingly, \n",
      "this method performs rather poorly. On a typical corpus, it will tag\n",
      "only about an eighth of the tokens correctly, as we see here:\n",
      ">>> default_tagger.evaluate(brown_tagged_sents)\n",
      "0.13089484257215028\n",
      "Default taggers assign their tag to every single word, even words that have never been\n",
      "encountered before. As it happens, once we have processed several thousand words of\n",
      "English text, most new words will be nouns. As we will see, this means that default\n",
      "taggers can help to improve the robustness of a language processing system. We will\n",
      "return to them shortly.\n",
      "The Regular Expression Tagger\n",
      "The regular expression tagger assigns tags to tokens on the basis of matching patterns.\n",
      "For instance, we might guess that any word ending in ed is the past participle of a verb,\n",
      "and any word ending with ’s is a possessive noun. We can express these as a list of\n",
      "regular expressions:\n",
      ">>> patterns = [\n",
      "...     (r'.*ing$', 'VBG'),               # gerunds\n",
      "...     (r'.*ed$', 'VBD'),                # simple past\n",
      "...     (r'.*es$', 'VBZ'),                # 3rd singular present\n",
      "...     (r'.*ould$', 'MD'),               # modals\n",
      "...     (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
      "...     (r'.*s$', 'NNS'),                 # plural nouns\n",
      "...     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
      "...     (r'.*', 'NN')                     # nouns (default)\n",
      "... ]\n",
      "Note that these are processed in order, and the first one that matches is applied. Now\n",
      "we can set up a tagger and use it to tag a sentence...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 220, 'page_label': '199', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4572}\n",
      "\n",
      "--- Chunk 4573 ---\n",
      "Content:\n",
      ".     (r'.*s$', 'NNS'),                 # plural nouns\n",
      "...     (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
      "...     (r'.*', 'NN')                     # nouns (default)\n",
      "... ]\n",
      "Note that these are processed in order, and the first one that matches is applied. Now\n",
      "we can set up a tagger and use it to tag a sentence. After this step, it is correct about a\n",
      "fifth of the time.\n",
      ">>> regexp_tagger = nltk.RegexpTagger(patterns)\n",
      ">>> regexp_tagger.tag(brown_sents[3])\n",
      "[('``', 'NN'), ('Only', 'NN'), ('a', 'NN'), ('relative', 'NN'), ('handful', 'NN'),\n",
      "('of', 'NN'), ('such', 'NN'), ('reports', 'NNS'), ('was', 'NNS'), ('received', 'VBD'),\n",
      "(\"''\", 'NN'), (',', 'NN'), ('the', 'NN'), ('jury', 'NN'), ('said', 'NN'), (',', 'NN'),\n",
      "('``', 'NN'), ('considering', 'VBG'), ('the', 'NN'), ('widespread', 'NN'), ...]\n",
      ">>> regexp_tagger.evaluate(brown_tagged_sents)\n",
      "0.20326391789486245\n",
      "The final regular expression « .*» is a catch-all that tags everything as a noun. This is\n",
      "equivalent to the default tagger (only much less efficient). Instead of respecifying this\n",
      "as part of the regular expression tagger, is there a way to combine this tagger with the\n",
      "default tagger? We will see how to do this shortly.\n",
      "5.4  Automatic Tagging | 199...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 220, 'page_label': '199', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4573}\n",
      "\n",
      "--- Chunk 4574 ---\n",
      "Content:\n",
      "Your Turn: See if you can come up with patterns to improve the per-\n",
      "formance of the regular expression tagger just shown. (Note that Sec-\n",
      "tion 6.1 describes a way to partially automate such work.)\n",
      "The Lookup Tagger\n",
      "A lot of high-frequency words do not have the NN tag. Let’s find the hundred most\n",
      "frequent words and store their most likely tag. We can then use this information as the\n",
      "model for a “lookup tagger” (an NLTK UnigramTagger):\n",
      ">>> fd = nltk.FreqDist(brown.words(categories='news'))\n",
      ">>> cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
      ">>> most_freq_words = fd.keys()[:100]\n",
      ">>> likely_tags = dict((word, cfd[word].max()) for word in most_freq_words)\n",
      ">>> baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
      ">>> baseline_tagger.evaluate(brown_tagged_sents)\n",
      "0.45578495136941344\n",
      "It should come as no surprise by now that simply knowing the tags for the 100 most\n",
      "frequent words enables us to tag a large fraction of tokens correctly (nearly half, in fact).\n",
      "Let’s see what it does on some untagged input text:\n",
      ">>> sent = brown.sents(categories='news')[3]\n",
      ">>> baseline_tagger.tag(sent)\n",
      "[('``', '``'), ('Only', None), ('a', 'AT'), ('relative', None),\n",
      "('handful', None), ('of', 'IN'), ('such', None), ('reports', None),\n",
      "('was', 'BEDZ'), ('received', None), (\"''\", \"''\"), (',', ','),\n",
      "('the', 'AT'), ('jury', None), ('said', 'VBD'), (',', ','),\n",
      "('``', '``'), ('considering', None), ('the', 'AT'), ('widespread', None),\n",
      "('interest', None), ('in', 'IN'), ('the', 'AT'), ('election', None),\n",
      "(',', ','), ('the', 'AT'), ('number', None), ('of', 'IN'),\n",
      "('voters', None), ('and', 'CC'), ('the', 'AT'), ('size', None),\n",
      "('of', 'IN'), ('this', 'DT'), ('city', None), (\"''\", \"''\"), ('.', '.')]\n",
      "Many words have been assigned a tag of None, because they were not among the 100\n",
      "most frequent words. In these cases we would like to assign the default tag of NN. In\n",
      "other words, we want to use the lookup table first, and if it is unable to assign a tag,\n",
      "then use the default tagger, a process known as backoff (Section 5.5). We do this by\n",
      "specifying one tagger as a parameter to the other, as shown next. Now the lookup tagger\n",
      "will only store word-tag pairs for words other than nouns, and whenever it cannot\n",
      "assign a tag to a word, it will invoke the default tagger.\n",
      ">>> baseline_tagger = nltk.UnigramTagger(model=likely_tags,\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 221, 'page_label': '200', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4574}\n",
      "\n",
      "--- Chunk 4575 ---\n",
      "Content:\n",
      ". In these cases we would like to assign the default tag of NN. In\n",
      "other words, we want to use the lookup table first, and if it is unable to assign a tag,\n",
      "then use the default tagger, a process known as backoff (Section 5.5). We do this by\n",
      "specifying one tagger as a parameter to the other, as shown next. Now the lookup tagger\n",
      "will only store word-tag pairs for words other than nouns, and whenever it cannot\n",
      "assign a tag to a word, it will invoke the default tagger.\n",
      ">>> baseline_tagger = nltk.UnigramTagger(model=likely_tags,\n",
      "...                                      backoff=nltk.DefaultTagger('NN'))\n",
      "Let’s put all this together and write a program to create and evaluate lookup taggers\n",
      "having a range of sizes (Example 5-4).\n",
      "200 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 221, 'page_label': '200', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4575}\n",
      "\n",
      "--- Chunk 4576 ---\n",
      "Content:\n",
      "Example 5-4. Lookup tagger performance with varying model size.\n",
      "def performance(cfd, wordlist):\n",
      "    lt = dict((word, cfd[word].max()) for word in wordlist)\n",
      "    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger('NN'))\n",
      "    return baseline_tagger.evaluate(brown.tagged_sents(categories='news'))\n",
      "def display():\n",
      "    import pylab\n",
      "    words_by_freq = list(nltk.FreqDist(brown.words(categories='news')))\n",
      "    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
      "    sizes = 2 ** pylab.arange(15)\n",
      "    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]\n",
      "    pylab.plot(sizes, perfs, '-bo')\n",
      "    pylab.title('Lookup Tagger Performance with Varying Model Size')\n",
      "    pylab.xlabel('Model Size')\n",
      "    pylab.ylabel('Performance')\n",
      "    pylab.show()\n",
      ">>> display()                                  \n",
      "Observe \n",
      "in Figure 5-4 that performance initially increases rapidly as the model size\n",
      "grows, eventually reaching a plateau, when large increases in model size yield little\n",
      "improvement in performance. (This example used the pylab plotting package, dis-\n",
      "cussed in Section 4.8.)\n",
      "Evaluation\n",
      "In the previous examples, you will have noticed an emphasis on accuracy scores. In\n",
      "fact, evaluating the performance of such tools is a central theme in NLP. Recall the\n",
      "processing pipeline in Figure 1-5; any errors in the output of one module are greatly\n",
      "multiplied in the downstream modules.\n",
      "We evaluate the performance of a tagger relative to the tags a human expert would\n",
      "assign. Since we usually don’t have access to an expert and impartial human judge, we\n",
      "make do instead with gold standard test data. This is a corpus which has been man-\n",
      "ually annotated and accepted as a standard against which the guesses of an automatic\n",
      "system are assessed. The tagger is regarded as being correct if the tag it guesses for a\n",
      "given word is the same as the gold standard tag.\n",
      "Of course, the humans who designed and carried out the original gold standard anno-\n",
      "tation were only human. Further analysis might show mistakes in the gold standard,\n",
      "or may eventually lead to a revised tagset and more elaborate guidelines. Nevertheless,\n",
      "the gold standard is by definition “correct” as far as the evaluation of an automatic\n",
      "tagger is concerned.\n",
      "5.4  Automatic Tagging | 201...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 222, 'page_label': '201', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4576}\n",
      "\n",
      "--- Chunk 4577 ---\n",
      "Content:\n",
      "Developing an annotated corpus is a major undertaking. Apart from the\n",
      "data, \n",
      "it generates sophisticated tools, documentation, and practices for\n",
      "ensuring high-quality annotation. The tagsets and other coding schemes\n",
      "inevitably depend on some theoretical position that is not shared by all.\n",
      "However, corpus creators often go to great lengths to make their work\n",
      "as theory-neutral as possible in order to maximize the usefulness of their\n",
      "work. We will discuss the challenges of creating a corpus in Chapter 11.\n",
      "5.5  N-Gram Tagging\n",
      "Unigram Tagging\n",
      "Unigram taggers are based on a simple statistical algorithm: for each token, assign the\n",
      "tag that is most likely for that particular token. For example, it will assign the tag JJ to\n",
      "any occurrence of the word frequent, since frequent is used as an adjective (e.g., a fre-\n",
      "quent word) more often than it is used as a verb (e.g., I frequent this cafe). A unigram\n",
      "tagger behaves just like a lookup tagger (Section 5.4), except there is a more convenient\n",
      "Figure 5-4. Lookup tagger\n",
      "202 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 223, 'page_label': '202', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4577}\n",
      "\n",
      "--- Chunk 4578 ---\n",
      "Content:\n",
      "technique for setting it up, called training. In the following code sample, we train a\n",
      "unigram tagger, use it to tag a sentence, and then evaluate:\n",
      ">>> from nltk.corpus import brown\n",
      ">>> brown_tagged_sents = brown.tagged_sents(categories='news')\n",
      ">>> brown_sents = brown.sents(categories='news')\n",
      ">>> unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
      ">>> unigram_tagger.tag(brown_sents[2007])\n",
      "[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'),\n",
      "('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'),\n",
      "(',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'),\n",
      "('floor', 'NN'), ('so', 'QL'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'),\n",
      "('direct', 'JJ'), ('.', '.')]\n",
      ">>> unigram_tagger.evaluate(brown_tagged_sents)\n",
      "0.9349006503968017\n",
      "We train a UnigramTagger by specifying tagged sentence data as a parameter when we\n",
      "initialize the tagger. The training process involves inspecting the tag of each word and\n",
      "storing the most likely tag for any word in a dictionary that is stored inside the tagger.\n",
      "Separating the Training and Testing Data\n",
      "Now that we are training a tagger on some data, we must be careful not to test it on\n",
      "the same data, as we did in the previous example. A tagger that simply memorized its\n",
      "training data and made no attempt to construct a general model would get a perfect\n",
      "score, but would be useless for tagging new text. Instead, we should split the data,\n",
      "training on 90% and testing on the remaining 10%:\n",
      ">>> size = int(len(brown_tagged_sents) * 0.9)\n",
      ">>> size\n",
      "4160\n",
      ">>> train_sents = brown_tagged_sents[:size]\n",
      ">>> test_sents = brown_tagged_sents[size:]\n",
      ">>> unigram_tagger = nltk.UnigramTagger(train_sents)\n",
      ">>> unigram_tagger.evaluate(test_sents)\n",
      "0.81202033290142528\n",
      "Although the score is worse, we now have a better picture of the usefulness of this\n",
      "tagger, i.e., its performance on previously unseen text.\n",
      "General N-Gram Tagging\n",
      "When we perform a language processing task based on unigrams, we are using one\n",
      "item of context. In the case of tagging, we consider only the current token, in isolation\n",
      "from any larger context. Given such a model, the best we can do is tag each word with\n",
      "its a priori most likely tag. This means we would tag a word such as wind with the same\n",
      "tag, regardless of whether it appears in the context the wind or to wind.\n",
      "An n-gram tagger is a generalization of a unigram tagger whose context is the current\n",
      "word together with the part-of-speech tags of the n-1 preceding tokens, as shown in\n",
      "Figure 5-5. The tag to be chosen, tn, is circled, and the context is shaded in grey...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 224, 'page_label': '203', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4578}\n",
      "\n",
      "--- Chunk 4579 ---\n",
      "Content:\n",
      ". In the case of tagging, we consider only the current token, in isolation\n",
      "from any larger context. Given such a model, the best we can do is tag each word with\n",
      "its a priori most likely tag. This means we would tag a word such as wind with the same\n",
      "tag, regardless of whether it appears in the context the wind or to wind.\n",
      "An n-gram tagger is a generalization of a unigram tagger whose context is the current\n",
      "word together with the part-of-speech tags of the n-1 preceding tokens, as shown in\n",
      "Figure 5-5. The tag to be chosen, tn, is circled, and the context is shaded in grey. In the\n",
      "example of an n-gram tagger shown in Figure 5-5, we have n=3; that is, we consider\n",
      "5.5  N-Gram Tagging | 203...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 224, 'page_label': '203', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4579}\n",
      "\n",
      "--- Chunk 4580 ---\n",
      "Content:\n",
      "the tags of the two preceding words in addition to the current word. An n-gram tagger\n",
      "picks the tag that is most likely in the given context.\n",
      "A 1-gram tagger is another term for a unigram tagger: i.e., the context\n",
      "used \n",
      "to tag a token is just the text of the token itself. 2-gram taggers are\n",
      "also called bigram taggers, and 3-gram taggers are called trigram taggers.\n",
      "The NgramTagger class uses a tagged training corpus to determine which part-of-speech\n",
      "tag is most likely for each context. Here we see a special case of an n-gram tagger,\n",
      "namely a bigram tagger. First we train it, then use it to tag untagged sentences:\n",
      ">>> bigram_tagger = nltk.BigramTagger(train_sents)\n",
      ">>> bigram_tagger.tag(brown_sents[2007])\n",
      "[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'),\n",
      "('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'),\n",
      "('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'),\n",
      "('ground', 'NN'), ('floor', 'NN'), ('so', 'CS'), ('that', 'CS'),\n",
      "('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]\n",
      ">>> unseen_sent = brown_sents[4203]\n",
      ">>> bigram_tagger.tag(unseen_sent)\n",
      "[('The', 'AT'), ('population', 'NN'), ('of', 'IN'), ('the', 'AT'), ('Congo', 'NP'),\n",
      "('is', 'BEZ'), ('13.5', None), ('million', None), (',', None), ('divided', None),\n",
      "('into', None), ('at', None), ('least', None), ('seven', None), ('major', None),\n",
      "('``', None), ('culture', None), ('clusters', None), (\"''\", None), ('and', None),\n",
      "('innumerable', None), ('tribes', None), ('speaking', None), ('400', None),\n",
      "('separate', None), ('dialects', None), ('.', None)]\n",
      "Notice that the bigram tagger manages to tag every word in a sentence it saw during\n",
      "training, but does badly on an unseen sentence. As soon as it encounters a new word\n",
      "(i.e., 13.5), it is unable to assign a tag. It cannot tag the following word (i.e., million),\n",
      "even if it was seen during training, simply because it never saw it during training with\n",
      "a None tag on the previous word. Consequently, the tagger fails to tag the rest of the\n",
      "sentence. Its overall accuracy score is very low:\n",
      ">>> bigram_tagger.evaluate(test_sents)\n",
      "0.10276088906608193\n",
      "Figure 5-5. Tagger context.\n",
      "204 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 225, 'page_label': '204', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4580}\n",
      "\n",
      "--- Chunk 4581 ---\n",
      "Content:\n",
      "As n gets larger, the specificity of the contexts increases, as does the chance that the\n",
      "data we wish to tag contains contexts that were not present in the training data. This\n",
      "is known as the sparse data problem, and is quite pervasive in NLP. As a consequence,\n",
      "there is a trade-off between the accuracy and the coverage of our results (and this is\n",
      "related to the precision/recall trade-off in information retrieval).\n",
      "Caution!\n",
      "N-gram \n",
      "taggers should not consider context that crosses a sentence\n",
      "boundary. Accordingly, NLTK taggers are designed to work with lists\n",
      "of sentences, where each sentence is a list of words. At the start of a\n",
      "sentence, tn-1 and preceding tags are set to None.\n",
      "Combining Taggers\n",
      "One way to address the trade-off between accuracy and coverage is to use the more\n",
      "accurate algorithms when we can, but to fall back on algorithms with wider coverage\n",
      "when necessary. For example, we could combine the results of a bigram tagger, a\n",
      "unigram tagger, and a default tagger, as follows:\n",
      "1. Try tagging the token with the bigram tagger.\n",
      "2. If the bigram tagger is unable to find a tag for the token, try the unigram tagger.\n",
      "3. If the unigram tagger is also unable to find a tag, use a default tagger.\n",
      "Most NLTK taggers permit a backoff tagger to be specified. The backoff tagger may\n",
      "itself have a backoff tagger:\n",
      ">>> t0 = nltk.DefaultTagger('NN')\n",
      ">>> t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      ">>> t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      ">>> t2.evaluate(test_sents)\n",
      "0.84491179108940495\n",
      "Your Turn: Extend the preceding example by defining a TrigramTag\n",
      "ger called t3, which backs off to t2.\n",
      "Note that we specify the backoff tagger when the tagger is initialized so that training\n",
      "can take advantage of the backoff tagger. Thus, if the bigram tagger would assign the\n",
      "same tag as its unigram backoff tagger in a certain context, the bigram tagger discards\n",
      "the training instance. This keeps the bigram tagger model as small as possible. We can\n",
      "further specify that a tagger needs to see more than one instance of a context in order\n",
      "to retain it. For example, nltk.BigramTagger(sents, cutoff=2, backoff=t1) will dis-\n",
      "card contexts that have only been seen once or twice.\n",
      "5.5  N-Gram Tagging | 205...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 226, 'page_label': '205', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4581}\n",
      "\n",
      "--- Chunk 4582 ---\n",
      "Content:\n",
      "Tagging Unknown Words\n",
      "Our \n",
      "approach to tagging unknown words still uses backoff to a regular expression\n",
      "tagger or a default tagger. These are unable to make use of context. Thus, if our tagger\n",
      "encountered the word blog, not seen during training, it would assign it the same tag,\n",
      "regardless of whether this word appeared in the context the blog or to blog. How can\n",
      "we do better with these unknown words, or out-of-vocabulary items?\n",
      "A useful method to tag unknown words based on context is to limit the vocabulary of\n",
      "a tagger to the most frequent n words, and to replace every other word with a special\n",
      "word UNK using the method shown in Section 5.3. During training, a unigram tagger\n",
      "will probably learn that UNK is usually a noun. However, the n-gram taggers will detect\n",
      "contexts in which it has some other tag. For example, if the preceding word is to (tagged\n",
      "TO), then UNK will probably be tagged as a verb.\n",
      "Storing Taggers\n",
      "Training a tagger on a large corpus may take a significant time. Instead of training a\n",
      "tagger every time we need one, it is convenient to save a trained tagger in a file for later\n",
      "reuse. Let’s save our tagger t2 to a file t2.pkl:\n",
      ">>> from cPickle import dump\n",
      ">>> output = open('t2.pkl', 'wb')\n",
      ">>> dump(t2, output, -1)\n",
      ">>> output.close()\n",
      "Now, in a separate Python process, we can load our saved tagger:\n",
      ">>> from cPickle import load\n",
      ">>> input = open('t2.pkl', 'rb')\n",
      ">>> tagger = load(input)\n",
      ">>> input.close()\n",
      "Now let’s check that it can be used for tagging:\n",
      ">>> text = \"\"\"The board's action shows what free enterprise\n",
      "...     is up against in our complex maze of regulatory laws .\"\"\"\n",
      ">>> tokens = text.split()\n",
      ">>> tagger.tag(tokens)\n",
      "[('The', 'AT'), (\"board's\", 'NN$'), ('action', 'NN'), ('shows', 'NNS'),\n",
      "('what', 'WDT'), ('free', 'JJ'), ('enterprise', 'NN'), ('is', 'BEZ'),\n",
      "('up', 'RP'), ('against', 'IN'), ('in', 'IN'), ('our', 'PP$'), ('complex', 'JJ'),\n",
      "('maze', 'NN'), ('of', 'IN'), ('regulatory', 'NN'), ('laws', 'NNS'), ('.', '.')]\n",
      "Performance Limitations\n",
      "What is the upper limit to the performance of an n-gram tagger? Consider the case of\n",
      "a trigram tagger. How many cases of part-of-speech ambiguity does it encounter? We\n",
      "can determine the answer to this question empirically:\n",
      "206 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 227, 'page_label': '206', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4582}\n",
      "\n",
      "--- Chunk 4583 ---\n",
      "Content:\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...            ((x[1], y[1], z[0]), z[1])\n",
      "...            for sent in brown_tagged_sents\n",
      "...            for x, y, z in nltk.trigrams(sent))\n",
      ">>> ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
      ">>> sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()\n",
      "0.049297702068029296\n",
      "Thus, \n",
      "1 out of 20 trigrams is ambiguous. Given the current word and the previous two\n",
      "tags, in 5% of cases there is more than one tag that could be legitimately assigned to\n",
      "the current word according to the training data. Assuming we always pick the most\n",
      "likely tag in such ambiguous contexts, we can derive a lower bound on the performance\n",
      "of a trigram tagger.\n",
      "Another way to investigate the performance of a tagger is to study its mistakes. Some\n",
      "tags may be harder than others to assign, and it might be possible to treat them specially\n",
      "by pre- or post-processing the data. A convenient way to look at tagging errors is the\n",
      "confusion matrix. It charts expected tags (the gold standard) against actual tags gen-\n",
      "erated by a tagger:\n",
      ">>> test_tags = [tag for sent in brown.sents(categories='editorial')\n",
      "...                  for (word, tag) in t2.tag(sent)]\n",
      ">>> gold_tags = [tag for (word, tag) in brown.tagged_words(categories='editorial')]\n",
      ">>> print nltk.ConfusionMatrix(gold, test)                \n",
      "Based on such analysis we may decide to modify the tagset. Perhaps a distinction be-\n",
      "tween tags that is difficult to make can be dropped, since it is not important in the\n",
      "context of some larger processing task.\n",
      "Another way to analyze the performance bound on a tagger comes from the less than\n",
      "100% agreement between human annotators.\n",
      "In general, observe that the tagging process collapses distinctions: e.g., lexical identity\n",
      "is usually lost when all personal pronouns are tagged PRP. At the same time, the tagging\n",
      "process introduces new distinctions and removes ambiguities: e.g., deal tagged as VB or\n",
      "NN. This characteristic of collapsing certain distinctions and introducing new distinc-\n",
      "tions is an important feature of tagging which facilitates classification and prediction.\n",
      "When we introduce finer distinctions in a tagset, an n-gram tagger gets more detailed\n",
      "information about the left-context when it is deciding what tag to assign to a particular\n",
      "word. However, the tagger simultaneously has to do more work to classify the current\n",
      "token, simply because there are more tags to choose from. Conversely, with fewer dis-\n",
      "tinctions (as with the simplified tagset), the tagger has less information about context,\n",
      "and it has a smaller range of choices in classifying the current token.\n",
      "We have seen that ambiguity in the training data leads to an upper limit in tagger\n",
      "performance. Sometimes more context will resolve the ambiguity. In other cases, how-\n",
      "ever, as noted by (Abney, 1996), the ambiguity can be resolved only with reference to\n",
      "syntax or to world knowledge...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 228, 'page_label': '207', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4583}\n",
      "\n",
      "--- Chunk 4584 ---\n",
      "Content:\n",
      ". However, the tagger simultaneously has to do more work to classify the current\n",
      "token, simply because there are more tags to choose from. Conversely, with fewer dis-\n",
      "tinctions (as with the simplified tagset), the tagger has less information about context,\n",
      "and it has a smaller range of choices in classifying the current token.\n",
      "We have seen that ambiguity in the training data leads to an upper limit in tagger\n",
      "performance. Sometimes more context will resolve the ambiguity. In other cases, how-\n",
      "ever, as noted by (Abney, 1996), the ambiguity can be resolved only with reference to\n",
      "syntax or to world knowledge. Despite these imperfections, part-of-speech tagging has\n",
      "played a central role in the rise of statistical approaches to natural language processing.\n",
      "In the early 1990s, the surprising accuracy of statistical taggers was a striking\n",
      "5.5  N-Gram Tagging | 207...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 228, 'page_label': '207', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4584}\n",
      "\n",
      "--- Chunk 4585 ---\n",
      "Content:\n",
      "demonstration that it was possible to solve one small part of the language understand-\n",
      "ing problem, namely part-of-speech disambiguation, without reference to deeper sour-\n",
      "ces of linguistic knowledge. Can this idea be pushed further? In Chapter 7, we will see\n",
      "that it can.\n",
      "Tagging Across Sentence Boundaries\n",
      "An n-gram tagger uses recent tags to guide the choice of tag for the current word. When\n",
      "tagging the first word of a sentence, a trigram tagger will be using the part-of-speech\n",
      "tag of the previous two tokens, which will normally be the last word of the previous\n",
      "sentence and the sentence-ending punctuation. However, the lexical category that\n",
      "closed the previous sentence has no bearing on the one that begins the next sentence.\n",
      "To deal with this situation, we can train, run, and evaluate taggers using lists of tagged\n",
      "sentences, as shown in Example 5-5.\n",
      "Example 5-5. N-gram tagging at the sentence level.\n",
      "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
      "brown_sents = brown.sents(categories='news')\n",
      "size = int(len(brown_tagged_sents) * 0.9)\n",
      "train_sents = brown_tagged_sents[:size]\n",
      "test_sents = brown_tagged_sents[size:]\n",
      "t0 = nltk.DefaultTagger('NN')\n",
      "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      ">>> t2.evaluate(test_sents)\n",
      "0.84491179108940495\n",
      "5.6  Transformation-Based Tagging\n",
      "A potential issue with n-gram taggers is the size of their n-gram table (or language\n",
      "model). If tagging is to be employed in a variety of language technologies deployed on\n",
      "mobile computing devices, it is important to strike a balance between model size and\n",
      "tagger performance. An n-gram tagger with backoff may store trigram and bigram ta-\n",
      "bles, which are large, sparse arrays that may have hundreds of millions of entries.\n",
      "A second issue concerns context. The only information an n-gram tagger considers\n",
      "from prior context is tags, even though words themselves might be a useful source of\n",
      "information. It is simply impractical for n-gram models to be conditioned on the iden-\n",
      "tities of words in the context. In this section, we examine Brill tagging, an inductive\n",
      "tagging method which performs very well using models that are only a tiny fraction of\n",
      "the size of n-gram taggers.\n",
      "Brill tagging is a kind of transformation-based learning, named after its inventor. The\n",
      "general idea is very simple: guess the tag of each word, then go back and fix the mistakes.\n",
      "208 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 229, 'page_label': '208', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4585}\n",
      "\n",
      "--- Chunk 4586 ---\n",
      "Content:\n",
      "In this way, a Brill tagger successively transforms a bad tagging of a text into a better\n",
      "one. \n",
      "As with n-gram tagging, this is a supervised learning method, since we need an-\n",
      "notated training data to figure out whether the tagger’s guess is a mistake or not. How-\n",
      "ever, unlike n-gram tagging, it does not count observations but compiles a list of trans-\n",
      "formational correction rules.\n",
      "The process of Brill tagging is usually explained by analogy with painting. Suppose we\n",
      "were painting a tree, with all its details of boughs, branches, twigs, and leaves, against\n",
      "a uniform sky-blue background. Instead of painting the tree first and then trying to\n",
      "paint blue in the gaps, it is simpler to paint the whole canvas blue, then “correct” the\n",
      "tree section by over-painting the blue background. In the same fashion, we might paint\n",
      "the trunk a uniform brown before going back to over-paint further details with even\n",
      "finer brushes. Brill tagging uses the same idea: begin with broad brush strokes, and\n",
      "then fix up the details, with successively finer changes. Let’s look at an example in-\n",
      "volving the following sentence:\n",
      "(1) The President said he will ask Congress to increase grants to states for voca-\n",
      "tional rehabilitation.\n",
      "We will examine the operation of two rules: (a) replace NN with VB when the previous\n",
      "word is TO; (b) replace TO with IN when the next tag is NNS. Table 5-6 illustrates this\n",
      "process, first tagging with the unigram tagger, then applying the rules to fix the errors.\n",
      "Table 5-6. Steps in Brill tagging\n",
      "Phrase to increase grants to states for vocational rehabilitation\n",
      "Unigram TO NN NNS TO NNS IN JJ NN\n",
      "Rule 1  VB       \n",
      "Rule 2    IN     \n",
      "Output TO VB NNS IN NNS IN JJ NN\n",
      "Gold TO VB NNS IN NNS IN JJ NN\n",
      "In this table, we see two rules. All such rules are generated from a template of the\n",
      "following \n",
      "form: “replace T1 with T2 in the context C.” Typical contexts are the identity\n",
      "or the tag of the preceding or following word, or the appearance of a specific tag within\n",
      "two to three words of the current word. During its training phase, the tagger guesses\n",
      "values for T1, T2, and C, to create thousands of candidate rules. Each rule is scored\n",
      "according to its net benefit: the number of incorrect tags that it corrects, less the number\n",
      "of correct tags it incorrectly modifies.\n",
      "Brill taggers have another interesting property: the rules are linguistically interpretable.\n",
      "Compare this with the n-gram taggers, which employ a potentially massive table of n-\n",
      "grams. We cannot learn much from direct inspection of such a table, in comparison to\n",
      "the rules learned by the Brill tagger. Example 5-6 demonstrates NLTK’s Brill tagger.\n",
      "5.6  Transformation-Based Tagging | 209...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 230, 'page_label': '209', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4586}\n",
      "\n",
      "--- Chunk 4587 ---\n",
      "Content:\n",
      "Example 5-6...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 231, 'page_label': '210', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4587}\n",
      "\n",
      "--- Chunk 4588 ---\n",
      "Content:\n",
      ". Brill tagger demonstration: The tagger has a collection of templates of the form X → Y\n",
      "if the preceding word is Z; the variables in these templates are instantiated to particular words and\n",
      "tags to create “rules”; the score for a rule is the number of broken examples it corrects minus the\n",
      "number of correct cases it breaks; apart from training a tagger, the demonstration displays residual\n",
      "errors.\n",
      ">>> nltk.tag.brill.demo()\n",
      "Training Brill tagger on 80 sentences...\n",
      "Finding initial useful rules...\n",
      "    Found 6555 useful rules.\n",
      "           B      |\n",
      "   S   F   r   O  |        Score = Fixed - Broken\n",
      "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
      "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
      "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
      "   e   d   n   r  |  e\n",
      "------------------+-------------------------------------------------------\n",
      "  12  13   1   4  | NN -> VB if the tag of the preceding word is 'TO'\n",
      "   8   9   1  23  | NN -> VBD if the tag of the following word is 'DT'\n",
      "   8   8   0   9  | NN -> VBD if the tag of the preceding word is 'NNS'\n",
      "   6   9   3  16  | NN -> NNP if the tag of words i-2...i-1 is '-NONE-'\n",
      "   5   8   3   6  | NN -> NNP if the tag of the following word is 'NNP'\n",
      "   5   6   1   0  | NN -> NNP if the text of words i-2...i-1 is 'like'\n",
      "   5   5   0   3  | NN -> VBN if the text of the following word is '*-1'\n",
      "   ...\n",
      ">>> print(open(\"errors.out\").read())\n",
      "             left context |    word/test->gold     | right context\n",
      "--------------------------+------------------------+--------------------------\n",
      "                          |      Then/NN->RB       | ,/, in/IN the/DT guests/N\n",
      ", in/IN the/DT guests/NNS |       '/VBD->POS       | honor/NN ,/, the/DT speed\n",
      "'/POS honor/NN ,/, the/DT |    speedway/JJ->NN     | hauled/VBD out/RP four/CD\n",
      "NN ,/, the/DT speedway/NN |     hauled/NN->VBD     | out/RP four/CD drivers/NN\n",
      "DT speedway/NN hauled/VBD |      out/NNP->RP       | four/CD drivers/NNS ,/, c...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 231, 'page_label': '210', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4588}\n",
      "\n",
      "--- Chunk 4589 ---\n",
      "Content:\n",
      "'/POS honor/NN ,/, the/DT |    speedway/JJ->NN     | hauled/VBD out/RP four/CD\n",
      "NN ,/, the/DT speedway/NN |     hauled/NN->VBD     | out/RP four/CD drivers/NN\n",
      "DT speedway/NN hauled/VBD |      out/NNP->RP       | four/CD drivers/NNS ,/, c\n",
      "dway/NN hauled/VBD out/RP |      four/NNP->CD      | drivers/NNS ,/, crews/NNS\n",
      "hauled/VBD out/RP four/CD |    drivers/NNP->NNS    | ,/, crews/NNS and/CC even\n",
      "P four/CD drivers/NNS ,/, |     crews/NN->NNS      | and/CC even/RB the/DT off\n",
      "NNS and/CC even/RB the/DT |    official/NNP->JJ    | Indianapolis/NNP 500/CD a\n",
      "                          |     After/VBD->IN      | the/DT race/NN ,/, Fortun\n",
      "ter/IN the/DT race/NN ,/, |    Fortune/IN->NNP     | 500/CD executives/NNS dro\n",
      "s/NNS drooled/VBD like/IN |  schoolboys/NNP->NNS   | over/IN the/DT cars/NNS a\n",
      "olboys/NNS over/IN the/DT |      cars/NN->NNS      | and/CC drivers/NNS ./.\n",
      "5.7  How to Determine the Category of a Word\n",
      "Now that we have examined word classes in detail, we turn to a more basic question:\n",
      "how do we decide what category a word belongs to in the first place...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 231, 'page_label': '210', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4589}\n",
      "\n",
      "--- Chunk 4590 ---\n",
      "Content:\n",
      "? In general, linguists\n",
      "use morphological, syntactic, and semantic clues to determine the category of a word.\n",
      "210 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 231, 'page_label': '210', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4590}\n",
      "\n",
      "--- Chunk 4591 ---\n",
      "Content:\n",
      "Morphological Clues\n",
      "The \n",
      "internal structure of a word may give useful clues as to the word’s category. For\n",
      "example, -ness is a suffix that combines with an adjective to produce a noun, e.g., happy\n",
      "→ happiness, ill → illness. So if we encounter a word that ends in -ness, this is very likely\n",
      "to be a noun. Similarly, -ment is a suffix that combines with some verbs to produce a\n",
      "noun, e.g., govern → government and establish → establishment.\n",
      "English verbs can also be morphologically complex. For instance, the present par-\n",
      "ticiple of a verb ends in -ing, and expresses the idea of ongoing, incomplete action (e.g.,\n",
      "falling, eating). The -ing suffix also appears on nouns derived from verbs, e.g., the falling\n",
      "of the leaves (this is known as the gerund).\n",
      "Syntactic Clues\n",
      "Another source of information is the typical contexts in which a word can occur. For\n",
      "example, assume that we have already determined the category of nouns. Then we\n",
      "might say that a syntactic criterion for an adjective in English is that it can occur im-\n",
      "mediately before a noun, or immediately following the words be or very. According to\n",
      "these tests, near should be categorized as an adjective:\n",
      "(2) a. the near window\n",
      "b. The end is (very) near.\n",
      "Semantic Clues\n",
      "Finally, the meaning of a word is a useful clue as to its lexical category. For example,\n",
      "the best-known definition of a noun is semantic: “the name of a person, place, or thing.”\n",
      "Within modern linguistics, semantic criteria for word classes are treated with suspicion,\n",
      "mainly because they are hard to formalize. Nevertheless, semantic criteria underpin\n",
      "many of our intuitions about word classes, and enable us to make a good guess about\n",
      "the categorization of words in languages with which we are unfamiliar. For example,\n",
      "if all we know about the Dutch word verjaardag is that it means the same as the English\n",
      "word birthday, then we can guess that verjaardag is a noun in Dutch. However, some\n",
      "care is needed: although we might translate zij is vandaag jarig as it’s her birthday to-\n",
      "day, the word jarig is in fact an adjective in Dutch, and has no exact equivalent in\n",
      "English.\n",
      "New Words\n",
      "All languages acquire new lexical items. A list of words recently added to the Oxford\n",
      "Dictionary of English includes cyberslacker, fatoush, blamestorm, SARS, cantopop,\n",
      "bupkis, noughties, muggle, and robata. Notice that all these new words are nouns, and\n",
      "this is reflected in calling nouns an open class. By contrast, prepositions are regarded\n",
      "as a closed class. That is, there is a limited set of words belonging to the class (e.g.,\n",
      "above, along, at, below, beside, between, during, for, from, in, near, on, outside, over,\n",
      "5.7  How to Determine the Category of a Word | 211...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 232, 'page_label': '211', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4591}\n",
      "\n",
      "--- Chunk 4592 ---\n",
      "Content:\n",
      "past, through, towards, under, up, with), and membership of the set only changes very\n",
      "gradually over time.\n",
      "Morphology in Part-of-Speech Tagsets\n",
      "Common \n",
      "tagsets often capture some morphosyntactic information, that is, informa-\n",
      "tion about the kind of morphological markings that words receive by virtue of their\n",
      "syntactic role. Consider, for example, the selection of distinct grammatical forms of the\n",
      "word go illustrated in the following sentences:\n",
      "(3) a. Go away!\n",
      "b. He sometimes goes to the cafe.\n",
      "c. All the cakes have gone.\n",
      "d. We went on the excursion.\n",
      "Each of these forms—go, goes, gone, and went—is morphologically distinct from the\n",
      "others. Consider the form goes. This occurs in a restricted set of grammatical contexts,\n",
      "and requires a third person singular subject. Thus, the following sentences are\n",
      "ungrammatical.\n",
      "(4) a. *They sometimes goes to the cafe.\n",
      "b. *I sometimes goes to the cafe.\n",
      "By contrast, gone is the past participle form; it is required after have (and cannot be\n",
      "replaced in this context by goes), and cannot occur as the main verb of a clause.\n",
      "(5) a. *All the cakes have goes.\n",
      "b. *He sometimes gone to the cafe.\n",
      "We can easily imagine a tagset in which the four distinct grammatical forms just dis-\n",
      "cussed were all tagged as VB. Although this would be adequate for some purposes, a\n",
      "more fine-grained tagset provides useful information about these forms that can help\n",
      "other processors that try to detect patterns in tag sequences. The Brown tagset captures\n",
      "these distinctions, as summarized in Table 5-7.\n",
      "Table 5-7. Some morphosyntactic distinctions in the Brown tagset\n",
      "Form Category Tag\n",
      "go base VB\n",
      "goes third singular present VBZ\n",
      "gone past participle VBN\n",
      "going gerund VBG\n",
      "went\n",
      "simple past VBD\n",
      "212 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 233, 'page_label': '212', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4592}\n",
      "\n",
      "--- Chunk 4593 ---\n",
      "Content:\n",
      "In addition to this set of verb tags, the various forms of the verb to be have special tags:\n",
      "be/BE, being/BEG, am/BEM, are/BER, is/BEZ, been/BEN, were/BED, and was/BEDZ (plus extra\n",
      "tags for negative forms of the verb). All told, this fine-grained tagging of verbs means\n",
      "that an automatic tagger that uses this tagset is effectively carrying out a limited amount\n",
      "of morphological analysis.\n",
      "Most part-of-speech tagsets make use of the same basic categories, such as noun, verb,\n",
      "adjective, and preposition. However, tagsets differ both in how finely they divide words\n",
      "into categories, and in how they define their categories. For example, is might be tagged\n",
      "simply as a verb in one tagset, but as a distinct form of the lexeme be in another tagset\n",
      "(as in the Brown Corpus). This variation in tagsets is unavoidable, since part-of-speech\n",
      "tags are used in different ways for different tasks. In other words, there is no one “right\n",
      "way” to assign tags, only more or less useful ways depending on one’s goals.\n",
      "5.8  Summary\n",
      "• Words can be grouped into classes, such as nouns, verbs, adjectives, and adverbs.\n",
      "These classes are known as lexical categories or parts-of-speech. Parts-of-speech\n",
      "are assigned short labels, or tags, such as NN and VB.\n",
      "• The process of automatically assigning parts-of-speech to words in text is called\n",
      "part-of-speech tagging, POS tagging, or just tagging.\n",
      "• Automatic tagging is an important step in the NLP pipeline, and is useful in a variety\n",
      "of situations, including predicting the behavior of previously unseen words, ana-\n",
      "lyzing word usage in corpora, and text-to-speech systems.\n",
      "• Some linguistic corpora, such as the Brown Corpus, have been POS tagged.\n",
      "• A variety of tagging methods are possible, e.g., default tagger, regular expression\n",
      "tagger, unigram tagger, and n-gram taggers. These can be combined using a tech-\n",
      "nique known as backoff.\n",
      "• Taggers can be trained and evaluated using tagged corpora.\n",
      "• Backoff is a method for combining models: when a more specialized model (such\n",
      "as a bigram tagger) cannot assign a tag in a given context, we back off to a more\n",
      "general model (such as a unigram tagger).\n",
      "• Part-of-speech tagging is an important, early example of a sequence classification\n",
      "task in NLP: a classification decision at any one point in the sequence makes use\n",
      "of words and tags in the local context.\n",
      "• A dictionary is used to map between arbitrary types of information, such as a string\n",
      "and a number: freq['cat'] = 12. We create dictionaries using the brace notation:\n",
      "pos = {}, pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}.\n",
      "• N-gram taggers can be defined for large values of n, but once n is larger than 3, we\n",
      "usually encounter the sparse data problem; even with a large quantity of training\n",
      "data, we see only a tiny fraction of possible contexts.\n",
      "5.8  Summary | 213...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 234, 'page_label': '213', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4593}\n",
      "\n",
      "--- Chunk 4594 ---\n",
      "Content:\n",
      "• Transformation-based tagging involves learning a series of repair rules of the form\n",
      "“change tag s to tag t in context c,” where each rule fixes mistakes and possibly\n",
      "introduces a (smaller) number of errors.\n",
      "5.9  Further Reading\n",
      "Extra materials for this chapter are posted at http://www.nltk.org/, including links to\n",
      "freely available resources on the Web. For more examples of tagging with NLTK, please\n",
      "see the Tagging HOWTO at http://www.nltk.org/howto. Chapters 4 and 5 of (Jurafsky\n",
      "& Martin, 2008) contain more advanced material on n-grams and part-of-speech tag-\n",
      "ging. Other approaches to tagging involve machine learning methods ( Chapter 6). In\n",
      "Chapter 7, we will see a generalization of tagging called chunking in which a contiguous\n",
      "sequence of words is assigned a single tag.\n",
      "For tagset documentation, see nltk.help.upenn_tagset() and nltk.help.brown_tag\n",
      "set(). Lexical categories are introduced in linguistics textbooks, including those listed\n",
      "in Chapter 1 of this book.\n",
      "There are many other kinds of tagging. Words can be tagged with directives to a speech\n",
      "synthesizer, indicating which words should be emphasized. Words can be tagged with\n",
      "sense numbers, indicating which sense of the word was used. Words can also be tagged\n",
      "with morphological features. Examples of each of these kinds of tags are shown in the\n",
      "following list. For space reasons, we only show the tag for a single word. Note also that\n",
      "the first two examples use XML-style tags, where elements in angle brackets enclose\n",
      "the word that is tagged.\n",
      "Speech Synthesis Markup Language (W3C SSML)\n",
      "That is a <emphasis>big</emphasis> car!\n",
      "SemCor: Brown Corpus tagged with WordNet senses\n",
      "Space in any <wf pos=\"NN\" lemma=\"form\" wnsn=\"4\">form</wf> is completely meas\n",
      "ured by the three dimensions. (Wordnet form/nn sense 4: “shape, form, config-\n",
      "uration, contour, conformation”)\n",
      "Morphological tagging, from the Turin University Italian Treebank\n",
      "E' italiano , come progetto e realizzazione , il primo (PRIMO ADJ ORDIN M\n",
      "SING) porto turistico dell' Albania .\n",
      "Note that tagging is also performed at higher levels. Here is an example of dialogue act\n",
      "tagging, from the NPS Chat Corpus (Forsyth & Martell, 2007) included with NLTK.\n",
      "Each turn of the dialogue is categorized as to its communicative function:\n",
      "Statement  User117 Dude..., I wanted some of that\n",
      "ynQuestion User120 m I missing something?\n",
      "Bye        User117 I'm gonna go fix food, I'll be back later.\n",
      "System     User122 JOIN\n",
      "System     User2   slaps User122 around a bit with a large trout.\n",
      "Statement  User121 18/m pm me if u tryin to chat\n",
      "214 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 235, 'page_label': '214', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4594}\n",
      "\n",
      "--- Chunk 4595 ---\n",
      "Content:\n",
      "5.10  Exercises\n",
      "1. ○ Search \n",
      "the Web for “spoof newspaper headlines,” to find such gems as: British\n",
      "Left Waffles on Falkland Islands , and Juvenile Court to Try Shooting Defendant .\n",
      "Manually tag these headlines to see whether knowledge of the part-of-speech tags\n",
      "removes the ambiguity.\n",
      "2. ○ Working with someone else, take turns picking a word that can be either a noun\n",
      "or a verb (e.g., contest); the opponent has to predict which one is likely to be the\n",
      "most frequent in the Brown Corpus. Check the opponent’s prediction, and tally\n",
      "the score over several turns.\n",
      "3. ○ Tokenize and tag the following sentence: They wind back the clock, while we\n",
      "chase after the wind . What different pronunciations and parts-of-speech are\n",
      "involved?\n",
      "4. ○ Review the mappings in Table 5-4. Discuss any other examples of mappings you\n",
      "can think of. What type of information do they map from and to?\n",
      "5. ○ Using the Python interpreter in interactive mode, experiment with the dictionary\n",
      "examples in this chapter. Create a dictionary d, and add some entries. What hap-\n",
      "pens whether you try to access a non-existent entry, e.g., d['xyz']?\n",
      "6. ○ Try deleting an element from a dictionary d, using the syntax del d['abc']. Check\n",
      "that the item was deleted.\n",
      "7. ○ Create two dictionaries, d1 and d2, and add some entries to each. Now issue the\n",
      "command d1.update(d2). What did this do? What might it be useful for?\n",
      "8. ○ Create a dictionary e, to represent a single lexical entry for some word of your\n",
      "choice. Define keys such as headword, part-of-speech, sense, and example, and as-\n",
      "sign them suitable values.\n",
      "9. ○ Satisfy yourself that there are restrictions on the distribution of go and went, in\n",
      "the sense that they cannot be freely interchanged in the kinds of contexts illustrated\n",
      "in (3), Section 5.7.\n",
      "10. ○ Train a unigram tagger and run it on some new text. Observe that some words\n",
      "are not assigned a tag. Why not?\n",
      "11. ○ Learn about the affix tagger (type help(nltk.AffixTagger)). Train an affix tagger\n",
      "and run it on some new text. Experiment with different settings for the affix length\n",
      "and the minimum word length. Discuss your findings.\n",
      "12. ○ Train a bigram tagger with no backoff tagger, and run it on some of the training\n",
      "data. Next, run it on some new data. What happens to the performance of the\n",
      "tagger? Why?\n",
      "13. ○ We can use a dictionary to specify the values to be substituted into a formatting\n",
      "string. Read Python’s library documentation for formatting strings (http://docs.py\n",
      "5.10  Exercises | 215...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 236, 'page_label': '215', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4595}\n",
      "\n",
      "--- Chunk 4596 ---\n",
      "Content:\n",
      "thon.org/lib/typesseq-strings.html) and use this method to display today’s date in\n",
      "two different formats.\n",
      "14. ◑ Use sorted() and set() to get a sorted list of tags used in the Brown Corpus,\n",
      "removing duplicates.\n",
      "15. ◑ Write programs to process the Brown Corpus and find answers to the following\n",
      "questions:\n",
      "a. Which nouns are more common in their plural form, rather than their singular\n",
      "form? (Only consider regular plurals, formed with the -s suffix.)\n",
      "b. Which word has the greatest number of distinct tags? What are they, and what\n",
      "do they represent?\n",
      "c. List tags in order of decreasing frequency. What do the 20 most frequent tags\n",
      "represent?\n",
      "d. Which tags are nouns most commonly found after? What do these tags\n",
      "represent?\n",
      "16. ◑ Explore the following issues that arise in connection with the lookup tagger:\n",
      "a. What happens to the tagger performance for the various model sizes when a\n",
      "backoff tagger is omitted?\n",
      "b. Consider the curve in Figure 5-4; suggest a good size for a lookup tagger that\n",
      "balances memory and performance. Can you come up with scenarios where it\n",
      "would be preferable to minimize memory usage, or to maximize performance\n",
      "with no regard for memory usage?\n",
      "17. ◑ What is the upper limit of performance for a lookup tagger, assuming no limit\n",
      "to the size of its table? (Hint: write a program to work out what percentage of tokens\n",
      "of a word are assigned the most likely tag for that word, on average.)\n",
      "18. ◑ Generate some statistics for tagged data to answer the following questions:\n",
      "a. What proportion of word types are always assigned the same part-of-speech\n",
      "tag?\n",
      "b. How many words are ambiguous, in the sense that they appear with at least\n",
      "two tags?\n",
      "c. What percentage of word tokens in the Brown Corpus involve these ambiguous\n",
      "words?\n",
      "19. ◑ The evaluate() method works out how accurately the tagger performs on this\n",
      "text. For example, if the supplied tagged text was [('the', 'DT'), ('dog',\n",
      "'NN')] and the tagger produced the output [('the', 'NN'), ('dog', 'NN')], then\n",
      "the score would be 0.5. Let’s try to figure out how the evaluation method works:\n",
      "a. A tagger t takes a list of words as input, and produces a list of tagged words\n",
      "as output. However, t.evaluate() is given correctly tagged text as its only\n",
      "parameter. What must it do with this input before performing the tagging?\n",
      "216 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 237, 'page_label': '216', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4596}\n",
      "\n",
      "--- Chunk 4597 ---\n",
      "Content:\n",
      "b. Once the tagger has created newly tagged text, how might the evaluate()\n",
      "method go about comparing it with the original tagged text and computing\n",
      "the accuracy score?\n",
      "c. Now examine the source code to see how the method is implemented. Inspect\n",
      "nltk.tag.api.__file__ to discover the location of the source code, and open\n",
      "this file using an editor (be sure to use the api.py file and not the compiled\n",
      "api.pyc binary file).\n",
      "20. ◑ Write code to search the Brown Corpus for particular words and phrases ac-\n",
      "cording to tags, to answer the following questions:\n",
      "a. Produce an alphabetically sorted list of the distinct words tagged as MD.\n",
      "b. Identify words that can be plural nouns or third person singular verbs (e.g.,\n",
      "deals, flies).\n",
      "c. Identify three-word prepositional phrases of the form IN + DET + NN (e.g.,\n",
      "in the lab).\n",
      "d. What is the ratio of masculine to feminine pronouns?\n",
      "21. ◑ In Table 3-1, we saw a table involving frequency counts for the verbs adore, love,\n",
      "like, and prefer, and preceding qualifiers such as really. Investigate the full range\n",
      "of qualifiers (Brown tag QL) that appear before these four verbs.\n",
      "22. ◑ We defined the regexp_tagger that can be used as a fall-back tagger for unknown\n",
      "words. This tagger only checks for cardinal numbers. By testing for particular prefix\n",
      "or suffix strings, it should be possible to guess other tags. For example, we could\n",
      "tag any word that ends with -s as a plural noun. Define a regular expression tagger\n",
      "(using RegexpTagger()) that tests for at least five other patterns in the spelling of\n",
      "words. (Use inline documentation to explain the rules.)\n",
      "23. ◑ Consider the regular expression tagger developed in the exercises in the previous\n",
      "section. Evaluate the tagger using its accuracy() method, and try to come up with\n",
      "ways to improve its performance. Discuss your findings. How does objective eval-\n",
      "uation help in the development process?\n",
      "24. ◑ How serious is the sparse data problem? Investigate the performance of n-gram\n",
      "taggers as n increases from 1 to 6. Tabulate the accuracy score. Estimate the training\n",
      "data required for these taggers, assuming a vocabulary size of 105 and a tagset size\n",
      "of 102.\n",
      "25. ◑ Obtain some tagged data for another language, and train and evaluate a variety\n",
      "of taggers on it. If the language is morphologically complex, or if there are any\n",
      "orthographic clues (e.g., capitalization) to word classes, consider developing a reg-\n",
      "ular expression tagger for it (ordered after the unigram tagger, and before the de-\n",
      "fault tagger). How does the accuracy of your tagger(s) compare with the same\n",
      "taggers run on English data? Discuss any issues you encounter in applying these\n",
      "methods to the language.\n",
      "5.10  Exercises | 217...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 238, 'page_label': '217', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4597}\n",
      "\n",
      "--- Chunk 4598 ---\n",
      "Content:\n",
      "26. ◑ Example 5-4 plotted a curve showing change in the performance of a lookup\n",
      "tagger as the model size was increased. Plot the performance curve for a unigram\n",
      "tagger, as the amount of training data is varied.\n",
      "27. ◑ Inspect the confusion matrix for the bigram tagger t2 defined in Section 5.5, and\n",
      "identify one or more sets of tags to collapse. Define a dictionary to do the mapping,\n",
      "and evaluate the tagger on the simplified data.\n",
      "28. ◑ Experiment with taggers using the simplified tagset (or make one of your own\n",
      "by discarding all but the first character of each tag name). Such a tagger has fewer\n",
      "distinctions to make, but much less information on which to base its work. Discuss\n",
      "your findings.\n",
      "29. ◑ Recall the example of a bigram tagger which encountered a word it hadn’t seen\n",
      "during training, and tagged the rest of the sentence as None. It is possible for a\n",
      "bigram tagger to fail partway through a sentence even if it contains no unseen words\n",
      "(even if the sentence was used during training). In what circumstance can this\n",
      "happen? Can you write a program to find some examples of this?\n",
      "30. ◑ Preprocess the Brown News data by replacing low-frequency words with UNK,\n",
      "but leaving the tags untouched. Now train and evaluate a bigram tagger on this\n",
      "data. How much does this help? What is the contribution of the unigram tagger\n",
      "and default tagger now?\n",
      "31. ◑ Modify the program in Example 5-4 to use a logarithmic scale on the x-axis, by\n",
      "replacing pylab.plot() with pylab.semilogx(). What do you notice about the\n",
      "shape of the resulting plot? Does the gradient tell you anything?\n",
      "32. ◑ Consult the documentation for the Brill tagger demo function, using\n",
      "help(nltk.tag.brill.demo). Experiment with the tagger by setting different values\n",
      "for the parameters. Is there any trade-off between training time (corpus size) and\n",
      "performance?\n",
      "33. ◑ Write code that builds a dictionary of dictionaries of sets. Use it to store the set\n",
      "of POS tags that can follow a given word having a given POS tag, i.e., wordi → tagi →\n",
      "tagi+1.\n",
      "34. ● There are 264 distinct words in the Brown Corpus having exactly three possible\n",
      "tags.\n",
      "a. Print a table with the integers 1..10 in one column, and the number of distinct\n",
      "words in the corpus having 1..10 distinct tags in the other column.\n",
      "b. For the word with the greatest number of distinct tags, print out sentences\n",
      "from the corpus containing the word, one for each possible tag.\n",
      "35. ● Write a program to classify contexts involving the word must according to the\n",
      "tag of the following word. Can this be used to discriminate between the epistemic\n",
      "and deontic uses of must?\n",
      "36. ● Create a regular expression tagger and various unigram and n-gram taggers,\n",
      "incorporating backoff, and train them on part of the Brown Corpus.\n",
      "218 | Chapter 5:  Categorizing and Tagging Words...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 239, 'page_label': '218', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4598}\n",
      "\n",
      "--- Chunk 4599 ---\n",
      "Content:\n",
      "a. Create three different combinations of the taggers. Test the accuracy of each\n",
      "combined tagger. Which combination works best?\n",
      "b. Try varying the size of the training corpus. How does it affect your results?\n",
      "37. ● Our approach for tagging an unknown word has been to consider the letters of\n",
      "the word (using RegexpTagger()), or to ignore the word altogether and tag it as a\n",
      "noun (using nltk.DefaultTagger()). These methods will not do well for texts hav-\n",
      "ing new words that are not nouns. Consider the sentence I like to blog on Kim’s\n",
      "blog. If blog is a new word, then looking at the previous tag (TO versus NP$) would\n",
      "probably be helpful, i.e., we need a default tagger that is sensitive to the preceding\n",
      "tag.\n",
      "a. Create a new kind of unigram tagger that looks at the tag of the previous word,\n",
      "and ignores the current word. (The best way to do this is to modify the source\n",
      "code for UnigramTagger(), which presumes knowledge of object-oriented pro-\n",
      "gramming in Python.)\n",
      "b. Add this tagger to the sequence of backoff taggers (including ordinary trigram\n",
      "and bigram taggers that look at words), right before the usual default tagger.\n",
      "c. Evaluate the contribution of this new unigram tagger.\n",
      "38. ● Consider the code in Section 5.5, which determines the upper bound for accuracy\n",
      "of a trigram tagger. Review Abney’s discussion concerning the impossibility of\n",
      "exact tagging (Abney, 2006). Explain why correct tagging of these examples re-\n",
      "quires access to other kinds of information than just words and tags. How might\n",
      "you estimate the scale of this problem?\n",
      "39. ● Use some of the estimation techniques in nltk.probability, such as Lidstone or\n",
      "Laplace estimation, to develop a statistical tagger that does a better job than n-\n",
      "gram backoff taggers in cases where contexts encountered during testing were not\n",
      "seen during training.\n",
      "40. ● Inspect the diagnostic files created by the Brill tagger rules.out and\n",
      "errors.out. Obtain the demonstration code by accessing the source code (at http:\n",
      "//www.nltk.org/code) and create your own version of the Brill tagger. Delete some\n",
      "of the rule templates, based on what you learned from inspecting rules.out. Add\n",
      "some new rule templates which employ contexts that might help to correct the\n",
      "errors you saw in errors.out.\n",
      "41. ● Develop an n-gram backoff tagger that permits “anti-n-grams” such as [\"the\",\n",
      "\"the\"] to be specified when a tagger is initialized. An anti-n-gram is assigned a\n",
      "count of zero and is used to prevent backoff for this n-gram (e.g., to avoid esti-\n",
      "mating P(the | the) as just P(the)).\n",
      "42. ● Investigate three different ways to define the split between training and testing\n",
      "data when developing a tagger using the Brown Corpus: genre (category), source\n",
      "(fileid), and sentence. Compare their relative performance and discuss which\n",
      "method is the most legitimate. (You might use n-fold cross validation, discussed\n",
      "in Section 6.3, to improve the accuracy of the evaluations.)\n",
      "5.10  Exercises | 219...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 240, 'page_label': '219', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4599}\n",
      "\n",
      "--- Chunk 4600 ---\n",
      "Content:\n",
      "CHAPTER 6\n",
      "Learning to Classify Text\n",
      "Detecting patterns is a central part of Natural Language Processing. Words ending in\n",
      "-ed \n",
      "tend to be past tense verbs ( Chapter 5). Frequent use of will is indicative of news\n",
      "text (Chapter 3). These observable patterns—word structure and word frequency—\n",
      "happen to correlate with particular aspects of meaning, such as tense and topic. But\n",
      "how did we know where to start looking, which aspects of form to associate with which\n",
      "aspects of meaning?\n",
      "The goal of this chapter is to answer the following questions:\n",
      "1. How can we identify particular features of language data that are salient for clas-\n",
      "sifying it?\n",
      "2. How can we construct models of language that can be used to perform language\n",
      "processing tasks automatically?\n",
      "3. What can we learn about language from these models?\n",
      "Along the way we will study some important machine learning techniques, including\n",
      "decision trees, naive Bayes classifiers, and maximum entropy classifiers. We will gloss\n",
      "over the mathematical and statistical underpinnings of these techniques, focusing in-\n",
      "stead on how and when to use them (see Section 6.9 for more technical background).\n",
      "Before looking at these methods, we first need to appreciate the broad scope of this\n",
      "topic.\n",
      "6.1  Supervised Classification\n",
      "Classification is the task of choosing the correct class label for a given input. In basic\n",
      "classification tasks, each input is considered in isolation from all other inputs, and the\n",
      "set of labels is defined in advance. Some examples of classification tasks are:\n",
      "221...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 242, 'page_label': '221', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4600}\n",
      "\n",
      "--- Chunk 4601 ---\n",
      "Content:\n",
      "• Deciding whether an email is spam or not.\n",
      "• Deciding \n",
      "what the topic of a news article is, from a fixed list of topic areas such as\n",
      "“sports,” “technology,” and “politics.”\n",
      "• Deciding whether a given occurrence of the word bank is used to refer to a river\n",
      "bank, a financial institution, the act of tilting to the side, or the act of depositing\n",
      "something in a financial institution.\n",
      "The basic classification task has a number of interesting variants. For example, in multi-\n",
      "class classification, each instance may be assigned multiple labels; in open-class clas-\n",
      "sification, the set of labels is not defined in advance; and in sequence classification, a\n",
      "list of inputs are jointly classified.\n",
      "A classifier is called supervised if it is built based on training corpora containing the\n",
      "correct label for each input. The framework used by supervised classification is shown\n",
      "in Figure 6-1.\n",
      "Figure 6-1. Supervised classification. (a) During training, a feature extractor is used to convert each\n",
      "input \n",
      "value to a feature set. These feature sets, which capture the basic information about each input\n",
      "that should be used to classify it, are discussed in the next section. Pairs of feature sets and labels are\n",
      "fed into the machine learning algorithm to generate a model. (b) During prediction, the same feature\n",
      "extractor is used to convert unseen inputs to feature sets. These feature sets are then fed into the model,\n",
      "which generates predicted labels.\n",
      "In the rest of this section, we will look at how classifiers can be employed to solve a\n",
      "wide variety of tasks. Our discussion is not intended to be comprehensive, but to give\n",
      "a representative sample of tasks that can be performed with the help of text classifiers.\n",
      "Gender Identification\n",
      "In Section 2.4, we saw that male and female names have some distinctive characteristics.\n",
      "Names ending in a, e, and i are likely to be female, while names ending in k, o, r, s, and\n",
      "t are likely to be male. Let’s build a classifier to model these differences more precisely.\n",
      "222 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 243, 'page_label': '222', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4601}\n",
      "\n",
      "--- Chunk 4602 ---\n",
      "Content:\n",
      "The first step in creating a classifier is deciding what features of the input are relevant,\n",
      "and how to encode those features. For this example, we’ll start by just looking at the\n",
      "final letter of a given name. The following feature extractor function builds a dic-\n",
      "tionary containing relevant information about a given name:\n",
      ">>> def gender_features(word):\n",
      "...     return {'last_letter': word[-1]}\n",
      ">>> gender_features('Shrek')\n",
      "{'last_letter': 'k'}\n",
      "The dictionary that is returned by this function is called a feature set and maps from\n",
      "features’ names to their values. Feature names are case-sensitive strings that typically\n",
      "provide a short human-readable description of the feature. Feature values are values\n",
      "with simple types, such as Booleans, numbers, and strings.\n",
      "Most classification methods require that features be encoded using sim-\n",
      "ple \n",
      "value types, such as Booleans, numbers, and strings. But note that\n",
      "just because a feature has a simple type, this does not necessarily mean\n",
      "that the feature’s value is simple to express or compute; indeed, it is\n",
      "even possible to use very complex and informative values, such as the\n",
      "output of a second supervised classifier, as features.\n",
      "Now that we’ve defined a feature extractor, we need to prepare a list of examples and\n",
      "corresponding class labels:\n",
      ">>> from nltk.corpus import names\n",
      ">>> import random\n",
      ">>> names = ([(name, 'male') for name in names.words('male.txt')] +\n",
      "...          [(name, 'female') for name in names.words('female.txt')])\n",
      ">>> random.shuffle(names)\n",
      "Next, we use the feature extractor to process the names data, and divide the resulting\n",
      "list of feature sets into a training set and a test set. The training set is used to train a\n",
      "new “naive Bayes” classifier.\n",
      ">>> featuresets = [(gender_features(n), g) for (n,g) in names]\n",
      ">>> train_set, test_set = featuresets[500:], featuresets[:500]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "We will learn more about the naive Bayes classifier later in the chapter. For now, let’s\n",
      "just test it out on some names that did not appear in its training data:\n",
      ">>> classifier.classify(gender_features('Neo'))\n",
      "'male'\n",
      ">>> classifier.classify(gender_features('Trinity'))\n",
      "'female'\n",
      "Observe that these character names from The Matrix are correctly classified. Although\n",
      "this science fiction movie is set in 2199, it still conforms with our expectations about\n",
      "names and genders. We can systematically evaluate the classifier on a much larger\n",
      "quantity of unseen data:\n",
      "6.1  Supervised Classification | 223...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 244, 'page_label': '223', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4602}\n",
      "\n",
      "--- Chunk 4603 ---\n",
      "Content:\n",
      ">>> print nltk.classify.accuracy(classifier, test_set)\n",
      "0.758\n",
      "Finally, \n",
      "we can examine the classifier to determine which features it found most effec-\n",
      "tive for distinguishing the names’ genders:\n",
      ">>> classifier.show_most_informative_features(5)\n",
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     38.3 : 1.0\n",
      "             last_letter = 'k'              male : female =     31.4 : 1.0\n",
      "             last_letter = 'f'              male : female =     15.3 : 1.0\n",
      "             last_letter = 'p'              male : female =     10.6 : 1.0\n",
      "             last_letter = 'w'              male : female =     10.6 : 1.0\n",
      "This listing shows that the names in the training set that end in a are female 38 times\n",
      "more often than they are male, but names that end in k are male 31 times more often\n",
      "than they are female. These ratios are known as likelihood ratios, and can be useful\n",
      "for comparing different feature-outcome relationships.\n",
      "Your Turn: Modify the gender_features() function to provide the clas-\n",
      "sifier with features encoding the length of the name, its first letter, and\n",
      "any other features that seem like they might be informative. Retrain the\n",
      "classifier with these new features, and test its accuracy.\n",
      "When working with large corpora, constructing a single list that contains the features\n",
      "of every instance can use up a large amount of memory. In these cases, use the function\n",
      "nltk.classify.apply_features, which returns an object that acts like a list but does not\n",
      "store all the feature sets in memory:\n",
      ">>> from nltk.classify import apply_features\n",
      ">>> train_set = apply_features(gender_features, names[500:])\n",
      ">>> test_set = apply_features(gender_features, names[:500])\n",
      "Choosing the Right Features\n",
      "Selecting relevant features and deciding how to encode them for a learning method can\n",
      "have an enormous impact on the learning method’s ability to extract a good model.\n",
      "Much of the interesting work in building a classifier is deciding what features might be\n",
      "relevant, and how we can represent them. Although it’s often possible to get decent\n",
      "performance by using a fairly simple and obvious set of features, there are usually sig-\n",
      "nificant gains to be had by using carefully constructed features based on a thorough\n",
      "understanding of the task at hand.\n",
      "Typically, feature extractors are built through a process of trial-and-error, guided by\n",
      "intuitions about what information is relevant to the problem. It’s common to start with\n",
      "a “kitchen sink” approach, including all the features that you can think of, and then\n",
      "checking to see which features actually are helpful. We take this approach for name\n",
      "gender features in Example 6-1.\n",
      "224 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 245, 'page_label': '224', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4603}\n",
      "\n",
      "--- Chunk 4604 ---\n",
      "Content:\n",
      "Example 6-1. A feature extractor that overfits gender features. The featuresets returned by this feature\n",
      "extractor \n",
      "contain a large number of specific features, leading to overfitting for the relatively small\n",
      "Names Corpus.\n",
      "def gender_features2(name):\n",
      "    features = {}\n",
      "    features[\"firstletter\"] = name[0].lower()\n",
      "    features[\"lastletter\"] = name[–1].lower()\n",
      "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
      "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
      "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
      "    return features\n",
      ">>> gender_features2('John') \n",
      "{'count(j)': 1, 'has(d)': False, 'count(b)': 0, ...}\n",
      "However, there are usually limits to the number of features that you should use with a\n",
      "given learning algorithm—if you provide too many features, then the algorithm will\n",
      "have a higher chance of relying on idiosyncrasies of your training data that don’t gen-\n",
      "eralize well to new examples. This problem is known as overfitting, and can be espe-\n",
      "cially problematic when working with small training sets. For example, if we train a\n",
      "naive Bayes classifier using the feature extractor shown in Example 6-1, it will overfit\n",
      "the relatively small training set, resulting in a system whose accuracy is about 1% lower\n",
      "than the accuracy of a classifier that only pays attention to the final letter of each name:\n",
      ">>> featuresets = [(gender_features2(n), g) for (n,g) in names]\n",
      ">>> train_set, test_set = featuresets[500:], featuresets[:500]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      ">>> print nltk.classify.accuracy(classifier, test_set)\n",
      "0.748\n",
      "Once an initial set of features has been chosen, a very productive method for refining\n",
      "the feature set is error analysis. First, we select a development set, containing the\n",
      "corpus data for creating the model. This development set is then subdivided into the\n",
      "training set and the dev-test set.\n",
      ">>> train_names = names[1500:]\n",
      ">>> devtest_names = names[500:1500]\n",
      ">>> test_names = names[:500]\n",
      "The training set is used to train the model, and the dev-test set is used to perform error\n",
      "analysis. The test set serves in our final evaluation of the system. For reasons discussed\n",
      "later, it is important that we employ a separate dev-test set for error analysis, rather\n",
      "than just using the test set. The division of the corpus data into different subsets is\n",
      "shown in Figure 6-2.\n",
      "Having divided the corpus into appropriate datasets, we train a model using the training\n",
      "set \n",
      ", and then run it on the dev-test set \n",
      " .\n",
      ">>> train_set = [(gender_features(n), g) for (n,g) in train_names]\n",
      ">>> devtest_set = [(gender_features(n), g) for (n,g) in devtest_names]\n",
      ">>> test_set = [(gender_features(n), g) for (n,g) in test_names]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set) \n",
      "6.1  Supervised Classification | 225...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 246, 'page_label': '225', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4604}\n",
      "\n",
      "--- Chunk 4605 ---\n",
      "Content:\n",
      ">>> print nltk.classify.accuracy(classifier, devtest_set) \n",
      "0.765\n",
      "Figure 6-2. Organization of corpus data for training supervised classifiers. The corpus data is divided\n",
      "into \n",
      "two sets: the development set and the test set. The development set is often further subdivided into\n",
      "a training set and a dev-test set.\n",
      "Using the dev-test set, we can generate a list of the errors that the classifier makes when\n",
      "predicting name genders:\n",
      ">>> errors = []\n",
      ">>> for (name, tag) in devtest_names:\n",
      "...     guess = classifier.classify(gender_features(name))\n",
      "...     if guess != tag:\n",
      "...         errors.append( (tag, guess, name) )\n",
      "We can then examine individual error cases where the model predicted the wrong label,\n",
      "and try to determine what additional pieces of information would allow it to make the\n",
      "right decision (or which existing pieces of information are tricking it into making the\n",
      "wrong decision). The feature set can then be adjusted accordingly. The names classifier\n",
      "that we have built generates about 100 errors on the dev-test corpus:\n",
      ">>> for (tag, guess, name) in sorted(errors): # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE \n",
      "...     print 'correct=%-8s guess=%-8s name=%-30s' % \n",
      "(tag, guess, name)\n",
      "       ...\n",
      "correct=female   guess=male     name=Cindelyn\n",
      "       ...\n",
      "correct=female   guess=male     name=Katheryn\n",
      "correct=female   guess=male     name=Kathryn\n",
      "       ...\n",
      "correct=male     guess=female   name=Aldrich\n",
      "       ...\n",
      "correct=male     guess=female   name=Mitch\n",
      "       ...\n",
      "correct=male     guess=female   name=Rich\n",
      "       ...\n",
      "226 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 247, 'page_label': '226', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4605}\n",
      "\n",
      "--- Chunk 4606 ---\n",
      "Content:\n",
      "Looking through this list of errors makes it clear that some suffixes that are more than\n",
      "one \n",
      "letter can be indicative of name genders. For example, names ending in yn appear\n",
      "to be predominantly female, despite the fact that names ending in n tend to be male;\n",
      "and names ending in ch are usually male, even though names that end in h tend to be\n",
      "female. We therefore adjust our feature extractor to include features for two-letter\n",
      "suffixes:\n",
      ">>> def gender_features(word):\n",
      "...     return {'suffix1': word[-1:],\n",
      "...             'suffix2': word[-2:]}\n",
      "Rebuilding the classifier with the new feature extractor, we see that the performance\n",
      "on the dev-test dataset improves by almost three percentage points (from 76.5% to\n",
      "78.2%):\n",
      ">>> train_set = [(gender_features(n), g) for (n,g) in train_names]\n",
      ">>> devtest_set = [(gender_features(n), g) for (n,g) in devtest_names]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      ">>> print nltk.classify.accuracy(classifier, devtest_set)\n",
      "0.782\n",
      "This error analysis procedure can then be repeated, checking for patterns in the errors\n",
      "that are made by the newly improved classifier. Each time the error analysis procedure\n",
      "is repeated, we should select a different dev-test/training split, to ensure that the clas-\n",
      "sifier does not start to reflect idiosyncrasies in the dev-test set.\n",
      "But once we’ve used the dev-test set to help us develop the model, we can no longer\n",
      "trust that it will give us an accurate idea of how well the model would perform on new\n",
      "data. It is therefore important to keep the test set separate, and unused, until our model\n",
      "development is complete. At that point, we can use the test set to evaluate how well\n",
      "our model will perform on new input values.\n",
      "Document Classification\n",
      "In Section 2.1, we saw several examples of corpora where documents have been labeled\n",
      "with categories. Using these corpora, we can build classifiers that will automatically\n",
      "tag new documents with appropriate category labels. First, we construct a list of docu-\n",
      "ments, labeled with the appropriate categories. For this example, we’ve chosen the\n",
      "Movie Reviews Corpus, which categorizes each review as positive or negative.\n",
      ">>> from nltk.corpus import movie_reviews\n",
      ">>> documents = [(list(movie_reviews.words(fileid)), category)\n",
      "...              for category in movie_reviews.categories()\n",
      "...              for fileid in movie_reviews.fileids(category)]\n",
      ">>> random.shuffle(documents)\n",
      "Next, we define a feature extractor for documents, so the classifier will know which\n",
      "aspects of the data it should pay attention to (see Example 6-2). For document topic\n",
      "identification, we can define a feature for each word, indicating whether the document\n",
      "contains that word. To limit the number of features that the classifier needs to process,\n",
      "we begin by constructing a list of the 2,000 most frequent words in the overall\n",
      "6.1  Supervised Classification | 227...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 248, 'page_label': '227', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4606}\n",
      "\n",
      "--- Chunk 4607 ---\n",
      "Content:\n",
      "corpus \n",
      " . We can then define a feature extractor \n",
      "  that simply checks whether each\n",
      "of these words is present in a given document.\n",
      "Example \n",
      "6-2. A feature extractor for document classification, whose features indicate whether or not\n",
      "individual words are present in a given document.\n",
      "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
      "word_features = all_words.keys()[:2000] \n",
      "def document_features(document): \n",
      "    document_words = set(document) \n",
      "    features = {}\n",
      "    for word in word_features:\n",
      "        features['contains(%s)' % word] = (word in document_words)\n",
      "    return features\n",
      ">>> print document_features(movie_reviews.words('pos/cv957_8737.txt')) \n",
      "{'contains(waste)': False, 'contains(lot)': False, ...}\n",
      "We compute the set of all words in a document in \n",
      " , rather than just\n",
      "checking \n",
      "if word in document, because checking whether a word occurs\n",
      "in a set is much faster than checking whether it occurs in a list (see\n",
      "Section 4.7).\n",
      "Now that we’ve defined our feature extractor, we can use it to train a classifier to label\n",
      "new movie reviews (Example 6-3). To check how reliable the resulting classifier is, we\n",
      "compute its accuracy on the test set \n",
      " . And once again, we can use show_most_infor\n",
      "mative_features() \n",
      "to find out which features the classifier found to be most\n",
      "informative \n",
      " .\n",
      "Example 6-3...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 249, 'page_label': '228', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4607}\n",
      "\n",
      "--- Chunk 4608 ---\n",
      "Content:\n",
      ". To check how reliable the resulting classifier is, we\n",
      "compute its accuracy on the test set \n",
      " . And once again, we can use show_most_infor\n",
      "mative_features() \n",
      "to find out which features the classifier found to be most\n",
      "informative \n",
      " .\n",
      "Example 6-3. Training and testing a classifier for document classification.\n",
      "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
      "train_set, test_set = featuresets[100:], featuresets[:100]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      ">>> print nltk.classify.accuracy(classifier, test_set) \n",
      "0.81\n",
      ">>> classifier.show_most_informative_features(5) \n",
      "Most Informative Features\n",
      "   contains(outstanding) = True               pos : neg   =     11.1 : 1.0\n",
      "        contains(seagal) = True               neg : pos   =      7.7 : 1.0\n",
      "   contains(wonderfully) = True               pos : neg   =      6.8 : 1.0\n",
      "         contains(damon) = True               pos : neg   =      5.9 : 1.0\n",
      "        contains(wasted) = True               neg : pos   =      5.8 : 1.0\n",
      "Apparently in this corpus, a review that mentions Seagal is almost 8 times more likely\n",
      "to \n",
      "be negative than positive, while a review that mentions Damon is about 6 times more\n",
      "likely to be positive.\n",
      "228 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 249, 'page_label': '228', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4608}\n",
      "\n",
      "--- Chunk 4609 ---\n",
      "Content:\n",
      "Part-of-Speech Tagging\n",
      "In Chapter 5, \n",
      "we built a regular expression tagger that chooses a part-of-speech tag for\n",
      "a word by looking at the internal makeup of the word. However, this regular expression\n",
      "tagger had to be handcrafted. Instead, we can train a classifier to work out which suf-\n",
      "fixes are most informative. Let’s begin by finding the most common suffixes:\n",
      ">>> from nltk.corpus import brown\n",
      ">>> suffix_fdist = nltk.FreqDist()\n",
      ">>> for word in brown.words():\n",
      "...     word = word.lower()\n",
      "...     suffix_fdist.inc(word[-1:])\n",
      "...     suffix_fdist.inc(word[-2:])\n",
      "...     suffix_fdist.inc(word[-3:])\n",
      ">>> common_suffixes = suffix_fdist.keys()[:100]\n",
      ">>> print common_suffixes \n",
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the',\n",
      " 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l',\n",
      " 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or',\n",
      " 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', ...]\n",
      "Next, we’ll define a feature extractor function that checks a given word for these\n",
      "suffixes:\n",
      ">>> def pos_features(word):\n",
      "...     features = {}\n",
      "...     for suffix in common_suffixes:\n",
      "...         features['endswith(%s)' % suffix] = word.lower().endswith(suffix)\n",
      "...     return features\n",
      "Feature extraction functions behave like tinted glasses, highlighting some of the prop-\n",
      "erties (colors) in our data and making it impossible to see other properties. The classifier\n",
      "will rely exclusively on these highlighted properties when determining how to label\n",
      "inputs. In this case, the classifier will make its decisions based only on information\n",
      "about which of the common suffixes (if any) a given word has.\n",
      "Now that we’ve defined our feature extractor, we can use it to train a new “decision\n",
      "tree” classifier (to be discussed in Section 6.4):\n",
      ">>> tagged_words = brown.tagged_words(categories='news')\n",
      ">>> featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n",
      ">>> size = int(len(featuresets) * 0.1)\n",
      ">>> train_set, test_set = featuresets[size:], featuresets[:size]\n",
      ">>> classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
      ">>> nltk.classify.accuracy(classifier, test_set)\n",
      "0.62705121829935351\n",
      ">>> classifier.classify(pos_features('cats'))\n",
      "'NNS'\n",
      "One nice feature of decision tree models is that they are often fairly easy to interpret.\n",
      "We can even instruct NLTK to print them out as pseudocode:\n",
      "6.1  Supervised Classification | 229...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 250, 'page_label': '229', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4609}\n",
      "\n",
      "--- Chunk 4610 ---\n",
      "Content:\n",
      ">>> print classifier.pseudocode(depth=4)\n",
      "if endswith(,) == True: return ','\n",
      "if endswith(,) == False:\n",
      "  if endswith(the) == True: return 'AT'\n",
      "  if endswith(the) == False:\n",
      "    if endswith(s) == True:\n",
      "      if endswith(is) == True: return 'BEZ'\n",
      "      if endswith(is) == False: return 'VBZ'\n",
      "    if endswith(s) == False:\n",
      "      if endswith(.) == True: return '.'\n",
      "      if endswith(.) == False: return 'NN'\n",
      "Here, \n",
      "we can see that the classifier begins by checking whether a word ends with a\n",
      "comma—if so, then it will receive the special tag \",\". Next, the classifier checks whether\n",
      "the word ends in \"the\", in which case it’s almost certainly a determiner. This “suffix”\n",
      "gets used early by the decision tree because the word the is so common. Continuing\n",
      "on, the classifier checks if the word ends in s. If so, then it’s most likely to receive the\n",
      "verb tag VBZ (unless it’s the word is, which has the special tag BEZ), and if not, then it’s\n",
      "most likely a noun (unless it’s the punctuation mark “.”). The actual classifier contains\n",
      "further nested if-then statements below the ones shown here, but the depth=4 argument\n",
      "just displays the top portion of the decision tree.\n",
      "Exploiting Context\n",
      "By augmenting the feature extraction function, we could modify this part-of-speech\n",
      "tagger to leverage a variety of other word-internal features, such as the length of the\n",
      "word, the number of syllables it contains, or its prefix. However, as long as the feature\n",
      "extractor just looks at the target word, we have no way to add features that depend on\n",
      "the context in which the word appears. But contextual features often provide powerful\n",
      "clues about the correct tag—for example, when tagging the word fly, knowing that the\n",
      "previous word is a will allow us to determine that it is functioning as a noun, not a verb.\n",
      "In order to accommodate features that depend on a word’s context, we must revise the\n",
      "pattern that we used to define our feature extractor. Instead of just passing in the word\n",
      "to be tagged, we will pass in a complete (untagged) sentence, along with the index of\n",
      "the target word. This approach is demonstrated in Example 6-4, which employs a con-\n",
      "text-dependent feature extractor to define a part-of-speech tag classifier.\n",
      "230 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 251, 'page_label': '230', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4610}\n",
      "\n",
      "--- Chunk 4611 ---\n",
      "Content:\n",
      "Example 6-4. A part-of-speech classifier whose feature detector examines the context in which a word\n",
      "appears \n",
      "in order to determine which part-of-speech tag should be assigned. In particular, the identity\n",
      "of the previous word is included as a feature.\n",
      "def pos_features(sentence, i): \n",
      "    features = {\"suffix(1)\": sentence[i][-1:],\n",
      "                \"suffix(2)\": sentence[i][-2:],\n",
      "                \"suffix(3)\": sentence[i][-3:]}\n",
      "    if i == 0:\n",
      "        features[\"prev-word\"] = \"<START>\"\n",
      "    else:\n",
      "        features[\"prev-word\"] = sentence[i-1]\n",
      "    return features\n",
      ">>> pos_features(brown.sents()[0], 8)\n",
      "{'suffix(3)': 'ion', 'prev-word': 'an', 'suffix(2)': 'on', 'suffix(1)': 'n'}\n",
      ">>> tagged_sents = brown.tagged_sents(categories='news')\n",
      ">>> featuresets = []\n",
      ">>> for tagged_sent in tagged_sents:\n",
      "...     untagged_sent = nltk.tag.untag(tagged_sent)\n",
      "...     for i, (word, tag) in enumerate(tagged_sent):\n",
      "...         featuresets.append( \n",
      "(pos_features(untagged_sent, i), tag) )\n",
      ">>> size = int(len(featuresets) * 0.1)\n",
      ">>> train_set, test_set = featuresets[size:], featuresets[:size]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      ">>> nltk.classify.accuracy(classifier, test_set)\n",
      "0.78915962207856782\n",
      "It’s \n",
      "clear that exploiting contextual features improves the performance of our part-of-\n",
      "speech tagger. For example, the classifier learns that a word is likely to be a noun if it\n",
      "comes immediately after the word large or the word gubernatorial. However, it is unable\n",
      "to learn the generalization that a word is probably a noun if it follows an adjective,\n",
      "because it doesn’t have access to the previous word’s part-of-speech tag. In general,\n",
      "simple classifiers always treat each input as independent from all other inputs. In many\n",
      "contexts, this makes perfect sense. For example, decisions about whether names tend\n",
      "to be male or female can be made on a case-by-case basis. However, there are often\n",
      "cases, such as part-of-speech tagging, where we are interested in solving classification\n",
      "problems that are closely related to one another.\n",
      "Sequence Classification\n",
      "In order to capture the dependencies between related classification tasks, we can use\n",
      "joint classifier models, which choose an appropriate labeling for a collection of related\n",
      "inputs. In the case of part-of-speech tagging, a variety of different sequence\n",
      "classifier models can be used to jointly choose part-of-speech tags for all the words in\n",
      "a given sentence.\n",
      "6.1  Supervised Classification | 231...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 252, 'page_label': '231', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4611}\n",
      "\n",
      "--- Chunk 4612 ---\n",
      "Content:\n",
      "One sequence classification strategy, known as consecutive classification or greedy\n",
      "sequence classification, is to find the most likely class label for the first input, then\n",
      "to use that answer to help find the best label for the next input. The process can then\n",
      "be repeated until all of the inputs have been labeled. This is the approach that was taken\n",
      "by the bigram tagger from Section 5.5, which began by choosing a part-of-speech tag\n",
      "for the first word in the sentence, and then chose the tag for each subsequent word\n",
      "based on the word itself and the predicted tag for the previous word.\n",
      "This strategy is demonstrated in Example 6-5 . First, we must augment our feature\n",
      "extractor function to take a history argument, which provides a list of the tags that\n",
      "we’ve predicted for the sentence so far \n",
      ". Each tag in history corresponds with a word\n",
      "in sentence. But note that history will only contain tags for words we’ve already clas-\n",
      "sified, that is, words to the left of the target word. Thus, although it is possible to look\n",
      "at some features of words to the right of the target word, it is not possible to look at\n",
      "the tags for those words (since we haven’t generated them yet).\n",
      "Having defined a feature extractor, we can proceed to build our sequence\n",
      "classifier \n",
      ". During training, we use the annotated tags to provide the appropriate\n",
      "history to the feature extractor, but when tagging new sentences, we generate the his-\n",
      "tory list based on the output of the tagger itself.\n",
      "Example 6-5...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 253, 'page_label': '232', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4612}\n",
      "\n",
      "--- Chunk 4613 ---\n",
      "Content:\n",
      ". Thus, although it is possible to look\n",
      "at some features of words to the right of the target word, it is not possible to look at\n",
      "the tags for those words (since we haven’t generated them yet).\n",
      "Having defined a feature extractor, we can proceed to build our sequence\n",
      "classifier \n",
      ". During training, we use the annotated tags to provide the appropriate\n",
      "history to the feature extractor, but when tagging new sentences, we generate the his-\n",
      "tory list based on the output of the tagger itself.\n",
      "Example 6-5. Part-of-speech tagging with a consecutive classifier.\n",
      "def pos_features(sentence, i, history): \n",
      "    features = {\"suffix(1)\": sentence[i][-1:],\n",
      "                \"suffix(2)\": sentence[i][-2:],\n",
      "                \"suffix(3)\": sentence[i][-3:]}\n",
      "    if i == 0:\n",
      "        features[\"prev-word\"] = \"<START>\"\n",
      "        features[\"prev-tag\"] = \"<START>\"\n",
      "    else:\n",
      "        features[\"prev-word\"] = sentence[i-1]\n",
      "        features[\"prev-tag\"] = history[i-1]\n",
      "    return features\n",
      "class ConsecutivePosTagger(nltk.TaggerI): \n",
      "    def __init__(self, train_sents):\n",
      "        train_set = []\n",
      "        for tagged_sent in train_sents:\n",
      "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
      "            history = []\n",
      "            for i, (word, tag) in enumerate(tagged_sent):\n",
      "                featureset = pos_features(untagged_sent, i, history)\n",
      "                train_set.append( (featureset, tag) )\n",
      "                history.append(tag)\n",
      "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      " \n",
      "    def tag(self, sentence):\n",
      "        history = []\n",
      "        for i, word in enumerate(sentence):\n",
      "            featureset = pos_features(sentence, i, history)\n",
      "232 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 253, 'page_label': '232', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4613}\n",
      "\n",
      "--- Chunk 4614 ---\n",
      "Content:\n",
      "tag = self.classifier.classify(featureset)\n",
      "            history.append(tag)\n",
      "        return zip(sentence, history)\n",
      ">>> tagged_sents = brown.tagged_sents(categories='news')\n",
      ">>> size = int(len(tagged_sents) * 0.1)\n",
      ">>> train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
      ">>> tagger = ConsecutivePosTagger(train_sents)\n",
      ">>> print tagger.evaluate(test_sents)\n",
      "0.79796012981\n",
      "Other Methods for Sequence Classification\n",
      "One \n",
      "shortcoming of this approach is that we commit to every decision that we make.\n",
      "For example, if we decide to label a word as a noun, but later find evidence that it should\n",
      "have been a verb, there’s no way to go back and fix our mistake. One solution to this\n",
      "problem is to adopt a transformational strategy instead. Transformational joint classi-\n",
      "fiers work by creating an initial assignment of labels for the inputs, and then iteratively\n",
      "refining that assignment in an attempt to repair inconsistencies between related inputs.\n",
      "The Brill tagger, described in Section 5.6, is a good example of this strategy.\n",
      "Another solution is to assign scores to all of the possible sequences of part-of-speech\n",
      "tags, and to choose the sequence whose overall score is highest. This is the approach\n",
      "taken by Hidden Markov Models. Hidden Markov Models are similar to consecutive\n",
      "classifiers in that they look at both the inputs and the history of predicted tags. How-\n",
      "ever, rather than simply finding the single best tag for a given word, they generate a\n",
      "probability distribution over tags. These probabilities are then combined to calculate\n",
      "probability scores for tag sequences, and the tag sequence with the highest probability\n",
      "is chosen. Unfortunately, the number of possible tag sequences is quite large. Given a\n",
      "tag set with 30 tags, there are about 600 trillion (3010) ways to label a 10-word sentence.\n",
      "In order to avoid considering all these possible sequences separately, Hidden Markov\n",
      "Models require that the feature extractor only look at the most recent tag (or the most\n",
      "recent n tags, where n is fairly small). Given that restriction, it is possible to use dynamic\n",
      "programming (Section 4.7) to efficiently find the most likely tag sequence. In particular,\n",
      "for each consecutive word index i, a score is computed for each possible current and\n",
      "previous tag. This same basic approach is taken by two more advanced models, called\n",
      "Maximum Entropy Markov Models  and Linear-Chain Conditional Random\n",
      "Field Models; but different algorithms are used to find scores for tag sequences.\n",
      "6.2  Further Examples of Supervised Classification\n",
      "Sentence Segmentation\n",
      "Sentence segmentation can be viewed as a classification task for punctuation: whenever\n",
      "we encounter a symbol that could possibly end a sentence, such as a period or a question\n",
      "mark, we have to decide whether it terminates the preceding sentence.\n",
      "6.2  Further Examples of Supervised Classification | 233...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 254, 'page_label': '233', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4614}\n",
      "\n",
      "--- Chunk 4615 ---\n",
      "Content:\n",
      "The first step is to obtain some data that has already been segmented into sentences\n",
      "and convert it into a form that is suitable for extracting features:\n",
      ">>> sents = nltk.corpus.treebank_raw.sents()\n",
      ">>> tokens = []\n",
      ">>> boundaries = set()\n",
      ">>> offset = 0\n",
      ">>> for sent in nltk.corpus.treebank_raw.sents():\n",
      "...     tokens.extend(sent)\n",
      "...     offset += len(sent)\n",
      "...     boundaries.add(offset-1)\n",
      "Here, tokens \n",
      "is a merged list of tokens from the individual sentences, and boundaries\n",
      "is a set containing the indexes of all sentence-boundary tokens. Next, we need to specify\n",
      "the features of the data that will be used in order to decide whether punctuation indi-\n",
      "cates a sentence boundary:\n",
      ">>> def punct_features(tokens, i):\n",
      "...     return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
      "...             'prevword': tokens[i-1].lower(),\n",
      "...             'punct': tokens[i],\n",
      "...             'prev-word-is-one-char': len(tokens[i-1]) == 1}\n",
      "Based on this feature extractor, we can create a list of labeled featuresets by selecting\n",
      "all the punctuation tokens, and tagging whether they are boundary tokens or not:\n",
      ">>> featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
      "...                for i in range(1, len(tokens)-1)\n",
      "...                if tokens[i] in '.?!']\n",
      "Using these featuresets, we can train and evaluate a punctuation classifier:\n",
      ">>> size = int(len(featuresets) * 0.1)\n",
      ">>> train_set, test_set = featuresets[size:], featuresets[:size]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      ">>> nltk.classify.accuracy(classifier, test_set)\n",
      "0.97419354838709682\n",
      "To use this classifier to perform sentence segmentation, we simply check each punc-\n",
      "tuation mark to see whether it’s labeled as a boundary, and divide the list of words at\n",
      "the boundary marks. The listing in Example 6-6 shows how this can be done.\n",
      "Example 6-6. Classification-based sentence segmenter.\n",
      "def segment_sentences(words):\n",
      "    start = 0\n",
      "    sents = []\n",
      "    for i, word in words:\n",
      "        if word in '.?!' and classifier.classify(words, i) == True:\n",
      "            sents.append(words[start:i+1])\n",
      "            start = i+1\n",
      "    if start < len(words):\n",
      "        sents.append(words[start:])\n",
      "234 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 255, 'page_label': '234', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4615}\n",
      "\n",
      "--- Chunk 4616 ---\n",
      "Content:\n",
      "Identifying Dialogue Act Types\n",
      "When \n",
      "processing dialogue, it can be useful to think of utterances as a type of action\n",
      "performed by the speaker. This interpretation is most straightforward for performative\n",
      "statements such as I forgive you or I bet you can’t climb that hill. But greetings, questions,\n",
      "answers, assertions, and clarifications can all be thought of as types of speech-based\n",
      "actions. Recognizing the dialogue acts underlying the utterances in a dialogue can be\n",
      "an important first step in understanding the conversation.\n",
      "The NPS Chat Corpus, which was demonstrated in Section 2.1, consists of over 10,000\n",
      "posts from instant messaging sessions. These posts have all been labeled with one of\n",
      "15 dialogue act types, such as “Statement,” “Emotion,” “ynQuestion,” and “Contin-\n",
      "uer.” We can therefore use this data to build a classifier that can identify the dialogue\n",
      "act types for new instant messaging posts. The first step is to extract the basic messaging\n",
      "data. We will call xml_posts() to get a data structure representing the XML annotation\n",
      "for each post:\n",
      ">>> posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
      "Next, we’ll define a simple feature extractor that checks what words the post contains:\n",
      ">>> def dialogue_act_features(post):\n",
      "...     features = {}\n",
      "...     for word in nltk.word_tokenize(post):\n",
      "...         features['contains(%s)' % word.lower()] = True\n",
      "...     return features\n",
      "Finally, we construct the training and testing data by applying the feature extractor to\n",
      "each post (using post.get('class') to get a post’s dialogue act type), and create a new\n",
      "classifier:\n",
      ">>> featuresets = [(dialogue_act_features(post.text), post.get('class'))\n",
      "...                for post in posts]\n",
      ">>> size = int(len(featuresets) * 0.1)\n",
      ">>> train_set, test_set = featuresets[size:], featuresets[:size]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      ">>> print nltk.classify.accuracy(classifier, test_set)\n",
      "0.66\n",
      "Recognizing Textual Entailment\n",
      "Recognizing textual entailment (RTE) is the task of determining whether a given piece\n",
      "of text T entails another text called the “hypothesis” (as already discussed in Sec-\n",
      "tion 1.5). To date, there have been four RTE Challenges, where shared development\n",
      "and test data is made available to competing teams. Here are a couple of examples of\n",
      "text/hypothesis pairs from the Challenge 3 development dataset. The label True indi-\n",
      "cates that the entailment holds, and False indicates that it fails to hold.\n",
      "6.2  Further Examples of Supervised Classification | 235...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 256, 'page_label': '235', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4616}\n",
      "\n",
      "--- Chunk 4617 ---\n",
      "Content:\n",
      "Challenge 3, Pair 34 (True)\n",
      "T: Parviz Davudi was representing Iran at a meeting of the Shanghai Co-operation\n",
      "Organisation (SCO), the fledgling association that binds Russia, China and four\n",
      "former Soviet republics of central Asia together to fight terrorism.\n",
      "H: China is a member of SCO.\n",
      "Challenge 3, Pair 81 (False)\n",
      "T: \n",
      "According to NC Articles of Organization, the members of LLC company are\n",
      "H. Nelson Beavers, III, H. Chester Beavers and Jennie Beavers Stewart.\n",
      "H: Jennie Beavers Stewart is a share-holder of Carolina Analytical Laboratory.\n",
      "It should be emphasized that the relationship between text and hypothesis is not in-\n",
      "tended to be logical entailment, but rather whether a human would conclude that the\n",
      "text provides reasonable evidence for taking the hypothesis to be true.\n",
      "We \n",
      "can treat RTE as a classification task, in which we try to predict the True/False label\n",
      "for each pair. Although it seems likely that successful approaches to this task will in-\n",
      "volve a combination of parsing, semantics, and real-world knowledge, many early at-\n",
      "tempts at RTE achieved reasonably good results with shallow analysis, based on sim-\n",
      "ilarity between the text and hypothesis at the word level. In the ideal case, we would\n",
      "expect that if there is an entailment, then all the information expressed by the hypoth-\n",
      "esis should also be present in the text. Conversely, if there is information found in the\n",
      "hypothesis that is absent from the text, then there will be no entailment.\n",
      "In our RTE feature detector ( Example 6-7), we let words (i.e., word types) serve as\n",
      "proxies for information, and our features count the degree of word overlap, and the\n",
      "degree to which there are words in the hypothesis but not in the text (captured by the\n",
      "method hyp_extra()). Not all words are equally important—named entity mentions,\n",
      "such as the names of people, organizations, and places, are likely to be more significant,\n",
      "which motivates us to extract distinct information for words and nes (named entities).\n",
      "In addition, some high-frequency function words are filtered out as “stopwords.”\n",
      "Example 6-7. “Recognizing Text Entailment” feature extractor: The RTEFeatureExtractor class\n",
      "builds a bag of words for both the text and the hypothesis after throwing away some stopwords, then\n",
      "calculates overlap and difference.\n",
      "def rte_features(rtepair):\n",
      "    extractor = nltk.RTEFeatureExtractor(rtepair)\n",
      "    features = {}\n",
      "    features['word_overlap'] = len(extractor.overlap('word'))\n",
      "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n",
      "    features['ne_overlap'] = len(extractor.overlap('ne'))\n",
      "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n",
      "    return features\n",
      "To illustrate the content of these features, we examine some attributes of the text/\n",
      "hypothesis Pair 34 shown earlier:\n",
      "236 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 257, 'page_label': '236', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4617}\n",
      "\n",
      "--- Chunk 4618 ---\n",
      "Content:\n",
      ">>> rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\n",
      ">>> extractor = nltk.RTEFeatureExtractor(rtepair)\n",
      ">>> print extractor.text_words\n",
      "set(['Russia', 'Organisation', 'Shanghai', 'Asia', 'four', 'at',\n",
      "'operation', 'SCO', ...])\n",
      ">>> print extractor.hyp_words\n",
      "set(['member', 'SCO', 'China'])\n",
      ">>> print extractor.overlap('word')\n",
      "set([])\n",
      ">>> print extractor.overlap('ne')\n",
      "set(['SCO', 'China'])\n",
      ">>> print extractor.hyp_extra('word')\n",
      "set(['member'])\n",
      "These \n",
      "features indicate that all important words in the hypothesis are contained in the\n",
      "text, and thus there is some evidence for labeling this as True.\n",
      "The module nltk.classify.rte_classify reaches just over 58% accuracy on the com-\n",
      "bined RTE test data using methods like these. Although this figure is not very\n",
      "impressive, it requires significant effort, and more linguistic processing, to achieve\n",
      "much better results.\n",
      "Scaling Up to Large Datasets\n",
      "Python provides an excellent environment for performing basic text processing and\n",
      "feature extraction. However, it is not able to perform the numerically intensive calcu-\n",
      "lations required by machine learning methods nearly as quickly as lower-level languages\n",
      "such as C. Thus, if you attempt to use the pure-Python machine learning implemen-\n",
      "tations (such as nltk.NaiveBayesClassifier) on large datasets, you may find that the\n",
      "learning algorithm takes an unreasonable amount of time and memory to complete.\n",
      "If you plan to train classifiers with large amounts of training data or a large number of\n",
      "features, we recommend that you explore NLTK’s facilities for interfacing with external\n",
      "machine learning packages. Once these packages have been installed, NLTK can trans-\n",
      "parently invoke them (via system calls) to train classifier models significantly faster than\n",
      "the pure-Python classifier implementations. See the NLTK web page for a list of rec-\n",
      "ommended machine learning packages that are supported by NLTK.\n",
      "6.3  Evaluation\n",
      "In order to decide whether a classification model is accurately capturing a pattern, we\n",
      "must evaluate that model. The result of this evaluation is important for deciding how\n",
      "trustworthy the model is, and for what purposes we can use it. Evaluation can also be\n",
      "an effective tool for guiding us in making future improvements to the model.\n",
      "The Test Set\n",
      "Most evaluation techniques calculate a score for a model by comparing the labels that\n",
      "it generates for the inputs in a test set (or evaluation set) with the correct labels for\n",
      "6.3  Evaluation | 237...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 258, 'page_label': '237', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4618}\n",
      "\n",
      "--- Chunk 4619 ---\n",
      "Content:\n",
      "those inputs. This test set typically has the same format as the training set. However,\n",
      "it \n",
      "is very important that the test set be distinct from the training corpus: if we simply\n",
      "reused the training set as the test set, then a model that simply memorized its input,\n",
      "without learning how to generalize to new examples, would receive misleadingly high\n",
      "scores.\n",
      "When building the test set, there is often a trade-off between the amount of data avail-\n",
      "able for testing and the amount available for training. For classification tasks that have\n",
      "a small number of well-balanced labels and a diverse test set, a meaningful evaluation\n",
      "can be performed with as few as 100 evaluation instances. But if a classification task\n",
      "has a large number of labels or includes very infrequent labels, then the size of the test\n",
      "set should be chosen to ensure that the least frequent label occurs at least 50 times.\n",
      "Additionally, if the test set contains many closely related instances—such as instances\n",
      "drawn from a single document—then the size of the test set should be increased to\n",
      "ensure that this lack of diversity does not skew the evaluation results. When large\n",
      "amounts of annotated data are available, it is common to err on the side of safety by\n",
      "using 10% of the overall data for evaluation.\n",
      "Another consideration when choosing the test set is the degree of similarity between\n",
      "instances in the test set and those in the development set. The more similar these two\n",
      "datasets are, the less confident we can be that evaluation results will generalize to other\n",
      "datasets. For example, consider the part-of-speech tagging task. At one extreme, we\n",
      "could create the training set and test set by randomly assigning sentences from a data\n",
      "source that reflects a single genre, such as news:\n",
      ">>> import random\n",
      ">>> from nltk.corpus import brown\n",
      ">>> tagged_sents = list(brown.tagged_sents(categories='news'))\n",
      ">>> random.shuffle(tagged_sents)\n",
      ">>> size = int(len(tagged_sents) * 0.1)\n",
      ">>> train_set, test_set = tagged_sents[size:], tagged_sents[:size]\n",
      "In this case, our test set will be very similar to our training set. The training set and test\n",
      "set are taken from the same genre, and so we cannot be confident that evaluation results\n",
      "would generalize to other genres. What’s worse, because of the call to\n",
      "random.shuffle(), the test set contains sentences that are taken from the same docu-\n",
      "ments that were used for training. If there is any consistent pattern within a document\n",
      "(say, if a given word appears with a particular part-of-speech tag especially frequently),\n",
      "then that difference will be reflected in both the development set and the test set. A\n",
      "somewhat better approach is to ensure that the training set and test set are taken from\n",
      "different documents:\n",
      ">>> file_ids = brown.fileids(categories='news')\n",
      ">>> size = int(len(file_ids) * 0.1)\n",
      ">>> train_set = brown.tagged_sents(file_ids[size:])\n",
      ">>> test_set = brown.tagged_sents(file_ids[:size])\n",
      "If we want to perform a more stringent evaluation, we can draw the test set from docu-\n",
      "ments that are less closely related to those in the training set:\n",
      "238 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 259, 'page_label': '238', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4619}\n",
      "\n",
      "--- Chunk 4620 ---\n",
      "Content:\n",
      ">>> train_set = brown.tagged_sents(categories='news')\n",
      ">>> test_set = brown.tagged_sents(categories='fiction')\n",
      "If \n",
      "we build a classifier that performs well on this test set, then we can be confident that\n",
      "it has the power to generalize well beyond the data on which it was trained.\n",
      "Accuracy\n",
      "The simplest metric that can be used to evaluate a classifier, accuracy, measures the\n",
      "percentage of inputs in the test set that the classifier correctly labeled. For example, a\n",
      "name gender classifier that predicts the correct name 60 times in a test set containing\n",
      "80 names would have an accuracy of 60/80 = 75%. The function nltk.classify.accu\n",
      "racy() will calculate the accuracy of a classifier model on a given test set:\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      ">>> print 'Accuracy: %4.2f' % nltk.classify.accuracy(classifier, test_set) \n",
      "0.75\n",
      "When interpreting the accuracy score of a classifier, it is important to consider the\n",
      "frequencies of the individual class labels in the test set. For example, consider a classifier\n",
      "that determines the correct word sense for each occurrence of the word bank. If we\n",
      "evaluate this classifier on financial newswire text, then we may find that the financial-\n",
      "institution sense appears 19 times out of 20. In that case, an accuracy of 95% would\n",
      "hardly be impressive, since we could achieve that accuracy with a model that always\n",
      "returns the financial-institution sense. However, if we instead evaluate the classifier\n",
      "on a more balanced corpus, where the most frequent word sense has a frequency of\n",
      "40%, then a 95% accuracy score would be a much more positive result. (A similar issue\n",
      "arises when measuring inter-annotator agreement in Section 11.2.)\n",
      "Precision and Recall\n",
      "Another instance where accuracy scores can be misleading is in “search” tasks, such as\n",
      "information retrieval, where we are attempting to find documents that are relevant to\n",
      "a particular task. Since the number of irrelevant documents far outweighs the number\n",
      "of relevant documents, the accuracy score for a model that labels every document as\n",
      "irrelevant would be very close to 100%.\n",
      "It is therefore conventional to employ a different set of measures for search tasks, based\n",
      "on the number of items in each of the four categories shown in Figure 6-3:\n",
      "• True positives are relevant items that we correctly identified as relevant.\n",
      "• True negatives are irrelevant items that we correctly identified as irrelevant.\n",
      "• False positives (or Type I errors) are irrelevant items that we incorrectly identi-\n",
      "fied as relevant.\n",
      "• False negatives (or Type II errors) are relevant items that we incorrectly identi-\n",
      "fied as irrelevant.\n",
      "6.3  Evaluation | 239...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 260, 'page_label': '239', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4620}\n",
      "\n",
      "--- Chunk 4621 ---\n",
      "Content:\n",
      "Given these four numbers, we can define the following metrics:\n",
      "• Precision, \n",
      "which indicates how many of the items that we identified were relevant,\n",
      "is TP/(TP+FP).\n",
      "• Recall, which indicates how many of the relevant items that we identified, is\n",
      "TP/(TP+FN).\n",
      "• The F-Measure (or F-Score), which combines the precision and recall to give a\n",
      "single score, is defined to be the harmonic mean of the precision and recall\n",
      "(2 × Precision × Recall)/(Precision+Recall).\n",
      "Confusion Matrices\n",
      "When performing classification tasks with three or more labels, it can be informative\n",
      "to subdivide the errors made by the model based on which types of mistake it made. A\n",
      "confusion matrix is a table where each cell [ i,j] indicates how often label j was pre-\n",
      "dicted when the correct label was i. Thus, the diagonal entries (i.e., cells [i,j]) indicate\n",
      "labels that were correctly predicted, and the off-diagonal entries indicate errors. In the\n",
      "following example, we generate a confusion matrix for the unigram tagger developed\n",
      "in Section 5.4:\n",
      "Figure 6-3. True and false positives and negatives.\n",
      "240 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 261, 'page_label': '240', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4621}\n",
      "\n",
      "--- Chunk 4622 ---\n",
      "Content:\n",
      ">>> def tag_list(tagged_sents):\n",
      "...     return [tag for sent in tagged_sents for (word, tag) in sent]\n",
      ">>> def apply_tagger(tagger, corpus):\n",
      "...     return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]\n",
      ">>> gold = tag_list(brown.tagged_sents(categories='editorial'))\n",
      ">>> test = tag_list(apply_tagger(t2, brown.tagged_sents(categories='editorial'))) \n",
      ">>> cm = nltk.ConfusionMatrix(gold, test)\n",
      "    |                                         N                      |\n",
      "    |      N      I      A      J             N             V      N |\n",
      "    |      N      N      T      J      .      S      ,      B      P |\n",
      "----+----------------------------------------------------------------+\n",
      " NN | <11.8%>  0.0%      .   0.2%      .   0.0%      .   0.3%   0.0% |\n",
      " IN |   0.0%  <9.0%>     .      .      .   0.0%      .      .      . |\n",
      " AT |      .      .  <8.6%>     .      .      .      .      .      . |\n",
      " JJ |   1.6%      .      .  <4.0%>     .      .      .   0.0%   0.0% |\n",
      "  . |      .      .      .      .  <4.8%>     .      .      .      . |\n",
      " NS |   1.5%      .      .      .      .  <3.2%>     .      .   0.0% |\n",
      "  , |      .      .      .      .      .      .  <4.4%>     ....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 262, 'page_label': '241', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4622}\n",
      "\n",
      "--- Chunk 4623 ---\n",
      "Content:\n",
      ".      .      . |\n",
      " NS |   1.5%      .      .      .      .  <3.2%>     .      .   0.0% |\n",
      "  , |      .      .      .      .      .      .  <4.4%>     .      . |\n",
      "  B |   0.9%      .      .   0.0%      .      .      .  <2.4%>     . |\n",
      " NP |   1.0%      .      .   0.0%      .      .      .      .  <1.9%>|\n",
      "----+----------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "The \n",
      "confusion matrix indicates that common errors include a substitution of NN for\n",
      "JJ (for 1.6% of words), and of NN for NNS (for 1.5% of words). Note that periods (.)\n",
      "indicate cells whose value is 0, and that the diagonal entries—which correspond to\n",
      "correct classifications—are marked with angle brackets.\n",
      "Cross-Validation\n",
      "In order to evaluate our models, we must reserve a portion of the annotated data for\n",
      "the test set. As we already mentioned, if the test set is too small, our evaluation may\n",
      "not be accurate. However, making the test set larger usually means making the training\n",
      "set smaller, which can have a significant impact on performance if a limited amount of\n",
      "annotated data is available.\n",
      "One solution to this problem is to perform multiple evaluations on different test sets,\n",
      "then to combine the scores from those evaluations, a technique known as cross-\n",
      "validation. In particular, we subdivide the original corpus into N subsets called\n",
      "folds. For each of these folds, we train a model using all of the data except the data in\n",
      "that fold, and then test that model on the fold. Even though the individual folds might\n",
      "be too small to give accurate evaluation scores on their own, the combined evaluation\n",
      "score is based on a large amount of data and is therefore quite reliable.\n",
      "A second, and equally important, advantage of using cross-validation is that it allows\n",
      "us to examine how widely the performance varies across different training sets. If we\n",
      "get very similar scores for all N training sets, then we can be fairly confident that the\n",
      "score is accurate. On the other hand, if scores vary widely across the N training sets,\n",
      "then we should probably be skeptical about the accuracy of the evaluation score.\n",
      "6.3  Evaluation | 241...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 262, 'page_label': '241', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4623}\n",
      "\n",
      "--- Chunk 4624 ---\n",
      "Content:\n",
      "6.4  Decision Trees\n",
      "In \n",
      "the next three sections, we’ll take a closer look at three machine learning methods\n",
      "that can be used to automatically build classification models: decision trees, naive Bayes\n",
      "classifiers, and Maximum Entropy classifiers. As we’ve seen, it’s possible to treat these\n",
      "learning methods as black boxes, simply training models and using them for prediction\n",
      "without understanding how they work. But there’s a lot to be learned from taking a\n",
      "closer look at how these learning methods select models based on the data in a training\n",
      "set. An understanding of these methods can help guide our selection of appropriate\n",
      "features, and especially our decisions about how those features should be encoded.\n",
      "And an understanding of the generated models can allow us to extract information\n",
      "about which features are most informative, and how those features relate to one an-\n",
      "other.\n",
      "A decision tree is a simple flowchart that selects labels for input values. This flowchart\n",
      "consists of decision nodes, which check feature values, and leaf nodes, which assign\n",
      "labels. To choose the label for an input value, we begin at the flowchart’s initial decision\n",
      "node, known as its root node. This node contains a condition that checks one of the\n",
      "input value’s features, and selects a branch based on that feature’s value. Following the\n",
      "branch that describes our input value, we arrive at a new decision node, with a new\n",
      "condition on the input value’s features. We continue following the branch selected by\n",
      "each node’s condition, until we arrive at a leaf node which provides a label for the input\n",
      "value. Figure 6-4 shows an example decision tree model for the name gender task.\n",
      "Once we have a decision tree, it is straightforward to use it to assign labels to new input\n",
      "values. What’s less straightforward is how we can build a decision tree that models a\n",
      "given training set. But before we look at the learning algorithm for building decision\n",
      "trees, we’ll consider a simpler task: picking the best “decision stump” for a corpus. A\n",
      "Figure 6-4. Decision Tree model for the name gender task. Note that tree diagrams are conventionally\n",
      "drawn “upside down,” with the root at the top, and the leaves at the bottom.\n",
      "242 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 263, 'page_label': '242', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4624}\n",
      "\n",
      "--- Chunk 4625 ---\n",
      "Content:\n",
      "decision stump is a decision tree with a single node that decides how to classify inputs\n",
      "based on a single feature. It contains one leaf for each possible feature value, specifying\n",
      "the class label that should be assigned to inputs whose features have that value. In order\n",
      "to build a decision stump, we must first decide which feature should be used. The\n",
      "simplest method is to just build a decision stump for each possible feature, and see\n",
      "which one achieves the highest accuracy on the training data, although there are other\n",
      "alternatives that we will discuss later. Once we’ve picked a feature, we can build the\n",
      "decision stump by assigning a label to each leaf based on the most frequent label for\n",
      "the selected examples in the training set (i.e., the examples where the selected feature\n",
      "has that value).\n",
      "Given the algorithm for choosing decision stumps, the algorithm for growing larger\n",
      "decision trees is straightforward. We begin by selecting the overall best decision stump\n",
      "for the classification task. We then check the accuracy of each of the leaves on the\n",
      "training set. Leaves that do not achieve sufficient accuracy are then replaced by new\n",
      "decision stumps, trained on the subset of the training corpus that is selected by the path\n",
      "to the leaf. For example, we could grow the decision tree in Figure 6-4 by replacing the\n",
      "leftmost leaf with a new decision stump, trained on the subset of the training set names\n",
      "that do not start with a k or end with a vowel or an l.\n",
      "Entropy and Information Gain\n",
      "As was mentioned before, there are several methods for identifying the most informa-\n",
      "tive feature for a decision stump. One popular alternative, called information gain,\n",
      "measures how much more organized the input values become when we divide them up\n",
      "using a given feature. To measure how disorganized the original set of input values are,\n",
      "we calculate entropy of their labels, which will be high if the input values have highly\n",
      "varied labels, and low if many input values all have the same label. In particular, entropy\n",
      "is defined as the sum of the probability of each label times the log probability of that\n",
      "same label:\n",
      "(1) H = Σl ∈ labelsP(l) × log2P(l).\n",
      "For example, Figure 6-5 shows how the entropy of labels in the name gender prediction\n",
      "task depends on the ratio of male to female names. Note that if most input values have\n",
      "the same label (e.g., if P(male) is near 0 or near 1), then entropy is low. In particular,\n",
      "labels that have low frequency do not contribute much to the entropy (since P(l) is\n",
      "small), and labels with high frequency also do not contribute much to the entropy (since\n",
      "log2P(l) is small). On the other hand, if the input values have a wide variety of labels,\n",
      "then there are many labels with a “medium” frequency, where neither P(l) nor\n",
      "log2P(l) is small, so the entropy is high. Example 6-8 demonstrates how to calculate\n",
      "the entropy of a list of labels.\n",
      "6.4  Decision Trees | 243...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 264, 'page_label': '243', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4625}\n",
      "\n",
      "--- Chunk 4626 ---\n",
      "Content:\n",
      "Figure 6-5. The entropy of labels in the name gender prediction task, as a function of the percentage\n",
      "of names in a given set that are male.\n",
      "Example 6-8. Calculating the entropy of a list of labels.\n",
      "import math\n",
      "def entropy(labels):\n",
      "    freqdist = nltk.FreqDist(labels)\n",
      "    probs = [freqdist.freq(l) for l in nltk.FreqDist(labels)]\n",
      "    return -sum([p * math.log(p,2) for p in probs])\n",
      ">>> print entropy(['male', 'male', 'male', 'male']) \n",
      "0.0\n",
      ">>> print entropy(['male', 'female', 'male', 'male'])\n",
      "0.811278124459\n",
      " \n",
      ">>> print entropy(['female', 'male', 'female', 'male'])\n",
      "1.0\n",
      ">>> print entropy(['female', 'female', 'male', 'female'])\n",
      "0.811278124459\n",
      ">>> print entropy(['female', 'female', 'female', 'female'])\n",
      "0.0\n",
      "Once \n",
      "we have calculated the entropy of the labels of the original set of input values, we\n",
      "can determine how much more organized the labels become once we apply the decision\n",
      "stump. To do so, we calculate the entropy for each of the decision stump’s leaves, and\n",
      "take the average of those leaf entropy values (weighted by the number of samples in\n",
      "each leaf). The information gain is then equal to the original entropy minus this new,\n",
      "reduced entropy. The higher the information gain, the better job the decision stump\n",
      "does of dividing the input values into coherent groups, so we can build decision trees\n",
      "by selecting the decision stumps with the highest information gain.\n",
      "Another consideration for decision trees is efficiency. The simple algorithm for selecting\n",
      "decision stumps described earlier must construct a candidate decision stump for every\n",
      "possible feature, and this process must be repeated for every node in the constructed\n",
      "244 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 265, 'page_label': '244', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4626}\n",
      "\n",
      "--- Chunk 4627 ---\n",
      "Content:\n",
      "decision tree. A number of algorithms have been developed to cut down on the training\n",
      "time by storing and reusing information about previously evaluated examples.\n",
      "Decision \n",
      "trees have a number of useful qualities. To begin with, they’re simple to un-\n",
      "derstand, and easy to interpret. This is especially true near the top of the decision tree,\n",
      "where it is usually possible for the learning algorithm to find very useful features. De-\n",
      "cision trees are especially well suited to cases where many hierarchical categorical dis-\n",
      "tinctions can be made. For example, decision trees can be very effective at capturing\n",
      "phylogeny trees.\n",
      "However, decision trees also have a few disadvantages. One problem is that, since each\n",
      "branch in the decision tree splits the training data, the amount of training data available\n",
      "to train nodes lower in the tree can become quite small. As a result, these lower decision\n",
      "nodes may overfit the training set, learning patterns that reflect idiosyncrasies of the\n",
      "training set rather than linguistically significant patterns in the underlying problem.\n",
      "One solution to this problem is to stop dividing nodes once the amount of training data\n",
      "becomes too small. Another solution is to grow a full decision tree, but then to\n",
      "prune decision nodes that do not improve performance on a dev-test.\n",
      "A second problem with decision trees is that they force features to be checked in a\n",
      "specific order, even when features may act relatively independently of one another. For\n",
      "example, when classifying documents into topics (such as sports, automotive, or mur-\n",
      "der mystery), features such as hasword(football) are highly indicative of a specific label,\n",
      "regardless of what the other feature values are. Since there is limited space near the top\n",
      "of the decision tree, most of these features will need to be repeated on many different\n",
      "branches in the tree. And since the number of branches increases exponentially as we\n",
      "go down the tree, the amount of repetition can be very large.\n",
      "A related problem is that decision trees are not good at making use of features that are\n",
      "weak predictors of the correct label. Since these features make relatively small\n",
      "incremental improvements, they tend to occur very low in the decision tree. But by the\n",
      "time the decision tree learner has descended far enough to use these features, there is\n",
      "not enough training data left to reliably determine what effect they should have. If we\n",
      "could instead look at the effect of these features across the entire training set, then we\n",
      "might be able to make some conclusions about how they should affect the choice of\n",
      "label.\n",
      "The fact that decision trees require that features be checked in a specific order limits\n",
      "their ability to exploit features that are relatively independent of one another. The naive\n",
      "Bayes classification method, which we’ll discuss next, overcomes this limitation by\n",
      "allowing all features to act “in parallel.”\n",
      "6.5  Naive Bayes Classifiers\n",
      "In naive Bayes classifiers, every feature gets a say in determining which label should\n",
      "be assigned to a given input value. To choose a label for an input value, the naive Bayes\n",
      "6.5  Naive Bayes Classifiers | 245...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 266, 'page_label': '245', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4627}\n",
      "\n",
      "--- Chunk 4628 ---\n",
      "Content:\n",
      "classifier begins by calculating the prior probability of each label, which is determined\n",
      "by checking the frequency of each label in the training set. The contribution from each\n",
      "feature is then combined with this prior probability, to arrive at a likelihood estimate\n",
      "for each label. The label whose likelihood estimate is the highest is then assigned to the\n",
      "input value. Figure 6-6 illustrates this process.\n",
      "Figure 6-6. An abstract illustration of the procedure used by the naive Bayes classifier to choose the\n",
      "topic \n",
      "for a document. In the training corpus, most documents are automotive, so the classifier starts\n",
      "out at a point closer to the “automotive” label. But it then considers the effect of each feature. In this\n",
      "example, the input document contains the word dark, which is a weak indicator for murder mysteries,\n",
      "but it also contains the word football, which is a strong indicator for sports documents. After every\n",
      "feature has made its contribution, the classifier checks which label it is closest to, and assigns that\n",
      "label to the input.\n",
      "Individual features make their contribution to the overall decision by “voting against”\n",
      "labels that don’t occur with that feature very often. In particular, the likelihood score\n",
      "for each label is reduced by multiplying it by the probability that an input value with\n",
      "that label would have the feature. For example, if the word run occurs in 12% of the\n",
      "sports documents, 10% of the murder mystery documents, and 2% of the automotive\n",
      "documents, then the likelihood score for the sports label will be multiplied by 0.12, the\n",
      "likelihood score for the murder mystery label will be multiplied by 0.1, and the likeli-\n",
      "hood score for the automotive label will be multiplied by 0.02. The overall effect will\n",
      "be to reduce the score of the murder mystery label slightly more than the score of the\n",
      "sports label, and to significantly reduce the automotive label with respect to the other\n",
      "two labels. This process is illustrated in Figures 6-7 and 6-8.\n",
      "246 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 267, 'page_label': '246', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4628}\n",
      "\n",
      "--- Chunk 4629 ---\n",
      "Content:\n",
      "Figure 6-7. Calculating label likelihoods with naive Bayes. Naive Bayes begins by calculating the prior\n",
      "probability \n",
      "of each label, based on how frequently each label occurs in the training data. Every feature\n",
      "then contributes to the likelihood estimate for each label, by multiplying it by the probability that\n",
      "input values with that label will have that feature. The resulting likelihood score can be thought of as\n",
      "an estimate of the probability that a randomly selected value from the training set would have both\n",
      "the given label and the set of features, assuming that the feature probabilities are all independent.\n",
      "Figure 6-8. A Bayesian Network Graph illustrating the generative process that is assumed by the naive\n",
      "Bayes \n",
      "classifier. To generate a labeled input, the model first chooses a label for the input, and then it\n",
      "generates each of the input’s features based on that label. Every feature is assumed to be entirely\n",
      "independent of every other feature, given the label.\n",
      "Underlying Probabilistic Model\n",
      "Another way of understanding the naive Bayes classifier is that it chooses the most likely\n",
      "label for an input, under the assumption that every input value is generated by first\n",
      "choosing a class label for that input value, and then generating each feature, entirely\n",
      "independent of every other feature. Of course, this assumption is unrealistic; features\n",
      "are often highly dependent on one another. We’ll return to some of the consequences\n",
      "of this assumption at the end of this section. This simplifying assumption, known as\n",
      "the naive Bayes assumption (or independence assumption), makes it much easier\n",
      "6.5  Naive Bayes Classifiers | 247...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 268, 'page_label': '247', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4629}\n",
      "\n",
      "--- Chunk 4630 ---\n",
      "Content:\n",
      "to combine the contributions of the different features, since we don’t need to worry\n",
      "about how they should interact with one another.\n",
      "Based \n",
      "on this assumption, we can calculate an expression for P(label|features), the\n",
      "probability that an input will have a particular label given that it has a particular set of\n",
      "features. To choose a label for a new input, we can then simply pick the label l that\n",
      "maximizes P(l|features).\n",
      "To begin, we note that P(label|features) is equal to the probability that an input has a\n",
      "particular label and the specified set of features, divided by the probability that it has\n",
      "the specified set of features:\n",
      "(2) P(label|features) = P(features, label)/P(features)\n",
      "Next, we note that P(features) will be the same for every choice of label, so if we are\n",
      "simply interested in finding the most likely label, it suffices to calculate P(features,\n",
      "label), which we’ll call the label likelihood.\n",
      "If we want to generate a probability estimate for each label, rather than\n",
      "just \n",
      "choosing the most likely label, then the easiest way to compute\n",
      "P(features) is to simply calculate the sum over labels of P(features, label):\n",
      "(3) P(features) = Σlabel ∈ labels P(features, label)\n",
      "The label likelihood can be expanded out as the probability of the label times the prob-\n",
      "ability of the features given the label:\n",
      "(4) P(features, label) = P(label) × P(features|label)\n",
      "Furthermore, since the features are all independent of one another (given the label), we\n",
      "can separate out the probability of each individual feature:\n",
      "(5) P(features, label) = P(label) × ⊓f ∈ featuresP(f|label)\n",
      "This is exactly the equation we discussed earlier for calculating the label likelihood:\n",
      "P(label) is the prior probability for a given label, and each P(f|label) is the contribution\n",
      "of a single feature to the label likelihood.\n",
      "Zero Counts and Smoothing\n",
      "The simplest way to calculate P(f|label), the contribution of a feature f toward the label\n",
      "likelihood for a label label, is to take the percentage of training instances with the given\n",
      "label that also have the given feature:\n",
      "(6) P(f|label) = count(f, label)/count(label)\n",
      "248 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 269, 'page_label': '248', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4630}\n",
      "\n",
      "--- Chunk 4631 ---\n",
      "Content:\n",
      "However, this simple approach can become problematic when a feature never occurs\n",
      "with \n",
      "a given label in the training set. In this case, our calculated value for P(f|label) will\n",
      "be zero, which will cause the label likelihood for the given label to be zero. Thus, the\n",
      "input will never be assigned this label, regardless of how well the other features fit the\n",
      "label.\n",
      "The basic problem here is with our calculation of P(f|label), the probability that an\n",
      "input will have a feature, given a label. In particular, just because we haven’t seen a\n",
      "feature/label combination occur in the training set, doesn’t mean it’s impossible for\n",
      "that combination to occur. For example, we may not have seen any murder mystery\n",
      "documents that contained the word football, but we wouldn’t want to conclude that\n",
      "it’s completely impossible for such documents to exist.\n",
      "Thus, although count(f,label)/count(label) is a good estimate for P(f|label) when count(f,\n",
      "label) is relatively high, this estimate becomes less reliable when count(f) becomes\n",
      "smaller. Therefore, when building naive Bayes models, we usually employ more so-\n",
      "phisticated techniques, known as smoothing techniques, for calculating P(f|label), the\n",
      "probability of a feature given a label. For example, the Expected Likelihood Estima-\n",
      "tion for the probability of a feature given a label basically adds 0.5 to each\n",
      "count(f,label) value, and the Heldout Estimation uses a heldout corpus to calculate\n",
      "the relationship between feature frequencies and feature probabilities. The nltk.prob\n",
      "ability module provides support for a wide variety of smoothing techniques.\n",
      "Non-Binary Features\n",
      "We have assumed here that each feature is binary, i.e., that each input either has a\n",
      "feature or does not. Label-valued features (e.g., a color feature, which could be red,\n",
      "green, blue, white, or orange) can be converted to binary features by replacing them\n",
      "with binary features, such as “color-is-red”. Numeric features can be converted to bi-\n",
      "nary features by binning, which replaces them with features such as “4<x<6.”\n",
      "Another alternative is to use regression methods to model the probabilities of numeric\n",
      "features. For example, if we assume that the height feature has a bell curve distribution,\n",
      "then we could estimate P(height|label) by finding the mean and variance of the heights\n",
      "of the inputs with each label. In this case, P(f=v|label) would not be a fixed value, but\n",
      "would vary depending on the value of v.\n",
      "The Naivete of Independence\n",
      "The reason that naive Bayes classifiers are called “naive” is that it’s unreasonable to\n",
      "assume that all features are independent of one another (given the label). In particular,\n",
      "almost all real-world problems contain features with varying degrees of dependence on\n",
      "one another. If we had to avoid any features that were dependent on one another, it\n",
      "would be very difficult to construct good feature sets that provide the required infor-\n",
      "mation to the machine learning algorithm.\n",
      "6.5  Naive Bayes Classifiers | 249...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 270, 'page_label': '249', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4631}\n",
      "\n",
      "--- Chunk 4632 ---\n",
      "Content:\n",
      "So what happens when we ignore the independence assumption, and use the naive\n",
      "Bayes \n",
      "classifier with features that are not independent? One problem that arises is that\n",
      "the classifier can end up “double-counting” the effect of highly correlated features,\n",
      "pushing the classifier closer to a given label than is justified.\n",
      "To see how this can occur, consider a name gender classifier that contains two identical\n",
      "features, f1 and f2. In other words, f2 is an exact copy of f1, and contains no new infor-\n",
      "mation. When the classifier is considering an input, it will include the contribution of\n",
      "both f1 and f2 when deciding which label to choose. Thus, the information content of\n",
      "these two features will be given more weight than it deserves.\n",
      "Of course, we don’t usually build naive Bayes classifiers that contain two identical\n",
      "features. However, we do build classifiers that contain features which are dependent\n",
      "on one another. For example, the features ends-with(a) and ends-with(vowel) are de-\n",
      "pendent on one another, because if an input value has the first feature, then it must\n",
      "also have the second feature. For features like these, the duplicated information may\n",
      "be given more weight than is justified by the training set.\n",
      "The Cause of Double-Counting\n",
      "The reason for the double-counting problem is that during training, feature contribu-\n",
      "tions are computed separately; but when using the classifier to choose labels for new\n",
      "inputs, those feature contributions are combined. One solution, therefore, is to con-\n",
      "sider the possible interactions between feature contributions during training. We could\n",
      "then use those interactions to adjust the contributions that individual features make.\n",
      "To make this more precise, we can rewrite the equation used to calculate the likelihood\n",
      "of a label, separating out the contribution made by each feature (or label):\n",
      "(7) P(features, label) = w[label] × ⊓f ∈ features w[f, label]\n",
      "Here, w[label] is the “starting score” for a given label, and w[f, label] is the contribution\n",
      "made by a given feature towards a label’s likelihood. We call these values w[label] and\n",
      "w[f, label] the parameters or weights for the model. Using the naive Bayes algorithm,\n",
      "we set each of these parameters independently:\n",
      "(8) w[label] = P(label)\n",
      "(9) w[f, label] = P(f|label)\n",
      "However, in the next section, we’ll look at a classifier that considers the possible in-\n",
      "teractions between these parameters when choosing their values.\n",
      "6.6  Maximum Entropy Classifiers\n",
      "The Maximum Entropy classifier uses a model that is very similar to the model em-\n",
      "ployed by the naive Bayes classifier. But rather than using probabilities to set the\n",
      "250 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 271, 'page_label': '250', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4632}\n",
      "\n",
      "--- Chunk 4633 ---\n",
      "Content:\n",
      "model’s parameters, it uses search techniques to find a set of parameters that will max-\n",
      "imize the performance of the classifier. In particular, it looks for the set of parameters\n",
      "that maximizes the total likelihood of the training corpus, which is defined as:\n",
      "(10) P(features) = Σx ∈ corpus P(label(x)|features(x))\n",
      "Where P(label|features), the probability that an input whose features are features will\n",
      "have class label label, is defined as:\n",
      "(11) P(label|features) = P(label, features)/Σlabel P(label, features)\n",
      "Because of the potentially complex interactions between the effects of related features,\n",
      "there is no way to directly calculate the model parameters that maximize the likelihood\n",
      "of the training set. Therefore, Maximum Entropy classifiers choose the model param-\n",
      "eters using iterative optimization techniques, which initialize the model’s parameters\n",
      "to random values, and then repeatedly refine those parameters to bring them closer to\n",
      "the optimal solution. These iterative optimization techniques guarantee that each re-\n",
      "finement of the parameters will bring them closer to the optimal values, but do not\n",
      "necessarily provide a means of determining when those optimal values have been\n",
      "reached. Because the parameters for Maximum Entropy classifiers are selected using\n",
      "iterative optimization techniques, they can take a long time to learn. This is especially\n",
      "true when the size of the training set, the number of features, and the number of labels\n",
      "are all large.\n",
      "Some iterative optimization techniques are much faster than others.\n",
      "When \n",
      "training Maximum Entropy models, avoid the use of Generalized\n",
      "Iterative Scaling (GIS) or Improved Iterative Scaling (IIS), which are both\n",
      "considerably slower than the Conjugate Gradient (CG) and the BFGS\n",
      "optimization methods.\n",
      "The Maximum Entropy Model\n",
      "The Maximum Entropy classifier model is a generalization of the model used by the\n",
      "naive Bayes classifier. Like the naive Bayes model, the Maximum Entropy classifier\n",
      "calculates the likelihood of each label for a given input value by multiplying together\n",
      "the parameters that are applicable for the input value and label. The naive Bayes clas-\n",
      "sifier model defines a parameter for each label, specifying its prior probability, and a\n",
      "parameter for each (feature, label) pair, specifying the contribution of individual fea-\n",
      "tures toward a label’s likelihood.\n",
      "In contrast, the Maximum Entropy classifier model leaves it up to the user to decide\n",
      "what combinations of labels and features should receive their own parameters. In par-\n",
      "ticular, it is possible to use a single parameter to associate a feature with more than one\n",
      "label; or to associate more than one feature with a given label. This will sometimes\n",
      "6.6  Maximum Entropy Classifiers | 251...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 272, 'page_label': '251', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4633}\n",
      "\n",
      "--- Chunk 4634 ---\n",
      "Content:\n",
      "allow the model to “generalize” over some of the differences between related labels or\n",
      "features.\n",
      "Each \n",
      "combination of labels and features that receives its own parameter is called a\n",
      "joint-feature. Note that joint-features are properties of labeled values, whereas (sim-\n",
      "ple) features are properties of unlabeled values.\n",
      "In literature that describes and discusses Maximum Entropy models,\n",
      "the \n",
      "term “features” often refers to joint-features; the term “contexts”\n",
      "refers to what we have been calling (simple) features.\n",
      "Typically, the joint-features that are used to construct Maximum Entropy models ex-\n",
      "actly mirror those that are used by the naive Bayes model. In particular, a joint-feature\n",
      "is defined for each label, corresponding to w[label], and for each combination of (sim-\n",
      "ple) feature and label, corresponding to w[f, label]. Given the joint-features for a Max-\n",
      "imum Entropy model, the score assigned to a label for a given input is simply the\n",
      "product of the parameters associated with the joint-features that apply to that input\n",
      "and label:\n",
      "(12) P(input, label) = ⊓joint-features(input,label)w[joint-feature]\n",
      "Maximizing Entropy\n",
      "The intuition that motivates Maximum Entropy classification is that we should build\n",
      "a model that captures the frequencies of individual joint-features, without making any\n",
      "unwarranted assumptions. An example will help to illustrate this principle.\n",
      "Suppose we are assigned the task of picking the correct word sense for a given word,\n",
      "from a list of 10 possible senses (labeled A–J). At first, we are not told anything more\n",
      "about the word or the senses. There are many probability distributions that we could\n",
      "choose for the 10 senses, such as:\n",
      "A B C D E F G H I J\n",
      "(i) 10% 10% 10% 10% 10% 10% 10% 10% 10% 10%\n",
      "(ii) 5% 15% 0% 30% 0% 8% 12% 0% 6% 24%\n",
      "(iii) 0% 100% 0% 0% 0% 0% 0% 0% 0% 0%\n",
      "Although any of these distributions might be correct, we are likely to choose distribution\n",
      "(i), because without any more information, there is no reason to believe that any word\n",
      "sense is more likely than any other. On the other hand, distributions (ii) and (iii) reflect\n",
      "assumptions that are not supported by what we know.\n",
      "One way to capture this intuition that distribution (i) is more “fair” than the other two\n",
      "is to invoke the concept of entropy. In the discussion of decision trees, we described\n",
      "252 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 273, 'page_label': '252', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4634}\n",
      "\n",
      "--- Chunk 4635 ---\n",
      "Content:\n",
      "entropy as a measure of how “disorganized” a set of labels was. In particular, if a single\n",
      "label \n",
      "dominates then entropy is low, but if the labels are more evenly distributed then\n",
      "entropy is high. In our example, we chose distribution (i) because its label probabilities\n",
      "are evenly distributed—in other words, because its entropy is high. In general, the\n",
      "Maximum Entropy principle states that, among the distributions that are consistent\n",
      "with what we know, we should choose the distribution whose entropy is highest.\n",
      "Next, suppose that we are told that sense A appears 55% of the time. Once again, there\n",
      "are many distributions that are consistent with this new piece of information, such as:\n",
      "A B C D E F G H I J\n",
      "(iv) 55% 45% 0% 0% 0% 0% 0% 0% 0% 0%\n",
      "(v) 55% 5% 5% 5% 5% 5% 5% 5% 5% 5%\n",
      "(vi) 55% 3% 1% 2% 9% 5% 0% 25% 0% 0%\n",
      "But again, we will likely choose the distribution that makes the fewest unwarranted\n",
      "assumptions—in this case, distribution (v).\n",
      "Finally, \n",
      "suppose that we are told that the word up appears in the nearby context 10%\n",
      "of the time, and that when it does appear in the context there’s an 80% chance that\n",
      "sense A or C will be used. In this case, we will have a harder time coming up with an\n",
      "appropriate distribution by hand; however, we can verify that the following distribution\n",
      "looks appropriate:\n",
      " A B C D E F G H I J\n",
      "(vii) +up 5.1% 0.25% 2.9% 0.25% 0.25% 0.25% 0.25% 0.25% 0.25% 0.25%\n",
      "–up 49.9% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46%\n",
      "In particular, the distribution is consistent with what we know: if we add up the prob-\n",
      "abilities \n",
      "in column A, we get 55%; if we add up the probabilities of row 1, we get 10%;\n",
      "and if we add up the boxes for senses A and C in the +up row, we get 8% (or 80% of\n",
      "the +up cases). Furthermore, the remaining probabilities appear to be “evenly\n",
      "distributed.”\n",
      "Throughout this example, we have restricted ourselves to distributions that are con-\n",
      "sistent with what we know; among these, we chose the distribution with the highest\n",
      "entropy. This is exactly what the Maximum Entropy classifier does as well. In\n",
      "particular, for each joint-feature, the Maximum Entropy model calculates the “empir-\n",
      "ical frequency” of that feature—i.e., the frequency with which it occurs in the training\n",
      "set. It then searches for the distribution which maximizes entropy, while still predicting\n",
      "the correct frequency for each joint-feature.\n",
      "6.6  Maximum Entropy Classifiers | 253...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 274, 'page_label': '253', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4635}\n",
      "\n",
      "--- Chunk 4636 ---\n",
      "Content:\n",
      "Generative Versus Conditional Classifiers\n",
      "An \n",
      "important difference between the naive Bayes classifier and the Maximum Entropy\n",
      "classifier concerns the types of questions they can be used to answer. The naive Bayes\n",
      "classifier is an example of a generative classifier, which builds a model that predicts\n",
      "P(input, label), the joint probability of an ( input, label) pair. As a result, generative\n",
      "models can be used to answer the following questions:\n",
      "1. What is the most likely label for a given input?\n",
      "2. How likely is a given label for a given input?\n",
      "3. What is the most likely input value?\n",
      "4. How likely is a given input value?\n",
      "5. How likely is a given input value with a given label?\n",
      "6. What is the most likely label for an input that might have one of two values (but\n",
      "we don’t know which)?\n",
      "The Maximum Entropy classifier, on the other hand, is an example of a conditional\n",
      "classifier. Conditional classifiers build models that predict P(label|input)—the proba-\n",
      "bility of a label given the input value. Thus, conditional models can still be used to\n",
      "answer questions 1 and 2. However, conditional models cannot be used to answer the\n",
      "remaining questions 3–6.\n",
      "In general, generative models are strictly more powerful than conditional models, since\n",
      "we can calculate the conditional probability P(label|input) from the joint probability\n",
      "P(input, label), but not vice versa. However, this additional power comes at a price.\n",
      "Because the model is more powerful, it has more “free parameters” that need to be\n",
      "learned. However, the size of the training set is fixed. Thus, when using a more powerful\n",
      "model, we end up with less data that can be used to train each parameter’s value, making\n",
      "it harder to find the best parameter values. As a result, a generative model may not do\n",
      "as good a job at answering questions 1 and 2 as a conditional model, since the condi-\n",
      "tional model can focus its efforts on those two questions. However, if we do need\n",
      "answers to questions like 3–6, then we have no choice but to use a generative model.\n",
      "The difference between a generative model and a conditional model is analogous to the\n",
      "difference between a topographical map and a picture of a skyline. Although the topo-\n",
      "graphical map can be used to answer a wider variety of questions, it is significantly\n",
      "more difficult to generate an accurate topographical map than it is to generate an ac-\n",
      "curate skyline.\n",
      "6.7  Modeling Linguistic Patterns\n",
      "Classifiers can help us to understand the linguistic patterns that occur in natural lan-\n",
      "guage, by allowing us to create explicit models that capture those patterns. Typically,\n",
      "these models are using supervised classification techniques, but it is also possible to\n",
      "254 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 275, 'page_label': '254', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4636}\n",
      "\n",
      "--- Chunk 4637 ---\n",
      "Content:\n",
      "build analytically motivated models. Either way, these explicit models serve two im-\n",
      "portant \n",
      "purposes: they help us to understand linguistic patterns, and they can be used\n",
      "to make predictions about new language data.\n",
      "The extent to which explicit models can give us insights into linguistic patterns depends\n",
      "largely on what kind of model is used. Some models, such as decision trees, are relatively\n",
      "transparent, and give us direct information about which factors are important in mak-\n",
      "ing decisions and about which factors are related to one another. Other models, such\n",
      "as multilevel neural networks, are much more opaque. Although it can be possible to\n",
      "gain insight by studying them, it typically takes a lot more work.\n",
      "But all explicit models can make predictions about new unseen language data that was\n",
      "not included in the corpus used to build the model. These predictions can be evaluated\n",
      "to assess the accuracy of the model. Once a model is deemed sufficiently accurate, it\n",
      "can then be used to automatically predict information about new language data. These\n",
      "predictive models can be combined into systems that perform many useful language\n",
      "processing tasks, such as document classification, automatic translation, and question\n",
      "answering.\n",
      "What Do Models Tell Us?\n",
      "It’s important to understand what we can learn about language from an automatically\n",
      "constructed model. One important consideration when dealing with models of lan-\n",
      "guage is the distinction between descriptive models and explanatory models. Descrip-\n",
      "tive models capture patterns in the data, but they don’t provide any information about\n",
      "why the data contains those patterns. For example, as we saw in Table 3-1, the syno-\n",
      "nyms absolutely and definitely are not interchangeable: we say absolutely adore not\n",
      "definitely adore, and definitely prefer, not absolutely prefer. In contrast, explanatory\n",
      "models attempt to capture properties and relationships that cause the linguistic pat-\n",
      "terns. For example, we might introduce the abstract concept of “polar adjective” as an\n",
      "adjective that has an extreme meaning, and categorize some adjectives, such as adore\n",
      "and detest as polar. Our explanatory model would contain the constraint that abso-\n",
      "lutely can combine only with polar adjectives, and definitely can only combine with\n",
      "non-polar adjectives. In summary, descriptive models provide information about cor-\n",
      "relations in the data, while explanatory models go further to postulate causal\n",
      "relationships.\n",
      "Most models that are automatically constructed from a corpus are descriptive models;\n",
      "in other words, they can tell us what features are relevant to a given pattern or con-\n",
      "struction, but they can’t necessarily tell us how those features and patterns relate to\n",
      "one another. If our goal is to understand the linguistic patterns, then we can use this\n",
      "information about which features are related as a starting point for further experiments\n",
      "designed to tease apart the relationships between features and patterns. On the other\n",
      "hand, if we’re just interested in using the model to make predictions (e.g., as part of a\n",
      "language processing system), then we can use the model to make predictions about\n",
      "new data without worrying about the details of underlying causal relationships.\n",
      "6.7  Modeling Linguistic Patterns | 255...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 276, 'page_label': '255', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4637}\n",
      "\n",
      "--- Chunk 4638 ---\n",
      "Content:\n",
      "6.8  Summary\n",
      "• Modeling \n",
      "the linguistic data found in corpora can help us to understand linguistic\n",
      "patterns, and can be used to make predictions about new language data.\n",
      "• Supervised classifiers use labeled training corpora to build models that predict the\n",
      "label of an input based on specific features of that input.\n",
      "• Supervised classifiers can perform a wide variety of NLP tasks, including document\n",
      "classification, part-of-speech tagging, sentence segmentation, dialogue act type\n",
      "identification, and determining entailment relations, and many other tasks.\n",
      "• When training a supervised classifier, you should split your corpus into three da-\n",
      "tasets: a training set for building the classifier model, a dev-test set for helping select\n",
      "and tune the model’s features, and a test set for evaluating the final model’s\n",
      "performance.\n",
      "• When evaluating a supervised classifier, it is important that you use fresh data that\n",
      "was not included in the training or dev-test set. Otherwise, your evaluation results\n",
      "may be unrealistically optimistic.\n",
      "• Decision trees are automatically constructed tree-structured flowcharts that are\n",
      "used to assign labels to input values based on their features. Although they’re easy\n",
      "to interpret, they are not very good at handling cases where feature values interact\n",
      "in determining the proper label.\n",
      "• In naive Bayes classifiers, each feature independently contributes to the decision\n",
      "of which label should be used. This allows feature values to interact, but can be\n",
      "problematic when two or more features are highly correlated with one another.\n",
      "• Maximum Entropy classifiers use a basic model that is similar to the model used\n",
      "by naive Bayes; however, they employ iterative optimization to find the set of fea-\n",
      "ture weights that maximizes the probability of the training set.\n",
      "• Most of the models that are automatically constructed from a corpus are descrip-\n",
      "tive, that is, they let us know which features are relevant to a given pattern or\n",
      "construction, but they don’t give any information about causal relationships be-\n",
      "tween those features and patterns.\n",
      "6.9  Further Reading\n",
      "Please consult http://www.nltk.org/ for further materials on this chapter and on how to\n",
      "install external machine learning packages, such as Weka, Mallet, TADM, and MegaM.\n",
      "For more examples of classification and machine learning with NLTK, please see the\n",
      "classification HOWTOs at http://www.nltk.org/howto.\n",
      "For a general introduction to machine learning, we recommend (Alpaydin, 2004). For\n",
      "a more mathematically intense introduction to the theory of machine learning, see\n",
      "(Hastie, Tibshirani & Friedman, 2009). Excellent books on using machine learning\n",
      "256 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 277, 'page_label': '256', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4638}\n",
      "\n",
      "--- Chunk 4639 ---\n",
      "Content:\n",
      "techniques for NLP include (Abney, 2008), (Daelemans & Bosch, 2005), (Feldman &\n",
      "Sanger, \n",
      "2007), (Segaran, 2007), and (Weiss et al., 2004). For more on smoothing tech-\n",
      "niques for language problems, see (Manning & Schütze, 1999). For more on sequence\n",
      "modeling, and especially hidden Markov models, see (Manning & Schütze, 1999) or\n",
      "(Jurafsky & Martin, 2008). Chapter 13 of (Manning, Raghavan & Schütze, 2008) dis-\n",
      "cusses the use of naive Bayes for classifying texts.\n",
      "Many of the machine learning algorithms discussed in this chapter are numerically\n",
      "intensive, and as a result, they will run slowly when coded naively in Python. For in-\n",
      "formation on increasing the efficiency of numerically intensive algorithms in Python,\n",
      "see (Kiusalaas, 2005).\n",
      "The classification techniques described in this chapter can be applied to a very wide\n",
      "variety of problems. For example, (Agirre & Edmonds, 2007) uses classifiers to perform\n",
      "word-sense disambiguation; and (Melamed, 2001) uses classifiers to create parallel\n",
      "texts. Recent textbooks that cover text classification include (Manning, Raghavan &\n",
      "Schütze, 2008) and (Croft, Metzler & Strohman, 2009).\n",
      "Much of the current research in the application of machine learning techniques to NLP\n",
      "problems is driven by government-sponsored “challenges,” where a set of research\n",
      "organizations are all provided with the same development corpus and asked to build a\n",
      "system, and the resulting systems are compared based on a reserved test set. Examples\n",
      "of these challenge competitions include CoNLL Shared Tasks, the Recognizing Textual\n",
      "Entailment competitions, the ACE competitions, and the AQUAINT competitions.\n",
      "Consult http://www.nltk.org/ for a list of pointers to the web pages for these challenges.\n",
      "6.10  Exercises\n",
      "1. ○ Read up on one of the language technologies mentioned in this section, such as\n",
      "word sense disambiguation, semantic role labeling, question answering, machine\n",
      "translation, or named entity recognition. Find out what type and quantity of an-\n",
      "notated data is required for developing such systems. Why do you think a large\n",
      "amount of data is required?\n",
      "2. ○ Using any of the three classifiers described in this chapter, and any features you\n",
      "can think of, build the best name gender classifier you can. Begin by splitting the\n",
      "Names Corpus into three subsets: 500 words for the test set, 500 words for the\n",
      "dev-test set, and the remaining 6,900 words for the training set. Then, starting with\n",
      "the example name gender classifier, make incremental improvements. Use the dev-\n",
      "test set to check your progress. Once you are satisfied with your classifier, check\n",
      "its final performance on the test set. How does the performance on the test set\n",
      "compare to the performance on the dev-test set? Is this what you’d expect?\n",
      "3. ○ The Senseval 2 Corpus contains data intended to train word-sense disambigua-\n",
      "tion classifiers. It contains data for four words: hard, interest, line, and serve.\n",
      "Choose one of these four words, and load the corresponding data:\n",
      "6.10  Exercises | 257...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 278, 'page_label': '257', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4639}\n",
      "\n",
      "--- Chunk 4640 ---\n",
      "Content:\n",
      ">>> from nltk.corpus import senseval\n",
      ">>> instances = senseval.instances('hard.pos')\n",
      ">>> size = int(len(instances) * 0.1)\n",
      ">>> train_set, test_set = instances[size:], instances[:size]\n",
      "Using \n",
      "this dataset, build a classifier that predicts the correct sense tag for a given\n",
      "instance. See the corpus HOWTO at http://www.nltk.org/howto for information\n",
      "on using the instance objects returned by the Senseval 2 Corpus.\n",
      "4. ○ Using the movie review document classifier discussed in this chapter, generate\n",
      "a list of the 30 features that the classifier finds to be most informative. Can you\n",
      "explain why these particular features are informative? Do you find any of them\n",
      "surprising?\n",
      "5. ○ Select one of the classification tasks described in this chapter, such as name\n",
      "gender detection, document classification, part-of-speech tagging, or dialogue act\n",
      "classification. Using the same training and test data, and the same feature extractor,\n",
      "build three classifiers for the task: a decision tree, a naive Bayes classifier, and a\n",
      "Maximum Entropy classifier. Compare the performance of the three classifiers on\n",
      "your selected task. How do you think that your results might be different if you\n",
      "used a different feature extractor?\n",
      "6. ○ The synonyms strong and powerful pattern differently (try combining them with\n",
      "chip and sales). What features are relevant in this distinction? Build a classifier that\n",
      "predicts when each word should be used.\n",
      "7. ◑ The dialogue act classifier assigns labels to individual posts, without considering\n",
      "the context in which the post is found. However, dialogue acts are highly depend-\n",
      "ent on context, and some sequences of dialogue act are much more likely than\n",
      "others. For example, a ynQuestion dialogue act is much more likely to be answered\n",
      "by a yanswer than by a greeting. Make use of this fact to build a consecutive clas-\n",
      "sifier for labeling dialogue acts. Be sure to consider what features might be useful.\n",
      "See the code for the consecutive classifier for part-of-speech tags in Example 6-5\n",
      "to get some ideas.\n",
      "8. ◑ Word features can be very useful for performing document classification, since\n",
      "the words that appear in a document give a strong indication about what its se-\n",
      "mantic content is. However, many words occur very infrequently, and some of the\n",
      "most informative words in a document may never have occurred in our training\n",
      "data. One solution is to make use of a lexicon, which describes how different words\n",
      "relate to one another. Using the WordNet lexicon, augment the movie review\n",
      "document classifier presented in this chapter to use features that generalize the\n",
      "words that appear in a document, making it more likely that they will match words\n",
      "found in the training data.\n",
      "9. ● The PP Attachment Corpus is a corpus describing prepositional phrase attach-\n",
      "ment decisions. Each instance in the corpus is encoded as a PPAttachment object:\n",
      "258 | Chapter 6:  Learning to Classify Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 279, 'page_label': '258', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4640}\n",
      "\n",
      "--- Chunk 4641 ---\n",
      "Content:\n",
      ">>> from nltk.corpus import ppattach\n",
      ">>> ppattach.attachments('training') \n",
      "[PPAttachment(sent='0', verb='join', noun1='board',\n",
      "              prep='as', noun2='director', attachment='V'),\n",
      " PPAttachment(sent='1', verb='is', noun1='chairman',\n",
      "              prep='of', noun2='N.V.', attachment='N'),\n",
      " ...]\n",
      ">>> inst = ppattach.attachments('training')[1]\n",
      ">>> (inst.noun1, inst.prep, inst.noun2)\n",
      "('chairman', 'of', 'N.V.')\n",
      "Select only the instances where inst.attachment is N:\n",
      ">>> nattach = [inst for inst in ppattach.attachments('training')\n",
      "...            if inst.attachment == 'N']\n",
      "Using \n",
      "this subcorpus, build a classifier that attempts to predict which preposition\n",
      "is used to connect a given pair of nouns. For example, given the pair of nouns\n",
      "team and researchers, the classifier should predict the preposition of. See the corpus\n",
      "HOWTO at http://www.nltk.org/howto for more information on using the PP At-\n",
      "tachment Corpus.\n",
      "10. ● Suppose you wanted to automatically generate a prose description of a scene,\n",
      "and already had a word to uniquely describe each entity, such as the book, and\n",
      "simply wanted to decide whether to use in or on in relating various items, e.g., the\n",
      "book is in the cupboard versus the book is on the shelf. Explore this issue by looking\n",
      "at corpus data and writing programs as needed. Consider the following examples:\n",
      "(13) a. in the car versus on the train\n",
      "b. in town versus on campus\n",
      "c. in the picture versus on the screen\n",
      "d. in Macbeth versus on Letterman\n",
      "6.10  Exercises | 259...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 280, 'page_label': '259', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4641}\n",
      "\n",
      "--- Chunk 4642 ---\n",
      "Content:\n",
      "CHAPTER 7\n",
      "Extracting Information from Text\n",
      "For any given question, it’s likely that someone has written the answer down some-\n",
      "where. \n",
      "The amount of natural language text that is available in electronic form is truly\n",
      "staggering, and is increasing every day. However, the complexity of natural language\n",
      "can make it very difficult to access the information in that text. The state of the art in\n",
      "NLP is still a long way from being able to build general-purpose representations of\n",
      "meaning from unrestricted text. If we instead focus our efforts on a limited set of ques-\n",
      "tions or “entity relations,” such as “where are different facilities located” or “who is\n",
      "employed by what company,” we can make significant progress. The goal of this chap-\n",
      "ter is to answer the following questions:\n",
      "1. How can we build a system that extracts structured data from unstructured text?\n",
      "2. What are some robust methods for identifying the entities and relationships de-\n",
      "scribed in a text?\n",
      "3. Which corpora are appropriate for this work, and how do we use them for training\n",
      "and evaluating our models?\n",
      "Along the way, we’ll apply techniques from the last two chapters to the problems of\n",
      "chunking and named entity recognition.\n",
      "7.1  Information Extraction\n",
      "Information comes in many shapes and sizes. One important form is structured\n",
      "data, where there is a regular and predictable organization of entities and relationships.\n",
      "For example, we might be interested in the relation between companies and locations.\n",
      "Given a particular company, we would like to be able to identify the locations where\n",
      "it does business; conversely, given a location, we would like to discover which com-\n",
      "panies do business in that location. If our data is in tabular form, such as the example\n",
      "in Table 7-1, then answering these queries is straightforward.\n",
      "261...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 282, 'page_label': '261', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4642}\n",
      "\n",
      "--- Chunk 4643 ---\n",
      "Content:\n",
      "Table 7-1. Locations data\n",
      "OrgName LocationName\n",
      "Omnicom New York\n",
      "DDB Needham New York\n",
      "Kaplan Thaler Group New York\n",
      "BBDO South Atlanta\n",
      "Georgia-Pacific\n",
      "Atlanta\n",
      "If this location data was stored in Python as a list of tuples (entity, relation,\n",
      "entity), \n",
      "then the question “Which organizations operate in Atlanta?” could be trans-\n",
      "lated as follows:\n",
      ">>> print [org for (e1, rel, e2) if rel=='IN' and e2=='Atlanta'] \n",
      "['BBDO South', 'Georgia-Pacific']\n",
      "Things are more tricky if we try to get similar information out of text. For example,\n",
      "consider the following snippet (from nltk.corpus.ieer, for fileid NYT19980315.0085).\n",
      "(1) The fourth Wells account moving to another agency is the packaged paper-\n",
      "products division of Georgia-Pacific Corp., which arrived at Wells only last fall.\n",
      "Like Hertz and the History Channel, it is also leaving for an Omnicom-owned\n",
      "agency, the BBDO South unit of BBDO Worldwide. BBDO South in Atlanta,\n",
      "which handles corporate advertising for Georgia-Pacific, will assume additional\n",
      "duties for brands like Angel Soft toilet tissue and Sparkle paper towels, said\n",
      "Ken Haldin, a spokesman for Georgia-Pacific in Atlanta.\n",
      "If you read through (1), you will glean the information required to answer the example\n",
      "question. But how do we get a machine to understand enough about (1) to return the\n",
      "list ['BBDO South', 'Georgia-Pacific'] as an answer? This is obviously a much harder\n",
      "task. Unlike Table 7-1, (1) contains no structure that links organization names with\n",
      "location names.\n",
      "One approach to this problem involves building a very general representation of mean-\n",
      "ing (Chapter 10). In this chapter we take a different approach, deciding in advance that\n",
      "we will only look for very specific kinds of information in text, such as the relation\n",
      "between organizations and locations. Rather than trying to use text like (1) to answer\n",
      "the question directly, we first convert the unstructured data of natural language sen-\n",
      "tences into the structured data of Table 7-1. Then we reap the benefits of powerful\n",
      "query tools such as SQL. This method of getting meaning from text is called Infor-\n",
      "mation Extraction.\n",
      "Information Extraction has many applications, including business intelligence, resume\n",
      "harvesting, media analysis, sentiment detection, patent search, and email scanning. A\n",
      "particularly important area of current research involves the attempt to extract\n",
      "262 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 283, 'page_label': '262', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4643}\n",
      "\n",
      "--- Chunk 4644 ---\n",
      "Content:\n",
      "structured data out of electronically available scientific literature, especially in the do-\n",
      "main of biology and medicine.\n",
      "Information Extraction Architecture\n",
      "Figure 7-1 shows the architecture for a simple information extraction system. It begins\n",
      "by processing a document using several of the procedures discussed in Chapters 3 and\n",
      "5: first, the raw text of the document is split into sentences using a sentence segmenter,\n",
      "and each sentence is further subdivided into words using a tokenizer. Next, each sen-\n",
      "tence is tagged with part-of-speech tags, which will prove very helpful in the next step,\n",
      "named entity recognition. In this step, we search for mentions of potentially inter-\n",
      "esting entities in each sentence. Finally, we use relation recognition to search for likely\n",
      "relations between different entities in the text.\n",
      "Figure 7-1. Simple pipeline architecture for an information extraction system. This system takes the\n",
      "raw \n",
      "text of a document as its input, and generates a list of ( entity, relation, entity) tuples as its\n",
      "output. For example, given a document that indicates that the company Georgia-Pacific is located in\n",
      "Atlanta, it might generate the tuple ([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']).\n",
      "To perform the first three tasks, we can define a function that simply connects together\n",
      "NLTK’s default sentence segmenter \n",
      ", word tokenizer \n",
      " , and part-of-speech\n",
      "tagger \n",
      " :\n",
      ">>> def ie_preprocess(document):\n",
      "...    sentences = nltk.sent_tokenize(document) \n",
      "...    sentences = [nltk.word_tokenize(sent) for sent in sentences] \n",
      "...    sentences = [nltk.pos_tag(sent) for sent in sentences] \n",
      "7.1  Information Extraction | 263...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 284, 'page_label': '263', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4644}\n",
      "\n",
      "--- Chunk 4645 ---\n",
      "Content:\n",
      "Remember that our program samples assume you begin your interactive\n",
      "session or your program with import nltk, re, pprint.\n",
      "Next, \n",
      "in named entity recognition, we segment and label the entities that might par-\n",
      "ticipate in interesting relations with one another. Typically, these will be definite noun\n",
      "phrases such as the knights who say “ni” , or proper names such as Monty Python. In\n",
      "some tasks it is useful to also consider indefinite nouns or noun chunks, such as every\n",
      "student or cats, and these do not necessarily refer to entities in the same way as definite\n",
      "NPs and proper names.\n",
      "Finally, in relation extraction, we search for specific patterns between pairs of entities\n",
      "that occur near one another in the text, and use those patterns to build tuples recording\n",
      "the relationships between the entities.\n",
      "7.2  Chunking\n",
      "The basic technique we will use for entity recognition is chunking, which segments\n",
      "and labels multitoken sequences as illustrated in Figure 7-2. The smaller boxes show\n",
      "the word-level tokenization and part-of-speech tagging, while the large boxes show\n",
      "higher-level chunking. Each of these larger boxes is called a chunk. Like tokenization,\n",
      "which omits whitespace, chunking usually selects a subset of the tokens. Also like\n",
      "tokenization, the pieces produced by a chunker do not overlap in the source text.\n",
      "In this section, we will explore chunking in some depth, beginning with the definition\n",
      "and representation of chunks. We will see regular expression and n-gram approaches\n",
      "to chunking, and will develop and evaluate chunkers using the CoNLL-2000 Chunking\n",
      "Corpus. We will then return in Sections 7.5 and 7.6 to the tasks of named entity rec-\n",
      "ognition and relation extraction.\n",
      "Noun Phrase Chunking\n",
      "We will begin by considering the task of noun phrase chunking, or NP-chunking,\n",
      "where we search for chunks corresponding to individual noun phrases. For example,\n",
      "here is some Wall Street Journal text with NP-chunks marked using brackets:\n",
      "Figure 7-2. Segmentation and labeling at both the Token and Chunk levels.\n",
      "264 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 285, 'page_label': '264', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4645}\n",
      "\n",
      "--- Chunk 4646 ---\n",
      "Content:\n",
      "(2) [ The/DT market/NN ] for/IN [ system-management/NN software/NN ] for/\n",
      "IN \n",
      "[ Digital/NNP ] [ ’s/POS hardware/NN ] is/VBZ fragmented/JJ enough/RB\n",
      "that/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP Associates/NNPS ]\n",
      "should/MD do/VB well/RB there/RB ./.\n",
      "As we can see, NP-chunks are often smaller pieces than complete noun phrases. For\n",
      "example, the market for system-management software for Digital’s hardware is a single\n",
      "noun phrase (containing two nested noun phrases), but it is captured in NP-chunks by\n",
      "the simpler chunk the market. One of the motivations for this difference is that NP-\n",
      "chunks are defined so as not to contain other NP-chunks. Consequently, any preposi-\n",
      "tional phrases or subordinate clauses that modify a nominal will not be included in the\n",
      "corresponding NP-chunk, since they almost certainly contain further noun phrases.\n",
      "One of the most useful sources of information for NP-chunking is part-of-speech tags.\n",
      "This is one of the motivations for performing part-of-speech tagging in our information\n",
      "extraction system. We demonstrate this approach using an example sentence that has\n",
      "been part-of-speech tagged in Example 7-1. In order to create an NP-chunker, we will\n",
      "first define a chunk grammar, consisting of rules that indicate how sentences should\n",
      "be chunked. In this case, we will define a simple grammar with a single regular\n",
      "expression rule \n",
      ". This rule says that an NP chunk should be formed whenever the\n",
      "chunker \n",
      "finds an optional determiner ( DT) followed by any number of adjectives ( JJ)\n",
      "and then a noun (NN). Using this grammar, we create a chunk parser \n",
      " , and test it on\n",
      "our \n",
      "example sentence \n",
      " . The result is a tree, which we can either print \n",
      " , or display\n",
      "graphically \n",
      " .\n",
      "Example 7-1. Example of a simple regular expression–based NP chunker.\n",
      ">>> sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), \n",
      "... (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
      ">>> grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n",
      ">>> cp = nltk.RegexpParser(grammar) \n",
      ">>> result = cp.parse(sentence) \n",
      ">>> print result \n",
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n",
      ">>> result.draw() \n",
      "7.2  Chunking | 265...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 286, 'page_label': '265', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4646}\n",
      "\n",
      "--- Chunk 4647 ---\n",
      "Content:\n",
      "Tag Patterns\n",
      "The \n",
      "rules that make up a chunk grammar use tag patterns to describe sequences of\n",
      "tagged words. A tag pattern is a sequence of part-of-speech tags delimited using angle\n",
      "brackets, e.g.,<DT>?<JJ>*<NN>. Tag patterns are similar to regular expression patterns\n",
      "(Section 3.4). Now, consider the following noun phrases from the Wall Street Journal:\n",
      "another/DT sharp/JJ dive/NN\n",
      "trade/NN figures/NNS\n",
      "any/DT new/JJ policy/NN measures/NNS\n",
      "earlier/JJR stages/NNS\n",
      "Panamanian/JJ dictator/NN Manuel/NNP Noriega/NNP\n",
      "We can match these noun phrases using a slight refinement of the first tag pattern\n",
      "above, i.e., <DT>?<JJ.*>*<NN.*>+. This will chunk any sequence of tokens beginning\n",
      "with an optional determiner, followed by zero or more adjectives of any type (including\n",
      "relative adjectives like earlier/JJR), followed by one or more nouns of any type. How-\n",
      "ever, it is easy to find many more complicated examples which this rule will not cover:\n",
      "his/PRP$ Mansion/NNP House/NNP speech/NN\n",
      "the/DT price/NN cutting/VBG\n",
      "3/CD %/NN to/TO 4/CD %/NN\n",
      "more/JJR than/IN 10/CD %/NN\n",
      "the/DT fastest/JJS developing/VBG trends/NNS\n",
      "'s/POS skill/NN\n",
      "Your Turn: Try to come up with tag patterns to cover these cases. Test\n",
      "them using the graphical interface nltk.app.chunkparser(). Continue\n",
      "to refine your tag patterns with the help of the feedback given by this\n",
      "tool.\n",
      "Chunking with Regular Expressions\n",
      "To find the chunk structure for a given sentence, the RegexpParser chunker begins with\n",
      "a flat structure in which no tokens are chunked. The chunking rules are applied in turn,\n",
      "successively updating the chunk structure. Once all of the rules have been invoked, the\n",
      "resulting chunk structure is returned.\n",
      "Example 7-2 shows a simple chunk grammar consisting of two rules. The first rule\n",
      "matches an optional determiner or possessive pronoun, zero or more adjectives, then\n",
      "266 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 287, 'page_label': '266', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4647}\n",
      "\n",
      "--- Chunk 4648 ---\n",
      "Content:\n",
      "a noun. The second rule matches one or more proper nouns. We also define an example\n",
      "sentence to be chunked \n",
      " , and run the chunker on this input \n",
      " .\n",
      "Example 7-2. Simple noun phrase chunker.\n",
      "grammar = r\"\"\"\n",
      "  NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and nouns\n",
      "      {<NNP>+}                # chunk sequences of proper nouns\n",
      "\"\"\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), \n",
      "                 (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n",
      ">>> print cp.parse(sentence) \n",
      "(S\n",
      "  (NP Rapunzel/NNP)\n",
      "  let/VBD\n",
      "  down/RP\n",
      "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n",
      "The $ symbol is a special character in regular expressions, and must be\n",
      "backslash escaped in order to match the tag PP$.\n",
      "If a tag pattern matches at overlapping locations, the leftmost match takes precedence.\n",
      "For example, if we apply a rule that matches two consecutive nouns to a text containing\n",
      "three consecutive nouns, then only the first two nouns will be chunked:\n",
      ">>> nouns = [(\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n",
      ">>> grammar = \"NP: {<NN><NN>}  # Chunk two consecutive nouns\"\n",
      ">>> cp = nltk.RegexpParser(grammar)\n",
      ">>> print cp.parse(nouns)\n",
      "(S (NP money/NN market/NN) fund/NN)\n",
      "Once we have created the chunk for money market, we have removed the context that\n",
      "would have permitted fund to be included in a chunk. This issue would have been\n",
      "avoided with a more permissive chunk rule, e.g., NP: {<NN>+}.\n",
      "We have added a comment to each of our chunk rules. These are op-\n",
      "tional; \n",
      "when they are present, the chunker prints these comments as\n",
      "part of its tracing output.\n",
      "Exploring Text Corpora\n",
      "In Section 5.2, we saw how we could interrogate a tagged corpus to extract phrases\n",
      "matching a particular sequence of part-of-speech tags. We can do the same work more\n",
      "easily with a chunker, as follows:\n",
      "7.2  Chunking | 267...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 288, 'page_label': '267', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4648}\n",
      "\n",
      "--- Chunk 4649 ---\n",
      "Content:\n",
      ">>> cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')\n",
      ">>> brown = nltk.corpus.brown\n",
      ">>> for sent in brown.tagged_sents():\n",
      "...     tree = cp.parse(sent)\n",
      "...     for subtree in tree.subtrees():\n",
      "...         if subtree.node == 'CHUNK': print subtree\n",
      "...\n",
      "(CHUNK combined/VBN to/TO achieve/VB)\n",
      "(CHUNK continue/VB to/TO place/VB)\n",
      "(CHUNK serve/VB to/TO protect/VB)\n",
      "(CHUNK wanted/VBD to/TO wait/VB)\n",
      "(CHUNK allowed/VBN to/TO place/VB)\n",
      "(CHUNK expected/VBN to/TO become/VB)\n",
      "...\n",
      "(CHUNK seems/VBZ to/TO overtake/VB)\n",
      "(CHUNK want/VB to/TO buy/VB)\n",
      "Your Turn:  Encapsulate the previous example inside a function\n",
      "find_chunks() that takes a chunk string like \"CHUNK: {<V.*> <TO>\n",
      "<V.*>}\" as an argument. Use it to search the corpus for several other\n",
      "patterns, such as four or more nouns in a row, e.g., \"NOUNS:\n",
      "{<N.*>{4,}}\".\n",
      "Chinking\n",
      "Sometimes it is easier to define what we want to exclude from a chunk. We can define\n",
      "a chink to be a sequence of tokens that is not included in a chunk. In the following\n",
      "example, barked/VBD at/IN is a chink:\n",
      "[ the/DT little/JJ yellow/JJ dog/NN ] barked/VBD at/IN [ the/DT cat/NN ]\n",
      "Chinking is the process of removing a sequence of tokens from a chunk. If the matching\n",
      "sequence of tokens spans an entire chunk, then the whole chunk is removed; if the\n",
      "sequence of tokens appears in the middle of the chunk, these tokens are removed,\n",
      "leaving two chunks where there was only one before. If the sequence is at the periphery\n",
      "of the chunk, these tokens are removed, and a smaller chunk remains. These three\n",
      "possibilities are illustrated in Table 7-2.\n",
      "Table 7-2. Three chinking rules applied to the same chunk\n",
      "Entire chunk Middle of a chunk End of a chunk\n",
      "Input [a/DT little/JJ dog/NN] [a/DT little/JJ dog/NN] [a/DT little/JJ dog/NN]\n",
      "Operation Chink “DT JJ NN” Chink “JJ” Chink “NN”\n",
      "Pattern }DT JJ NN{ }JJ{ }NN{\n",
      "Output a/DT little/JJ dog/NN [a/DT] little/JJ [dog/NN] [a/DT little/JJ] dog/NN\n",
      "268 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 289, 'page_label': '268', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4649}\n",
      "\n",
      "--- Chunk 4650 ---\n",
      "Content:\n",
      "In Example 7-3, we put the entire sentence into a single chunk, then excise the chinks.\n",
      "Example 7-3. Simple chinker.\n",
      "grammar = r\"\"\"\n",
      "  NP:\n",
      "    {<.*>+}          # Chunk everything\n",
      "    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
      "  \"\"\"\n",
      "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
      "       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
      "cp = nltk.RegexpParser(grammar)\n",
      ">>> print cp.parse(sentence)\n",
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n",
      "Representing Chunks: Tags Versus Trees\n",
      "As befits their intermediate status between tagging and parsing ( Chapter 8), chunk\n",
      "structures can be represented using either tags or trees. The most widespread file rep-\n",
      "resentation uses IOB tags. In this scheme, each token is tagged with one of three special\n",
      "chunk tags, I (inside), O (outside), or B (begin). A token is tagged as B if it marks the\n",
      "beginning of a chunk. Subsequent tokens within the chunk are tagged I. All other\n",
      "tokens are tagged O. The B and I tags are suffixed with the chunk type, e.g., B-NP, I-\n",
      "NP. Of course, it is not necessary to specify a chunk type for tokens that appear outside\n",
      "a chunk, so these are just labeled O. An example of this scheme is shown in Figure 7-3.\n",
      "Figure 7-3. Tag representation of chunk structures.\n",
      "IOB \n",
      "tags have become the standard way to represent chunk structures in files, and we\n",
      "will also be using this format. Here is how the information in Figure 7-3 would appear\n",
      "in a file:\n",
      "We PRP B-NP\n",
      "saw VBD O\n",
      "the DT B-NP\n",
      "little JJ I-NP\n",
      "yellow JJ I-NP\n",
      "dog NN I-NP\n",
      "7.2  Chunking | 269...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 290, 'page_label': '269', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4650}\n",
      "\n",
      "--- Chunk 4651 ---\n",
      "Content:\n",
      "In this representation there is one token per line, each with its part-of-speech tag and\n",
      "chunk \n",
      "tag. This format permits us to represent more than one chunk type, so long as\n",
      "the chunks do not overlap. As we saw earlier, chunk structures can also be represented\n",
      "using trees. These have the benefit that each chunk is a constituent that can be manip-\n",
      "ulated directly. An example is shown in Figure 7-4.\n",
      "Figure 7-4. Tree representation of chunk structures.\n",
      "NLTK uses trees for its internal representation of chunks, but provides\n",
      "methods for converting between such trees and the IOB format.\n",
      "7.3  Developing and Evaluating Chunkers\n",
      "Now \n",
      "you have a taste of what chunking does, but we haven’t explained how to evaluate\n",
      "chunkers. As usual, this requires a suitably annotated corpus. We begin by looking at\n",
      "the mechanics of converting IOB format into an NLTK tree, then at how this is done\n",
      "on a larger scale using a chunked corpus. We will see how to score the accuracy of a\n",
      "chunker relative to a corpus, then look at some more data-driven ways to search for\n",
      "NP chunks. Our focus throughout will be on expanding the coverage of a chunker.\n",
      "Reading IOB Format and the CoNLL-2000 Chunking Corpus\n",
      "Using the corpora module we can load Wall Street Journal text that has been tagged\n",
      "then chunked using the IOB notation. The chunk categories provided in this corpus\n",
      "are NP, VP, and PP. As we have seen, each sentence is represented using multiple lines,\n",
      "as shown here:\n",
      "he PRP B-NP\n",
      "accepted VBD B-VP\n",
      "the DT B-NP\n",
      "position NN I-NP\n",
      "...\n",
      "270 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 291, 'page_label': '270', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4651}\n",
      "\n",
      "--- Chunk 4652 ---\n",
      "Content:\n",
      "A conversion function chunk.conllstr2tree() builds a tree representation from one of\n",
      "these multiline strings. Moreover, it permits us to choose any subset of the three chunk\n",
      "types to use, here just for NP chunks:\n",
      ">>> text = '''\n",
      "... he PRP B-NP\n",
      "... accepted VBD B-VP\n",
      "... the DT B-NP\n",
      "... position NN I-NP\n",
      "... of IN B-PP\n",
      "... vice NN B-NP\n",
      "... chairman NN I-NP\n",
      "... of IN B-PP\n",
      "... Carlyle NNP B-NP\n",
      "... Group NNP I-NP\n",
      "... , , O\n",
      "... a DT B-NP\n",
      "... merchant NN I-NP\n",
      "... banking NN I-NP\n",
      "... concern NN I-NP\n",
      "... . . O\n",
      "... '''\n",
      ">>> nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()\n",
      "We can use the NLTK corpus module to access a larger amount of chunked text. The\n",
      "CoNLL-2000 \n",
      "Chunking Corpus contains 270k words of Wall Street Journal text, divi-\n",
      "ded into “train” and “test” portions, annotated with part-of-speech tags and chunk tags\n",
      "in the IOB format. We can access the data using nltk.corpus.conll2000. Here is an\n",
      "example that reads the 100th sentence of the “train” portion of the corpus:\n",
      ">>> from nltk.corpus import conll2000\n",
      ">>> print conll2000.chunked_sents('train.txt')[99]\n",
      "(S\n",
      "  (PP Over/IN)\n",
      "  (NP a/DT cup/NN)\n",
      "  (PP of/IN)\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  (VP told/VBD)\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n",
      "As you can see, the CoNLL-2000 Chunking Corpus contains three chunk types: NP\n",
      "chunks, which we have already seen; VP chunks, such as has already delivered; and PP\n",
      "7.3  Developing and Evaluating Chunkers | 271...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 292, 'page_label': '271', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4652}\n",
      "\n",
      "--- Chunk 4653 ---\n",
      "Content:\n",
      "chunks, such as because of. Since we are only interested in the NP chunks right now, we\n",
      "can use the chunk_types argument to select them:\n",
      ">>> print conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99]\n",
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n",
      "Simple Evaluation and Baselines\n",
      "Now that we can access a chunked corpus, we can evaluate chunkers. We start off by\n",
      "establishing a baseline for the trivial chunk parser cp that creates no chunks:\n",
      ">>> from nltk.corpus import conll2000\n",
      ">>> cp = nltk.RegexpParser(\"\")\n",
      ">>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
      ">>> print cp.evaluate(test_sents)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%\n",
      "    Precision:      0.0%\n",
      "    Recall:         0.0%\n",
      "    F-Measure:      0.0%\n",
      "The IOB tag accuracy indicates that more than a third of the words are tagged with O,\n",
      "i.e., not in an NP chunk. However, since our tagger did not find any chunks, its precision,\n",
      "recall, and F-measure are all zero. Now let’s try a naive regular expression chunker that\n",
      "looks for tags beginning with letters that are characteristic of noun phrase tags (e.g.,\n",
      "CD, DT, and JJ).\n",
      ">>> grammar = r\"NP: {<[CDJNP].*>+}\"\n",
      ">>> cp = nltk.RegexpParser(grammar)\n",
      ">>> print cp.evaluate(test_sents)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%\n",
      "    Precision:     70.6%\n",
      "    Recall:        67.8%\n",
      "    F-Measure:     69.2%\n",
      "As you can see, this approach achieves decent results. However, we can improve on it\n",
      "by adopting a more data-driven approach, where we use the training corpus to find the\n",
      "chunk tag (I, O, or B) that is most likely for each part-of-speech tag. In other words, we\n",
      "can build a chunker using a unigram tagger (Section 5.4). But rather than trying to\n",
      "determine the correct part-of-speech tag for each word, we are trying to determine the\n",
      "correct chunk tag, given each word’s part-of-speech tag.\n",
      "272 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 293, 'page_label': '272', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4653}\n",
      "\n",
      "--- Chunk 4654 ---\n",
      "Content:\n",
      "In Example 7-4, we define the UnigramChunker class, which uses a unigram tagger to\n",
      "label sentences with chunk tags. Most of the code in this class is simply used to convert\n",
      "back and forth between the chunk tree representation used by NLTK’s ChunkParserI\n",
      "interface, and the IOB representation used by the embedded tagger. The class defines\n",
      "two methods: a constructor \n",
      ", which is called when we build a new UnigramChunker;\n",
      "and the parse method \n",
      " , which is used to chunk new sentences.\n",
      "Example 7-4. Noun phrase chunking with a unigram tagger.\n",
      "class UnigramChunker(nltk.ChunkParserI):\n",
      "    def __init__(self, train_sents): \n",
      "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
      "                      for sent in train_sents]\n",
      "        self.tagger = nltk.UnigramTagger(train_data) \n",
      "    def parse(self, sentence): \n",
      "        pos_tags = [pos for (word,pos) in sentence]\n",
      "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
      "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
      "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
      "                     in zip(sentence, chunktags)]\n",
      "        return nltk.chunk.conlltags2tree(conlltags)\n",
      "The \n",
      "constructor \n",
      "  expects a list of training sentences, which will be in the form of\n",
      "chunk \n",
      "trees. It first converts training data to a form that’s suitable for training the tagger,\n",
      "using tree2conlltags to map each chunk tree to a list of word,tag,chunk triples. It then\n",
      "uses that converted training data to train a unigram tagger, and stores it in self.tag\n",
      "ger for later use.\n",
      "The parse method \n",
      "  takes a tagged sentence as its input, and begins by extracting the\n",
      "part-of-speech \n",
      "tags from that sentence. It then tags the part-of-speech tags with IOB\n",
      "chunk tags, using the tagger self.tagger that was trained in the constructor. Next, it\n",
      "extracts the chunk tags, and combines them with the original sentence, to yield\n",
      "conlltags...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 294, 'page_label': '273', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4654}\n",
      "\n",
      "--- Chunk 4655 ---\n",
      "Content:\n",
      ". It then\n",
      "uses that converted training data to train a unigram tagger, and stores it in self.tag\n",
      "ger for later use.\n",
      "The parse method \n",
      "  takes a tagged sentence as its input, and begins by extracting the\n",
      "part-of-speech \n",
      "tags from that sentence. It then tags the part-of-speech tags with IOB\n",
      "chunk tags, using the tagger self.tagger that was trained in the constructor. Next, it\n",
      "extracts the chunk tags, and combines them with the original sentence, to yield\n",
      "conlltags. Finally, it uses conlltags2tree to convert the result back into a chunk tree.\n",
      "Now that we have UnigramChunker, we can train it using the CoNLL-2000 Chunking\n",
      "Corpus, and test its resulting performance:\n",
      ">>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
      ">>> train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
      ">>> unigram_chunker = UnigramChunker(train_sents)\n",
      ">>> print unigram_chunker.evaluate(test_sents)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%\n",
      "    Precision:     79.9%\n",
      "    Recall:        86.8%\n",
      "    F-Measure:     83.2%\n",
      "This chunker does reasonably well, achieving an overall F-measure score of 83%. Let’s\n",
      "take a look at what it’s learned, by using its unigram tagger to assign a tag to each of\n",
      "the part-of-speech tags that appear in the corpus:\n",
      "7.3  Developing and Evaluating Chunkers | 273...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 294, 'page_label': '273', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4655}\n",
      "\n",
      "--- Chunk 4656 ---\n",
      "Content:\n",
      ">>> postags = sorted(set(pos for sent in train_sents\n",
      "...                      for (word,pos) in sent.leaves()))\n",
      ">>> print unigram_chunker.tagger.tag(postags)\n",
      "[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'),\n",
      " (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'),\n",
      " ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'),\n",
      " ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'),\n",
      " ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'),\n",
      " ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'),\n",
      " ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'),\n",
      " ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'),\n",
      " ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'),\n",
      " ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\n",
      "It \n",
      "has discovered that most punctuation marks occur outside of NP chunks, with the\n",
      "exception of # and $, both of which are used as currency markers. It has also found that\n",
      "determiners (DT) and possessives (PRP$ and WP$) occur at the beginnings of NP chunks,\n",
      "while noun types (NN, NNP, NNPS, NNS) mostly occur inside of NP chunks.\n",
      "Having built a unigram chunker, it is quite easy to build a bigram chunker: we simply\n",
      "change the class name to BigramChunker, and modify line \n",
      "  in Example 7-4  to construct\n",
      "a BigramTagger rather than a UnigramTagger. The resulting chunker has slightly higher\n",
      "performance than the unigram chunker:\n",
      ">>> bigram_chunker = BigramChunker(train_sents)\n",
      ">>> print bigram_chunker.evaluate(test_sents)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%\n",
      "    Precision:     82.3%\n",
      "    Recall:        86.8%\n",
      "    F-Measure:     84.5%\n",
      "Training Classifier-Based Chunkers\n",
      "Both the regular expression–based chunkers and the n-gram chunkers decide what\n",
      "chunks to create entirely based on part-of-speech tags. However, sometimes part-of-\n",
      "speech tags are insufficient to determine how a sentence should be chunked. For ex-\n",
      "ample, consider the following two statements:\n",
      "(3) a. Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.\n",
      "b...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 295, 'page_label': '274', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4656}\n",
      "\n",
      "--- Chunk 4657 ---\n",
      "Content:\n",
      ". However, sometimes part-of-\n",
      "speech tags are insufficient to determine how a sentence should be chunked. For ex-\n",
      "ample, consider the following two statements:\n",
      "(3) a. Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.\n",
      "b. Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.\n",
      "These two sentences have the same part-of-speech tags, yet they are chunked differ-\n",
      "ently. In the first sentence, the farmer and rice are separate chunks, while the corre-\n",
      "sponding material in the second sentence, the computer monitor , is a single chunk.\n",
      "Clearly, we need to make use of information about the content of the words, in addition\n",
      "to just their part-of-speech tags, if we wish to maximize chunking performance.\n",
      "One way that we can incorporate information about the content of words is to use a\n",
      "classifier-based tagger to chunk the sentence. Like the n-gram chunker considered in\n",
      "the previous section, this classifier-based chunker will work by assigning IOB tags to\n",
      "274 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 295, 'page_label': '274', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4657}\n",
      "\n",
      "--- Chunk 4658 ---\n",
      "Content:\n",
      "the words in a sentence, and then converting those tags to chunks. For the classifier-\n",
      "based \n",
      "tagger itself, we will use the same approach that we used in Section 6.1 to build\n",
      "a part-of-speech tagger.\n",
      "The basic code for the classifier-based NP chunker is shown in Example 7-5. It consists\n",
      "of two classes. The first class \n",
      "  is almost identical to the ConsecutivePosTagger class\n",
      "from Example 6-5. The only two differences are that it calls a different feature extractor\n",
      " and that it uses a MaxentClassifier rather than a NaiveBayesClassifier \n",
      " . The sec-\n",
      "ond class \n",
      "  is basically a wrapper around the tagger class that turns it into a chunker.\n",
      "During \n",
      "training, this second class maps the chunk trees in the training corpus into tag\n",
      "sequences; in the parse() method, it converts the tag sequence provided by the tagger\n",
      "back into a chunk tree.\n",
      "Example 7-5...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 296, 'page_label': '275', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4658}\n",
      "\n",
      "--- Chunk 4659 ---\n",
      "Content:\n",
      ". The sec-\n",
      "ond class \n",
      "  is basically a wrapper around the tagger class that turns it into a chunker.\n",
      "During \n",
      "training, this second class maps the chunk trees in the training corpus into tag\n",
      "sequences; in the parse() method, it converts the tag sequence provided by the tagger\n",
      "back into a chunk tree.\n",
      "Example 7-5. Noun phrase chunking with a consecutive classifier.\n",
      "class ConsecutiveNPChunkTagger(nltk.TaggerI): \n",
      "    def __init__(self, train_sents):\n",
      "        train_set = []\n",
      "        for tagged_sent in train_sents:\n",
      "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
      "            history = []\n",
      "            for i, (word, tag) in enumerate(tagged_sent):\n",
      "                featureset = npchunk_features(untagged_sent, i, history) \n",
      "                train_set.append( (featureset, tag) )\n",
      "                history.append(tag)\n",
      "        self.classifier = nltk.MaxentClassifier.train( \n",
      "            train_set, algorithm='megam', trace=0)\n",
      "    def tag(self, sentence):\n",
      "        history = []\n",
      "        for i, word in enumerate(sentence):\n",
      "            featureset = npchunk_features(sentence, i, history)\n",
      "            tag = self.classifier.classify(featureset)\n",
      "            history.append(tag)\n",
      "        return zip(sentence, history)\n",
      "class ConsecutiveNPChunker(nltk.ChunkParserI): \n",
      "    def __init__(self, train_sents):\n",
      "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
      "                         nltk.chunk.tree2conlltags(sent)]\n",
      "                        for sent in train_sents]\n",
      "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
      "    def parse(self, sentence):\n",
      "        tagged_sents = self.tagger.tag(sentence)\n",
      "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
      "        return nltk.chunk.conlltags2tree(conlltags)\n",
      "The \n",
      "only piece left to fill in is the feature extractor. We begin by defining a simple\n",
      "feature extractor, which just provides the part-of-speech tag of the current token...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 296, 'page_label': '275', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4659}\n",
      "\n",
      "--- Chunk 4660 ---\n",
      "Content:\n",
      ". We begin by defining a simple\n",
      "feature extractor, which just provides the part-of-speech tag of the current token. Using\n",
      "7.3  Developing and Evaluating Chunkers | 275...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 296, 'page_label': '275', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4660}\n",
      "\n",
      "--- Chunk 4661 ---\n",
      "Content:\n",
      "this feature extractor, our classifier-based chunker is very similar to the unigram chunk-\n",
      "er, as is reflected in its performance:\n",
      ">>> def npchunk_features(sentence, i, history):\n",
      "...     word, pos = sentence[i]\n",
      "...     return {\"pos\": pos}\n",
      ">>> chunker = ConsecutiveNPChunker(train_sents)\n",
      ">>> print chunker.evaluate(test_sents)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%\n",
      "    Precision:     79.9%\n",
      "    Recall:        86.7%\n",
      "    F-Measure:     83.2%\n",
      "We \n",
      "can also add a feature for the previous part-of-speech tag. Adding this feature allows\n",
      "the classifier to model interactions between adjacent tags, and results in a chunker that\n",
      "is closely related to the bigram chunker.\n",
      ">>> def npchunk_features(sentence, i, history):\n",
      "...     word, pos = sentence[i]\n",
      "...     if i == 0:\n",
      "...         prevword, prevpos = \"<START>\", \"<START>\"\n",
      "...     else:\n",
      "...         prevword, prevpos = sentence[i-1]\n",
      "...     return {\"pos\": pos, \"prevpos\": prevpos}\n",
      ">>> chunker = ConsecutiveNPChunker(train_sents)\n",
      ">>> print chunker.evaluate(test_sents)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.6%\n",
      "    Precision:     81.9%\n",
      "    Recall:        87.1%\n",
      "    F-Measure:     84.4%\n",
      "Next, we’ll try adding a feature for the current word, since we hypothesized that word\n",
      "content should be useful for chunking. We find that this feature does indeed improve\n",
      "the chunker’s performance, by about 1.5 percentage points (which corresponds to\n",
      "about a 10% reduction in the error rate).\n",
      ">>> def npchunk_features(sentence, i, history):\n",
      "...     word, pos = sentence[i]\n",
      "...     if i == 0:\n",
      "...         prevword, prevpos = \"<START>\", \"<START>\"\n",
      "...     else:\n",
      "...         prevword, prevpos = sentence[i-1]\n",
      "...     return {\"pos\": pos, \"word\": word, \"prevpos\": prevpos}\n",
      ">>> chunker = ConsecutiveNPChunker(train_sents)\n",
      ">>> print chunker.evaluate(test_sents)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.2%\n",
      "    Precision:     83.4%\n",
      "    Recall:        88.6%\n",
      "    F-Measure:     85.9%\n",
      "276 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 297, 'page_label': '276', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4661}\n",
      "\n",
      "--- Chunk 4662 ---\n",
      "Content:\n",
      "Finally, we can try extending the feature extractor with a variety of additional features,\n",
      "such \n",
      "as lookahead features \n",
      " , paired features \n",
      " , and complex contextual features \n",
      " .\n",
      "This \n",
      "last feature, called tags-since-dt, creates a string describing the set of all part-of-\n",
      "speech tags that have been encountered since the most recent determiner.\n",
      ">>> def npchunk_features(sentence, i, history):\n",
      "...     word, pos = sentence[i]\n",
      "...     if i == 0:\n",
      "...         prevword, prevpos = \"<START>\", \"<START>\"\n",
      "...     else:\n",
      "...         prevword, prevpos = sentence[i-1]\n",
      "...     if i == len(sentence)-1:\n",
      "...         nextword, nextpos = \"<END>\", \"<END>\"\n",
      "...     else:\n",
      "...         nextword, nextpos = sentence[i+1]\n",
      "...     return {\"pos\": pos,\n",
      "...             \"word\": word,\n",
      "...             \"prevpos\": prevpos,\n",
      "...             \"nextpos\": nextpos, \n",
      "...             \"prevpos+pos\": \"%s+%s\" % (prevpos, pos),  \n",
      "...             \"pos+nextpos\": \"%s+%s\" % (pos, nextpos),\n",
      "...             \"tags-since-dt\": tags_since_dt(sentence, i)}  \n",
      ">>> def tags_since_dt(sentence, i):\n",
      "...     tags = set()\n",
      "...     for word, pos in sentence[:i]:\n",
      "...         if pos == 'DT':\n",
      "...             tags = set()\n",
      "...         else:\n",
      "...             tags.add(pos)\n",
      "...     return '+'.join(sorted(tags))\n",
      ">>> chunker = ConsecutiveNPChunker(train_sents)\n",
      ">>> print chunker.evaluate(test_sents)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  95.9%\n",
      "    Precision:     88.3%\n",
      "    Recall:        90.7%\n",
      "    F-Measure:     89.5%\n",
      "Your Turn: Try adding different features to the feature extractor func-\n",
      "tion npchunk_features, and see if you can further improve the perform-\n",
      "ance of the NP chunker.\n",
      "7.4  Recursion in Linguistic Structure\n",
      "Building Nested Structure with Cascaded Chunkers\n",
      "So far, our chunk structures have been relatively flat. Trees consist of tagged tokens,\n",
      "optionally grouped under a chunk node such as NP. However, it is possible to build\n",
      "chunk structures of arbitrary depth, simply by creating a multistage chunk grammar\n",
      "7.4  Recursion in Linguistic Structure | 277...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 298, 'page_label': '277', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4662}\n",
      "\n",
      "--- Chunk 4663 ---\n",
      "Content:\n",
      "containing recursive rules. Example 7-6 has patterns for noun phrases, prepositional\n",
      "phrases, verb phrases, and sentences. This is a four-stage chunk grammar, and can be\n",
      "used to create structures having a depth of at most four.\n",
      "Example 7-6. A chunker that handles NP, PP, VP, and S.\n",
      "grammar = r\"\"\"\n",
      "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
      "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
      "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
      "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
      "  \"\"\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n",
      "    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
      ">>> print cp.parse(sentence)\n",
      "(S\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n",
      "Unfortunately this result misses the VP headed by saw. It has other shortcomings, too.\n",
      "Let’s see what happens when we apply this chunker to a sentence having deeper nesting.\n",
      "Notice that it fails to identify the VP chunk starting at \n",
      ".\n",
      ">>> sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"),\n",
      "...     (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"),\n",
      "...     (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
      ">>> print cp.parse(sentence)\n",
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD \n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n",
      "The \n",
      "solution to these problems is to get the chunker to loop over its patterns: after\n",
      "trying all of them, it repeats the process. We add an optional second argument loop to\n",
      "specify the number of times the set of patterns should be run:\n",
      ">>> cp = nltk.RegexpParser(grammar, loop=2)\n",
      ">>> print cp.parse(sentence)\n",
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (CLAUSE\n",
      "    (NP Mary/NN)\n",
      "    (VP\n",
      "      saw/VBD\n",
      "      (CLAUSE\n",
      "278 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 299, 'page_label': '278', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4663}\n",
      "\n",
      "--- Chunk 4664 ---\n",
      "Content:\n",
      "(NP the/DT cat/NN)\n",
      "        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\n",
      "This cascading process enables us to create deep structures. However,\n",
      "creating \n",
      "and debugging a cascade is difficult, and there comes a point\n",
      "where it is more effective to do full parsing (see Chapter 8). Also, the\n",
      "cascading process can only produce trees of fixed depth (no deeper than\n",
      "the number of stages in the cascade), and this is insufficient for complete\n",
      "syntactic analysis.\n",
      "Trees\n",
      "A tree is a set of connected labeled nodes, each reachable by a unique path from a\n",
      "distinguished root node. Here’s an example of a tree (note that they are standardly\n",
      "drawn upside-down):\n",
      "(4)\n",
      "We use a ‘family’ metaphor to talk about the relationships of nodes in a tree: for ex-\n",
      "ample, S \n",
      "is the parent of VP; conversely VP is a child of S. Also, since NP and VP are both\n",
      "children of S, they are also siblings. For convenience, there is also a text format for\n",
      "specifying trees:\n",
      "(S\n",
      "   (NP Alice)\n",
      "   (VP\n",
      "      (V chased)\n",
      "      (NP\n",
      "         (Det the)\n",
      "         (N rabbit))))\n",
      "Although we will focus on syntactic trees, trees can be used to encode any homogeneous\n",
      "hierarchical structure that spans a sequence of linguistic forms (e.g., morphological\n",
      "structure, discourse structure). In the general case, leaves and node values do not have\n",
      "to be strings.\n",
      "In NLTK, we create a tree by giving a node label and a list of children:\n",
      "7.4  Recursion in Linguistic Structure | 279...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 300, 'page_label': '279', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4664}\n",
      "\n",
      "--- Chunk 4665 ---\n",
      "Content:\n",
      ">>> tree1 = nltk.Tree('NP', ['Alice'])\n",
      ">>> print tree1\n",
      "(NP Alice)\n",
      ">>> tree2 = nltk.Tree('NP', ['the', 'rabbit'])\n",
      ">>> print tree2\n",
      "(NP the rabbit)\n",
      "We can incorporate these into successively larger trees as follows:\n",
      ">>> tree3 = nltk.Tree('VP', ['chased', tree2])\n",
      ">>> tree4 = nltk.Tree('S', [tree1, tree3])\n",
      ">>> print tree4\n",
      "(S (NP Alice) (VP chased (NP the rabbit)))\n",
      "Here are some of the methods available for tree objects:\n",
      ">>> print tree4[1]\n",
      "(VP chased (NP the rabbit))\n",
      ">>> tree4[1].node\n",
      "'VP'\n",
      ">>> tree4.leaves()\n",
      "['Alice', 'chased', 'the', 'rabbit']\n",
      ">>> tree4[1][1][1]\n",
      "'rabbit'\n",
      "The \n",
      "bracketed representation for complex trees can be difficult to read. In these cases,\n",
      "the draw method can be very useful. It opens a new window, containing a graphical\n",
      "representation of the tree. The tree display window allows you to zoom in and out, to\n",
      "collapse and expand subtrees, and to print the graphical representation to a postscript\n",
      "file (for inclusion in a document).\n",
      ">>> tree3.draw()\n",
      "Tree Traversal\n",
      "It \n",
      "is standard to use a recursive function to traverse a tree. The listing in Example 7-7\n",
      "demonstrates this.\n",
      "Example 7-7. A recursive function to traverse a tree.\n",
      "def traverse(t):\n",
      "    try:\n",
      "        t.node\n",
      "    except AttributeError:\n",
      "        print t,\n",
      " \n",
      "    else:\n",
      "280 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 301, 'page_label': '280', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4665}\n",
      "\n",
      "--- Chunk 4666 ---\n",
      "Content:\n",
      "# Now we know that t.node is defined\n",
      "        print '(', t.node,\n",
      "        for child in t:\n",
      "            traverse(child)\n",
      "        print ')',\n",
      ">>> t = nltk.Tree('(S (NP Alice) (VP chased (NP the rabbit)))')\n",
      ">>> traverse(t)\n",
      "( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) )\n",
      "We have used a technique called duck typing to detect that t is a tree\n",
      "(i.e., t.node is defined).\n",
      "7.5  Named Entity Recognition\n",
      "At the start of this chapter, we briefly introduced named entities (NEs). Named entities\n",
      "are definite noun phrases that refer to specific types of individuals, such as organiza-\n",
      "tions, persons, dates, and so on. Table 7-3 lists some of the more commonly used types\n",
      "of NEs. These should be self-explanatory, except for “FACILITY”: human-made arti-\n",
      "facts in the domains of architecture and civil engineering; and “GPE”: geo-political\n",
      "entities such as city, state/province, and country.\n",
      "Table 7-3. Commonly used types of named entity\n",
      "NE type Examples\n",
      "ORGANIZATION Georgia-Pacific Corp., WHO\n",
      "PERSON Eddy Bonte, President Obama\n",
      "LOCATION Murray River, Mount Everest\n",
      "DATE June, 2008-06-29\n",
      "TIME two fifty a m, 1:30 p.m.\n",
      "MONEY 175 million Canadian Dollars, GBP 10.40\n",
      "PERCENT twenty pct, 18.75 %\n",
      "FACILITY Washington Monument, Stonehenge\n",
      "GPE South East Asia, Midlothian\n",
      "The goal of a named entity recognition (NER) system is to identify all textual men-\n",
      "tions of the named entities. This can be broken down into two subtasks: identifying\n",
      "the boundaries of the NE, and identifying its type. While named entity recognition is\n",
      "frequently a prelude to identifying relations in Information Extraction, it can also con-\n",
      "tribute to other tasks. For example, in Question Answering (QA), we try to improve\n",
      "the precision of Information Retrieval by recovering not whole pages, but just those\n",
      "parts which contain an answer to the user’s question. Most QA systems take the\n",
      "7.5  Named Entity Recognition | 281...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 302, 'page_label': '281', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4666}\n",
      "\n",
      "--- Chunk 4667 ---\n",
      "Content:\n",
      "documents returned by standard Information Retrieval, and then attempt to isolate the\n",
      "minimal text snippet in the document containing the answer. Now suppose the\n",
      "question was Who was the first President of the US?, and one of the documents that was\n",
      "retrieved contained the following passage:\n",
      "(5) The Washington Monument is the most prominent structure in Washington,\n",
      "D.C. and one of the city’s early attractions. It was built in honor of George\n",
      "Washington, who led the country to independence and then became its first\n",
      "President.\n",
      "Analysis of the question leads us to expect that an answer should be of the form X was\n",
      "the first President of the US , where X is not only a noun phrase, but also refers to a\n",
      "named entity of type PER. This should allow us to ignore the first sentence in the passage.\n",
      "Although it contains two occurrences of Washington, named entity recognition should\n",
      "tell us that neither of them has the correct type.\n",
      "How do we go about identifying named entities? One option would be to look up each\n",
      "word in an appropriate list of names. For example, in the case of locations, we could\n",
      "use a gazetteer, or geographical dictionary, such as the Alexandria Gazetteer or the\n",
      "Getty Gazetteer. However, doing this blindly runs into problems, as shown in Fig-\n",
      "ure 7-5.\n",
      "Figure 7-5. Location detection by simple lookup for a news story: Looking up every word in a gazetteer\n",
      "is error-prone; case distinctions may help, but these are not always present.\n",
      "Observe \n",
      "that the gazetteer has good coverage of locations in many countries, and in-\n",
      "correctly finds locations like Sanchez in the Dominican Republic and On in Vietnam.\n",
      "Of course we could omit such locations from the gazetteer, but then we won’t be able\n",
      "to identify them when they do appear in a document.\n",
      "It gets even harder in the case of names for people or organizations. Any list of such\n",
      "names will probably have poor coverage. New organizations come into existence every\n",
      "282 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 303, 'page_label': '282', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4667}\n",
      "\n",
      "--- Chunk 4668 ---\n",
      "Content:\n",
      "day, so if we are trying to deal with contemporary newswire or blog entries, it is unlikely\n",
      "that we will be able to recognize many of the entities using gazetteer lookup.\n",
      "Another major source of difficulty is caused by the fact that many named entity terms\n",
      "are \n",
      "ambiguous. Thus May and North are likely to be parts of named entities for DATE\n",
      "and LOCATION, respectively, but could both be part of a PERSON; conversely Chris-\n",
      "tian Dior looks like a PERSON but is more likely to be of type ORGANIZATION. A\n",
      "term like Yankee will be an ordinary modifier in some contexts, but will be marked as\n",
      "an entity of type ORGANIZATION in the phrase Yankee infielders.\n",
      "Further challenges are posed by multiword names like Stanford University , and by\n",
      "names that contain other names, such as Cecil H. Green Library and Escondido Village\n",
      "Conference Service Center. In named entity recognition, therefore, we need to be able\n",
      "to identify the beginning and end of multitoken sequences.\n",
      "Named entity recognition is a task that is well suited to the type of classifier-based\n",
      "approach that we saw for noun phrase chunking. In particular, we can build a tagger\n",
      "that labels each word in a sentence using the IOB format, where chunks are labeled by\n",
      "their appropriate type. Here is part of the CONLL 2002 ( conll2002) Dutch training\n",
      "data:\n",
      "Eddy N B-PER\n",
      "Bonte N I-PER\n",
      "is V O\n",
      "woordvoerder N O\n",
      "van Prep O\n",
      "diezelfde Pron O\n",
      "Hogeschool N B-ORG\n",
      ". Punc O\n",
      "In this representation, there is one token per line, each with its part-of-speech tag and\n",
      "its named entity tag. Based on this training corpus, we can construct a tagger that can\n",
      "be used to label new sentences, and use the nltk.chunk.conlltags2tree() function to\n",
      "convert the tag sequences into a chunk tree.\n",
      "NLTK provides a classifier that has already been trained to recognize named entities,\n",
      "accessed with the function nltk.ne_chunk(). If we set the parameter binary=True \n",
      ",\n",
      "then \n",
      "named entities are just tagged as NE; otherwise, the classifier adds category labels\n",
      "such as PERSON, ORGANIZATION, and GPE.\n",
      ">>> sent = nltk.corpus.treebank.tagged_sents()[22]\n",
      ">>> print nltk.ne_chunk(sent, binary=True) \n",
      "  \n",
      "(S\n",
      "  The/DT\n",
      "  (NE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  ...\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (NE Brooke/NNP T./NNP Mossman/NNP)\n",
      "  ...)\n",
      "7.5  Named Entity Recognition | 283...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 304, 'page_label': '283', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4668}\n",
      "\n",
      "--- Chunk 4669 ---\n",
      "Content:\n",
      ">>> print nltk.ne_chunk(sent) \n",
      "(S\n",
      "  The/DT\n",
      "  (GPE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  ...\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
      "  ...)\n",
      "7.6  Relation Extraction\n",
      "Once \n",
      "named entities have been identified in a text, we then want to extract the relations\n",
      "that exist between them. As indicated earlier, we will typically be looking for relations\n",
      "between specified types of named entity. One way of approaching this task is to initially\n",
      "look for all triples of the form (X, α, Y), where X and Y are named entities of the required\n",
      "types, and α is the string of words that intervenes between X and Y. We can then use\n",
      "regular expressions to pull out just those instances of α that express the relation that\n",
      "we are looking for. The following example searches for strings that contain the word\n",
      "in. The special regular expression (?!\\b.+ing\\b) is a negative lookahead assertion that\n",
      "allows us to disregard strings such as success in supervising the transition of, where in\n",
      "is followed by a gerund.\n",
      ">>> IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
      ">>> for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
      "...     for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,\n",
      "...                                      corpus='ieer', pattern = IN):\n",
      "...         print nltk.sem.show_raw_rtuple(rel)\n",
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n",
      "Searching for the keyword in works reasonably well, though it will also retrieve false\n",
      "positives such as [ORG: House Transportation Committee] , secured the most money\n",
      "in the [LOC: New York]; there is unlikely to be a simple string-based method of ex-\n",
      "cluding filler strings such as this.\n",
      "284 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 305, 'page_label': '284', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4669}\n",
      "\n",
      "--- Chunk 4670 ---\n",
      "Content:\n",
      "As shown earlier, the Dutch section of the CoNLL 2002 Named Entity Corpus contains\n",
      "not \n",
      "just named entity annotation, but also part-of-speech tags. This allows us to devise\n",
      "patterns that are sensitive to these tags, as shown in the next example. The method\n",
      "show_clause() prints out the relations in a clausal form, where the binary relation sym-\n",
      "bol is specified as the value of parameter relsym \n",
      ".\n",
      ">>> from nltk.corpus import conll2002\n",
      ">>> vnv = \"\"\"\n",
      "... (\n",
      "... is/V|    # 3rd sing present and\n",
      "... was/V|   # past forms of the verb zijn ('be')\n",
      "... werd/V|  # and also present\n",
      "... wordt/V  # past of worden ('become')\n",
      "... )\n",
      "... .*       # followed by anything\n",
      "... van/Prep # followed by van ('of')\n",
      "... \"\"\"\n",
      ">>> VAN = re.compile(vnv, re.VERBOSE)\n",
      ">>> for doc in conll2002.chunked_sents('ned.train'):\n",
      "...     for r in nltk.sem.extract_rels('PER', 'ORG', doc,\n",
      "...                                    corpus='conll2002', pattern=VAN):\n",
      "...         print  nltk.sem.show_clause(r, relsym=\"VAN\") \n",
      "VAN(\"cornet_d'elzius\", 'buitenlandse_handel')\n",
      "VAN('johan_rottiers', 'kardinaal_van_roey_instituut')\n",
      "VAN('annie_lennox', 'eurythmics')\n",
      "Your Turn: Replace the last line \n",
      "  with print show_raw_rtuple(rel,\n",
      "lcon=True, rcon=True). \n",
      "This will show you the actual words that inter-\n",
      "vene between the two NEs and also their left and right context, within\n",
      "a default 10-word window. With the help of a Dutch dictionary, you\n",
      "might be able to figure out why the result VAN('annie_lennox', 'euryth\n",
      "mics') is a false hit.\n",
      "7.7  Summary\n",
      "• Information extraction systems search large bodies of unrestricted text for specific\n",
      "types of entities and relations, and use them to populate well-organized databases.\n",
      "These databases can then be used to find answers for specific questions.\n",
      "• The typical architecture for an information extraction system begins by segment-\n",
      "ing, tokenizing, and part-of-speech tagging the text. The resulting data is then\n",
      "searched for specific types of entity. Finally, the information extraction system\n",
      "looks at entities that are mentioned near one another in the text, and tries to de-\n",
      "termine whether specific relationships hold between those entities.\n",
      "• Entity recognition is often performed using chunkers, which segment multitoken\n",
      "sequences, and label them with the appropriate entity type. Common entity types\n",
      "include ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, and\n",
      "GPE (geo-political entity).\n",
      "7.7  Summary | 285...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 306, 'page_label': '285', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4670}\n",
      "\n",
      "--- Chunk 4671 ---\n",
      "Content:\n",
      "• Chunkers can be constructed using rule-based systems, such as the RegexpParser\n",
      "class provided by NLTK; or using machine learning techniques, such as the\n",
      "ConsecutiveNPChunker presented in this chapter. In either case, part-of-speech tags\n",
      "are often a very important feature when searching for chunks.\n",
      "• Although chunkers are specialized to create relatively flat data structures, where\n",
      "no two chunks are allowed to overlap, they can be cascaded together to build nested\n",
      "structures.\n",
      "• Relation extraction can be performed using either rule-based systems, which typ-\n",
      "ically look for specific patterns in the text that connect entities and the intervening\n",
      "words; or using machine-learning systems, which typically attempt to learn such\n",
      "patterns automatically from a training corpus.\n",
      "7.8  Further Reading\n",
      "Extra materials for this chapter are posted at http://www.nltk.org/, including links to\n",
      "freely available resources on the Web. For more examples of chunking with NLTK,\n",
      "please see the Chunking HOWTO at http://www.nltk.org/howto.\n",
      "The popularity of chunking is due in great part to pioneering work by Abney, e.g.,\n",
      "(Abney, 1996a). Abney’s Cass chunker is described in http://www.vinartus.net/spa/97a\n",
      ".pdf.\n",
      "The word chink initially meant a sequence of stopwords, according to a 1975 paper\n",
      "by Ross and Tukey (Abney, 1996a).\n",
      "The IOB format (or sometimes BIO Format) was developed for NP chunking by (Ram-\n",
      "shaw & Marcus, 1995), and was used for the shared NP bracketing task run by the\n",
      "Conference on Natural Language Learning  (CoNLL) in 1999. The same format was\n",
      "adopted by CoNLL 2000 for annotating a section of Wall Street Journal text as part of\n",
      "a shared task on NP chunking.\n",
      "Section 13.5 of (Jurafsky & Martin, 2008) contains a discussion of chunking. Chapter\n",
      "22 covers information extraction, including named entity recognition. For information\n",
      "about text mining in biology and medicine, see (Ananiadou & McNaught, 2006).\n",
      "For more information on the Getty and Alexandria gazetteers, see http://en.wikipedia\n",
      ".org/wiki/Getty_Thesaurus_of_Geographic_Names and http://www.alexandria.ucsb\n",
      ".edu/gazetteer/.\n",
      "7.9  Exercises\n",
      "1. ○ The IOB format categorizes tagged tokens as I, O, and B. Why are three tags\n",
      "necessary? What problem would be caused if we used I and O tags exclusively?\n",
      "286 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 307, 'page_label': '286', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4671}\n",
      "\n",
      "--- Chunk 4672 ---\n",
      "Content:\n",
      "2. ○ Write a tag pattern to match noun phrases containing plural head nouns, e.g.,\n",
      "many/JJ researchers/NNS, two/CD weeks/NNS, both/DT new/JJ positions/NNS. Try\n",
      "to do this by generalizing the tag pattern that handled singular noun phrases.\n",
      "3. ○ Pick one of the three chunk types in the CoNLL-2000 Chunking Corpus. Inspect\n",
      "the data and try to observe any patterns in the POS tag sequences that make up\n",
      "this kind of chunk. Develop a simple chunker using the regular expression chunker\n",
      "nltk.RegexpParser. Discuss any tag sequences that are difficult to chunk reliably.\n",
      "4. ○ An early definition of chunk was the material that occurs between chinks. De-\n",
      "velop a chunker that starts by putting the whole sentence in a single chunk, and\n",
      "then does the rest of its work solely by chinking. Determine which tags (or tag\n",
      "sequences) are most likely to make up chinks with the help of your own utility\n",
      "program. Compare the performance and simplicity of this approach relative to a\n",
      "chunker based entirely on chunk rules.\n",
      "5. ◑ Write a tag pattern to cover noun phrases that contain gerunds, e.g., the/DT\n",
      "receiving/VBG end/NN, assistant/NN managing/VBG editor/NN. Add these patterns\n",
      "to the grammar, one per line. Test your work using some tagged sentences of your\n",
      "own devising.\n",
      "6. ◑ Write one or more tag patterns to handle coordinated noun phrases, e.g., July/\n",
      "NNP and/CC August/NNP, all/DT your/PRP$ managers/NNS and/CC supervisors/NNS,\n",
      "company/NN courts/NNS and/CC adjudicators/NNS.\n",
      "7. ◑ Carry out the following evaluation tasks for any of the chunkers you have de-\n",
      "veloped earlier. (Note that most chunking corpora contain some internal incon-\n",
      "sistencies, such that any reasonable rule-based approach will produce errors.)\n",
      "a. Evaluate your chunker on 100 sentences from a chunked corpus, and report\n",
      "the precision, recall, and F-measure.\n",
      "b. Use the chunkscore.missed() and chunkscore.incorrect() methods to identify\n",
      "the errors made by your chunker. Discuss.\n",
      "c. Compare the performance of your chunker to the baseline chunker discussed\n",
      "in the evaluation section of this chapter.\n",
      "8. ◑ Develop a chunker for one of the chunk types in the CoNLL Chunking Corpus\n",
      "using a regular expression–based chunk grammar RegexpChunk. Use any combina-\n",
      "tion of rules for chunking, chinking, merging, or splitting.\n",
      "9. ◑ Sometimes a word is incorrectly tagged, e.g., the head noun in 12/CD or/CC so/\n",
      "RB cases/VBZ. Instead of requiring manual correction of tagger output, good\n",
      "chunkers are able to work with the erroneous output of taggers. Look for other\n",
      "examples of correctly chunked noun phrases with incorrect tags.\n",
      "10. ◑ The bigram chunker scores about 90% accuracy. Study its errors and try to work\n",
      "out why it doesn’t get 100% accuracy. Experiment with trigram chunking. Are you\n",
      "able to improve the performance any more?\n",
      "7.9  Exercises | 287...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 308, 'page_label': '287', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4672}\n",
      "\n",
      "--- Chunk 4673 ---\n",
      "Content:\n",
      "11. ● Apply the n-gram and Brill tagging methods to IOB chunk tagging. Instead of\n",
      "assigning POS tags to words, here we will assign IOB tags to the POS tags. E.g., if\n",
      "the tag DT (determiner) often occurs at the start of a chunk, it will be tagged B\n",
      "(begin). Evaluate the performance of these chunking methods relative to the regular\n",
      "expression chunking methods covered in this chapter.\n",
      "12. ● We saw in Chapter 5 that it is possible to establish an upper limit to tagging\n",
      "performance by looking for ambiguous n-grams, which are n-grams that are tagged\n",
      "in more than one possible way in the training data. Apply the same method to\n",
      "determine an upper bound on the performance of an n-gram chunker.\n",
      "13. ● Pick one of the three chunk types in the CoNLL Chunking Corpus. Write func-\n",
      "tions to do the following tasks for your chosen type:\n",
      "a. List all the tag sequences that occur with each instance of this chunk type.\n",
      "b. Count the frequency of each tag sequence, and produce a ranked list in order\n",
      "of decreasing frequency; each line should consist of an integer (the frequency)\n",
      "and the tag sequence.\n",
      "c. Inspect the high-frequency tag sequences. Use these as the basis for developing\n",
      "a better chunker.\n",
      "14. ● The baseline chunker presented in the evaluation section tends to create larger\n",
      "chunks than it should. For example, the phrase [every/DT time/NN] [she/PRP]\n",
      "sees/VBZ [a/DT newspaper/NN] contains two consecutive chunks, and our baseline\n",
      "chunker will incorrectly combine the first two: [every/DT time/NN she/PRP]. Write\n",
      "a program that finds which of these chunk-internal tags typically occur at the start\n",
      "of a chunk, then devise one or more rules that will split up these chunks. Combine\n",
      "these with the existing baseline chunker and re-evaluate it, to see if you have dis-\n",
      "covered an improved baseline.\n",
      "15. ● Develop an NP chunker that converts POS tagged text into a list of tuples, where\n",
      "each tuple consists of a verb followed by a sequence of noun phrases and prepo-\n",
      "sitions, e.g., the little cat sat on the mat becomes ('sat', 'on', 'NP')...\n",
      "16. ● The Penn Treebank Corpus sample contains a section of tagged Wall Street\n",
      "Journal text that has been chunked into noun phrases. The format uses square\n",
      "brackets, and we have encountered it several times in this chapter. The corpus can\n",
      "be accessed using: for sent in nltk.corpus.treebank_chunk.chunked_sents(fil\n",
      "eid). These are flat trees, just as we got using nltk.cor\n",
      "pus.conll2000.chunked_sents().\n",
      "a. The functions nltk.tree.pprint() and nltk.chunk.tree2conllstr() can be\n",
      "used to create Treebank and IOB strings from a tree. Write functions\n",
      "chunk2brackets() and chunk2iob() that take a single chunk tree as their sole\n",
      "argument, and return the required multiline string representation.\n",
      "b. Write command-line conversion utilities bracket2iob.py and iob2bracket.py\n",
      "that take a file in Treebank or CoNLL format (respectively) and convert it to\n",
      "the other format. (Obtain some raw Treebank or CoNLL data from the NLTK\n",
      "288 | Chapter 7:  Extracting Information from Text...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 309, 'page_label': '288', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4673}\n",
      "\n",
      "--- Chunk 4674 ---\n",
      "Content:\n",
      "Corpora, save it to a file, and then use for line in open(filename) to access\n",
      "it from Python.)\n",
      "17. ● An \n",
      "n-gram chunker can use information other than the current part-of-speech\n",
      "tag and the n-1 previous chunk tags. Investigate other models of the context, such\n",
      "as the n-1 previous part-of-speech tags, or some combination of previous chunk\n",
      "tags along with previous and following part-of-speech tags.\n",
      "18. ● Consider the way an n-gram tagger uses recent tags to inform its tagging choice.\n",
      "Now observe how a chunker may reuse this sequence information. For example,\n",
      "both tasks will make use of the information that nouns tend to follow adjectives\n",
      "(in English). It would appear that the same information is being maintained in two\n",
      "places. Is this likely to become a problem as the size of the rule sets grows? If so,\n",
      "speculate about any ways that this problem might be addressed.\n",
      "7.9  Exercises | 289...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 310, 'page_label': '289', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4674}\n",
      "\n",
      "--- Chunk 4675 ---\n",
      "Content:\n",
      "CHAPTER 8\n",
      "Analyzing Sentence Structure\n",
      "Earlier chapters focused on words: how to identify them, analyze their structure, assign\n",
      "them \n",
      "to lexical categories, and access their meanings. We have also seen how to identify\n",
      "patterns in word sequences or n-grams. However, these methods only scratch the sur-\n",
      "face of the complex constraints that govern sentences. We need a way to deal with the\n",
      "ambiguity that natural language is famous for. We also need to be able to cope with\n",
      "the fact that there are an unlimited number of possible sentences, and we can only write\n",
      "finite programs to analyze their structures and discover their meanings.\n",
      "The goal of this chapter is to answer the following questions:\n",
      "1. How can we use a formal grammar to describe the structure of an unlimited set of\n",
      "sentences?\n",
      "2. How do we represent the structure of sentences using syntax trees?\n",
      "3. How do parsers analyze a sentence and automatically build a syntax tree?\n",
      "Along the way, we will cover the fundamentals of English syntax, and see that there\n",
      "are systematic aspects of meaning that are much easier to capture once we have iden-\n",
      "tified the structure of sentences.\n",
      "291...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 312, 'page_label': '291', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4675}\n",
      "\n",
      "--- Chunk 4676 ---\n",
      "Content:\n",
      "8.1  Some Grammatical Dilemmas\n",
      "Linguistic Data and Unlimited Possibilities\n",
      "Previous \n",
      "chapters have shown you how to process and analyze text corpora, and we\n",
      "have stressed the challenges for NLP in dealing with the vast amount of electronic\n",
      "language data that is growing daily. Let’s consider this data more closely, and make the\n",
      "thought experiment that we have a gigantic corpus consisting of everything that has\n",
      "been either uttered or written in English over, say, the last 50 years. Would we be\n",
      "justified in calling this corpus “the language of modern English”? There are a number\n",
      "of reasons why we might answer no. Recall that in Chapter 3, we asked you to search\n",
      "the Web for instances of the pattern the of. Although it is easy to find examples on the\n",
      "Web containing this word sequence, such as New man at the of IMG  (see http://www\n",
      ".telegraph.co.uk/sport/2387900/New-man-at-the-of-IMG.html), speakers of English\n",
      "will say that most such examples are errors, and therefore not part of English after all.\n",
      "Accordingly, we can argue that “modern English” is not equivalent to the very big set\n",
      "of word sequences in our imaginary corpus. Speakers of English can make judgments\n",
      "about these sequences, and will reject some of them as being ungrammatical.\n",
      "Equally, it is easy to compose a new sentence and have speakers agree that it is perfectly\n",
      "good English. For example, sentences have an interesting property that they can be\n",
      "embedded inside larger sentences. Consider the following sentences:\n",
      "(1) a. Usain Bolt broke the 100m record.\n",
      "b. The Jamaica Observer reported that Usain Bolt broke the 100m record.\n",
      "c. Andre said The Jamaica Observer reported that Usain Bolt broke the 100m\n",
      "record.\n",
      "d. I think Andre said the Jamaica Observer reported that Usain Bolt broke\n",
      "the 100m record.\n",
      "If we replaced whole sentences with the symbol S, we would see patterns like Andre\n",
      "said S and I think S. These are templates for taking a sentence and constructing a bigger\n",
      "sentence. There are other templates we can use, such as S but S and S when S. With a\n",
      "bit of ingenuity we can construct some really long sentences using these templates.\n",
      "Here’s an impressive example from a Winnie the Pooh story by A.A. Milne, In Which\n",
      "Piglet Is Entirely Surrounded by Water:\n",
      "[You can imagine Piglet’s joy when at last the ship came in sight of him.] In after-years\n",
      "he liked to think that he had been in Very Great Danger during the Terrible Flood, but\n",
      "the only danger he had really been in was the last half-hour of his imprisonment, when\n",
      "Owl, who had just flown up, sat on a branch of his tree to comfort him, and told him a\n",
      "very long story about an aunt who had once laid a seagull’s egg by mistake, and the story\n",
      "went on and on, rather like this sentence, until Piglet who was listening out of his window\n",
      "without much hope, went to sleep quietly and naturally, slipping slowly out of the win-\n",
      "dow towards the water until he was only hanging on by his toes, at which moment,\n",
      "292 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 313, 'page_label': '292', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4676}\n",
      "\n",
      "--- Chunk 4677 ---\n",
      "Content:\n",
      "luckily, a sudden loud squawk from Owl, which was really part of the story, being what\n",
      "his aunt said, woke the Piglet up and just gave him time to jerk himself back into safety\n",
      "and \n",
      "say, “How interesting, and did she?” when—well, you can imagine his joy when at\n",
      "last he saw the good ship, Brain of Pooh (Captain, C. Robin; 1st Mate, P. Bear) coming\n",
      "over the sea to rescue him…\n",
      "This long sentence actually has a simple structure that begins S but S when S. We can\n",
      "see from this example that language provides us with constructions which seem to allow\n",
      "us to extend sentences indefinitely. It is also striking that we can understand sentences\n",
      "of arbitrary length that we’ve never heard before: it’s not hard to concoct an entirely\n",
      "novel sentence, one that has probably never been used before in the history of the\n",
      "language, yet all speakers of the language will understand it.\n",
      "The purpose of a grammar is to give an explicit description of a language. But the way\n",
      "in \n",
      "which we think of a grammar is closely intertwined with what we consider to be a\n",
      "language. Is it a large but finite set of observed utterances and written texts? Is it some-\n",
      "thing more abstract like the implicit knowledge that competent speakers have about\n",
      "grammatical sentences? Or is it some combination of the two? We won’t take a stand\n",
      "on this issue, but instead will introduce the main approaches.\n",
      "In this chapter, we will adopt the formal framework of “generative grammar,” in which\n",
      "a “language” is considered to be nothing more than an enormous collection of all\n",
      "grammatical sentences, and a grammar is a formal notation that can be used for “gen-\n",
      "erating” the members of this set. Grammars use recursive productions of the form\n",
      "S → S and S, as we will explore in Section 8.3. In Chapter 10 we will extend this, to\n",
      "automatically build up the meaning of a sentence out of the meanings of its parts.\n",
      "Ubiquitous Ambiguity\n",
      "A well-known example of ambiguity is shown in (2), from the Groucho Marx movie,\n",
      "Animal Crackers (1930):\n",
      "(2) While hunting in Africa, I shot an elephant in my pajamas. How an elephant\n",
      "got into my pajamas I’ll never know.\n",
      "Let’s take a closer look at the ambiguity in the phrase: I shot an elephant in my paja-\n",
      "mas. First we need to define a simple grammar:\n",
      ">>> groucho_grammar = nltk.parse_cfg(\"\"\"\n",
      "... S -> NP VP\n",
      "... PP -> P NP\n",
      "... NP -> Det N | Det N PP | 'I'\n",
      "... VP -> V NP | VP PP\n",
      "... Det -> 'an' | 'my'\n",
      "... N -> 'elephant' | 'pajamas'\n",
      "... V -> 'shot'\n",
      "... P -> 'in'\n",
      "... \"\"\")\n",
      "8.1  Some Grammatical Dilemmas | 293...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 314, 'page_label': '293', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4677}\n",
      "\n",
      "--- Chunk 4678 ---\n",
      "Content:\n",
      "This grammar permits the sentence to be analyzed in two ways, depending on whether\n",
      "the prepositional phrase in my pajamas describes the elephant or the shooting event.\n",
      ">>> sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
      ">>> parser = nltk.ChartParser(groucho_grammar)\n",
      ">>> trees = parser.nbest_parse(sent)\n",
      ">>> for tree in trees:\n",
      "...     print tree\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "The \n",
      "program produces two bracketed structures, which we can depict as trees, as\n",
      "shown in (3):\n",
      "(3) a.\n",
      "b.\n",
      "294 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 315, 'page_label': '294', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4678}\n",
      "\n",
      "--- Chunk 4679 ---\n",
      "Content:\n",
      "Notice that there’s no ambiguity concerning the meaning of any of the words; e.g., the\n",
      "word shot \n",
      "doesn’t refer to the act of using a gun in the first sentence and using a camera\n",
      "in the second sentence.\n",
      "Your Turn: Consider the following sentences and see if you can think\n",
      "of two quite different interpretations: Fighting animals could be danger-\n",
      "ous. Visiting relatives can be tiresome. Is ambiguity of the individual\n",
      "words to blame? If not, what is the cause of the ambiguity?\n",
      "This chapter presents grammars and parsing, as the formal and computational methods\n",
      "for investigating and modeling the linguistic phenomena we have been discussing. As\n",
      "we shall see, patterns of well-formedness and ill-formedness in a sequence of words\n",
      "can be understood with respect to the phrase structure and dependencies. We can\n",
      "develop formal models of these structures using grammars and parsers. As before, a\n",
      "key motivation is natural language understanding. How much more of the meaning of\n",
      "a text can we access when we can reliably recognize the linguistic structures it contains?\n",
      "Having read in a text, can a program “understand” it enough to be able to answer simple\n",
      "questions about “what happened” or “who did what to whom”? Also as before, we will\n",
      "develop simple programs to process annotated corpora and perform useful tasks.\n",
      "8.2  What’s the Use of Syntax?\n",
      "Beyond n-grams\n",
      "We gave an example in Chapter 2 of how to use the frequency information in bigrams\n",
      "to generate text that seems perfectly acceptable for small sequences of words but rapidly\n",
      "degenerates into nonsense. Here’s another pair of examples that we created by com-\n",
      "puting the bigrams over the text of a children’s story, The Adventures of Buster\n",
      "Brown (included in the Project Gutenberg Selection Corpus):\n",
      "(4) a. He roared with me the pail slip down his back\n",
      "b. The worst part and clumsy looking for whoever heard light\n",
      "You intuitively know that these sequences are “word-salad,” but you probably find it\n",
      "hard to pin down what’s wrong with them. One benefit of studying grammar is that it\n",
      "provides a conceptual framework and vocabulary for spelling out these intuitions. Let’s\n",
      "take a closer look at the sequence the worst part and clumsy looking. This looks like a\n",
      "coordinate structure, where two phrases are joined by a coordinating conjunction\n",
      "such as and, but, or or. Here’s an informal (and simplified) statement of how coordi-\n",
      "nation works syntactically:\n",
      "Coordinate Structure: if v1 and v2 are both phrases of grammatical category X, then v1\n",
      "and v2 is also a phrase of category X.\n",
      "8.2  What’s the Use of Syntax? | 295...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 316, 'page_label': '295', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4679}\n",
      "\n",
      "--- Chunk 4680 ---\n",
      "Content:\n",
      "Here are a couple of examples. In the first, two NPs (noun phrases) have been conjoined\n",
      "to make an NP, while in the second, two APs (adjective phrases) have been conjoined to\n",
      "make an AP.\n",
      "(5) a. The book’s ending was (NP the worst part and the best part) for me.\n",
      "b. On land they are (AP slow and clumsy looking).\n",
      "What we can’t do is conjoin an NP and an AP, which is why the worst part and clumsy\n",
      "looking is ungrammatical. Before we can formalize these ideas, we need to understand\n",
      "the concept of constituent structure.\n",
      "Constituent structure is based on the observation that words combine with other words\n",
      "to form units. The evidence that a sequence of words forms such a unit is given by\n",
      "substitutability—that is, a sequence of words in a well-formed sentence can be replaced\n",
      "by a shorter sequence without rendering the sentence ill-formed. To clarify this idea,\n",
      "consider the following sentence:\n",
      "(6) The little bear saw the fine fat trout in the brook.\n",
      "The fact that we can substitute He for The little bear indicates that the latter sequence\n",
      "is a unit. By contrast, we cannot replace little bear saw in the same way. (We use an\n",
      "asterisk at the start of a sentence to indicate that it is ungrammatical.)\n",
      "(7) a. He saw the fine fat trout in the brook.\n",
      "b. *The he the fine fat trout in the brook.\n",
      "In Figure 8-1, we systematically substitute longer sequences by shorter ones in a way\n",
      "which preserves grammaticality. Each sequence that forms a unit can in fact be replaced\n",
      "by a single word, and we end up with just two elements.\n",
      "Figure 8-1. Substitution of word sequences: Working from the top row, we can replace particular\n",
      "sequences \n",
      "of words (e.g., the brook) with individual words (e.g., it); repeating this process, we arrive\n",
      "at a grammatical two-word sentence.\n",
      "296 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 317, 'page_label': '296', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4680}\n",
      "\n",
      "--- Chunk 4681 ---\n",
      "Content:\n",
      "In Figure 8-2, we have added grammatical category labels to the words we saw in the\n",
      "earlier figure. The labels NP, VP, and PP stand for noun phrase , verb phrase , and\n",
      "prepositional phrase, respectively.\n",
      "If we now strip out the words apart from the topmost row, add an S node, and flip the\n",
      "figure over, we end up with a standard phrase structure tree, shown in (8). Each node\n",
      "in this tree (including the words) is called a constituent. The immediate constitu-\n",
      "ents of S are NP and VP.\n",
      "(8)\n",
      "As we saw in Section 8.1, sentences can have arbitrary length. Conse-\n",
      "quently, phrase structure trees can have arbitrary depth. The cascaded\n",
      "chunk parsers we saw in Section 7.4  can only produce structures of\n",
      "bounded depth, so chunking methods aren’t applicable here.\n",
      "Figure 8-2. Substitution of word sequences plus grammatical categories: This diagram reproduces\n",
      "Figure \n",
      "8-1 along with grammatical categories corresponding to noun phrases (NP), verb phrases\n",
      "(VP), prepositional phrases (PP), and nominals (Nom).\n",
      "8.2  What’s the Use of Syntax? | 297...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 318, 'page_label': '297', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4681}\n",
      "\n",
      "--- Chunk 4682 ---\n",
      "Content:\n",
      "As we will see in the next section, a grammar specifies how the sentence can be subdi-\n",
      "vided \n",
      "into its immediate constituents, and how these can be further subdivided until\n",
      "we reach the level of individual words.\n",
      "8.3  Context-Free Grammar\n",
      "A Simple Grammar\n",
      "Let’s start off by looking at a simple context-free grammar (CFG). By convention,\n",
      "the lefthand side of the first production is the start-symbol of the grammar, typically\n",
      "S, and all well-formed trees must have this symbol as their root label. In NLTK, context-\n",
      "free grammars are defined in the nltk.grammar module. In Example 8-1 we define a\n",
      "grammar and show how to parse a simple sentence admitted by the grammar.\n",
      "Example 8-1. A simple context-free grammar.\n",
      "grammar1 = nltk.parse_cfg(\"\"\"\n",
      "  S -> NP VP\n",
      "  VP -> V NP | V NP PP\n",
      "  PP -> P NP\n",
      "  V -> \"saw\" | \"ate\" | \"walked\"\n",
      "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
      "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
      "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
      "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
      "  \"\"\")\n",
      ">>> sent = \"Mary saw Bob\".split()\n",
      ">>> rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
      ">>> for tree in rd_parser.nbest_parse(sent):\n",
      "...      print tree\n",
      "(S (NP Mary) (VP (V saw) (NP Bob)))\n",
      "The grammar in Example 8-1 contains productions involving various syntactic cate-\n",
      "gories, as laid out in Table 8-1. The recursive descent parser used here can also be\n",
      "inspected via a graphical interface, as illustrated in Figure 8-3; we discuss this parser\n",
      "in more detail in Section 8.4.\n",
      "Table 8-1. Syntactic categories\n",
      "Symbol Meaning Example\n",
      "S sentence the man walked\n",
      "NP noun phrase a dog\n",
      "VP verb phrase saw a park\n",
      "PP prepositional phrase with a telescope\n",
      "Det determiner the\n",
      "N noun dog\n",
      "298 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 319, 'page_label': '298', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4682}\n",
      "\n",
      "--- Chunk 4683 ---\n",
      "Content:\n",
      "Symbol Meaning Example\n",
      "V verb walked\n",
      "P preposition in\n",
      "A production like VP -> V NP | V NP PP has a disjunction on the righthand side, shown\n",
      "by the |, and is an abbreviation for the two productions VP -> V NP and VP -> V NP PP.\n",
      "If we parse the sentence The dog saw a man in the park using the grammar shown in\n",
      "Example 8-1, we end up with two trees, similar to those we saw for (3):\n",
      "(9) a.\n",
      "b.\n",
      "Since our grammar licenses two trees for this sentence, the sentence is said to be struc-\n",
      "turally ambiguous. \n",
      "The ambiguity in question is called a prepositional phrase at-\n",
      "tachment ambiguity, as we saw earlier in this chapter. As you may recall, it is an\n",
      "ambiguity about attachment since the PP in the park needs to be attached to one of two\n",
      "places in the tree: either as a child of VP or else as a child of NP. When the PP is attached\n",
      "to VP, the intended interpretation is that the seeing event happened in the park.\n",
      "8.3  Context-Free Grammar | 299...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 320, 'page_label': '299', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4683}\n",
      "\n",
      "--- Chunk 4684 ---\n",
      "Content:\n",
      "However, if the PP is attached to NP, then it was the man who was in the park, and the\n",
      "agent of the seeing (the dog) might have been sitting on the balcony of an apartment\n",
      "overlooking the park.\n",
      "Writing Your Own Grammars\n",
      "If you are interested in experimenting with writing CFGs, you will find it helpful to\n",
      "create and edit your grammar in a text file, say, mygrammar.cfg. You can then load it\n",
      "into NLTK and parse with it as follows:\n",
      ">>> grammar1 = nltk.data.load('file:mygrammar.cfg')\n",
      ">>> sent = \"Mary saw Bob\".split()\n",
      ">>> rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
      ">>> for tree in rd_parser.nbest_parse(sent):\n",
      "...      print tree\n",
      "Make sure that you put a .cfg suffix on the filename, and that there are no spaces in the\n",
      "string 'file:mygrammar.cfg'. If the command print tree produces no output, this is\n",
      "probably because your sentence sent is not admitted by your grammar. In this case,\n",
      "call the parser with tracing set to be on: rd_parser = nltk.RecursiveDescent\n",
      "Figure 8-3. Recursive descent parser demo: This tool allows you to watch the operation of a recursive\n",
      "descent parser as it grows the parse tree and matches it against the input words.\n",
      "300 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 321, 'page_label': '300', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4684}\n",
      "\n",
      "--- Chunk 4685 ---\n",
      "Content:\n",
      "Parser(grammar1, trace=2). You can also check what productions are currently in the\n",
      "grammar with the command for p in grammar1.productions(): print p.\n",
      "When you write CFGs for parsing in NLTK, you cannot combine grammatical cate-\n",
      "gories with lexical items on the righthand side of the same production. Thus, a pro-\n",
      "duction such as PP -> 'of' NP is disallowed. In addition, you are not permitted to place\n",
      "multiword lexical items on the righthand side of a production. So rather than writing\n",
      "NP -> 'New York', you have to resort to something like NP -> 'New_York' instead.\n",
      "Recursion in Syntactic Structure\n",
      "A grammar is said to be recursive if a category occurring on the lefthand side of a\n",
      "production also appears on the righthand side of a production, as illustrated in Exam-\n",
      "ple 8-2. The production Nom -> Adj Nom (where Nom is the category of nominals) involves\n",
      "direct recursion on the category Nom, whereas indirect recursion on S arises from the\n",
      "combination of two productions, namely S -> NP VP and VP -> V S.\n",
      "Example 8-2. A recursive context-free grammar.\n",
      "grammar2 = nltk.parse_cfg(\"\"\"\n",
      "  S  -> NP VP\n",
      "  NP -> Det Nom | PropN\n",
      "  Nom -> Adj Nom | N\n",
      "  VP -> V Adj | V NP | V S | V NP PP\n",
      "  PP -> P NP\n",
      "  PropN -> 'Buster' | 'Chatterer' | 'Joe'\n",
      "  Det -> 'the' | 'a'\n",
      "  N -> 'bear' | 'squirrel' | 'tree' | 'fish' | 'log'\n",
      "  Adj  -> 'angry' | 'frightened' |  'little' | 'tall'\n",
      "  V ->  'chased'  | 'saw' | 'said' | 'thought' | 'was' | 'put'\n",
      "  P -> 'on'\n",
      "  \"\"\")\n",
      "To see how recursion arises from this grammar, consider the following trees. (10a)\n",
      "involves nested nominal phrases, while (10b) contains nested sentences.\n",
      "8.3  Context-Free Grammar | 301...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 322, 'page_label': '301', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4685}\n",
      "\n",
      "--- Chunk 4686 ---\n",
      "Content:\n",
      "(10) a.\n",
      "b.\n",
      "We’ve only illustrated two levels of recursion here, but there’s no upper limit on the\n",
      "depth. \n",
      "You can experiment with parsing sentences that involve more deeply nested\n",
      "structures. Beware that the RecursiveDescentParser is unable to handle left-\n",
      "recursive productions of the form X -> X Y; we will return to this in Section 8.4.\n",
      "8.4  Parsing with Context-Free Grammar\n",
      "A parser processes input sentences according to the productions of a grammar, and\n",
      "builds one or more constituent structures that conform to the grammar. A grammar is\n",
      "a declarative specification of well-formedness—it is actually just a string, not a pro-\n",
      "gram. A parser is a procedural interpretation of the grammar. It searches through the\n",
      "space of trees licensed by a grammar to find one that has the required sentence along\n",
      "its fringe.\n",
      "302 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 323, 'page_label': '302', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4686}\n",
      "\n",
      "--- Chunk 4687 ---\n",
      "Content:\n",
      "A parser permits a grammar to be evaluated against a collection of test sentences, help-\n",
      "ing \n",
      "linguists to discover mistakes in their grammatical analysis. A parser can serve as a\n",
      "model of psycholinguistic processing, helping to explain the difficulties that humans\n",
      "have with processing certain syntactic constructions. Many natural language applica-\n",
      "tions involve parsing at some point; for example, we would expect the natural language\n",
      "questions submitted to a question-answering system to undergo parsing as an initial\n",
      "step.\n",
      "In this section, we see two simple parsing algorithms, a top-down method called re-\n",
      "cursive descent parsing, and a bottom-up method called shift-reduce parsing. We also\n",
      "see some more sophisticated algorithms, a top-down method with bottom-up filtering\n",
      "called left-corner parsing, and a dynamic programming technique called chart parsing.\n",
      "Recursive Descent Parsing\n",
      "The simplest kind of parser interprets a grammar as a specification of how to break a\n",
      "high-level goal into several lower-level subgoals. The top-level goal is to find an S. The\n",
      "S → NP VP production permits the parser to replace this goal with two subgoals: find an\n",
      "NP, then find a VP. Each of these subgoals can be replaced in turn by sub-subgoals, using\n",
      "productions that have NP and VP on their lefthand side. Eventually, this expansion\n",
      "process leads to subgoals such as: find the word telescope. Such subgoals can be directly\n",
      "compared against the input sequence, and succeed if the next word is matched. If there\n",
      "is no match, the parser must back up and try a different alternative.\n",
      "The recursive descent parser builds a parse tree during this process. With the initial\n",
      "goal (find an S), the S root node is created. As the process recursively expands its goals\n",
      "using the productions of the grammar, the parse tree is extended downwards (hence\n",
      "the name recursive descent). We can see this in action using the graphical demonstration\n",
      "nltk.app.rdparser(). Six stages of the execution of this parser are shown in Figure 8-4.\n",
      "During this process, the parser is often forced to choose between several possible pro-\n",
      "ductions. For example, in going from step 3 to step 4, it tries to find productions with\n",
      "N on the lefthand side. The first of these is N → man. When this does not work it\n",
      "backtracks, and tries other N productions in order, until it gets to N → dog, which\n",
      "matches the next word in the input sentence. Much later, as shown in step 5, it finds\n",
      "a complete parse. This is a tree that covers the entire sentence, without any dangling\n",
      "edges. Once a parse has been found, we can get the parser to look for additional parses.\n",
      "Again it will backtrack and explore other choices of production in case any of them\n",
      "result in a parse.\n",
      "NLTK provides a recursive descent parser:\n",
      ">>> rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
      ">>> sent = 'Mary saw a dog'.split()\n",
      ">>> for t in rd_parser.nbest_parse(sent):\n",
      "...     print t\n",
      "(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\n",
      "8.4  Parsing with Context-Free Grammar | 303...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 324, 'page_label': '303', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4687}\n",
      "\n",
      "--- Chunk 4688 ---\n",
      "Content:\n",
      "RecursiveDescentParser() takes an optional parameter trace. If trace\n",
      "is greater than zero, then the parser will report the steps that it takes as\n",
      "it parses a text.\n",
      "Recursive descent parsing has three key shortcomings. First, left-recursive productions\n",
      "like NP -> NP PP send it into an infinite loop. Second, the parser wastes a lot of time\n",
      "considering words and structures that do not correspond to the input sentence. Third,\n",
      "the backtracking process may discard parsed constituents that will need to be rebuilt\n",
      "again later. For example, backtracking over VP -> V NP will discard the subtree created\n",
      "for the NP. If the parser then proceeds with VP -> V NP PP, then the NP subtree must be\n",
      "created all over again.\n",
      "Recursive descent parsing is a kind of top-down parsing. Top-down parsers use a\n",
      "grammar to predict what the input will be, before inspecting the input! However, since\n",
      "the input is available to the parser all along, it would be more sensible to consider the\n",
      "input sentence from the very beginning. This approach is called bottom-up parsing,\n",
      "and we will see an example in the next section.\n",
      "Shift-Reduce Parsing\n",
      "A simple kind of bottom-up parser is the shift-reduce parser. In common with all\n",
      "bottom-up parsers, a shift-reduce parser tries to find sequences of words and phrases\n",
      "that correspond to the righthand side of a grammar production, and replace them with\n",
      "the lefthand side, until the whole sentence is reduced to an S.\n",
      "Figure 8-4. Six stages of a recursive descent parser: The parser begins with a tree consisting of the\n",
      "node \n",
      "S; at each stage it consults the grammar to find a production that can be used to enlarge the tree;\n",
      "when a lexical production is encountered, its word is compared against the input; after a complete\n",
      "parse has been found, the parser backtracks to look for more parses.\n",
      "304 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 325, 'page_label': '304', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4688}\n",
      "\n",
      "--- Chunk 4689 ---\n",
      "Content:\n",
      "The shift-reduce parser repeatedly pushes the next input word onto a stack ( Sec-\n",
      "tion 4.1); \n",
      "this is the shift operation. If the top n items on the stack match the n items\n",
      "on the righthand side of some production, then they are all popped off the stack, and\n",
      "the item on the lefthand side of the production is pushed onto the stack. This replace-\n",
      "ment of the top n items with a single item is the reduce operation. The operation may\n",
      "be applied only to the top of the stack; reducing items lower in the stack must be done\n",
      "before later items are pushed onto the stack. The parser finishes when all the input is\n",
      "consumed and there is only one item remaining on the stack, a parse tree with an S\n",
      "node as its root. The shift-reduce parser builds a parse tree during the above process.\n",
      "Each time it pops n items off the stack, it combines them into a partial parse tree, and\n",
      "pushes this back onto the stack. We can see the shift-reduce parsing algorithm in action\n",
      "using the graphical demonstration nltk.app.srparser(). Six stages of the execution of\n",
      "this parser are shown in Figure 8-5.\n",
      "Figure 8-5. Six stages of a shift-reduce parser: The parser begins by shifting the first input word onto\n",
      "its \n",
      "stack; once the top items on the stack match the righthand side of a grammar production, they can\n",
      "be replaced with the lefthand side of that production; the parser succeeds once all input is consumed\n",
      "and one S item remains on the stack.\n",
      "NLTK provides ShiftReduceParser(), a simple implementation of a shift-reduce parser.\n",
      "This parser does not implement any backtracking, so it is not guaranteed to find a parse\n",
      "for a text, even if one exists. Furthermore, it will only find at most one parse, even if\n",
      "more parses exist. We can provide an optional trace parameter that controls how ver-\n",
      "bosely the parser reports the steps that it takes as it parses a text:\n",
      "8.4  Parsing with Context-Free Grammar | 305...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 326, 'page_label': '305', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4689}\n",
      "\n",
      "--- Chunk 4690 ---\n",
      "Content:\n",
      ">>> sr_parse = nltk.ShiftReduceParser(grammar1)\n",
      ">>> sent = 'Mary saw a dog'.split()\n",
      ">>> print sr_parse.parse(sent)\n",
      "  (S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\n",
      "Your Turn: Run this parser in tracing mode to see the sequence of shift\n",
      "and reduce operations, using sr_parse = nltk.ShiftReduceParser(gram\n",
      "mar1, trace=2).\n",
      "A shift-reduce parser can reach a dead end and fail to find any parse, even if the input\n",
      "sentence is well-formed according to the grammar. When this happens, no input re-\n",
      "mains, and the stack contains items that cannot be reduced to an S. The problem arises\n",
      "because there are choices made earlier that cannot be undone by the parser (although\n",
      "users of the graphical demonstration can undo their choices). There are two kinds of\n",
      "choices to be made by the parser: (a) which reduction to do when more than one is\n",
      "possible and (b) whether to shift or reduce when either action is possible.\n",
      "A shift-reduce parser may be extended to implement policies for resolving such con-\n",
      "flicts. For example, it may address shift-reduce conflicts by shifting only when no re-\n",
      "ductions are possible, and it may address reduce-reduce conflicts by favoring the re-\n",
      "duction operation that removes the most items from the stack. (A generalization of the\n",
      "shift-reduce parser, a “lookahead LR parser,” is commonly used in programming lan-\n",
      "guage compilers.)\n",
      "The advantages of shift-reduce parsers over recursive descent parsers is that they only\n",
      "build structure that corresponds to the words in the input. Furthermore, they only build\n",
      "each substructure once; e.g., NP(Det(the), N(man)) is only built and pushed onto the\n",
      "stack a single time, regardless of whether it will later be used by the VP -> V NP PP\n",
      "reduction or the NP -> NP PP reduction.\n",
      "The Left-Corner Parser\n",
      "One of the problems with the recursive descent parser is that it goes into an infinite\n",
      "loop when it encounters a left-recursive production. This is because it applies the\n",
      "grammar productions blindly, without considering the actual input sentence. A left-\n",
      "corner parser is a hybrid between the bottom-up and top-down approaches we have\n",
      "seen.\n",
      "A left-corner parser is a top-down parser with bottom-up filtering. Unlike an ordinary\n",
      "recursive descent parser, it does not get trapped in left-recursive productions. Before\n",
      "starting its work, a left-corner parser preprocesses the context-free grammar to build a\n",
      "table where each row contains two cells, the first holding a non-terminal, and the sec-\n",
      "ond holding the collection of possible left corners of that non-terminal. Table 8-2 il-\n",
      "lustrates this for the grammar from grammar2.\n",
      "306 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 327, 'page_label': '306', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4690}\n",
      "\n",
      "--- Chunk 4691 ---\n",
      "Content:\n",
      "Table 8-2. Left corners in grammar2\n",
      "Category Left corners (pre-terminals)\n",
      "S NP\n",
      "NP Det, PropN\n",
      "VP V\n",
      "PP\n",
      "P\n",
      "Each time a production is considered by the parser, it checks that the next input word\n",
      "is compatible with at least one of the pre-terminal categories in the left-corner table.\n",
      "Well-Formed Substring Tables\n",
      "The \n",
      "simple parsers discussed in the previous sections suffer from limitations in both\n",
      "completeness and efficiency. In order to remedy these, we will apply the algorithm\n",
      "design technique of dynamic programming to the parsing problem. As we saw in\n",
      "Section 4.7, dynamic programming stores intermediate results and reuses them when\n",
      "appropriate, achieving significant efficiency gains. This technique can be applied to\n",
      "syntactic parsing, allowing us to store partial solutions to the parsing task and then\n",
      "look them up as necessary in order to efficiently arrive at a complete solution. This\n",
      "approach to parsing is known as chart parsing. We introduce the main idea in this\n",
      "section; see the online materials available for this chapter for more implementation\n",
      "details.\n",
      "Dynamic programming allows us to build the PP in my pajamas just once. The first time\n",
      "we build it we save it in a table, then we look it up when we need to use it as a sub-\n",
      "constituent of either the object NP or the higher VP. This table is known as a well-formed\n",
      "substring table, or WFST for short. (The term “substring” refers to a contiguous se-\n",
      "quence of words within a sentence.) We will show how to construct the WFST bottom-\n",
      "up so as to systematically record what syntactic constituents have been found.\n",
      "Let’s set our input to be the sentence in (2). The numerically specified spans of the\n",
      "WFST are reminiscent of Python’s slice notation ( Section 3.2). Another way to think\n",
      "about the data structure is shown in Figure 8-6, a data structure known as a chart.\n",
      "Figure 8-6. The chart data structure: Words are the edge labels of a linear graph structure.\n",
      "In \n",
      "a WFST, we record the position of the words by filling in cells in a triangular matrix:\n",
      "the vertical axis will denote the start position of a substring, while the horizontal axis\n",
      "will denote the end position (thus shot will appear in the cell with coordinates (1, 2)).\n",
      "To simplify this presentation, we will assume each word has a unique lexical category,\n",
      "8.4  Parsing with Context-Free Grammar | 307...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 328, 'page_label': '307', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4691}\n",
      "\n",
      "--- Chunk 4692 ---\n",
      "Content:\n",
      "and we will store this (not the word) in the matrix. So cell (1, 2) will contain the entry\n",
      "V. \n",
      "More generally, if our input string is a1a2 ... an, and our grammar contains a pro-\n",
      "duction of the form A → ai, then we add A to the cell (i-1, i).\n",
      "So, for every word in text, we can look up in our grammar what category it belongs to.\n",
      ">>> text = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
      "[V -> 'shot']\n",
      "For our WFST, we create an (n-1) × (n-1) matrix as a list of lists in Python, and initialize\n",
      "it with the lexical categories of each token in the init_wfst() function in Exam-\n",
      "ple 8-3. We also define a utility function display() to pretty-print the WFST for us. As\n",
      "expected, there is a V in cell (1, 2).\n",
      "Example 8-3...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 329, 'page_label': '308', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4692}\n",
      "\n",
      "--- Chunk 4693 ---\n",
      "Content:\n",
      ". As\n",
      "expected, there is a V in cell (1, 2).\n",
      "Example 8-3. Acceptor using well-formed substring table.\n",
      "def init_wfst(tokens, grammar):\n",
      "    numtokens = len(tokens)\n",
      "    wfst = [[None for i in range(numtokens+1)] for j in range(numtokens+1)]\n",
      "    for i in range(numtokens):\n",
      "        productions = grammar.productions(rhs=tokens[i])\n",
      "        wfst[i][i+1] = productions[0].lhs()\n",
      "    return wfst\n",
      "def complete_wfst(wfst, tokens, grammar, trace=False):\n",
      "    index = dict((p.rhs(), p.lhs()) for p in grammar.productions())\n",
      "    numtokens = len(tokens)\n",
      "    for span in range(2, numtokens+1):\n",
      "        for start in range(numtokens+1-span):\n",
      "            end = start + span\n",
      "            for mid in range(start+1, end):\n",
      "                nt1, nt2 = wfst[start][mid], wfst[mid][end]\n",
      "                if nt1 and nt2 and (nt1,nt2) in index:\n",
      "                    wfst[start][end] = index[(nt1,nt2)]\n",
      "                    if trace:\n",
      "                        print \"[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]\" % \\\n",
      "                        (start, nt1, mid, nt2, end, start, index[(nt1,nt2)], end)\n",
      "    return wfst\n",
      "def display(wfst, tokens):\n",
      "    print '\\nWFST ' + ' '.join([(\"%-4d\" % i) for i in range(1, len(wfst))])\n",
      "    for i in range(len(wfst)-1):\n",
      "        print \"%d   \" % i,\n",
      "        for j in range(1, len(wfst)):\n",
      "            print \"%-4s\" % (wfst[i][j] or '.'),\n",
      "        print\n",
      ">>> tokens = \"I shot an elephant in my pajamas\".split()\n",
      ">>> wfst0 = init_wfst(tokens, groucho_grammar)\n",
      ">>> display(wfst0, tokens)\n",
      "WFST 1    2    3    4    5    6    7\n",
      "0    NP...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 329, 'page_label': '308', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4693}\n",
      "\n",
      "--- Chunk 4694 ---\n",
      "Content:\n",
      ".    .    .    .    .    .\n",
      "1    .    V    .    .    .    .    .\n",
      "2    .    .    Det  .    .    .    .\n",
      "3    .    .    .    N    .    .    .\n",
      "308 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 329, 'page_label': '308', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4694}\n",
      "\n",
      "--- Chunk 4695 ---\n",
      "Content:\n",
      "4    .    .    .    .    P    .    .\n",
      "5    .    .    .    .    .    Det  .\n",
      "6    .    .    .    .    .    .    N\n",
      ">>> wfst1 = complete_wfst(wfst0, tokens, groucho_grammar)\n",
      ">>> display(wfst1, tokens)\n",
      "WFST 1    2    3    4    5    6    7\n",
      "0    NP   .    .    S    .    .    S\n",
      "1    .    V    .    VP   .    .    VP\n",
      "2    .    .    Det  NP   .    .    .\n",
      "3    .    .    .    N    .    .    .\n",
      "4    .    .    .    .    P    .    PP\n",
      "5    .    .    .    .    .    Det  NP\n",
      "6    .    .    .    .    .    .    N\n",
      "Returning \n",
      "to our tabular representation, given that we have Det in cell (2, 3) for the\n",
      "word an, and N in cell (3, 4) for the word elephant, what should we put into cell (2, 4)\n",
      "for an elephant? We need to find a production of the form A → Det N. Consulting the\n",
      "grammar, we know that we can enter NP in cell (0, 2).\n",
      "More generally, we can enter A in (i, j) if there is a production A → B C, and we find\n",
      "non-terminal B in (i, k) and C in (k, j). The program in Example 8-3 uses this rule to\n",
      "complete the WFST...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 330, 'page_label': '309', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4695}\n",
      "\n",
      "--- Chunk 4696 ---\n",
      "Content:\n",
      ". Consulting the\n",
      "grammar, we know that we can enter NP in cell (0, 2).\n",
      "More generally, we can enter A in (i, j) if there is a production A → B C, and we find\n",
      "non-terminal B in (i, k) and C in (k, j). The program in Example 8-3 uses this rule to\n",
      "complete the WFST. By setting trace to True when calling the function\n",
      "complete_wfst(), we see tracing output that shows the WFST being constructed:\n",
      ">>> wfst1 = complete_wfst(wfst0, tokens, groucho_grammar, trace=True)\n",
      "[2] Det [3]   N [4] ==> [2]  NP [4]\n",
      "[5] Det [6]   N [7] ==> [5]  NP [7]\n",
      "[1]   V [2]  NP [4] ==> [1]  VP [4]\n",
      "[4]   P [5]  NP [7] ==> [4]  PP [7]\n",
      "[0]  NP [1]  VP [4] ==> [0]   S [4]\n",
      "[1]  VP [4]  PP [7] ==> [1]  VP [7]\n",
      "[0]  NP [1]  VP [7] ==> [0]   S [7]\n",
      "For example, this says that since we found Det at wfst[0][1] and N at wfst[1][2], we\n",
      "can add NP to wfst[0][2].\n",
      "To help us easily retrieve productions by their righthand sides, we create\n",
      "an \n",
      "index for the grammar. This is an example of a space-time trade-off:\n",
      "we do a reverse lookup on the grammar, instead of having to check\n",
      "through entire list of productions each time we want to look up via the\n",
      "righthand side.\n",
      "We conclude that there is a parse for the whole input string once we have constructed\n",
      "an S node in cell (0, 7), showing that we have found a sentence that covers the whole\n",
      "input. The final state of the WFST is depicted in Figure 8-7.\n",
      "Notice that we have not used any built-in parsing functions here. We’ve implemented\n",
      "a complete primitive chart parser from the ground up!\n",
      "WFSTs have several shortcomings. First, as you can see, the WFST is not itself a parse\n",
      "tree, so the technique is strictly speaking recognizing that a sentence is admitted by a\n",
      "8.4  Parsing with Context-Free Grammar | 309...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 330, 'page_label': '309', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4696}\n",
      "\n",
      "--- Chunk 4697 ---\n",
      "Content:\n",
      "grammar, rather than parsing it. Second, it requires every non-lexical grammar pro-\n",
      "duction \n",
      "to be binary. Although it is possible to convert an arbitrary CFG into this form,\n",
      "we would prefer to use an approach without such a requirement. Third, as a bottom-\n",
      "up approach it is potentially wasteful, being able to propose constituents in locations\n",
      "that would not be licensed by the grammar.\n",
      "Finally, the WFST did not represent the structural ambiguity in the sentence (i.e., the\n",
      "two verb phrase readings). The VP in cell (2,8) was actually entered twice, once for a V\n",
      "NP reading, and once for a VP PP reading. These are different hypotheses, and the second\n",
      "overwrote the first (as it happens, this didn’t matter since the lefthand side was the\n",
      "same). Chart parsers use a slightly richer data structure and some interesting algorithms\n",
      "to solve these problems (see Section 8.8).\n",
      "Your Turn: Try out the interactive chart parser application\n",
      "nltk.app.chartparser().\n",
      "8.5  Dependencies and Dependency Grammar\n",
      "Phrase structure grammar is concerned with how words and sequences of words com-\n",
      "bine to form constituents. A distinct and complementary approach, dependency\n",
      "grammar, focuses instead on how words relate to other words. Dependency is a binary\n",
      "asymmetric relation that holds between a head and its dependents. The head of a\n",
      "sentence is usually taken to be the tensed verb, and every other word is either dependent\n",
      "on the sentence head or connects to it through a path of dependencies.\n",
      "A dependency representation is a labeled directed graph, where the nodes are the lexical\n",
      "items and the labeled arcs represent dependency relations from heads to dependents.\n",
      "Figure 8-8  illustrates a dependency graph, where arrows point from heads to their\n",
      "dependents.\n",
      "Figure 8-7. The chart data structure: Non-terminals are represented as extra edges in the chart.\n",
      "310 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 331, 'page_label': '310', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4697}\n",
      "\n",
      "--- Chunk 4698 ---\n",
      "Content:\n",
      "The arcs in Figure 8-8 are labeled with the grammatical function that holds between a\n",
      "dependent and its head. For example, I is the SBJ (subject) of shot (which is the head\n",
      "of the whole sentence), and in is an NMOD (noun modifier of elephant). In contrast to\n",
      "phrase structure grammar, therefore, dependency grammars can be used to directly\n",
      "express grammatical functions as a type of dependency.\n",
      "Here’s one way of encoding a dependency grammar in NLTK—note that it only cap-\n",
      "tures bare dependency information without specifying the type of dependency:\n",
      ">>> groucho_dep_grammar = nltk.parse_dependency_grammar(\"\"\"\n",
      "... 'shot' -> 'I' | 'elephant' | 'in'\n",
      "... 'elephant' -> 'an' | 'in'\n",
      "... 'in' -> 'pajamas'\n",
      "... 'pajamas' -> 'my'\n",
      "... \"\"\")\n",
      ">>> print groucho_dep_grammar\n",
      "Dependency grammar with 7 productions\n",
      "  'shot' -> 'I'\n",
      "  'shot' -> 'elephant'\n",
      "  'shot' -> 'in'\n",
      "  'elephant' -> 'an'\n",
      "  'elephant' -> 'in'\n",
      "  'in' -> 'pajamas'\n",
      "  'pajamas' -> 'my'\n",
      "A dependency graph is projective if, when all the words are written in linear order, the\n",
      "edges can be drawn above the words without crossing. This is equivalent to saying that\n",
      "a word and all its descendants (dependents and dependents of its dependents, etc.)\n",
      "form a contiguous sequence of words within the sentence. Figure 8-8 is projective, and\n",
      "we can parse many sentences in English using a projective dependency parser. The next\n",
      "example shows how groucho_dep_grammar provides an alternative approach to captur-\n",
      "ing the attachment ambiguity that we examined earlier with phrase structure grammar.\n",
      ">>> pdp = nltk.ProjectiveDependencyParser(groucho_dep_grammar)\n",
      ">>> sent = 'I shot an elephant in my pajamas'.split()\n",
      ">>> trees = pdp.parse(sent)\n",
      ">>> for tree in trees:\n",
      "...     print tree\n",
      "(shot I (elephant an (in (pajamas my))))\n",
      "(shot I (elephant an) (in (pajamas my)))\n",
      "Figure 8-8. Dependency structure: Arrows point from heads to their dependents; labels indicate the\n",
      "grammatical function of the dependent as subject, object, or modifier.\n",
      "8.5  Dependencies and Dependency Grammar | 311...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 332, 'page_label': '311', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4698}\n",
      "\n",
      "--- Chunk 4699 ---\n",
      "Content:\n",
      "These bracketed dependency structures can also be displayed as trees, where dep-\n",
      "endents are shown as children of their heads.\n",
      "(11)\n",
      "In languages with more flexible word order than English, non-projective dependencies\n",
      "are more frequent.\n",
      "Various \n",
      "criteria have been proposed for deciding what is the head H and what is the\n",
      "dependent D in a construction C. Some of the most important are the following:\n",
      "1. H determines the distribution class of C; or alternatively, the external syntactic\n",
      "properties of C are due to H.\n",
      "2. H determines the semantic type of C.\n",
      "3. H is obligatory while D may be optional.\n",
      "4. H selects D and determines whether it is obligatory or optional.\n",
      "5. The morphological form of D is determined by H (e.g., agreement or case\n",
      "government).\n",
      "When we say in a phrase structure grammar that the immediate constituents of a PP\n",
      "are P and NP, we are implicitly appealing to the head/dependent distinction. A prepo-\n",
      "sitional phrase is a phrase whose head is a preposition; moreover, the NP is a dependent\n",
      "of P. The same distinction carries over to the other types of phrase that we have dis-\n",
      "cussed. The key point to note here is that although phrase structure grammars seem\n",
      "very different from dependency grammars, they implicitly embody a recognition of\n",
      "dependency relations. Although CFGs are not intended to directly capture dependen-\n",
      "cies, more recent linguistic frameworks have increasingly adopted formalisms which\n",
      "combine aspects of both approaches.\n",
      "Valency and the Lexicon\n",
      "Let us take a closer look at verbs and their dependents. The grammar in Example 8-2\n",
      "correctly generates examples like (12).\n",
      "312 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 333, 'page_label': '312', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4699}\n",
      "\n",
      "--- Chunk 4700 ---\n",
      "Content:\n",
      "(12) a. The squirrel was frightened.\n",
      "b.\n",
      "Chatterer saw the bear.\n",
      "c. Chatterer thought Buster was angry.\n",
      "d. Joe put the fish on the log.\n",
      "These possibilities correspond to the productions in Table 8-3.\n",
      "Table 8-3. VP productions and their lexical heads\n",
      "Production Lexical head\n",
      "VP -> V Adj was\n",
      "VP -> V NP saw\n",
      "VP -> V S thought\n",
      "VP -> V NP PP put\n",
      "That is, was can occur with a following Adj, saw can occur with a following NP,\n",
      "thought can occur with a following S, and put can occur with a following NP and PP. The\n",
      "dependents Adj, NP, S, and PP are often called complements of the respective verbs,\n",
      "and there are strong constraints on what verbs can occur with what complements. By\n",
      "contrast with (12), the word sequences in (13) are ill-formed:\n",
      "(13) a. *The squirrel was Buster was angry.\n",
      "b. *Chatterer saw frightened.\n",
      "c. *Chatterer thought the bear.\n",
      "d. *Joe put on the log.\n",
      "With a little imagination, it is possible to invent contexts in which un-\n",
      "usual \n",
      "combinations of verbs and complements are interpretable. How-\n",
      "ever, we assume that the examples in (13) are to be interpreted in neutral\n",
      "contexts.\n",
      "In the tradition of dependency grammar, the verbs in Table 8-3 are said to have different\n",
      "valencies. Valency restrictions are not just applicable to verbs, but also to the other\n",
      "classes of heads.\n",
      "Within frameworks based on phrase structure grammar, various techniques have been\n",
      "proposed for excluding the ungrammatical examples in (13). In a CFG, we need some\n",
      "way of constraining grammar productions which expand VP so that verbs co-occur\n",
      "only with their correct complements. We can do this by dividing the class of verbs into\n",
      "“subcategories,” each of which is associated with a different set of complements. For\n",
      "example, transitive verbs such as chased and saw require a following NP object com-\n",
      "plement; that is, they are subcategorized for NP direct objects. If we introduce a new\n",
      "8.5  Dependencies and Dependency Grammar | 313...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 334, 'page_label': '313', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4700}\n",
      "\n",
      "--- Chunk 4701 ---\n",
      "Content:\n",
      "category label for transitive verbs, namely TV (for transitive verb), then we can use it in\n",
      "the following productions:\n",
      "VP -> TV NP\n",
      "TV -> 'chased' | 'saw'\n",
      "Now *Joe thought the bear  is excluded since we haven’t listed thought as a TV, but\n",
      "Chatterer saw the bear is still allowed. Table 8-4 provides more examples of labels for\n",
      "verb subcategories.\n",
      "Table 8-4. Verb subcategories\n",
      "Symbol Meaning Example\n",
      "IV Intransitive verb barked\n",
      "TV Transitive verb saw a man\n",
      "DatV Dative verb gave a dog to a man\n",
      "SV Sentential verb said that a dog barked\n",
      "Valency is a property of lexical items, and we will discuss it further in Chapter 9.\n",
      "Complements \n",
      "are often contrasted with modifiers (or adjuncts), although both are\n",
      "kinds of dependents. Prepositional phrases, adjectives, and adverbs typically function\n",
      "as modifiers. Unlike complements, modifiers are optional, can often be iterated, and\n",
      "are not selected for by heads in the same way as complements. For example, the adverb\n",
      "really can be added as a modifier to all the sentences in (14):\n",
      "(14) a. The squirrel really was frightened.\n",
      "b. Chatterer really saw the bear.\n",
      "c. Chatterer really thought Buster was angry.\n",
      "d. Joe really put the fish on the log.\n",
      "The structural ambiguity of PP attachment, which we have illustrated in both phrase\n",
      "structure and dependency grammars, corresponds semantically to an ambiguity in the\n",
      "scope of the modifier.\n",
      "Scaling Up\n",
      "So far, we have only considered “toy grammars,” small grammars that illustrate the key\n",
      "aspects of parsing. But there is an obvious question as to whether the approach can be\n",
      "scaled up to cover large corpora of natural languages. How hard would it be to construct\n",
      "such a set of productions by hand? In general, the answer is: very hard. Even if we allow\n",
      "ourselves to use various formal devices that give much more succinct representations\n",
      "of grammar productions, it is still extremely difficult to keep control of the complex\n",
      "interactions between the many productions required to cover the major constructions\n",
      "of a language. In other words, it is hard to modularize grammars so that one portion\n",
      "can be developed independently of the other parts. This in turn means that it is difficult\n",
      "314 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 335, 'page_label': '314', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4701}\n",
      "\n",
      "--- Chunk 4702 ---\n",
      "Content:\n",
      "to distribute the task of grammar writing across a team of linguists. Another difficulty\n",
      "is \n",
      "that as the grammar expands to cover a wider and wider range of constructions, there\n",
      "is a corresponding increase in the number of analyses that are admitted for any one\n",
      "sentence. In other words, ambiguity increases with coverage.\n",
      "Despite these problems, some large collaborative projects have achieved interesting and\n",
      "impressive results in developing rule-based grammars for several languages. Examples\n",
      "are the Lexical Functional Grammar (LFG) Pargram project, the Head-Driven Phrase\n",
      "Structure Grammar (HPSG) LinGO Matrix framework, and the Lexicalized Tree Ad-\n",
      "joining Grammar XTAG Project.\n",
      "8.6  Grammar Development\n",
      "Parsing builds trees over sentences, according to a phrase structure grammar. Now, all\n",
      "the examples we gave earlier only involved toy grammars containing a handful of pro-\n",
      "ductions. What happens if we try to scale up this approach to deal with realistic corpora\n",
      "of language? In this section, we will see how to access treebanks, and look at the chal-\n",
      "lenge of developing broad-coverage grammars.\n",
      "Treebanks and Grammars\n",
      "The corpus module defines the treebank corpus reader, which contains a 10% sample\n",
      "of the Penn Treebank Corpus.\n",
      ">>> from nltk.corpus import treebank\n",
      ">>> t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
      ">>> print t\n",
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR\n",
      "        (IN as)\n",
      "        (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n",
      "We can use this data to help develop a grammar. For example, the program in Exam-\n",
      "ple 8-4 uses a simple filter to find verbs that take sentential complements. Assuming\n",
      "we already have a production of the form VP -> SV S, this information enables us to\n",
      "identify particular verbs that would be included in the expansion of SV.\n",
      "8.6  Grammar Development | 315...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 336, 'page_label': '315', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4702}\n",
      "\n",
      "--- Chunk 4703 ---\n",
      "Content:\n",
      "Example 8-4. Searching a treebank to find sentential complements.\n",
      "def filter(tree):\n",
      "    child_nodes = [child.node for child in tree\n",
      "                   if isinstance(child, nltk.Tree)]\n",
      "    return  (tree.node == 'VP') and ('S' in child_nodes)\n",
      ">>> from nltk.corpus import treebank\n",
      ">>> [subtree for tree in treebank.parsed_sents()\n",
      "...          for subtree in tree.subtrees(filter)]\n",
      " [Tree('VP', [Tree('VBN', ['named']), Tree('S', [Tree('NP-SBJ', ...]), ...]), ...]\n",
      "The \n",
      "PP Attachment Corpus, nltk.corpus.ppattach, is another source of information\n",
      "about the valency of particular verbs. Here we illustrate a technique for mining this\n",
      "corpus. It finds pairs of prepositional phrases where the preposition and noun are fixed,\n",
      "but where the choice of verb determines whether the prepositional phrase is attached\n",
      "to the VP or to the NP.\n",
      ">>> entries = nltk.corpus.ppattach.attachments('training')\n",
      ">>> table = nltk.defaultdict(lambda: nltk.defaultdict(set))\n",
      ">>> for entry in entries:\n",
      "...     key = entry.noun1 + '-' + entry.prep + '-' + entry.noun2\n",
      "...     table[key][entry.attachment].add(entry.verb)\n",
      "...\n",
      ">>> for key in sorted(table):\n",
      "...     if len(table[key]) > 1:\n",
      "...         print key, 'N:', sorted(table[key]['N']), 'V:', sorted(table[key]['V'])\n",
      "Among the output lines of this program we find offer-from-group N: ['rejected'] V:\n",
      "['received'], which indicates that received expects a separate PP complement attached\n",
      "to the VP, while rejected does not. As before, we can use this information to help con-\n",
      "struct the grammar.\n",
      "The NLTK corpus collection includes data from the PE08 Cross-Framework and Cross\n",
      "Domain Parser Evaluation Shared Task. A collection of larger grammars has been pre-\n",
      "pared for the purpose of comparing different parsers, which can be obtained by down-\n",
      "loading the large_grammars package (e.g., python -m nltk.downloader large_grammars).\n",
      "The NLTK corpus collection also includes a sample from the Sinica Treebank Corpus,\n",
      "consisting of 10,000 parsed sentences drawn from the Academia Sinica Balanced Corpus\n",
      "of Modern Chinese. Let’s load and display one of the trees in this corpus.\n",
      ">>> nltk.corpus.sinica_treebank.parsed_sents()[3450].draw()\n",
      "316 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 337, 'page_label': '316', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4703}\n",
      "\n",
      "--- Chunk 4704 ---\n",
      "Content:\n",
      "Pernicious Ambiguity\n",
      "Unfortunately, \n",
      "as the coverage of the grammar increases and the length of the input\n",
      "sentences grows, the number of parse trees grows rapidly. In fact, it grows at an astro-\n",
      "nomical rate.\n",
      "Let’s explore this issue with the help of a simple example. The word fish is both a noun\n",
      "and a verb. We can make up the sentence fish fish fish, meaning fish like to fish for other\n",
      "fish. (Try this with police if you prefer something more sensible.) Here is a toy grammar\n",
      "for the “fish” sentences.\n",
      ">>> grammar = nltk.parse_cfg(\"\"\"\n",
      "... S -> NP V NP\n",
      "... NP -> NP Sbar\n",
      "... Sbar -> NP V\n",
      "... NP -> 'fish'\n",
      "... V -> 'fish'\n",
      "... \"\"\")\n",
      "Now we can try parsing a longer sentence, fish fish fish fish fish , which among other\n",
      "things, means “fish that other fish fish are in the habit of fishing fish themselves.” We\n",
      "use the NLTK chart parser, which is presented earlier in this chapter. This sentence has\n",
      "two readings.\n",
      ">>> tokens = [\"fish\"] * 5\n",
      ">>> cp = nltk.ChartParser(grammar)\n",
      ">>> for tree in cp.nbest_parse(tokens):\n",
      "...     print tree\n",
      "(S (NP (NP fish) (Sbar (NP fish) (V fish))) (V fish) (NP fish))\n",
      "(S (NP fish) (V fish) (NP (NP fish) (Sbar (NP fish) (V fish))))\n",
      "As the length of this sentence goes up (3, 5, 7, ...) we get the following numbers of parse\n",
      "trees: 1; 2; 5; 14; 42; 132; 429; 1,430; 4,862; 16,796; 58,786; 208,012; …. (These are\n",
      "the Catalan numbers, which we saw in an exercise in Chapter 4.) The last of these is\n",
      "for a sentence of length 23, the average length of sentences in the WSJ section of Penn\n",
      "Treebank. For a sentence of length 50 there would be over 1012 parses, and this is only\n",
      "half the length of the Piglet sentence ( Section 8.1), which young children process ef-\n",
      "fortlessly. No practical NLP system could construct millions of trees for a sentence and\n",
      "choose the appropriate one in the context. It’s clear that humans don’t do this either!\n",
      "Note that the problem is not with our choice of example. (Church & Patil, 1982) point\n",
      "out that the syntactic ambiguity of PP attachment in sentences like (15) also grows in\n",
      "proportion to the Catalan numbers.\n",
      "(15) Put the block in the box on the table.\n",
      "So much for structural ambiguity; what about lexical ambiguity? As soon as we try to\n",
      "construct a broad-coverage grammar, we are forced to make lexical entries highly am-\n",
      "biguous for their part-of-speech. In a toy grammar, a is only a determiner, dog is only\n",
      "a noun, and runs is only a verb. However, in a broad-coverage grammar, a is also a\n",
      "8.6  Grammar Development | 317...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 338, 'page_label': '317', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4704}\n",
      "\n",
      "--- Chunk 4705 ---\n",
      "Content:\n",
      "noun (e.g., part a), dog is also a verb (meaning to follow closely), and runs is also a noun\n",
      "(e.g., ski runs). In fact, all words can be referred to by name: e.g., the verb ‘ate’ is spelled\n",
      "with three letters; in speech we do not need to supply quotation marks. Furthermore,\n",
      "it is possible to verb most nouns. Thus a parser for a broad-coverage grammar will be\n",
      "overwhelmed with ambiguity. Even complete gibberish will often have a reading, e.g.,\n",
      "the a are of I. As (Abney, 1996) has pointed out, this is not word salad but a grammatical\n",
      "noun phrase, in which are is a noun meaning a hundredth of a hectare (or 100 sq m),\n",
      "and a and I are nouns designating coordinates, as shown in Figure 8-9.\n",
      "Figure 8-9. The a are of I: A schematic drawing of 27 paddocks, each being one are in size, and each\n",
      "identified using coordinates; the top-left cell is the a are of column A (after Abney).\n",
      "Even \n",
      "though this phrase is unlikely, it is still grammatical, and a broad-coverage parser\n",
      "should be able to construct a parse tree for it. Similarly, sentences that seem to be\n",
      "unambiguous, such as John saw Mary, turn out to have other readings we would not\n",
      "have anticipated (as Abney explains). This ambiguity is unavoidable, and leads to hor-\n",
      "rendous inefficiency in parsing seemingly innocuous sentences. The solution to these\n",
      "problems is provided by probabilistic parsing, which allows us to rank the parses of an\n",
      "ambiguous sentence on the basis of evidence from corpora.\n",
      "Weighted Grammar\n",
      "As we have just seen, dealing with ambiguity is a key challenge in developing broad-\n",
      "coverage parsers. Chart parsers improve the efficiency of computing multiple parses of\n",
      "the same sentences, but they are still overwhelmed by the sheer number of possible\n",
      "parses. Weighted grammars and probabilistic parsing algorithms have provided an ef-\n",
      "fective solution to these problems.\n",
      "Before looking at these, we need to understand why the notion of grammaticality could\n",
      "be gradient. Considering the verb give. This verb requires both a direct object (the thing\n",
      "being given) and an indirect object (the recipient). These complements can be given in\n",
      "either order, as illustrated in (16). In the “prepositional dative” form in (16a), the direct\n",
      "object appears first, followed by a prepositional phrase containing the indirect object.\n",
      "318 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 339, 'page_label': '318', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4705}\n",
      "\n",
      "--- Chunk 4706 ---\n",
      "Content:\n",
      "(16) a. Kim gave a bone to the dog.\n",
      "b.\n",
      "Kim gave the dog a bone.\n",
      "In the “double object” form in (16b), the indirect object appears first, followed by the\n",
      "direct object. In this case, either order is acceptable. However, if the indirect object is\n",
      "a pronoun, there is a strong preference for the double object construction:\n",
      "(17) a. Kim gives the heebie-jeebies to me ( prepositional dative).\n",
      "b. Kim gives me the heebie-jeebies (double object).\n",
      "Using the Penn Treebank sample, we can examine all instances of prepositional dative\n",
      "and double object constructions involving give, as shown in Example 8-5.\n",
      "Example 8-5. Usage of give and gave in the Penn Treebank sample.\n",
      "def give(t):\n",
      "    return t.node == 'VP' and len(t) > 2 and t[1].node == 'NP'\\\n",
      "           and (t[2].node == 'PP-DTV' or t[2].node == 'NP')\\\n",
      "           and ('give' in t[0].leaves() or 'gave' in t[0].leaves())\n",
      "def sent(t):\n",
      "    return ' '.join(token for token in t.leaves() if token[0] not in '*-0')\n",
      "def print_node(t, width):\n",
      "        output = \"%s %s: %s / %s: %s\" %\\\n",
      "            (sent(t[0]), t[1].node, sent(t[1]), t[2].node, sent(t[2]))\n",
      "        if len(output) > width:\n",
      "            output = output[:width] + \"...\"\n",
      "        print output\n",
      ">>> for tree in nltk.corpus.treebank.parsed_sents():\n",
      "...     for t in tree.subtrees(give):\n",
      "...         print_node(t, 72)\n",
      "gave NP: the chefs / NP: a standing ovation\n",
      "give NP: advertisers / NP: discounts for maintaining or increasing ad sp...\n",
      "give NP: it / PP-DTV: to the politicians\n",
      "gave NP: them / NP: similar help\n",
      "give NP: them / NP:\n",
      "give NP: only French history questions / PP-DTV: to students in a Europe...\n",
      "give NP: federal judges / NP: a raise\n",
      "give NP: consumers / NP: the straight scoop on the U.S. waste crisis\n",
      "gave NP: Mitsui / NP: access to a high-tech medical product\n",
      "give NP: Mitsubishi / NP: a window on the U.S...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 340, 'page_label': '319', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4706}\n",
      "\n",
      "--- Chunk 4707 ---\n",
      "Content:\n",
      ". waste crisis\n",
      "gave NP: Mitsui / NP: access to a high-tech medical product\n",
      "give NP: Mitsubishi / NP: a window on the U.S. glass industry\n",
      "give NP: much thought / PP-DTV: to the rates she was receiving , nor to ...\n",
      "give NP: your Foster Savings Institution / NP: the gift of hope and free...\n",
      "give NP: market operators / NP: the authority to suspend trading in futu...\n",
      "gave NP: quick approval / PP-DTV: to $ 3.18 billion in supplemental appr...\n",
      "give NP: the Transportation Department / NP: up to 50 days to review any...\n",
      "give NP: the president / NP: such power\n",
      "give NP: me / NP: the heebie-jeebies\n",
      "give NP: holders / NP: the right , but not the obligation , to buy a cal...\n",
      "gave NP: Mr. Thomas / NP: only a `` qualified '' rating , rather than ``...\n",
      "give NP: the president / NP: line-item veto power\n",
      "8.6  Grammar Development | 319...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 340, 'page_label': '319', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4707}\n",
      "\n",
      "--- Chunk 4708 ---\n",
      "Content:\n",
      "We can observe a strong tendency for the shortest complement to appear first. How-\n",
      "ever, \n",
      "this does not account for a form like give NP: federal judges / NP: a raise,\n",
      "where animacy may play a role. In fact, there turns out to be a large number of\n",
      "contributing factors, as surveyed by (Bresnan & Hay, 2008). Such preferences can be\n",
      "represented in a weighted grammar.\n",
      "A probabilistic context-free grammar (or PCFG) is a context-free grammar that as-\n",
      "sociates a probability with each of its productions. It generates the same set of parses\n",
      "for a text that the corresponding context-free grammar does, and assigns a probability\n",
      "to each parse. The probability of a parse generated by a PCFG is simply the product of\n",
      "the probabilities of the productions used to generate it.\n",
      "The simplest way to define a PCFG is to load it from a specially formatted string con-\n",
      "sisting of a sequence of weighted productions, where weights appear in brackets, as\n",
      "shown in Example 8-6.\n",
      "Example 8-6. Defining a probabilistic context-free grammar (PCFG).\n",
      "grammar = nltk.parse_pcfg(\"\"\"\n",
      "    S    -> NP VP              [1.0]\n",
      "    VP   -> TV NP              [0.4]\n",
      "    VP   -> IV                 [0.3]\n",
      "    VP   -> DatV NP NP         [0.3]\n",
      "    TV   -> 'saw'              [1.0]\n",
      "    IV   -> 'ate'              [1.0]\n",
      "    DatV -> 'gave'             [1.0]\n",
      "    NP   -> 'telescopes'       [0.8]\n",
      "    NP   -> 'Jack'             [0.2]\n",
      "    \"\"\")\n",
      ">>> print grammar\n",
      "Grammar with 9 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    VP -> TV NP [0.4]\n",
      "    VP -> IV [0.3]\n",
      "    VP -> DatV NP NP [0.3]\n",
      "    TV -> 'saw' [1.0]\n",
      "    IV -> 'ate' [1.0]\n",
      "    DatV -> 'gave' [1.0]\n",
      "    NP -> 'telescopes' [0.8]\n",
      "    NP -> 'Jack' [0.2]\n",
      "It is sometimes convenient to combine multiple productions into a single line, e.g.,\n",
      "VP -> TV NP [0.4] | IV [0.3] | DatV NP NP [0.3]. In order to ensure that the trees\n",
      "generated by the grammar form a probability distribution, PCFG grammars impose the\n",
      "constraint that all productions with a given lefthand side must have probabilities that\n",
      "sum to one...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 341, 'page_label': '320', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4708}\n",
      "\n",
      "--- Chunk 4709 ---\n",
      "Content:\n",
      ". In order to ensure that the trees\n",
      "generated by the grammar form a probability distribution, PCFG grammars impose the\n",
      "constraint that all productions with a given lefthand side must have probabilities that\n",
      "sum to one. The grammar in Example 8-6 obeys this constraint: for S, there is only one\n",
      "production, with a probability of 1.0; for VP, 0.4+0.3+0.3=1.0; and for NP, 0.8+0.2=1.0.\n",
      "The parse tree returned by parse() includes probabilities:\n",
      "320 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 341, 'page_label': '320', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4709}\n",
      "\n",
      "--- Chunk 4710 ---\n",
      "Content:\n",
      ">>> viterbi_parser = nltk.ViterbiParser(grammar)\n",
      ">>> print viterbi_parser.parse(['Jack', 'saw', 'telescopes'])\n",
      "(S (NP Jack) (VP (TV saw) (NP telescopes))) (p=0.064)\n",
      "Now \n",
      "that parse trees are assigned probabilities, it no longer matters that there may be\n",
      "a huge number of possible parses for a given sentence. A parser will be responsible for\n",
      "finding the most likely parses.\n",
      "8.7  Summary\n",
      "• Sentences have internal organization that can be represented using a tree. Notable\n",
      "features of constituent structure are: recursion, heads, complements, and\n",
      "modifiers.\n",
      "• A grammar is a compact characterization of a potentially infinite set of sentences;\n",
      "we say that a tree is well-formed according to a grammar, or that a grammar licenses\n",
      "a tree.\n",
      "• A grammar is a formal model for describing whether a given phrase can be assigned\n",
      "a particular constituent or dependency structure.\n",
      "• Given a set of syntactic categories, a context-free grammar uses a set of productions\n",
      "to say how a phrase of some category A can be analyzed into a sequence of smaller\n",
      "parts α1 ... αn.\n",
      "• A dependency grammar uses productions to specify what the dependents are of a\n",
      "given lexical head.\n",
      "• Syntactic ambiguity arises when one sentence has more than one syntactic analysis\n",
      "(e.g., prepositional phrase attachment ambiguity).\n",
      "• A parser is a procedure for finding one or more trees corresponding to a grammat-\n",
      "ically well-formed sentence.\n",
      "• A simple top-down parser is the recursive descent parser, which recursively ex-\n",
      "pands the start symbol (usually S) with the help of the grammar productions, and\n",
      "tries to match the input sentence. This parser cannot handle left-recursive pro-\n",
      "ductions (e.g., productions such as NP -> NP PP). It is inefficient in the way it blindly\n",
      "expands categories without checking whether they are compatible with the input\n",
      "string, and in repeatedly expanding the same non-terminals and discarding the\n",
      "results.\n",
      "• A simple bottom-up parser is the shift-reduce parser, which shifts input onto a\n",
      "stack and tries to match the items at the top of the stack with the righthand side\n",
      "of grammar productions. This parser is not guaranteed to find a valid parse for the\n",
      "input, even if one exists, and builds substructures without checking whether it is\n",
      "globally consistent with the grammar.\n",
      "8.7  Summary | 321...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 342, 'page_label': '321', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4710}\n",
      "\n",
      "--- Chunk 4711 ---\n",
      "Content:\n",
      "8.8  Further Reading\n",
      "Extra \n",
      "materials for this chapter are posted at http://www.nltk.org/, including links to\n",
      "freely available resources on the Web. For more examples of parsing with NLTK, please\n",
      "see the Parsing HOWTO at http://www.nltk.org/howto.\n",
      "There are many introductory books on syntax. (O’Grady et al., 2004) is a general in-\n",
      "troduction to linguistics, while (Radford, 1988) provides a gentle introduction to trans-\n",
      "formational grammar, and can be recommended for its coverage of transformational\n",
      "approaches to unbounded dependency constructions. The most widely used term in\n",
      "linguistics for formal grammar is generative grammar, though it has nothing to do\n",
      "with generation (Chomsky, 1965).\n",
      "(Burton-Roberts, 1997) is a practically oriented textbook on how to analyze constitu-\n",
      "ency in English, with extensive exemplification and exercises. (Huddleston & Pullum,\n",
      "2002) provides an up-to-date and comprehensive analysis of syntactic phenomena in\n",
      "English.\n",
      "Chapter 12 of (Jurafsky & Martin, 2008) covers formal grammars of English; Sections\n",
      "13.1–3 cover simple parsing algorithms and techniques for dealing with ambiguity;\n",
      "Chapter 14 covers statistical parsing; and Chapter 16 covers the Chomsky hierarchy\n",
      "and the formal complexity of natural language. (Levin, 1993) has categorized English\n",
      "verbs into fine-grained classes, according to their syntactic properties.\n",
      "There are several ongoing efforts to build large-scale rule-based grammars, e.g., the\n",
      "LFG Pargram project (http://www2.parc.com/istl/groups/nltt/pargram/), the HPSG Lin-\n",
      "GO Matrix framework (http://www.delph-in.net/matrix/), and the XTAG Project (http:\n",
      "//www.cis.upenn.edu/~xtag/).\n",
      "8.9  Exercises\n",
      "1. ○ Can you come up with grammatical sentences that probably have never been\n",
      "uttered before? (Take turns with a partner.) What does this tell you about human\n",
      "language?\n",
      "2. ○ Recall Strunk and White’s prohibition against using a sentence-initial however\n",
      "to mean “although.” Do a web search for however used at the start of the sentence.\n",
      "How widely used is this construction?\n",
      "3. ○ Consider the sentence Kim arrived or Dana left and everyone cheered. Write down\n",
      "the parenthesized forms to show the relative scope of and and or. Generate tree\n",
      "structures corresponding to both of these interpretations.\n",
      "4. ○ The Tree class implements a variety of other useful methods. See the Tree help\n",
      "documentation for more details (i.e., import the Tree class and then type\n",
      "help(Tree)).\n",
      "5. ○ In this exercise you will manually construct some parse trees.\n",
      "322 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 343, 'page_label': '322', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4711}\n",
      "\n",
      "--- Chunk 4712 ---\n",
      "Content:\n",
      "a. Write code to produce two trees, one for each reading of the phrase old men\n",
      "and women.\n",
      "b. Encode any of the trees presented in this chapter as a labeled bracketing, and\n",
      "use nltk.Tree() to check that it is well-formed. Now use draw() to display the\n",
      "tree.\n",
      "c. As in (a), draw a tree for The woman saw a man last Thursday.\n",
      "6. ○ Write a recursive function to traverse a tree and return the depth of the tree, such\n",
      "that a tree with a single node would have depth zero. (Hint: the depth of a subtree\n",
      "is the maximum depth of its children, plus one.)\n",
      "7. ○ Analyze the A.A. Milne sentence about Piglet, by underlining all of the sentences\n",
      "it contains then replacing these with S (e.g., the first sentence becomes S when S).\n",
      "Draw a tree structure for this “compressed” sentence. What are the main syntactic\n",
      "constructions used for building such a long sentence?\n",
      "8. ○ In the recursive descent parser demo, experiment with changing the sentence to\n",
      "be parsed by selecting Edit Text in the Edit menu.\n",
      "9. ○ Can the grammar in grammar1 (Example 8-1) be used to describe sentences that\n",
      "are more than 20 words in length?\n",
      "10. ○ Use the graphical chart-parser interface to experiment with different rule invo-\n",
      "cation strategies. Come up with your own strategy that you can execute manually\n",
      "using the graphical interface. Describe the steps, and report any efficiency im-\n",
      "provements it has (e.g., in terms of the size of the resulting chart). Do these im-\n",
      "provements depend on the structure of the grammar? What do you think of the\n",
      "prospects for significant performance boosts from cleverer rule invocation\n",
      "strategies?\n",
      "11. ○ With pen and paper, manually trace the execution of a recursive descent parser\n",
      "and a shift-reduce parser, for a CFG you have already seen, or one of your own\n",
      "devising.\n",
      "12. ○ We have seen that a chart parser adds but never removes edges from a chart.\n",
      "Why?\n",
      "13. ○ Consider the sequence of words: Buffalo buffalo Buffalo buffalo buffalo buffalo\n",
      "Buffalo buffalo. This is a grammatically correct sentence, as explained at http://en\n",
      ".wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buf\n",
      "falo. Consider the tree diagram presented on this Wikipedia page, and write down\n",
      "a suitable grammar. Normalize case to lowercase, to simulate the problem that a\n",
      "listener has when hearing this sentence. Can you find other parses for this sentence?\n",
      "How does the number of parse trees grow as the sentence gets longer? (More ex-\n",
      "amples of these sentences can be found at http://en.wikipedia.org/wiki/List_of_ho\n",
      "mophonous_phrases.)\n",
      "14. ◑ You can modify the grammar in the recursive descent parser demo by selecting\n",
      "Edit Grammar in the Edit menu. Change the first expansion production, namely\n",
      "8.9  Exercises | 323...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 344, 'page_label': '323', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4712}\n",
      "\n",
      "--- Chunk 4713 ---\n",
      "Content:\n",
      "NP -> Det N PP, to NP -> NP PP. Using the Step button, try to build a parse tree.\n",
      "What happens?\n",
      "15. ◑ Extend the grammar in grammar2 with productions that expand prepositions as\n",
      "intransitive, transitive, and requiring a PP complement. Based on these produc-\n",
      "tions, use the method of the preceding exercise to draw a tree for the sentence Lee\n",
      "ran away home.\n",
      "16. ◑ Pick some common verbs and complete the following tasks:\n",
      "a. Write a program to find those verbs in the PP Attachment Corpus nltk.cor\n",
      "pus.ppattach. Find any cases where the same verb exhibits two different at-\n",
      "tachments, but where the first noun, or second noun, or preposition stays\n",
      "unchanged (as we saw in our discussion of syntactic ambiguity in Section 8.2).\n",
      "b. Devise CFG grammar productions to cover some of these cases.\n",
      "17. ◑ Write a program to compare the efficiency of a top-down chart parser compared\n",
      "with a recursive descent parser ( Section 8.4). Use the same grammar and input\n",
      "sentences for both. Compare their performance using the timeit module (see Sec-\n",
      "tion 4.7 for an example of how to do this).\n",
      "18. ◑ Compare the performance of the top-down, bottom-up, and left-corner parsers\n",
      "using the same grammar and three grammatical test sentences. Use timeit to log\n",
      "the amount of time each parser takes on the same sentence. Write a function that\n",
      "runs all three parsers on all three sentences, and prints a 3-by-3 grid of times, as\n",
      "well as row and column totals. Discuss your findings.\n",
      "19. ◑ Read up on “garden path” sentences. How might the computational work of a\n",
      "parser relate to the difficulty humans have with processing these sentences? (See\n",
      "http://en.wikipedia.org/wiki/Garden_path_sentence.)\n",
      "20. ◑ To compare multiple trees in a single window, we can use the draw_trees()\n",
      "method. Define some trees and try it out:\n",
      ">>> from nltk.draw.tree import draw_trees\n",
      ">>> draw_trees(tree1, tree2, tree3)\n",
      "21. ◑ Using tree positions, list the subjects of the first 100 sentences in the Penn tree-\n",
      "bank; to make the results easier to view, limit the extracted subjects to subtrees\n",
      "whose height is at most 2.\n",
      "22. ◑ Inspect the PP Attachment Corpus and try to suggest some factors that influence\n",
      "PP attachment.\n",
      "23. ◑ In Section 8.2, we claimed that there are linguistic regularities that cannot be\n",
      "described simply in terms of n-grams. Consider the following sentence, particularly\n",
      "the position of the phrase in his turn. Does this illustrate a problem for an approach\n",
      "based on n-grams?\n",
      "What was more, the in his turn somewhat youngish Nikolay Parfenovich also turned\n",
      "out to be the only person in the entire world to acquire a sincere liking to our “dis-\n",
      "criminated-against” public procurator. (Dostoevsky: The Brothers Karamazov)\n",
      "324 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 345, 'page_label': '324', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4713}\n",
      "\n",
      "--- Chunk 4714 ---\n",
      "Content:\n",
      "24. ◑ Write a recursive function that produces a nested bracketing for a tree, leaving\n",
      "out the leaf nodes and displaying the non-terminal labels after their subtrees. So\n",
      "the example in Section 8.6 about Pierre Vinken would produce: [[[NNP NNP]NP ,\n",
      "[ADJP [CD NNS]NP JJ]ADJP ,]NP-SBJ MD [VB [DT NN]NP [IN [DT JJ NN]NP]PP-CLR\n",
      "[NNP CD]NP-TMP]VP .]S. Consecutive categories should be separated by space.\n",
      "25. ◑ Download several electronic books from Project Gutenberg. Write a program to\n",
      "scan these texts for any extremely long sentences. What is the longest sentence you\n",
      "can find? What syntactic construction(s) are responsible for such long sentences?\n",
      "26. ◑ Modify the functions init_wfst() and complete_wfst() so that the contents of\n",
      "each cell in the WFST is a set of non-terminal symbols rather than a single non-\n",
      "terminal.\n",
      "27. ◑ Consider the algorithm in Example 8-3. Can you explain why parsing context-\n",
      "free grammar is proportional to n3, where n is the length of the input sentence?\n",
      "28. ◑ Process each tree of the Penn Treebank Corpus sample nltk.corpus.treebank\n",
      "and extract the productions with the help of Tree.productions(). Discard the pro-\n",
      "ductions that occur only once. Productions with the same lefthand side and similar\n",
      "righthand sides can be collapsed, resulting in an equivalent but more compact set\n",
      "of rules. Write code to output a compact grammar.\n",
      "29. ● One common way of defining the subject of a sentence S in English is as the noun\n",
      "phrase that is the child of S and the sibling of VP. Write a function that takes the tree\n",
      "for a sentence and returns the subtree corresponding to the subject of the sentence.\n",
      "What should it do if the root node of the tree passed to this function is not S, or if\n",
      "it lacks a subject?\n",
      "30. ● Write a function that takes a grammar (such as the one defined in Exam-\n",
      "ple 8-1 ) and returns a random sentence generated by the grammar. (Use gram\n",
      "mar.start() to find the start symbol of the grammar; grammar.productions(lhs) to\n",
      "get the list of productions from the grammar that have the specified lefthand side;\n",
      "and production.rhs() to get the righthand side of a production.)\n",
      "31. ● Implement a version of the shift-reduce parser using backtracking, so that it finds\n",
      "all possible parses for a sentence, what might be called a “recursive ascent parser.”\n",
      "Consult the Wikipedia entry for backtracking at http://en.wikipedia.org/wiki/Back\n",
      "tracking.\n",
      "32. ● As we saw in Chapter 7, it is possible to collapse chunks down to their chunk\n",
      "label. When we do this for sentences involving the word gave, we find patterns\n",
      "such as the following:\n",
      "gave NP\n",
      "gave up NP in NP\n",
      "gave NP up\n",
      "gave NP NP\n",
      "gave NP to NP\n",
      "8.9  Exercises | 325...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 346, 'page_label': '325', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4714}\n",
      "\n",
      "--- Chunk 4715 ---\n",
      "Content:\n",
      "a. Use this method to study the complementation patterns of a verb of interest,\n",
      "and write suitable grammar productions. (This task is sometimes called lexical\n",
      "acquisition.)\n",
      "b. Identify some English verbs that are near-synonyms, such as the dumped/filled/\n",
      "loaded example from (64) in Chapter 9. Use the chunking method to study the\n",
      "complementation patterns of these verbs. Create a grammar to cover these\n",
      "cases. Can the verbs be freely substituted for each other, or are there con-\n",
      "straints? Discuss your findings.\n",
      "33. ● Develop a left-corner parser based on the recursive descent parser, and inheriting\n",
      "from ParseI.\n",
      "34. ● Extend NLTK’s shift-reduce parser to incorporate backtracking, so that it is\n",
      "guaranteed to find all parses that exist (i.e., it is complete).\n",
      "35. ● Modify the functions init_wfst() and complete_wfst() so that when a non-\n",
      "terminal symbol is added to a cell in the WFST, it includes a record of the cells\n",
      "from which it was derived. Implement a function that will convert a WFST in this\n",
      "form to a parse tree.\n",
      "326 | Chapter 8:  Analyzing Sentence Structure...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 347, 'page_label': '326', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4715}\n",
      "\n",
      "--- Chunk 4716 ---\n",
      "Content:\n",
      "CHAPTER 9\n",
      "Building Feature-Based Grammars\n",
      "Natural languages have an extensive range of grammatical constructions which are hard\n",
      "to \n",
      "handle with the simple methods described in Chapter 8. In order to gain more flex-\n",
      "ibility, we change our treatment of grammatical categories like S, NP, and V. In place of\n",
      "atomic labels, we decompose them into structures like dictionaries, where features can\n",
      "take on a range of values.\n",
      "The goal of this chapter is to answer the following questions:\n",
      "1. How can we extend the framework of context-free grammars with features so as\n",
      "to gain more fine-grained control over grammatical categories and productions?\n",
      "2. What are the main formal properties of feature structures, and how do we use them\n",
      "computationally?\n",
      "3. What kinds of linguistic patterns and grammatical constructions can we now cap-\n",
      "ture with feature-based grammars?\n",
      "Along the way, we will cover more topics in English syntax, including phenomena such\n",
      "as agreement, subcategorization, and unbounded dependency constructions.\n",
      "9.1  Grammatical Features\n",
      "In Chapter 6, we described how to build classifiers that rely on detecting features of\n",
      "text. Such features may be quite simple, such as extracting the last letter of a word, or\n",
      "more complex, such as a part-of-speech tag that has itself been predicted by the clas-\n",
      "sifier. In this chapter, we will investigate the role of features in building rule-based\n",
      "grammars. In contrast to feature extractors, which record features that have been au-\n",
      "tomatically detected, we are now going to declare the features of words and phrases.\n",
      "We start off with a very simple example, using dictionaries to store features and their\n",
      "values.\n",
      ">>> kim = {'CAT': 'NP', 'ORTH': 'Kim', 'REF': 'k'}\n",
      ">>> chase = {'CAT': 'V', 'ORTH': 'chased', 'REL': 'chase'}\n",
      "327...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 348, 'page_label': '327', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4716}\n",
      "\n",
      "--- Chunk 4717 ---\n",
      "Content:\n",
      "The objects kim and chase both have a couple of shared features, CAT (grammatical\n",
      "category) and ORTH (orthography, i.e., spelling). In addition, each has a more semanti-\n",
      "cally oriented feature: kim['REF'] is intended to give the referent of kim, while\n",
      "chase['REL'] gives the relation expressed by chase. In the context of rule-based gram-\n",
      "mars, such pairings of features and values are known as feature structures, and we\n",
      "will shortly see alternative notations for them.\n",
      "Feature structures contain various kinds of information about grammatical entities.\n",
      "The information need not be exhaustive, and we might want to add further properties.\n",
      "For example, in the case of a verb, it is often useful to know what “semantic role” is\n",
      "played by the arguments of the verb. In the case of chase, the subject plays the role of\n",
      "“agent,” whereas the object has the role of “patient.” Let’s add this information, using\n",
      "'sbj' (subject) and 'obj' (object) as placeholders which will get filled once the verb\n",
      "combines with its grammatical arguments:\n",
      ">>> chase['AGT'] = 'sbj'\n",
      ">>> chase['PAT'] = 'obj'\n",
      "If we now process a sentence Kim chased Lee, we want to “bind” the verb’s agent role\n",
      "to the subject and the patient role to the object. We do this by linking to the REF feature\n",
      "of the relevant NP. In the following example, we make the simple-minded assumption\n",
      "that the NPs immediately to the left and right of the verb are the subject and object,\n",
      "respectively. We also add a feature structure for Lee to complete the example.\n",
      ">>> sent = \"Kim chased Lee\"\n",
      ">>> tokens = sent.split()\n",
      ">>> lee = {'CAT': 'NP', 'ORTH': 'Lee', 'REF': 'l'}\n",
      ">>> def lex2fs(word):\n",
      "...     for fs in [kim, lee, chase]:\n",
      "...         if fs['ORTH'] == word:\n",
      "...             return fs\n",
      ">>> subj, verb, obj = lex2fs(tokens[0]), lex2fs(tokens[1]), lex2fs(tokens[2])\n",
      " >>> verb['AGT'] = subj['REF'] # agent of 'chase' is Kim\n",
      " >>> verb['PAT'] = obj['REF']  # patient of 'chase' is Lee\n",
      " >>> for k in ['ORTH', 'REL', 'AGT', 'PAT']: # check featstruct of 'chase'\n",
      "...     print \"%-5s => %s\" % (k, verb[k])\n",
      "ORTH  => chased\n",
      "REL   => chase\n",
      "AGT   => k\n",
      "PAT   => l\n",
      "The same approach could be adopted for a different verb—say, surprise—though in\n",
      "this case, the subject would play the role of “source” (SRC), and the object plays the role\n",
      "of “experiencer” (EXP):\n",
      ">>> surprise = {'CAT': 'V', 'ORTH': 'surprised', 'REL': 'surprise',\n",
      "...             'SRC': 'sbj', 'EXP': 'obj'}\n",
      "Feature structures are pretty powerful, but the way in which we have manipulated them\n",
      "is extremely ad hoc...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 349, 'page_label': '328', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4717}\n",
      "\n",
      "--- Chunk 4718 ---\n",
      "Content:\n",
      ".             'SRC': 'sbj', 'EXP': 'obj'}\n",
      "Feature structures are pretty powerful, but the way in which we have manipulated them\n",
      "is extremely ad hoc. Our next task in this chapter is to show how the framework of\n",
      "context-free grammar and parsing can be expanded to accommodate feature structures,\n",
      "so that we can build analyses like this in a more generic and principled way. We will\n",
      "328 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 349, 'page_label': '328', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4718}\n",
      "\n",
      "--- Chunk 4719 ---\n",
      "Content:\n",
      "start off by looking at the phenomenon of syntactic agreement; we will show how\n",
      "agreement \n",
      "constraints can be expressed elegantly using features, and illustrate their use\n",
      "in a simple grammar.\n",
      "Since feature structures are a general data structure for representing information of any\n",
      "kind, we will briefly look at them from a more formal point of view, and illustrate the\n",
      "support for feature structures offered by NLTK. In the final part of the chapter, we\n",
      "demonstrate that the additional expressiveness of features opens up a wide spectrum\n",
      "of possibilities for describing sophisticated aspects of linguistic structure.\n",
      "Syntactic Agreement\n",
      "The following examples show pairs of word sequences, the first of which is grammatical\n",
      "and the second not. (We use an asterisk at the start of a word sequence to signal that\n",
      "it is ungrammatical.)\n",
      "(1) a. this dog\n",
      "b. *these dog\n",
      "(2) a. these dogs\n",
      "b. *this dogs\n",
      "In English, nouns are usually marked as being singular or plural. The form of the de-\n",
      "monstrative also varies: this (singular) and these (plural). Examples (1) and (2) show\n",
      "that there are constraints on the use of demonstratives and nouns within a noun phrase:\n",
      "either both are singular or both are plural. A similar constraint holds between subjects\n",
      "and predicates:\n",
      "(3) a. the dog runs\n",
      "b. *the dog run\n",
      "(4) a. the dogs run\n",
      "b. *the dogs runs\n",
      "Here we can see that morphological properties of the verb co-vary with syntactic prop-\n",
      "erties of the subject noun phrase. This co-variance is called agreement. If we look\n",
      "further at verb agreement in English, we will see that present tense verbs typically have\n",
      "two inflected forms: one for third person singular, and another for every other combi-\n",
      "nation of person and number, as shown in Table 9-1.\n",
      "9.1  Grammatical Features | 329...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 350, 'page_label': '329', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4719}\n",
      "\n",
      "--- Chunk 4720 ---\n",
      "Content:\n",
      "Table 9-1. Agreement paradigm for English regular verbs\n",
      " Singular Plural\n",
      "1st person I run we run\n",
      "2nd person you run you run\n",
      "3rd person he/she/it runs they run\n",
      "We can make the role of morphological properties a bit more explicit, as illustrated in\n",
      "(5) \n",
      "and (6). These representations indicate that the verb agrees with its subject in person\n",
      "and number. (We use 3 as an abbreviation for 3rd person, SG for singular, and PL for\n",
      "plural.)\n",
      "(5) the dog run-s\n",
      " dog.3.SG run-3.SG\n",
      "(6) the dog-s run\n",
      " dog.3.PL run-3.PL\n",
      "Let’s see what happens when we encode these agreement constraints in a context-free\n",
      "grammar. We will begin with the simple CFG in (7).\n",
      "(7) S   ->   NP VP\n",
      "NP  ->   Det N\n",
      "VP  ->   V\n",
      "Det  ->  'this'\n",
      "N    ->  'dog'\n",
      "V    ->  'runs'\n",
      "Grammar (7) \n",
      "allows us to generate the sentence this dog runs; however, what we really\n",
      "want to do is also generate these dogs run while blocking unwanted sequences like *this\n",
      "dogs run and *these dog runs. The most straightforward approach is to add new non-\n",
      "terminals and productions to the grammar:\n",
      "(8) S -> NP_SG VP_SG\n",
      "S -> NP_PL VP_PL\n",
      "NP_SG -> Det_SG N_SG\n",
      "NP_PL -> Det_PL N_PL\n",
      "VP_SG -> V_SG\n",
      "VP_PL -> V_PL\n",
      "Det_SG -> 'this'\n",
      "Det_PL -> 'these'\n",
      "N_SG -> 'dog'\n",
      "N_PL -> 'dogs'\n",
      "V_SG -> 'runs'\n",
      "V_PL -> 'run'\n",
      "In place of a single production expanding S, we now have two productions, one covering\n",
      "the sentences involving singular subject NPs and VPs, the other covering sentences with\n",
      "330 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 351, 'page_label': '330', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4720}\n",
      "\n",
      "--- Chunk 4721 ---\n",
      "Content:\n",
      "plural subject NPs and VPs. In fact, every production in (7) has two counterparts in\n",
      "(8). With a small grammar, this is not really such a problem, although it is aesthetically\n",
      "unappealing. However, with a larger grammar that covers a reasonable subset of Eng-\n",
      "lish constructions, the prospect of doubling the grammar size is very unattractive. Let’s\n",
      "suppose now that we used the same approach to deal with first, second, and third\n",
      "person agreement, for both singular and plural. This would lead to the original grammar\n",
      "being multiplied by a factor of 6, which we definitely want to avoid. Can we do better\n",
      "than this? In the next section, we will show that capturing number and person agree-\n",
      "ment need not come at the cost of “blowing up” the number of productions.\n",
      "Using Attributes and Constraints\n",
      "We spoke informally of linguistic categories having properties, for example, that a noun\n",
      "has the property of being plural. Let’s make this explicit:\n",
      "(9) N[NUM=pl]\n",
      "In (9), we have introduced some new notation which says that the category N has a\n",
      "(grammatical) feature called NUM (short for “number”) and that the value of this feature\n",
      "is pl (short for “plural”). We can add similar annotations to other categories, and use\n",
      "them in lexical entries:\n",
      "(10) Det[NUM=sg] -> 'this'\n",
      "Det[NUM=pl] -> 'these'\n",
      "N[NUM=sg] -> 'dog'\n",
      "N[NUM=pl] -> 'dogs'\n",
      "V[NUM=sg] -> 'runs'\n",
      "V[NUM=pl] -> 'run'\n",
      "Does this help at all? So far, it looks just like a slightly more verbose alternative to what\n",
      "was specified in (8). Things become more interesting when we allow variables over\n",
      "feature values, and use these to state constraints:\n",
      "(11) S -> NP[NUM=?n] VP[NUM=?n]\n",
      "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
      "VP[NUM=?n] -> V[NUM=?n]\n",
      "We are using ?n as a variable over values of NUM; it can be instantiated either to sg or\n",
      "pl, within a given production. We can read the first production as saying that whatever\n",
      "value NP takes for the feature NUM, VP must take the same value.\n",
      "In order to understand how these feature constraints work, it’s helpful to think about\n",
      "how one would go about building a tree. Lexical productions will admit the following\n",
      "local trees (trees of depth one):\n",
      "9.1  Grammatical Features | 331...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 352, 'page_label': '331', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4721}\n",
      "\n",
      "--- Chunk 4722 ---\n",
      "Content:\n",
      "(12) a.\n",
      "b.\n",
      "(13) a.\n",
      "b.\n",
      "Now NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n] says that whatever the NUM values of N and\n",
      "Det are, they have to be the same. Consequently, this production will permit (12a) and\n",
      "(13a) to be combined into an NP, as shown in (14a), and it will also allow (12b) and\n",
      "(13b) to be combined, as in (14b). By contrast, (15a) and (15b) are prohibited because\n",
      "the roots of their subtrees differ in their values for the NUM feature; this incompatibility\n",
      "of values is indicated informally with a FAIL value at the top node.\n",
      "(14) a.\n",
      "b.\n",
      "332 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 353, 'page_label': '332', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4722}\n",
      "\n",
      "--- Chunk 4723 ---\n",
      "Content:\n",
      "(15) a.\n",
      "b.\n",
      "Production VP[NUM=?n] -> V[NUM=?n] says that the NUM value of the head verb has to be\n",
      "the same as the NUM value of the VP parent. Combined with the production for expanding\n",
      "S, we derive the consequence that if the NUM value of the subject head noun is pl, then\n",
      "so is the NUM value of the VP’s head verb.\n",
      "(16)\n",
      "Grammar (10) illustrated lexical productions for determiners like this and these, which\n",
      "require a singular or plural head noun respectively. However, other determiners in\n",
      "English are not choosy about the grammatical number of the noun they combine with.\n",
      "One way of describing this would be to add two lexical entries to the grammar, one\n",
      "each for the singular and plural versions of a determiner such as the:\n",
      "Det[NUM=sg] -> 'the' | 'some' | 'several'\n",
      "Det[NUM=pl] -> 'the' | 'some' | 'several'\n",
      "However, a more elegant solution is to leave the NUM value underspecified and let it\n",
      "agree in number with whatever noun it combines with. Assigning a variable value to\n",
      "NUM is one way of achieving this result:\n",
      "Det[NUM=?n] -> 'the' | 'some' | 'several'\n",
      "But in fact we can be even more economical, and just omit any specification for NUM in\n",
      "such productions. We only need to explicitly enter a variable value when this constrains\n",
      "another value elsewhere in the same production.\n",
      "The grammar in Example 9-1 illustrates most of the ideas we have introduced so far in\n",
      "this chapter, plus a couple of new ones.\n",
      "9.1  Grammatical Features | 333...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 354, 'page_label': '333', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4723}\n",
      "\n",
      "--- Chunk 4724 ---\n",
      "Content:\n",
      "Example 9-1. Example feature-based grammar.\n",
      ">>> nltk.data.show_cfg('grammars/book_grammars/feat0.fcfg')\n",
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "# S expansion productions\n",
      "S -> NP[NUM=?n] VP[NUM=?n]\n",
      "# NP expansion productions\n",
      "NP[NUM=?n] -> N[NUM=?n]\n",
      "NP[NUM=?n] -> PropN[NUM=?n]\n",
      "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
      "NP[NUM=pl] -> N[NUM=pl]\n",
      "# VP expansion productions\n",
      "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
      "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "Det[NUM=sg] -> 'this' | 'every'\n",
      "Det[NUM=pl] -> 'these' | 'all'\n",
      "Det -> 'the' | 'some' | 'several'\n",
      "PropN[NUM=sg]-> 'Kim' | 'Jody'\n",
      "N[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\n",
      "N[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children'\n",
      "IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\n",
      "TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
      "IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\n",
      "TV[TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
      "IV[TENSE=past] -> 'disappeared' | 'walked'\n",
      "TV[TENSE=past] -> 'saw' | 'liked'\n",
      "Notice \n",
      "that a syntactic category can have more than one feature: for example,\n",
      "V[TENSE=pres, NUM=pl]. In general, we can add as many features as we like.\n",
      "A final detail about Example 9-1 is the statement %start S. This “directive” tells the\n",
      "parser to take S as the start symbol for the grammar.\n",
      "In general, when we are trying to develop even a very small grammar, it is convenient\n",
      "to put the productions in a file where they can be edited, tested, and revised. We have\n",
      "saved Example 9-1 as a file named feat0.fcfg in the NLTK data distribution. You can\n",
      "make your own copy of this for further experimentation using nltk.data.load().\n",
      "Feature-based grammars are parsed in NLTK using an Earley chart parser (see Sec-\n",
      "tion 9.5 for more information about this) and Example 9-2 illustrates how this is carried\n",
      "out. After tokenizing the input, we import the load_parser function \n",
      ", which takes a\n",
      "grammar \n",
      "filename as input and returns a chart parser cp \n",
      " . Calling the parser’s\n",
      "nbest_parse() method will return a list trees of parse trees; trees will be empty if the\n",
      "grammar \n",
      "fails to parse the input and otherwise will contain one or more parse trees,\n",
      "depending on whether the input is syntactically ambiguous.\n",
      "334 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 355, 'page_label': '334', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4724}\n",
      "\n",
      "--- Chunk 4725 ---\n",
      "Content:\n",
      "Example 9-2. Trace of feature-based chart parser.\n",
      ">>> tokens = 'Kim likes children'.split()\n",
      ">>> from nltk import load_parser \n",
      ">>> cp = load_parser('grammars/book_grammars/feat0.fcfg', trace=2)  \n",
      ">>> trees = cp.nbest_parse(tokens)\n",
      "|.Kim .like.chil.|\n",
      "|[----]    .    .| PropN[NUM='sg'] -> 'Kim' *\n",
      "|[----]    .    .| NP[NUM='sg'] -> PropN[NUM='sg'] *\n",
      "|[---->    .    .| S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}\n",
      "|.    [----]    .| TV[NUM='sg', TENSE='pres'] -> 'likes' *\n",
      "|.    [---->    .| VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] * NP[]\n",
      "                {?n: 'sg', ?t: 'pres'}\n",
      "|.    .    [----]| N[NUM='pl'] -> 'children' *\n",
      "|.    .    [----]| NP[NUM='pl'] -> N[NUM='pl'] *\n",
      "|.    .    [---->| S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'pl'}\n",
      "|.    [---------]| VP[NUM='sg', TENSE='pres']\n",
      "                -> TV[NUM='sg', TENSE='pres'] NP[] *\n",
      "|[==============]| S[] -> NP[NUM='sg'] VP[NUM='sg'] *\n",
      "The \n",
      "details of the parsing procedure are not that important for present purposes. How-\n",
      "ever, there is an implementation issue which bears on our earlier discussion of grammar\n",
      "size. One possible approach to parsing productions containing feature constraints is to\n",
      "compile out all admissible values of the features in question so that we end up with a\n",
      "large, fully specified CFG along the lines of (8). By contrast, the parser process illus-\n",
      "trated in the previous examples works directly with the underspecified productions\n",
      "given by the grammar. Feature values “flow upwards” from lexical entries, and variable\n",
      "values are then associated with those values via bindings (i.e., dictionaries) such as\n",
      "{?n: 'sg', ?t: 'pres'}...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 356, 'page_label': '335', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4725}\n",
      "\n",
      "--- Chunk 4726 ---\n",
      "Content:\n",
      ". How-\n",
      "ever, there is an implementation issue which bears on our earlier discussion of grammar\n",
      "size. One possible approach to parsing productions containing feature constraints is to\n",
      "compile out all admissible values of the features in question so that we end up with a\n",
      "large, fully specified CFG along the lines of (8). By contrast, the parser process illus-\n",
      "trated in the previous examples works directly with the underspecified productions\n",
      "given by the grammar. Feature values “flow upwards” from lexical entries, and variable\n",
      "values are then associated with those values via bindings (i.e., dictionaries) such as\n",
      "{?n: 'sg', ?t: 'pres'}. As the parser assembles information about the nodes of the\n",
      "tree it is building, these variable bindings are used to instantiate values in these nodes;\n",
      "thus the underspecified VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] NP[] becomes\n",
      "instantiated as VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP[] by\n",
      "looking up the values of ?n and ?t in the bindings.\n",
      "Finally, we can inspect the resulting parse trees (in this case, a single one).\n",
      ">>> for tree in trees: print tree\n",
      "(S[]\n",
      "  (NP[NUM='sg'] (PropN[NUM='sg'] Kim))\n",
      "  (VP[NUM='sg', TENSE='pres']\n",
      "    (TV[NUM='sg', TENSE='pres'] likes)\n",
      "    (NP[NUM='pl'] (N[NUM='pl'] children))))\n",
      "Terminology\n",
      "So far, we have only seen feature values like sg and pl. These simple values are usually\n",
      "called atomic—that is, they can’t be decomposed into subparts. A special case of\n",
      "atomic values are Boolean values, that is, values that just specify whether a property\n",
      "is true or false. For example, we might want to distinguish auxiliary verbs such as can,\n",
      "9.1  Grammatical Features | 335...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 356, 'page_label': '335', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4726}\n",
      "\n",
      "--- Chunk 4727 ---\n",
      "Content:\n",
      "may, will, and do with the Boolean feature AUX. Then the production V[TENSE=pres,\n",
      "aux=+] -> 'can' means that can receives the value pres for TENSE and + or true for\n",
      "AUX. There is a widely adopted convention that abbreviates the representation of Boo-\n",
      "lean features f; instead of aux=+ or aux=-, we use +aux and -aux respectively. These are\n",
      "just abbreviations, however, and the parser interprets them as though + and - are like\n",
      "any other atomic value. (17) shows some representative productions:\n",
      "(17) V[TENSE=pres, +aux] -> 'can'\n",
      "V[TENSE=pres, +aux] -> 'may'\n",
      "V[TENSE=pres, -aux] -> 'walks'\n",
      "V[TENSE=pres, -aux] -> 'likes'\n",
      "We have spoken of attaching “feature annotations” to syntactic categories. A more\n",
      "radical approach represents the whole category—that is, the non-terminal symbol plus\n",
      "the annotation—as a bundle of features. For example, N[NUM=sg] contains part-of-\n",
      "speech information which can be represented as POS=N. An alternative notation for this\n",
      "category, therefore, is [POS=N, NUM=sg].\n",
      "In addition to atomic-valued features, features may take values that are themselves\n",
      "feature structures. For example, we can group together agreement features (e.g., per-\n",
      "son, number, and gender) as a distinguished part of a category, serving as the value of\n",
      "AGR. In this case, we say that AGR has a complex value. (18) depicts the structure, in a\n",
      "format known as an attribute value matrix (AVM).\n",
      "(18) [POS = N           ]\n",
      "[                  ]\n",
      "[AGR = [PER = 3   ]]\n",
      "[      [NUM = pl  ]]\n",
      "[      [GND = fem ]]\n",
      "In passing, we should point out that there are alternative approaches for displaying\n",
      "AVMs; Figure 9-1 shows an example. Although feature structures rendered in the style\n",
      "of (18) are less visually pleasing, we will stick with this format, since it corresponds to\n",
      "the output we will be getting from NLTK.\n",
      "Figure 9-1. Rendering a feature structure as an attribute value matrix.\n",
      "336 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 357, 'page_label': '336', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4727}\n",
      "\n",
      "--- Chunk 4728 ---\n",
      "Content:\n",
      "On the topic of representation, we also note that feature structures, like dictionaries,\n",
      "assign no particular significance to the order of features. So (18) is equivalent to:\n",
      "(19) [AGR = [NUM = pl  ]]\n",
      "[      [PER = 3   ]]\n",
      "[      [GND = fem ]]\n",
      "[                  ]\n",
      "[POS = N           ]\n",
      "Once \n",
      "we have the possibility of using features like AGR, we can refactor a grammar like\n",
      "Example 9-1 so that agreement features are bundled together. A tiny grammar illus-\n",
      "trating this idea is shown in (20).\n",
      "(20) S -> NP[AGR=?n] VP[AGR=?n]\n",
      "NP[AGR=?n] -> PropN[AGR=?n]\n",
      "VP[TENSE=?t, AGR=?n] -> Cop[TENSE=?t, AGR=?n] Adj\n",
      "Cop[TENSE=pres,  AGR=[NUM=sg, PER=3]] -> 'is'\n",
      "PropN[AGR=[NUM=sg, PER=3]] -> 'Kim'\n",
      "Adj -> 'happy'\n",
      "9.2  Processing Feature Structures\n",
      "In this section, we will show how feature structures can be constructed and manipulated\n",
      "in NLTK. We will also discuss the fundamental operation of unification, which allows\n",
      "us to combine the information contained in two different feature structures.\n",
      "Feature structures in NLTK are declared with the FeatStruct() constructor. Atomic\n",
      "feature values can be strings or integers.\n",
      ">>> fs1 = nltk.FeatStruct(TENSE='past', NUM='sg')\n",
      ">>> print fs1\n",
      "[ NUM   = 'sg'   ]\n",
      "[ TENSE = 'past' ]\n",
      "A feature structure is actually just a kind of dictionary, and so we access its values by\n",
      "indexing in the usual way. We can use our familiar syntax to assign values to features:\n",
      ">>> fs1 = nltk.FeatStruct(PER=3, NUM='pl', GND='fem')\n",
      ">>> print fs1['GND']\n",
      "fem\n",
      ">>> fs1['CASE'] = 'acc'\n",
      "We can also define feature structures that have complex values, as discussed earlier.\n",
      ">>> fs2 = nltk.FeatStruct(POS='N', AGR=fs1)\n",
      ">>> print fs2\n",
      "[       [ CASE = 'acc' ] ]\n",
      "[ AGR = [ GND  = 'fem' ] ]\n",
      "[       [ NUM  = 'pl'  ] ]\n",
      "[       [ PER  = 3     ] ]\n",
      "[                        ]\n",
      "[ POS = 'N'              ]\n",
      "9.2  Processing Feature Structures | 337...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 358, 'page_label': '337', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4728}\n",
      "\n",
      "--- Chunk 4729 ---\n",
      "Content:\n",
      ">>> print fs2['AGR']\n",
      "[ CASE = 'acc' ]\n",
      "[ GND  = 'fem' ]\n",
      "[ NUM  = 'pl'  ]\n",
      "[ PER  = 3     ]\n",
      ">>> print fs2['AGR']['PER']\n",
      "3\n",
      "An \n",
      "alternative method of specifying feature structures is to use a bracketed string con-\n",
      "sisting of feature-value pairs in the format feature=value, where values may themselves\n",
      "be feature structures:\n",
      ">>> print nltk.FeatStruct(\"[POS='N', AGR=[PER=3, NUM='pl', GND='fem']]\")\n",
      "[       [ PER = 3     ] ]\n",
      "[ AGR = [ GND = 'fem' ] ]\n",
      "[       [ NUM = 'pl'  ] ]\n",
      "[                       ]\n",
      "[ POS = 'N'             ]\n",
      "Feature structures are not inherently tied to linguistic objects; they are general-purpose\n",
      "structures for representing knowledge. For example, we could encode information\n",
      "about a person in a feature structure:\n",
      ">>> print nltk.FeatStruct(name='Lee', telno='01 27 86 42 96', age=33)\n",
      "[ age   = 33               ]\n",
      "[ name  = 'Lee'            ]\n",
      "[ telno = '01 27 86 42 96' ]\n",
      "In the next couple of pages, we are going to use examples like this to explore standard\n",
      "operations over feature structures. This will briefly divert us from processing natural\n",
      "language, but we need to lay the groundwork before we can get back to talking about\n",
      "grammars. Hang on tight!\n",
      "It is often helpful to view feature structures as graphs, more specifically, as directed\n",
      "acyclic graphs (DAGs). (21) is equivalent to the preceding AVM.\n",
      "(21)\n",
      "The feature names appear as labels on the directed arcs, and feature values appear as\n",
      "labels on the nodes that are pointed to by the arcs.\n",
      "Just as before, feature values can be complex:\n",
      "338 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 359, 'page_label': '338', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4729}\n",
      "\n",
      "--- Chunk 4730 ---\n",
      "Content:\n",
      "(22)\n",
      "When we look at such graphs, it is natural to think in terms of paths through the graph.\n",
      "A feature path \n",
      "is a sequence of arcs that can be followed from the root node. We will\n",
      "represent paths as tuples of arc labels. Thus, ('ADDRESS', 'STREET') is a feature path\n",
      "whose value in (22) is the node labeled 'rue Pascal'.\n",
      "Now let’s consider a situation where Lee has a spouse named Kim, and Kim’s address\n",
      "is the same as Lee’s. We might represent this as (23).\n",
      "(23)\n",
      "However, rather than repeating the address information in the feature structure, we\n",
      "can “share” the same sub-graph between different arcs:\n",
      "9.2  Processing Feature Structures | 339...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 360, 'page_label': '339', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4730}\n",
      "\n",
      "--- Chunk 4731 ---\n",
      "Content:\n",
      "(24)\n",
      "In other words, the value of the path ('ADDRESS') in (24) is identical to the value of the\n",
      "path ('SPOUSE', 'ADDRESS'). DAGs such as (24) are said to involve structure shar-\n",
      "ing or reentrancy. When two paths have the same value, they are said to be\n",
      "equivalent.\n",
      "In order to indicate reentrancy in our matrix-style representations, we will prefix the\n",
      "first occurrence of a shared feature structure with an integer in parentheses, such as\n",
      "(1). Any later reference to that structure will use the notation ->(1), as shown here.\n",
      ">>> print nltk.FeatStruct(\"\"\"[NAME='Lee', ADDRESS=(1)[NUMBER=74, STREET='rue Pascal'],\n",
      "...                          SPOUSE=[NAME='Kim', ADDRESS->(1)]]\"\"\")\n",
      "[ ADDRESS = (1) [ NUMBER = 74           ] ]\n",
      "[               [ STREET = 'rue Pascal' ] ]\n",
      "[                                         ]\n",
      "[ NAME    = 'Lee'                         ]\n",
      "[                                         ]\n",
      "[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n",
      "[           [ NAME    = 'Kim' ]           ]\n",
      "The bracketed integer is sometimes called a tag or a coindex. The choice of integer is\n",
      "not significant. There can be any number of tags within a single feature structure.\n",
      ">>> print nltk.FeatStruct(\"[A='a', B=(1)[C='c'], D->(1), E->(1)]\")\n",
      "[ A = 'a'             ]\n",
      "[                     ]\n",
      "[ B = (1) [ C = 'c' ] ]\n",
      "[                     ]\n",
      "[ D -> (1)            ]\n",
      "[ E -> (1)            ]\n",
      "340 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 361, 'page_label': '340', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4731}\n",
      "\n",
      "--- Chunk 4732 ---\n",
      "Content:\n",
      "Subsumption and Unification\n",
      "It \n",
      "is standard to think of feature structures as providing partial information about\n",
      "some object, in the sense that we can order feature structures according to how general\n",
      "they are. For example, (25a) is more general (less specific) than (25b), which in turn is\n",
      "more general than (25c).\n",
      "(25) a. [NUMBER = 74]\n",
      "b. [NUMBER = 74          ]\n",
      "[STREET = 'rue Pascal']\n",
      "c. [NUMBER = 74          ]\n",
      "[STREET = 'rue Pascal']\n",
      "[CITY = 'Paris'       ]\n",
      "This ordering is called subsumption; a more general feature structure subsumes a less\n",
      "general one. If FS0 subsumes FS1 (formally, we write FS0 ⊑ FS1), then FS1 must have\n",
      "all the paths and path equivalences of FS0, and may have additional paths and equiv-\n",
      "alences as well. Thus, (23) subsumes (24) since the latter has additional path equiva-\n",
      "lences. It should be obvious that subsumption provides only a partial ordering on fea-\n",
      "ture structures, since some feature structures are incommensurable. For example,\n",
      "(26) neither subsumes nor is subsumed by (25a).\n",
      "(26) [TELNO = 01 27 86 42 96]\n",
      "So we have seen that some feature structures are more specific than others. How do we\n",
      "go about specializing a given feature structure? For example, we might decide that\n",
      "addresses should consist of not just a street number and a street name, but also a city.\n",
      "That is, we might want to merge graph (27a) with (27b) to yield (27c).\n",
      "9.2  Processing Feature Structures | 341...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 362, 'page_label': '341', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4732}\n",
      "\n",
      "--- Chunk 4733 ---\n",
      "Content:\n",
      "(27) a.\n",
      "b.\n",
      "c.\n",
      "Merging information from two feature structures is called unification and is supported\n",
      "by the unify() method.\n",
      ">>> fs1 = nltk.FeatStruct(NUMBER=74, STREET='rue Pascal')\n",
      ">>> fs2 = nltk.FeatStruct(CITY='Paris')\n",
      ">>> print fs1.unify(fs2)\n",
      "[ CITY   = 'Paris'      ]\n",
      "[ NUMBER = 74           ]\n",
      "[ STREET = 'rue Pascal' ]\n",
      "342 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 363, 'page_label': '342', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4733}\n",
      "\n",
      "--- Chunk 4734 ---\n",
      "Content:\n",
      "Unification is formally defined as a binary operation: FS0 ⊔ FS1. Unification is sym-\n",
      "metric, so FS0 ⊔ FS1 = FS1 ⊔ FS0. The same is true in Python:\n",
      ">>> print fs2.unify(fs1)\n",
      "[ CITY   = 'Paris'      ]\n",
      "[ NUMBER = 74           ]\n",
      "[ STREET = 'rue Pascal' ]\n",
      "If we unify two feature structures that stand in the subsumption relationship, then the\n",
      "result of unification is the most specific of the two:\n",
      "(28) If FS0 ⊑ FS1, then FS0 ⊔ FS1 = FS1\n",
      "For example, the result of unifying (25b) with (25c) is (25c).\n",
      "Unification between FS0 and FS1 will fail if the two feature structures share a path π\n",
      "where the value of π in FS0 is a distinct atom from the value of π in FS1. This is imple-\n",
      "mented by setting the result of unification to be None.\n",
      ">>> fs0 = nltk.FeatStruct(A='a')\n",
      ">>> fs1 = nltk.FeatStruct(A='b')\n",
      ">>> fs2 = fs0.unify(fs1)\n",
      ">>> print fs2\n",
      "None\n",
      "Now, if we look at how unification interacts with structure-sharing, things become\n",
      "really interesting. First, let’s define (23) in Python:\n",
      ">>> fs0 = nltk.FeatStruct(\"\"\"[NAME=Lee,\n",
      "...                           ADDRESS=[NUMBER=74,\n",
      "...                                    STREET='rue Pascal'],\n",
      "...                           SPOUSE= [NAME=Kim,\n",
      "...                                    ADDRESS=[NUMBER=74,\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 364, 'page_label': '343', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4734}\n",
      "\n",
      "--- Chunk 4735 ---\n",
      "Content:\n",
      ".                                    STREET='rue Pascal'],\n",
      "...                           SPOUSE= [NAME=Kim,\n",
      "...                                    ADDRESS=[NUMBER=74,\n",
      "...                                             STREET='rue Pascal']]]\"\"\")\n",
      ">>> print fs0\n",
      "[ ADDRESS = [ NUMBER = 74           ]               ]\n",
      "[           [ STREET = 'rue Pascal' ]               ]\n",
      "[                                                   ]\n",
      "[ NAME    = 'Lee'                                   ]\n",
      "[                                                   ]\n",
      "[           [ ADDRESS = [ NUMBER = 74           ] ] ]\n",
      "[ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]\n",
      "[           [                                     ] ]\n",
      "[           [ NAME    = 'Kim'                     ] ]\n",
      "What happens when we augment Kim’s address with a specification for CITY? Notice\n",
      "that fs1 needs to include the whole path from the root of the feature structure down\n",
      "to CITY.\n",
      ">>> fs1 = nltk.FeatStruct(\"[SPOUSE = [ADDRESS = [CITY = Paris]]]\")\n",
      ">>> print fs1.unify(fs0)\n",
      "[ ADDRESS = [ NUMBER = 74           ]               ]\n",
      "[           [ STREET = 'rue Pascal' ]               ]\n",
      "[                                                   ]\n",
      " \n",
      "9.2  Processing Feature Structures | 343...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 364, 'page_label': '343', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4735}\n",
      "\n",
      "--- Chunk 4736 ---\n",
      "Content:\n",
      "[ NAME    = 'Lee'                                   ]\n",
      "[                                                   ]\n",
      "[           [           [ CITY   = 'Paris'      ] ] ]\n",
      "[           [ ADDRESS = [ NUMBER = 74           ] ] ]\n",
      "[ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]\n",
      "[           [                                     ] ]\n",
      "[           [ NAME    = 'Kim'                     ] ]\n",
      "By \n",
      "contrast, the result is very different if fs1 is unified with the structure sharing version\n",
      "fs2 (also shown earlier as the graph (24)):\n",
      ">>> fs2 = nltk.FeatStruct(\"\"\"[NAME=Lee, ADDRESS=(1)[NUMBER=74, STREET='rue Pascal'],\n",
      "...                           SPOUSE=[NAME=Kim, ADDRESS->(1)]]\"\"\")\n",
      ">>> print fs1.unify(fs2)\n",
      "[               [ CITY   = 'Paris'      ] ]\n",
      "[ ADDRESS = (1) [ NUMBER = 74           ] ]\n",
      "[               [ STREET = 'rue Pascal' ] ]\n",
      "[                                         ]\n",
      "[ NAME    = 'Lee'                         ]\n",
      "[                                         ]\n",
      "[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n",
      "[           [ NAME    = 'Kim' ]           ]\n",
      "Rather than just updating what was in effect Kim’s “copy” of Lee’s address, we have\n",
      "now updated both their addresses at the same time...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 365, 'page_label': '344', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4736}\n",
      "\n",
      "--- Chunk 4737 ---\n",
      "Content:\n",
      ". More generally, if a unification\n",
      "involves specializing the value of some path π, that unification simultaneously spe-\n",
      "cializes the value of any path that is equivalent to π.\n",
      "As we have already seen, structure sharing can also be stated using variables such as\n",
      "?x.\n",
      ">>> fs1 = nltk.FeatStruct(\"[ADDRESS1=[NUMBER=74, STREET='rue Pascal']]\")\n",
      ">>> fs2 = nltk.FeatStruct(\"[ADDRESS1=?x, ADDRESS2=?x]\")\n",
      ">>> print fs2\n",
      "[ ADDRESS1 = ?x ]\n",
      "[ ADDRESS2 = ?x ]\n",
      ">>> print fs2.unify(fs1)\n",
      "[ ADDRESS1 = (1) [ NUMBER = 74           ] ]\n",
      "[                [ STREET = 'rue Pascal' ] ]\n",
      "[                                          ]\n",
      "[ ADDRESS2 -> (1)                          ]\n",
      "9.3  Extending a Feature-Based Grammar\n",
      "In this section, we return to feature-based grammar and explore a variety of linguistic\n",
      "issues, and demonstrate the benefits of incorporating features into the grammar.\n",
      "Subcategorization\n",
      "In Chapter 8, we augmented our category labels to represent different kinds of verbs,\n",
      "and used the labels IV and TV for intransitive and transitive verbs respectively. This\n",
      "allowed us to write productions like the following:\n",
      "344 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 365, 'page_label': '344', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4737}\n",
      "\n",
      "--- Chunk 4738 ---\n",
      "Content:\n",
      "(29) VP -> IV\n",
      "VP -> TV NP\n",
      "Although \n",
      "we know that IV and TV are two kinds of V, they are just atomic non-terminal\n",
      "symbols in a CFG and are as distinct from each other as any other pair of symbols. This\n",
      "notation doesn’t let us say anything about verbs in general; e.g., we cannot say “All\n",
      "lexical items of category V can be marked for tense,” since walk, say, is an item of\n",
      "category IV, not V. So, can we replace category labels such as TV and IV by V along with\n",
      "a feature that tells us whether the verb combines with a following NP object or whether\n",
      "it can occur without any complement?\n",
      "A simple approach, originally developed for a grammar framework called Generalized\n",
      "Phrase Structure Grammar (GPSG), tries to solve this problem by allowing lexical cat-\n",
      "egories to bear a SUBCAT feature, which tells us what subcategorization class the item\n",
      "belongs to. In contrast to the integer values for SUBCAT used by GPSG, the example here\n",
      "adopts more mnemonic values, namely intrans, trans, and clause:\n",
      "(30) VP[TENSE=?t, NUM=?n] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n]\n",
      "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=trans, TENSE=?t, NUM=?n] NP\n",
      "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=clause, TENSE=?t, NUM=?n] SBar\n",
      "V[SUBCAT=intrans, TENSE=pres, NUM=sg] -> 'disappears' | 'walks'\n",
      "V[SUBCAT=trans, TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
      "V[SUBCAT=clause, TENSE=pres, NUM=sg] -> 'says' | 'claims'\n",
      "V[SUBCAT=intrans, TENSE=pres, NUM=pl] -> 'disappear' | 'walk'\n",
      "V[SUBCAT=trans, TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
      "V[SUBCAT=clause, TENSE=pres, NUM=pl] -> 'say' | 'claim'\n",
      "V[SUBCAT=intrans, TENSE=past] -> 'disappeared' | 'walked'\n",
      "V[SUBCAT=trans, TENSE=past] -> 'saw' | 'liked'\n",
      "V[SUBCAT=clause, TENSE=past] -> 'said' | 'claimed'\n",
      "When we see a lexical category like V[SUBCAT=trans], we can interpret the SUBCAT spec-\n",
      "ification as a pointer to a production in which V[SUBCAT=trans] is introduced as the\n",
      "head child in a VP production. By convention, there is a correspondence between the\n",
      "values of SUBCAT and the productions that introduce lexical heads. On this approach,\n",
      "SUBCAT can appear only on lexical categories; it makes no sense, for example, to specify\n",
      "a SUBCAT value on VP. As required, walk and like both belong to the category V. Never-\n",
      "theless, walk will occur only in VPs expanded by a production with the feature\n",
      "SUBCAT=intrans on the righthand side, as opposed to like, which requires a\n",
      "SUBCAT=trans.\n",
      "In our third class of verbs in (30), we have specified a category SBar...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 366, 'page_label': '345', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4738}\n",
      "\n",
      "--- Chunk 4739 ---\n",
      "Content:\n",
      ". By convention, there is a correspondence between the\n",
      "values of SUBCAT and the productions that introduce lexical heads. On this approach,\n",
      "SUBCAT can appear only on lexical categories; it makes no sense, for example, to specify\n",
      "a SUBCAT value on VP. As required, walk and like both belong to the category V. Never-\n",
      "theless, walk will occur only in VPs expanded by a production with the feature\n",
      "SUBCAT=intrans on the righthand side, as opposed to like, which requires a\n",
      "SUBCAT=trans.\n",
      "In our third class of verbs in (30), we have specified a category SBar. This is a label for\n",
      "subordinate clauses, such as the complement of claim in the example You claim that\n",
      "you like children. We require two further productions to analyze such sentences:\n",
      "(31) SBar -> Comp S\n",
      "Comp -> 'that'\n",
      "9.3  Extending a Feature-Based Grammar | 345...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 366, 'page_label': '345', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4739}\n",
      "\n",
      "--- Chunk 4740 ---\n",
      "Content:\n",
      "The resulting structure is the following.\n",
      "(32)\n",
      "An alternative treatment of subcategorization, due originally to a framework known as\n",
      "categorial \n",
      "grammar, is represented in feature-based frameworks such as PATR and\n",
      "Head-driven Phrase Structure Grammar. Rather than using SUBCAT values as a way of\n",
      "indexing productions, the SUBCAT value directly encodes the valency of a head (the list\n",
      "of arguments that it can combine with). For example, a verb like put that takes NP and\n",
      "PP complements (put the book on the table) might be represented as (33):\n",
      "(33) V[SUBCAT=<NP, NP, PP>]\n",
      "This says that the verb can combine with three arguments. The leftmost element in the\n",
      "list is the subject NP, while everything else—an NP followed by a PP in this case—com-\n",
      "prises the subcategorized-for complements. When a verb like put is combined with\n",
      "appropriate complements, the requirements which are specified in the SUBCAT are dis-\n",
      "charged, and only a subject NP is needed. This category, which corresponds to what is\n",
      "traditionally thought of as VP, might be represented as follows:\n",
      "(34) V[SUBCAT=<NP>]\n",
      "Finally, a sentence is a kind of verbal category that has no requirements for further\n",
      "arguments, and hence has a SUBCAT whose value is the empty list. The tree (35) shows\n",
      "how these category assignments combine in a parse of Kim put the book on the table.\n",
      "(35)\n",
      "346 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 367, 'page_label': '346', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4740}\n",
      "\n",
      "--- Chunk 4741 ---\n",
      "Content:\n",
      "Heads Revisited\n",
      "We \n",
      "noted in the previous section that by factoring subcategorization information out\n",
      "of the main category label, we could express more generalizations about properties of\n",
      "verbs. Another property of this kind is the following: expressions of category V are heads\n",
      "of phrases of category VP. Similarly, Ns are heads of NPs, As (i.e., adjectives) are heads of\n",
      "APs, and Ps (i.e., prepositions) are heads of PPs. Not all phrases have heads—for exam-\n",
      "ple, it is standard to say that coordinate phrases (e.g., the book and the bell) lack heads.\n",
      "Nevertheless, we would like our grammar formalism to express the parent/head-child\n",
      "relation where it holds. At present, V and VP are just atomic symbols, and we need to\n",
      "find a way to relate them using features (as we did earlier to relate IV and TV).\n",
      "X-bar syntax addresses this issue by abstracting out the notion of phrasal level. It is\n",
      "usual to recognize three such levels. If N represents the lexical level, then N' represents\n",
      "the next level up, corresponding to the more traditional category Nom, and N'' represents\n",
      "the phrasal level, corresponding to the category NP. (36a) illustrates a representative\n",
      "structure, while (36b) is the more conventional counterpart.\n",
      "(36) a.\n",
      "b.\n",
      "The head of the structure (36a) is N, and N' and N'' are called (phrasal) projections of\n",
      "N. N'' is the maximal projection, and N is sometimes called the zero projection. One\n",
      "of \n",
      "the central claims of X-bar syntax is that all constituents share a structural similarity.\n",
      "Using X as a variable over N, V, A, and P, we say that directly subcategorized comple-\n",
      "ments of a lexical head X are always placed as siblings of the head, whereas adjuncts are\n",
      "placed as siblings of the intermediate category, X'. Thus, the configuration of the two\n",
      "P'' adjuncts in (37) contrasts with that of the complement P'' in (36a).\n",
      "9.3  Extending a Feature-Based Grammar | 347...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 368, 'page_label': '347', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4741}\n",
      "\n",
      "--- Chunk 4742 ---\n",
      "Content:\n",
      "(37)\n",
      "The productions in (38) illustrate how bar levels can be encoded using feature struc-\n",
      "tures. The nested structure in (37) is achieved by two applications of the recursive rule\n",
      "expanding N[BAR=1].\n",
      "(38) S -> N[BAR=2] V[BAR=2]\n",
      "N[BAR=2] -> Det N[BAR=1]\n",
      "N[BAR=1] -> N[BAR=1] P[BAR=2]\n",
      "N[BAR=1] -> N[BAR=0] P[BAR=2]\n",
      "Auxiliary Verbs and Inversion\n",
      "Inverted clauses—where the order of subject and verb is switched—occur in English\n",
      "interrogatives and also after “negative” adverbs:\n",
      "(39) a. Do you like children?\n",
      "b. Can Jody walk?\n",
      "(40) a. Rarely do you see Kim.\n",
      "b. Never have I seen this dog.\n",
      "However, we cannot place just any verb in pre-subject position:\n",
      "(41) a. *Like you children?\n",
      "b. *Walks Jody?\n",
      "(42) a. *Rarely see you Kim.\n",
      "b. *Never saw I this dog.\n",
      "Verbs that can be positioned initially in inverted clauses belong to the class known as\n",
      "auxiliaries, and as well as do, can, and have include be, will, and shall. One way of\n",
      "capturing such structures is with the following production:\n",
      "(43) S[+INV] -> V[+AUX] NP VP\n",
      "348 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 369, 'page_label': '348', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4742}\n",
      "\n",
      "--- Chunk 4743 ---\n",
      "Content:\n",
      "That is, a clause marked as [+inv] consists of an auxiliary verb followed by a VP. (In a\n",
      "more detailed grammar, we would need to place some constraints on the form of the\n",
      "VP, depending on the choice of auxiliary.) (44) illustrates the structure of an inverted\n",
      "clause:\n",
      "(44)\n",
      "Unbounded Dependency Constructions\n",
      "Consider the following contrasts:\n",
      "(45)\n",
      "a. You like Jody.\n",
      "b. *You like.\n",
      "(46) a. You put the card into the slot.\n",
      "b. *You put into the slot.\n",
      "c. *You put the card.\n",
      "d. *You put.\n",
      "The verb like requires an NP complement, while put requires both a following NP and\n",
      "PP. (45) and (46) show that these complements are obligatory: omitting them leads to\n",
      "ungrammaticality. Yet there are contexts in which obligatory complements can be\n",
      "omitted, as (47) and (48) illustrate.\n",
      "(47) a. Kim knows who you like.\n",
      "b. This music, you really like.\n",
      "(48) a. Which card do you put into the slot?\n",
      "b. Which slot do you put the card into?\n",
      "That is, an obligatory complement can be omitted if there is an appropriate filler in\n",
      "the sentence, such as the question word who in (47a), the preposed topic this music in\n",
      "(47b), or the wh phrases which card/slot in (48). It is common to say that sentences like\n",
      "those in (47) and (48) contain gaps where the obligatory complements have been\n",
      "omitted, and these gaps are sometimes made explicit using an underscore:\n",
      "(49) a. Which card do you put __ into the slot?\n",
      "b. Which slot do you put the card into __?\n",
      "9.3  Extending a Feature-Based Grammar | 349...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 370, 'page_label': '349', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4743}\n",
      "\n",
      "--- Chunk 4744 ---\n",
      "Content:\n",
      "So, a gap can occur if it is licensed by a filler. Conversely, fillers can occur only if there\n",
      "is an appropriate gap elsewhere in the sentence, as shown by the following examples:\n",
      "(50) a. *Kim knows who you like Jody.\n",
      "b. *This music, you really like hip-hop.\n",
      "(51) a. *Which card do you put this into the slot?\n",
      "b. *Which slot do you put the card into this one?\n",
      "The mutual co-occurrence between filler and gap is sometimes termed a “dependency.”\n",
      "One issue of considerable importance in theoretical linguistics has been the nature of\n",
      "the material that can intervene between a filler and the gap that it licenses; in particular,\n",
      "can we simply list a finite set of sequences that separate the two? The answer is no:\n",
      "there is no upper bound on the distance between filler and gap. This fact can be easily\n",
      "illustrated with constructions involving sentential complements, as shown in (52).\n",
      "(52) a. Who do you like __?\n",
      "b. Who do you claim that you like __?\n",
      "c. Who do you claim that Jody says that you like __?\n",
      "Since we can have indefinitely deep recursion of sentential complements, the gap can\n",
      "be embedded indefinitely far inside the whole sentence. This constellation of properties\n",
      "leads to the notion of an unbounded dependency construction, that is, a filler-gap\n",
      "dependency where there is no upper bound on the distance between filler and gap.\n",
      "A variety of mechanisms have been suggested for handling unbounded dependencies\n",
      "in formal grammars; here we illustrate the approach due to Generalized Phrase Struc-\n",
      "ture Grammar that involves slash categories. A slash category has the form Y/XP; we\n",
      "interpret this as a phrase of category Y that is missing a subconstituent of category XP.\n",
      "For example, S/NP is an S that is missing an NP. The use of slash categories is illustrated\n",
      "in (53).\n",
      "(53)\n",
      "The top part of the tree introduces the filler who (treated as an expression of category\n",
      "NP[+wh]) \n",
      "together with a corresponding gap-containing constituent S/NP. The gap\n",
      "350 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 371, 'page_label': '350', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4744}\n",
      "\n",
      "--- Chunk 4745 ---\n",
      "Content:\n",
      "information is then “percolated” down the tree via the VP/NP category, until it reaches\n",
      "the \n",
      "category NP/NP. At this point, the dependency is discharged by realizing the gap\n",
      "information as the empty string, immediately dominated by NP/NP.\n",
      "Do we need to think of slash categories as a completely new kind of object? Fortunately,\n",
      "we can accommodate them within our existing feature-based framework, by treating\n",
      "slash as a feature and the category to its right as a value; that is, S/NP is reducible to\n",
      "S[SLASH=NP]. In practice, this is also how the parser interprets slash categories.\n",
      "The grammar shown in Example 9-3 illustrates the main principles of slash categories,\n",
      "and also includes productions for inverted clauses. To simplify presentation, we have\n",
      "omitted any specification of tense on the verbs.\n",
      "Example 9-3. Grammar with productions for inverted clauses and long-distance dependencies,\n",
      "making use of slash categories.\n",
      ">>> nltk.data.show_cfg('grammars/book_grammars/feat1.fcfg')\n",
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "S[-INV] -> NP VP\n",
      "S[-INV]/?x -> NP VP/?x\n",
      "S[-INV] -> NP S/NP\n",
      "S[-INV] -> Adv[+NEG] S[+INV]\n",
      "S[+INV] -> V[+AUX] NP VP\n",
      "S[+INV]/?x -> V[+AUX] NP VP/?x\n",
      "SBar -> Comp S[-INV]\n",
      "SBar/?x -> Comp S[-INV]/?x\n",
      "VP -> V[SUBCAT=intrans, -AUX]\n",
      "VP -> V[SUBCAT=trans, -AUX] NP\n",
      "VP/?x -> V[SUBCAT=trans, -AUX] NP/?x\n",
      "VP -> V[SUBCAT=clause, -AUX] SBar\n",
      "VP/?x -> V[SUBCAT=clause, -AUX] SBar/?x\n",
      "VP -> V[+AUX] VP\n",
      "VP/?x -> V[+AUX] VP/?x\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "V[SUBCAT=intrans, -AUX] -> 'walk' | 'sing'\n",
      "V[SUBCAT=trans, -AUX] -> 'see' | 'like'\n",
      "V[SUBCAT=clause, -AUX] -> 'say' | 'claim'\n",
      "V[+AUX] -> 'do' | 'can'\n",
      "NP[-WH] -> 'you' | 'cats'\n",
      "NP[+WH] -> 'who'\n",
      "Adv[+NEG] -> 'rarely' | 'never'\n",
      "NP/NP ->\n",
      "Comp -> 'that'\n",
      "The grammar in Example 9-3 contains one “gap-introduction” production, namely S[-\n",
      "INV] -> NP S/NP. In order to percolate the slash feature correctly, we need to add slashes\n",
      "with variable values to both sides of the arrow in productions that expand S, VP, and\n",
      "NP. For example, VP/?x -> V SBar/?x is the slashed version of VP -> V SBar and says\n",
      "9.3  Extending a Feature-Based Grammar | 351...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 372, 'page_label': '351', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4745}\n",
      "\n",
      "--- Chunk 4746 ---\n",
      "Content:\n",
      "that a slash value can be specified on the VP parent of a constituent if the same value is\n",
      "also specified on the SBar child. Finally, NP/NP -> allows the slash information on NP to\n",
      "be discharged as the empty string. Using the grammar in Example 9-3, we can parse\n",
      "the sequence who do you claim that you like:\n",
      ">>> tokens = 'who do you claim that you like'.split()\n",
      ">>> from nltk import load_parser\n",
      ">>> cp = load_parser('grammars/book_grammars/feat1.fcfg')\n",
      ">>> for tree in cp.nbest_parse(tokens):\n",
      "...     print tree\n",
      "(S[-INV]\n",
      "  (NP[+WH] who)\n",
      "  (S[+INV]/NP[]\n",
      "    (V[+AUX] do)\n",
      "    (NP[-WH] you)\n",
      "    (VP[]/NP[]\n",
      "      (V[-AUX, SUBCAT='clause'] claim)\n",
      "      (SBar[]/NP[]\n",
      "        (Comp[] that)\n",
      "        (S[-INV]/NP[]\n",
      "          (NP[-WH] you)\n",
      "          (VP[]/NP[] (V[-AUX, SUBCAT='trans'] like) (NP[]/NP[] )))))))\n",
      "A more readable version of this tree is shown in (54).\n",
      "(54)\n",
      "The grammar in Example 9-3 will also allow us to parse sentences without gaps:\n",
      ">>> tokens = 'you claim that you like cats'.split()\n",
      ">>> for tree in cp.nbest_parse(tokens):\n",
      "...     print tree\n",
      "(S[-INV]\n",
      "  (NP[-WH] you)\n",
      "  (VP[]\n",
      "    (V[-AUX, SUBCAT='clause'] claim)\n",
      "    (SBar[]\n",
      "      (Comp[] that)\n",
      "      (S[-INV]\n",
      "        (NP[-WH] you)\n",
      "        (VP[] (V[-AUX, SUBCAT='trans'] like) (NP[-WH] cats))))))\n",
      "352 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 373, 'page_label': '352', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4746}\n",
      "\n",
      "--- Chunk 4747 ---\n",
      "Content:\n",
      "In addition, it admits inverted sentences that do not involve wh constructions:\n",
      ">>> tokens = 'rarely do you sing'.split()\n",
      ">>> for tree in cp.nbest_parse(tokens):\n",
      "...     print tree\n",
      "(S[-INV]\n",
      "  (Adv[+NEG] rarely)\n",
      "  (S[+INV]\n",
      "    (V[+AUX] do)\n",
      "    (NP[-WH] you)\n",
      "    (VP[] (V[-AUX, SUBCAT='intrans'] sing))))\n",
      "Case and Gender in German\n",
      "Compared \n",
      "with English, German has a relatively rich morphology for agreement. For\n",
      "example, the definite article in German varies with case, gender, and number, as shown\n",
      "in Table 9-2.\n",
      "Table 9-2. Morphological paradigm for the German definite article\n",
      "Case Masculine Feminine Neutral Plural\n",
      "Nominative der die das die\n",
      "Genitive des der des der\n",
      "Dative dem der dem den\n",
      "Accusative den die das die\n",
      "Subjects in German take the nominative case, and most verbs govern their objects in\n",
      "the \n",
      "accusative case. However, there are exceptions, such as helfen, that govern the\n",
      "dative case:\n",
      "(55) a. Die Katze sieht den Hund\n",
      "the.NOM.FEM.SG cat.3.FEM.SG see.3.SG the.ACC.MASC.SG dog.3.MASC.SG\n",
      "‘the cat sees the dog’\n",
      "b. *Die Katze sieht dem Hund\n",
      "the.NOM.FEM.SG cat.3.FEM.SG see.3.SG the.DAT.MASC.SG dog.3.MASC.SG\n",
      "c. Die Katze hilft dem Hund\n",
      "the.NOM.FEM.SG cat.3.FEM.SG help.3.SG the.DAT.MASC.SG dog.3.MASC.SG\n",
      "‘the cat helps the dog’\n",
      "d. *Die Katze hilft den Hund\n",
      "the.NOM.FEM.SG cat.3.FEM.SG help.3.SG the.ACC.MASC.SG dog.3.MASC.SG\n",
      "The grammar in Example 9-4 \n",
      " illustrates the interaction of agreement (comprising per-\n",
      "son, number, and gender) with case.\n",
      "9.3  Extending a Feature-Based Grammar | 353...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 374, 'page_label': '353', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4747}\n",
      "\n",
      "--- Chunk 4748 ---\n",
      "Content:\n",
      "Example 9-4...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 375, 'page_label': '354', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4748}\n",
      "\n",
      "--- Chunk 4749 ---\n",
      "Content:\n",
      ". Example feature-based grammar.\n",
      ">>> nltk.data.show_cfg('grammars/book_grammars/german.fcfg')\n",
      "% start S\n",
      " # Grammar Productions\n",
      " S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\n",
      " NP[CASE=?c, AGR=?a] -> PRO[CASE=?c, AGR=?a]\n",
      " NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\n",
      " VP[AGR=?a] -> IV[AGR=?a]\n",
      " VP[AGR=?a] -> TV[OBJCASE=?c, AGR=?a] NP[CASE=?c]\n",
      " # Lexical Productions\n",
      " # Singular determiners\n",
      " # masc\n",
      " Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -> 'der'\n",
      " Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -> 'dem'\n",
      " Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -> 'den'\n",
      " # fem\n",
      " Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n",
      " Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -> 'der'\n",
      " Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n",
      " # Plural determiners\n",
      " Det[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'die'\n",
      " Det[CASE=dat, AGR=[PER=3,NUM=pl]] -> 'den'\n",
      " Det[CASE=acc, AGR=[PER=3,NUM=pl]] -> 'die'\n",
      " # Nouns\n",
      " N[AGR=[GND=masc,PER=3,NUM=sg]] -> 'Hund'\n",
      " N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
      " N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunden'\n",
      " N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
      " N[AGR=[GND=fem,PER=3,NUM=sg]] -> 'Katze'\n",
      " N[AGR=[GND=fem,PER=3,NUM=pl]] -> 'Katzen'\n",
      " # Pronouns\n",
      " PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n",
      " PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n",
      " PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n",
      " PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n",
      " PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n",
      " PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 375, 'page_label': '354', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4749}\n",
      "\n",
      "--- Chunk 4750 ---\n",
      "Content:\n",
      "PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n",
      " PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n",
      " PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n",
      " PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n",
      " PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n",
      " PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'\n",
      " PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
      " PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
      " PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -> 'ihr'\n",
      " PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'sie'\n",
      " # Verbs\n",
      " IV[AGR=[NUM=sg,PER=1]] -> 'komme'\n",
      " IV[AGR=[NUM=sg,PER=2]] -> 'kommst'\n",
      " IV[AGR=[NUM=sg,PER=3]] -> 'kommt'\n",
      " IV[AGR=[NUM=pl, PER=1]] -> 'kommen'\n",
      " IV[AGR=[NUM=pl, PER=2]] -> 'kommt'\n",
      " IV[AGR=[NUM=pl, PER=3]] -> 'kommen'\n",
      " TV[OBJCASE=acc, AGR=[NUM=sg,PER=1]] -> 'sehe' | 'mag'\n",
      " TV[OBJCASE=acc, AGR=[NUM=sg,PER=2]] -> 'siehst' | 'magst'\n",
      " TV[OBJCASE=acc, AGR=[NUM=sg,PER=3]] -> 'sieht' | 'mag'\n",
      " TV[OBJCASE=dat, AGR=[NUM=sg,PER=1]] -> 'folge' | 'helfe'\n",
      " TV[OBJCASE=dat, AGR=[NUM=sg,PER=2]] -> 'folgst' | 'hilfst'\n",
      " TV[OBJCASE=dat, AGR=[NUM=sg,PER=3]] -> 'folgt' | 'hilft'\n",
      " TV[OBJCASE=acc, AGR=[NUM=pl,PER=1]] -> 'sehen' | 'moegen'\n",
      "354 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 375, 'page_label': '354', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4750}\n",
      "\n",
      "--- Chunk 4751 ---\n",
      "Content:\n",
      "TV[OBJCASE=acc, AGR=[NUM=pl,PER=2]] -> 'sieht' | 'moegt'\n",
      " TV[OBJCASE=acc, AGR=[NUM=pl,PER=3]] -> 'sehen' | 'moegen'\n",
      " TV[OBJCASE=dat, AGR=[NUM=pl,PER=1]] -> 'folgen' | 'helfen'\n",
      " TV[OBJCASE=dat, AGR=[NUM=pl,PER=2]] -> 'folgt' | 'helft'\n",
      " TV[OBJCASE=dat, AGR=[NUM=pl,PER=3]] -> 'folgen' | 'helfen'\n",
      "As \n",
      "you can see, the feature objcase is used to specify the case that a verb governs on its\n",
      "object. The next example illustrates the parse tree for a sentence containing a verb that\n",
      "governs the dative case:\n",
      ">>> tokens = 'ich folge den Katzen'.split()\n",
      ">>> cp = load_parser('grammars/book_grammars/german.fcfg')\n",
      ">>> for tree in cp.nbest_parse(tokens):\n",
      "...     print tree\n",
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=1], CASE='nom']\n",
      "    (PRO[AGR=[NUM='sg', PER=1], CASE='nom'] ich))\n",
      "  (VP[AGR=[NUM='sg', PER=1]]\n",
      "    (TV[AGR=[NUM='sg', PER=1], OBJCASE='dat'] folge)\n",
      "    (NP[AGR=[GND='fem', NUM='pl', PER=3], CASE='dat']\n",
      "      (Det[AGR=[NUM='pl', PER=3], CASE='dat'] den)\n",
      "      (N[AGR=[GND='fem', NUM='pl', PER=3]] Katzen))))\n",
      "In developing grammars, excluding ungrammatical word sequences is often as chal-\n",
      "lenging as parsing grammatical ones. In order to get an idea where and why a sequence\n",
      "fails to parse, setting the trace parameter of the load_parser() method can be crucial.\n",
      "Consider the following parse failure:\n",
      ">>> tokens = 'ich folge den Katze'.split()\n",
      ">>> cp = load_parser('grammars/book_grammars/german.fcfg', trace=2)\n",
      ">>> for tree in cp.nbest_parse(tokens):\n",
      "...     print tree\n",
      "|.ich.fol.den.Kat.|\n",
      "|[---]   .   .   .| PRO[AGR=[NUM='sg', PER=1], CASE='nom'] -> 'ich' *\n",
      "|[---]   .   .   .| NP[AGR=[NUM='sg', PER=1], CASE='nom']\n",
      "                  -> PRO[AGR=[NUM='sg', PER=1], CASE='nom'] *\n",
      "|[--->   .   .   .| S[] -> NP[AGR=?a, CASE='nom'] * VP[AGR=?a]\n",
      "                        {?a: [NUM='sg', PER=1]}\n",
      "|.   [---]...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 376, 'page_label': '355', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4751}\n",
      "\n",
      "--- Chunk 4752 ---\n",
      "Content:\n",
      ".   .| NP[AGR=[NUM='sg', PER=1], CASE='nom']\n",
      "                  -> PRO[AGR=[NUM='sg', PER=1], CASE='nom'] *\n",
      "|[--->   .   .   .| S[] -> NP[AGR=?a, CASE='nom'] * VP[AGR=?a]\n",
      "                        {?a: [NUM='sg', PER=1]}\n",
      "|.   [---]   .   .| TV[AGR=[NUM='sg', PER=1], OBJCASE='dat'] -> 'folge' *\n",
      "|.   [--->   .   .| VP[AGR=?a] -> TV[AGR=?a, OBJCASE=?c]\n",
      "                        * NP[CASE=?c] {?a: [NUM='sg', PER=1], ?c: 'dat'}\n",
      "|.   .   [---]   .| Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='acc'] -> 'den' *\n",
      "|.   .   [---]   .| Det[AGR=[NUM='pl', PER=3], CASE='dat'] -> 'den' *\n",
      "|.   .   [--->   .| NP[AGR=?a, CASE=?c] -> Det[AGR=?a, CASE=?c]\n",
      "                  * N[AGR=?a, CASE=?c] {?a: [NUM='pl', PER=3], ?c: 'dat'}\n",
      "|.   .   [--->   .| NP[AGR=?a, CASE=?c] -> Det[AGR=?a, CASE=?c] * N[AGR=?a, CASE=?c]\n",
      "                 {?a: [GND='masc', NUM='sg', PER=3], ?c: 'acc'}\n",
      "|.   .   .   [---]| N[AGR=[GND='fem', NUM='sg', PER=3]] -> 'Katze' *\n",
      "9.3  Extending a Feature-Based Grammar | 355...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 376, 'page_label': '355', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4752}\n",
      "\n",
      "--- Chunk 4753 ---\n",
      "Content:\n",
      "The last two Scanner lines in the trace show that den is recognized as admitting two\n",
      "possible categories: Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='acc'] and\n",
      "Det[AGR=[NUM='pl', PER=3], CASE='dat']. We know from the grammar in Exam-\n",
      "ple 9-4 that Katze has category N[AGR=[GND=fem, NUM=sg, PER=3]]. Thus there is no\n",
      "binding for the variable ?a in production:\n",
      "NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=? a] N[CASE=?c, AGR=?a] \n",
      "that will satisfy these constraints, since the AGR value of Katze will not unify with either\n",
      "of the AGR values of den, that is, with either [GND='masc', NUM='sg', PER=3] or\n",
      "[NUM='pl', PER=3].\n",
      "9.4  Summary\n",
      "• The traditional categories of context-free grammar are atomic symbols. An impor-\n",
      "tant motivation for feature structures is to capture fine-grained distinctions that\n",
      "would otherwise require a massive multiplication of atomic categories.\n",
      "• By using variables over feature values, we can express constraints in grammar pro-\n",
      "ductions that allow the realization of different feature specifications to be inter-\n",
      "dependent.\n",
      "• Typically we specify fixed values of features at the lexical level and constrain the\n",
      "values of features in phrases to unify with the corresponding values in their\n",
      "children.\n",
      "• Feature values are either atomic or complex. A particular subcase of atomic value\n",
      "is the Boolean value, represented by convention as [+/- feat].\n",
      "• Two features can share a value (either atomic or complex). Structures with shared\n",
      "values are said to be re-entrant. Shared values are represented by numerical indexes\n",
      "(or tags) in AVMs.\n",
      "• A path in a feature structure is a tuple of features corresponding to the labels on a\n",
      "sequence of arcs from the root of the graph representation.\n",
      "• Two paths are equivalent if they share a value.\n",
      "• Feature structures are partially ordered by subsumption. FS0 subsumes FS1 when\n",
      "FS0 is more general (less informative) than FS1.\n",
      "• The unification of two structures FS0 and FS1, if successful, is the feature structure\n",
      "FS2 that contains the combined information of both FS0 and FS1.\n",
      "• If unification specializes a path π in FS, then it also specializes every path π' equiv-\n",
      "alent to π.\n",
      "• We can use feature structures to build succinct analyses of a wide variety of lin-\n",
      "guistic phenomena, including verb subcategorization, inversion constructions,\n",
      "unbounded dependency constructions, and case government.\n",
      "356 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 377, 'page_label': '356', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4753}\n",
      "\n",
      "--- Chunk 4754 ---\n",
      "Content:\n",
      "9.5  Further Reading\n",
      "Please \n",
      "consult http://www.nltk.org/ for further materials on this chapter, including\n",
      "HOWTOs feature structures, feature grammars, Earley parsing, and grammar test\n",
      "suites.\n",
      "For an excellent introduction to the phenomenon of agreement, see (Corbett, 2006).\n",
      "The earliest use of features in theoretical linguistics was designed to capture phono-\n",
      "logical properties of phonemes. For example, a sound like /b/ might be decomposed\n",
      "into the structure [+labial, +voice]. An important motivation was to capture gener-\n",
      "alizations across classes of segments, for example, that /n/ gets realized as /m/ preceding\n",
      "any +labial consonant. Within Chomskyan grammar, it was standard to use atomic\n",
      "features for phenomena such as agreement, and also to capture generalizations across\n",
      "syntactic categories, by analogy with phonology. A radical expansion of the use of\n",
      "features in theoretical syntax was advocated by Generalized Phrase Structure Grammar\n",
      "(GPSG; [Gazdar et al., 1985]), particularly in the use of features with complex values.\n",
      "Coming more from the perspective of computational linguistics, (Kay, 1985) proposed\n",
      "that functional aspects of language could be captured by unification of attribute-value\n",
      "structures, and a similar approach was elaborated by (Grosz & Stickel, 1983) within\n",
      "the PATR-II formalism. Early work in Lexical-Functional grammar (LFG; [Kaplan &\n",
      "Bresnan, 1982]) introduced the notion of an f-structure that was primarily intended\n",
      "to represent the grammatical relations and predicate-argument structure associated\n",
      "with a constituent structure parse. (Shieber, 1986) provides an excellent introduction\n",
      "to this phase of research into feature-based grammars.\n",
      "One conceptual difficulty with algebraic approaches to feature structures arose when\n",
      "researchers attempted to model negation. An alternative perspective, pioneered by\n",
      "(Kasper & Rounds, 1986) and (Johnson, 1988), argues that grammars involve descrip-\n",
      "tions of feature structures rather than the structures themselves. These descriptions are\n",
      "combined using logical operations such as conjunction, and negation is just the usual\n",
      "logical operation over feature descriptions. This description-oriented perspective was\n",
      "integral to LFG from the outset (Kaplan, 1989), and was also adopted by later versions\n",
      "of Head-Driven Phrase Structure Grammar (HPSG; [Sag & Wasow, 1999]). A com-\n",
      "prehensive bibliography of HPSG literature can be found at http://www.cl.uni-bremen\n",
      ".de/HPSG-Bib/.\n",
      "Feature structures, as presented in this chapter, are unable to capture important con-\n",
      "straints on linguistic information. For example, there is no way of saying that the only\n",
      "permissible values for NUM are sg and pl, while a specification such as [NUM=masc] is\n",
      "anomalous. Similarly, we cannot say that the complex value of AGR must contain spec-\n",
      "ifications for the features PER, NUM, and GND, but cannot contain a specification such as\n",
      "[SUBCAT=trans]. Typed feature structures were developed to remedy this deficiency. \n",
      "A good early review of work on typed feature structures is (Emele & Zajac, 1990). A\n",
      "more comprehensive examination of the formal foundations can be found in\n",
      "9.5  Further Reading | 357...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 378, 'page_label': '357', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4754}\n",
      "\n",
      "--- Chunk 4755 ---\n",
      "Content:\n",
      "(Carpenter, 1992), while (Copestake, 2002) focuses on implementing an HPSG-orien-\n",
      "ted approach to typed feature structures.\n",
      "There is a copious literature on the analysis of German within feature-based grammar\n",
      "frameworks. (Nerbonne, Netter & Pollard, 1994) is a good starting point for the HPSG\n",
      "literature on this topic, while (Müller, 2002) gives a very extensive and detailed analysis\n",
      "of German syntax in HPSG.\n",
      "Chapter 15 of (Jurafsky & Martin, 2008) discusses feature structures, the unification\n",
      "algorithm, and the integration of unification into parsing algorithms.\n",
      "9.6  Exercises\n",
      "1. ○ What constraints are required to correctly parse word sequences like I am hap-\n",
      "py and she is happy but not *you is happy or *they am happy? Implement two sol-\n",
      "utions for the present tense paradigm of the verb be in English, first taking Gram-\n",
      "mar (8) as your starting point, and then taking Grammar (20) as the starting point.\n",
      "2. ○ Develop a variant of grammar in Example 9-1 that uses a feature COUNT to make\n",
      "the distinctions shown here:\n",
      "(56) a. The boy sings.\n",
      "b. *Boy sings.\n",
      "(57) a. The boys sing.\n",
      "b. Boys sing.\n",
      "(58) a. The water is precious.\n",
      "b. Water is precious.\n",
      "3. ○ Write a function subsumes() that holds of two feature structures fs1 and fs2 just\n",
      "in case fs1 subsumes fs2.\n",
      "4. ○ Modify the grammar illustrated in (30) to incorporate a BAR feature for dealing\n",
      "with phrasal projections.\n",
      "5. ○ Modify the German grammar in Example 9-4 to incorporate the treatment of\n",
      "subcategorization presented in Section 9.3.\n",
      "6. ◑ Develop a feature-based grammar that will correctly describe the following\n",
      "Spanish noun phrases:\n",
      "(59) un cuadro hermos-o\n",
      "INDEF.SG.MASC picture beautiful-SG.MASC\n",
      "‘a beautiful picture’\n",
      "(60) un-os cuadro-s hermos-os\n",
      "INDEF-PL.MASC picture-PL beautiful-PL.MASC\n",
      "‘beautiful pictures’\n",
      "358 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 379, 'page_label': '358', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4755}\n",
      "\n",
      "--- Chunk 4756 ---\n",
      "Content:\n",
      "(61) un-a cortina hermos-a\n",
      "INDEF-SG.FEM curtain beautiful-SG.FEM\n",
      "‘a beautiful curtain’\n",
      "(62) un-as cortina-s hermos-as\n",
      "INDEF-PL.FEM curtain beautiful-PL.FEM\n",
      "‘beautiful curtains’\n",
      "7. ◑ Develop \n",
      " a wrapper for the earley_parser so that a trace is only printed if the\n",
      "input sequence fails to parse.\n",
      "8. ◑ Consider the feature structures shown in Example 9-5.\n",
      "Example 9-5. Exploring feature structures.\n",
      "fs1 = nltk.FeatStruct(\"[A = ?x, B= [C = ?x]]\")\n",
      "fs2 = nltk.FeatStruct(\"[B = [D = d]]\")\n",
      "fs3 = nltk.FeatStruct(\"[B = [C = d]]\")\n",
      "fs4 = nltk.FeatStruct(\"[A = (1)[B = b], C->(1)]\")\n",
      "fs5 = nltk.FeatStruct(\"[A = (1)[D = ?x], C = [E -> (1), F = ?x] ]\")\n",
      "fs6 = nltk.FeatStruct(\"[A = [D = d]]\")\n",
      "fs7 = nltk.FeatStruct(\"[A = [D = d], C = [F = [D = d]]]\")\n",
      "fs8 = nltk.FeatStruct(\"[A = (1)[D = ?x, G = ?x], C = [B = ?x, E -> (1)] ]\")\n",
      "fs9 = nltk.FeatStruct(\"[A = [B = b], C = [E = [G = e]]]\")\n",
      "fs10 = nltk.FeatStruct(\"[A = (1)[B = b], C -> (1)]\")\n",
      "Work out on paper what the result is of the following unifications. (Hint: you might\n",
      "find it useful to draw the graph structures.)\n",
      "a. fs1 and fs2\n",
      "b. fs1 and fs3\n",
      "c. fs4 and fs5\n",
      "d. fs5 and fs6\n",
      "e. fs5 and fs7\n",
      "f. fs8 and fs9\n",
      "g. fs8 and fs10\n",
      "Check your answers using NLTK.\n",
      "9. ◑ List two feature structures that subsume [A=?x, B=?x].\n",
      "10. ◑ Ignoring structure sharing, give an informal algorithm for unifying two feature\n",
      "structures.\n",
      "11. ◑ Extend the German grammar in Example 9-4 so that it can handle so-called verb-\n",
      "second structures like the following:\n",
      "(63) Heute sieht der Hund die Katze.\n",
      "12. ◑ Seemingly synonymous verbs have slightly different syntactic properties (Levin,\n",
      "1993). Consider the following patterns of grammaticality for the verbs loaded,\n",
      "filled, and dumped. Can you write grammar productions to handle such data?\n",
      "9.6  Exercises | 359...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 380, 'page_label': '359', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4756}\n",
      "\n",
      "--- Chunk 4757 ---\n",
      "Content:\n",
      "(64) a. The farmer loaded the cart with sand\n",
      "b.\n",
      "The farmer loaded sand into the cart\n",
      "c. The farmer filled the cart with sand\n",
      "d. *The farmer filled sand into the cart\n",
      "e. *The farmer dumped the cart with sand\n",
      "f. The farmer dumped sand into the cart\n",
      "13. ● Morphological paradigms are rarely completely regular, in the sense of every cell\n",
      "in the matrix having a different realization. For example, the present tense conju-\n",
      "gation of the lexeme walk has only two distinct forms: walks for the third-person\n",
      "singular, and walk for all other combinations of person and number. A successful\n",
      "analysis should not require redundantly specifying that five out of the six possible\n",
      "morphological combinations have the same realization. Propose and implement a\n",
      "method for dealing with this.\n",
      "14. ● So-called head features are shared between the parent node and head child. For\n",
      "example, TENSE is a head feature that is shared between a VP and its head V child.\n",
      "See (Gazdar et al., 1985) for more details. Most of the features we have looked at\n",
      "are head features—exceptions are SUBCAT and SLASH. Since the sharing of head fea-\n",
      "tures is predictable, it should not need to be stated explicitly in the grammar\n",
      "productions. Develop an approach that automatically accounts for this regular\n",
      "behavior of head features.\n",
      "15. ● Extend NLTK’s treatment of feature structures to allow unification into list-\n",
      "valued features, and use this to implement an HPSG-style analysis of subcategori-\n",
      "zation, whereby the SUBCAT of a head category is the concatenation of its\n",
      "complements’ categories with the SUBCAT value of its immediate parent.\n",
      "16. ● Extend NLTK’s treatment of feature structures to allow productions with un-\n",
      "derspecified categories, such as S[-INV] -> ?x S/?x.\n",
      "17. ● Extend NLTK’s treatment of feature structures to allow typed feature structures.\n",
      "18. ● Pick some grammatical constructions described in (Huddleston & Pullum,\n",
      "2002), and develop a feature-based grammar to account for them.\n",
      "360 | Chapter 9:  Building Feature-Based Grammars...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 381, 'page_label': '360', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4757}\n",
      "\n",
      "--- Chunk 4758 ---\n",
      "Content:\n",
      "CHAPTER 10\n",
      "Analyzing the Meaning of Sentences\n",
      "We have seen how useful it is to harness the power of a computer to process text on a\n",
      "large \n",
      "scale. However, now that we have the machinery of parsers and feature-based\n",
      "grammars, can we do anything similarly useful by analyzing the meaning of sentences?\n",
      "The goal of this chapter is to answer the following questions:\n",
      "1. How can we represent natural language meaning so that a computer can process\n",
      "these representations?\n",
      "2. How can we associate meaning representations with an unlimited set of sentences?\n",
      "3. How can we use programs that connect the meaning representations of sentences\n",
      "to stores of knowledge?\n",
      "Along the way we will learn some formal techniques in the field of logical semantics,\n",
      "and see how these can be used for interrogating databases that store facts about the\n",
      "world.\n",
      "10.1  Natural Language Understanding\n",
      "Querying a Database\n",
      "Suppose we have a program that lets us type in a natural language question and gives\n",
      "us back the right answer:\n",
      "(1) a. Which country is Athens in?\n",
      "b. Greece.\n",
      "How hard is it to write such a program? And can we just use the same techniques that\n",
      "we’ve encountered so far in this book, or does it involve something new? In this section,\n",
      "we will show that solving the task in a restricted domain is pretty straightforward. But\n",
      "we will also see that to address the problem in a more general way, we have to open up\n",
      "a whole new box of ideas and techniques, involving the representation of meaning.\n",
      "361...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 382, 'page_label': '361', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4758}\n",
      "\n",
      "--- Chunk 4759 ---\n",
      "Content:\n",
      "So let’s start off by assuming that we have data about cities and countries in a structured\n",
      "form. \n",
      "To be concrete, we will use a database table whose first few rows are shown in\n",
      "Table 10-1.\n",
      "The data illustrated in Table 10-1 is drawn from the Chat-80 system\n",
      "(Warren & Pereira, 1982). Population figures are given in thousands,\n",
      "but note that the data used in these examples dates back at least to the\n",
      "1980s, and was already somewhat out of date at the point when (Warren\n",
      "& Pereira, 1982) was published.\n",
      "Table 10-1. city_table: A table of cities, countries, and populations\n",
      "City Country Population\n",
      "athens greece 1368\n",
      "bangkok thailand 1178\n",
      "barcelona spain 1280\n",
      "berlin east_germany 3481\n",
      "birmingham\n",
      "united_kingdom 1112\n",
      "The obvious way to retrieve answers from this tabular data involves writing queries in\n",
      "a database query language such as SQL.\n",
      "SQL (Structured Query Language) is a language designed for retrieving\n",
      "and \n",
      "managing data in relational databases. If you want to find out more\n",
      "about SQL, http://www.w3schools.com/sql/ is a convenient online\n",
      "reference.\n",
      "For example, executing the query (2) will pull out the value 'greece':\n",
      "(2) SELECT Country FROM city_table WHERE City = 'athens'\n",
      "This specifies a result set consisting of all values for the column Country in data rows\n",
      "where the value of the City column is 'athens'.\n",
      "How can we get the same effect using English as our input to the query system? The\n",
      "feature-based grammar formalism described in Chapter 9 makes it easy to translate\n",
      "from English to SQL. The grammar sql0.fcfg illustrates how to assemble a meaning\n",
      "representation for a sentence in tandem with parsing the sentence. Each phrase struc-\n",
      "ture rule is supplemented with a recipe for constructing a value for the feature SEM. You\n",
      "can see that these recipes are extremely simple; in each case, we use the string concat-\n",
      "enation operation + to splice the values for the child constituents to make a value for\n",
      "the parent constituent.\n",
      "362 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 383, 'page_label': '362', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4759}\n",
      "\n",
      "--- Chunk 4760 ---\n",
      "Content:\n",
      ">>> nltk.data.show_cfg('grammars/book_grammars/sql0.fcfg')\n",
      "% start S\n",
      "S[SEM=(?np + WHERE + ?vp)] -> NP[SEM=?np] VP[SEM=?vp]\n",
      "VP[SEM=(?v + ?pp)] -> IV[SEM=?v] PP[SEM=?pp]\n",
      "VP[SEM=(?v + ?ap)] -> IV[SEM=?v] AP[SEM=?ap]\n",
      "NP[SEM=(?det + ?n)] -> Det[SEM=?det] N[SEM=?n]\n",
      "PP[SEM=(?p + ?np)] -> P[SEM=?p] NP[SEM=?np]\n",
      "AP[SEM=?pp] -> A[SEM=?a] PP[SEM=?pp]\n",
      "NP[SEM='Country=\"greece\"'] -> 'Greece'\n",
      "NP[SEM='Country=\"china\"'] -> 'China'\n",
      "Det[SEM='SELECT'] -> 'Which' | 'What'\n",
      "N[SEM='City FROM city_table'] -> 'cities'\n",
      "IV[SEM=''] -> 'are'\n",
      "A[SEM=''] -> 'located'\n",
      "P[SEM=''] -> 'in'\n",
      "This allows us to parse a query into SQL:\n",
      ">>> from nltk import load_parser\n",
      ">>> cp = load_parser('grammars/book_grammars/sql0.fcfg')\n",
      ">>> query = 'What cities are located in China'\n",
      ">>> trees = cp.nbest_parse(query.split())\n",
      ">>> answer = trees[0].node['sem']\n",
      ">>> q = ' '.join(answer)\n",
      ">>> print q\n",
      "SELECT City FROM city_table WHERE Country=\"china\"\n",
      "Your Turn:  Run the parser with maximum tracing on, i.e., cp =\n",
      "load_parser('grammars/book_grammars/sql0.fcfg', trace=3), and ex-\n",
      "amine how the values of SEM are built up as complete edges are added\n",
      "to the chart.\n",
      "Finally, we execute the query over the database city.db and retrieve some results:\n",
      ">>> from nltk.sem import chat80\n",
      ">>> rows = chat80.sql_query('corpora/city_database/city.db', q)\n",
      ">>> for r in rows: print r[0], \n",
      "canton chungking dairen harbin kowloon mukden peking shanghai sian tientsin\n",
      "Since \n",
      "each row r is a one-element tuple, we print out the member of the tuple rather\n",
      "than the tuple itself \n",
      " .\n",
      "To \n",
      "summarize, we have defined a task where the computer returns useful data in re-\n",
      "sponse to a natural language query, and we implemented this by translating a small\n",
      "subset of English into SQL. We can say that our NLTK code already “understands”\n",
      "SQL, given that Python is able to execute SQL queries against a database, and by ex-\n",
      "tension it also “understands” queries such as What cities are located in China . This\n",
      "parallels being able to translate from Dutch into English as an example of natural lan-\n",
      "guage understanding. Suppose that you are a native speaker of English, and have started\n",
      "to learn Dutch. Your teacher asks if you understand what (3) means:\n",
      "(3) Margrietje houdt van Brunoke.\n",
      "10.1  Natural Language Understanding | 363...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 384, 'page_label': '363', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4760}\n",
      "\n",
      "--- Chunk 4761 ---\n",
      "Content:\n",
      "If you know the meanings of the individual words in (3), and know how these meanings\n",
      "are combined to make up the meaning of the whole sentence, you might say that (3)\n",
      "means the same as Margrietje loves Brunoke.\n",
      "An observer—let’s call her Olga—might well take this as evidence that you do grasp\n",
      "the meaning of (3). But this would depend on Olga herself understanding English. If\n",
      "she doesn’t, then your translation from Dutch to English is not going to convince her\n",
      "of your ability to understand Dutch. We will return to this issue shortly.\n",
      "The grammar sql0.fcfg, together with the NLTK Earley parser, is instrumental in car-\n",
      "rying out the translation from English to SQL. How adequate is this grammar? You saw\n",
      "that the SQL translation for the whole sentence was built up from the translations of\n",
      "the components. However, there does not seem to be a lot of justification for these\n",
      "component meaning representations. For example, if we look at the analysis of the\n",
      "noun phrase Which cities, the determiner and noun correspond respectively to the SQL\n",
      "fragments SELECT and City FROM city_table. But neither of these has a well-defined\n",
      "meaning in isolation from the other.\n",
      "There is another criticism we can level at the grammar: we have “hard-wired” an em-\n",
      "barrassing amount of detail about the database into it. We need to know the name of\n",
      "the relevant table (e.g., city_table) and the names of the fields. But our database could\n",
      "have contained exactly the same rows of data yet used a different table name and dif-\n",
      "ferent field names, in which case the SQL queries would not be executable. Equally,\n",
      "we could have stored our data in a different format, such as XML, in which case re-\n",
      "trieving the same results would require us to translate our English queries into an XML\n",
      "query language rather than SQL. These considerations suggest that we should be trans-\n",
      "lating English into something that is more abstract and generic than SQL.\n",
      "In order to sharpen the point, let’s consider another English query and its translation:\n",
      "(4) a. What cities are in China and have populations above 1,000,000?\n",
      "b. SELECT City FROM city_table WHERE Country = 'china' AND Population >\n",
      "1000\n",
      "Your Turn: Extend the grammar sql0.fcfg so that it will translate (4a)\n",
      "into (4b), and check the values returned by the query. Remember that\n",
      "figures in the Chat-80 database are given in thousands, hence 1000 in\n",
      "(4b) represents one million inhabitants.\n",
      "You will probably find it easiest to first extend the grammar to handle\n",
      "queries like What cities have populations above 1,000,000  before tack-\n",
      "ling conjunction. After you have had a go at this task, you can compare\n",
      "your solution to grammars/book_grammars/sql1.fcfg in the NLTK data\n",
      "distribution.\n",
      "364 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 385, 'page_label': '364', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4761}\n",
      "\n",
      "--- Chunk 4762 ---\n",
      "Content:\n",
      "Observe that the and conjunction in (4a) is translated into an AND in the SQL counter-\n",
      "part, (4b). \n",
      "The latter tells us to select results from rows where two conditions are true\n",
      "together: the value of the Country column is 'china' and the value of the Population\n",
      "column is greater than 1000. This interpretation for and involves a new idea: it talks\n",
      "about what is true in some particular situation, and tells us that Cond1 AND Cond2 is true\n",
      "in situation s if and only if condition Cond1 is true in s and condition Cond2 is true in s.\n",
      "Although this doesn’t account for the full range of meanings of and in English, it has\n",
      "the nice property that it is independent of any query language. In fact, we have given\n",
      "it the standard interpretation from classical logic. In the following sections, we will\n",
      "explore an approach in which sentences of natural language are translated into logic\n",
      "instead of an executable query language such as SQL. One advantage of logical for-\n",
      "malisms is that they are more abstract and therefore more generic. If we wanted to,\n",
      "once we had our translation into logic, we could then translate it into various other\n",
      "special-purpose languages. In fact, most serious attempts to query databases via natural\n",
      "language have used this methodology.\n",
      "Natural Language, Semantics, and Logic\n",
      "We started out trying to capture the meaning of (1a) by translating it into a query in\n",
      "another language, SQL, which the computer could interpret and execute. But this still\n",
      "begged the question whether the translation was correct. Stepping back from database\n",
      "query, we noted that the meaning of and seems to depend on being able to specify when\n",
      "statements are true or not in a particular situation. Instead of translating a sentence S\n",
      "from one language to another, we try to say what S is about by relating it to a situation\n",
      "in the world. Let’s pursue this further. Imagine there is a situation s where there are\n",
      "two entities, Margrietje and her favorite doll, Brunoke. In addition, there is a relation\n",
      "holding between the two entities, which we will call the love relation. If you understand\n",
      "the meaning of (3), then you know that it is true in situation s. In part, you know this\n",
      "because you know that Margrietje refers to Margrietje, Brunoke refers to Brunoke, and\n",
      "houdt van refers to the love relation.\n",
      "We have introduced two fundamental notions in semantics. The first is that declarative\n",
      "sentences are true or false in certain situations. The second is that definite noun phrases\n",
      "and proper nouns refer to things in the world . So (3) is true in a situation where Mar-\n",
      "grietje loves the doll Brunoke, here illustrated in Figure 10-1.\n",
      "Once we have adopted the notion of truth in a situation, we have a powerful tool for\n",
      "reasoning. In particular, we can look at sets of sentences, and ask whether they could\n",
      "be true together in some situation. For example, the sentences in (5) can be both true,\n",
      "whereas those in (6) and (7) cannot be. In other words, the sentences in (5) are con-\n",
      "sistent, whereas those in (6) and (7) are inconsistent.\n",
      "(5) a. Sylvania is to the north of Freedonia.\n",
      "b. Freedonia is a republic.\n",
      "10.1  Natural Language Understanding | 365...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 386, 'page_label': '365', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4762}\n",
      "\n",
      "--- Chunk 4763 ---\n",
      "Content:\n",
      "(6) a. The capital of Freedonia has a population of 9,000.\n",
      "b.\n",
      "No city in Freedonia has a population of 9,000.\n",
      "(7) a. Sylvania is to the north of Freedonia.\n",
      "b. Freedonia is to the north of Sylvania.\n",
      "We have chosen sentences about fictional countries (featured in the Marx Brothers’\n",
      "1933 movie Duck Soup) to emphasize that your ability to reason about these examples\n",
      "does not depend on what is true or false in the actual world. If you know the meaning\n",
      "of the word no, and also know that the capital of a country is a city in that country,\n",
      "then you should be able to conclude that the two sentences in (6) are inconsistent,\n",
      "regardless of where Freedonia is or what the population of its capital is. That is, there’s\n",
      "no possible situation in which both sentences could be true. Similarly, if you know that\n",
      "the relation expressed by to the north of  is asymmetric, then you should be able to\n",
      "conclude that the two sentences in (7) are inconsistent.\n",
      "Broadly speaking, logic-based approaches to natural language semantics focus on those\n",
      "aspects of natural language that guide our judgments of consistency and inconsistency.\n",
      "The syntax of a logical language is designed to make these features formally explicit.\n",
      "As a result, determining properties like consistency can often be reduced to symbolic\n",
      "manipulation, that is, to a task that can be carried out by a computer. In order to pursue\n",
      "this approach, we first want to develop a technique for representing a possible situation.\n",
      "We do this in terms of something that logicians call a “model.”\n",
      "Figure 10-1. Depiction of a situation in which Margrietje loves Brunoke.\n",
      "366 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 387, 'page_label': '366', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4763}\n",
      "\n",
      "--- Chunk 4764 ---\n",
      "Content:\n",
      "A model for a set W of sentences is a formal representation of a situation in which all\n",
      "the sentences in W are true. The usual way of representing models involves set theory.\n",
      "The domain D of discourse (all the entities we currently care about) is a set of individ-\n",
      "uals, while relations are treated as sets built up from D. Let’s look at a concrete example.\n",
      "Our domain D will consist of three children, Stefan, Klaus, and Evi, represented re-\n",
      "spectively as s, k, and e. We write this as D = {s, k, e}. The expression boy denotes\n",
      "the set consisting of Stefan and Klaus, the expression girl denotes the set consisting of\n",
      "Evi, and the expression is running denotes the set consisting of Stefan and Evi. Fig-\n",
      "ure 10-2 is a graphical rendering of the model.\n",
      "Figure 10-2. Diagram of a model containing a domain D and subsets of D corresponding to the\n",
      "predicates boy, girl, and is running.\n",
      "Later in this chapter we will use models to help evaluate the truth or falsity of English\n",
      "sentences, \n",
      "and in this way to illustrate some methods for representing meaning. How-\n",
      "ever, before going into more detail, let’s put the discussion into a broader perspective,\n",
      "and link back to a topic that we briefly raised in Section 1.5. Can a computer understand\n",
      "the meaning of a sentence? And how could we tell if it did? This is similar to asking\n",
      "“Can a computer think?” Alan Turing famously proposed to answer this by examining\n",
      "the ability of a computer to hold sensible conversations with a human (Turing, 1950).\n",
      "Suppose you are having a chat session with a person and a computer, but you are not\n",
      "told at the outset which is which. If you cannot identify which of your partners is the\n",
      "computer after chatting with each of them, then the computer has successfully imitated\n",
      "a human. If a computer succeeds in passing itself off as human in this “imitation game”\n",
      "(or “Turing Test” as it is popularly known), then according to Turing, we should be\n",
      "prepared to say that the computer can think and can be said to be intelligent. So Turing\n",
      "side-stepped the question of somehow examining the internal states of a computer by\n",
      "instead using its behavior as evidence of intelligence. By the same reasoning, we have\n",
      "assumed that in order to say that a computer understands English, it just needs to\n",
      "10.1  Natural Language Understanding | 367...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 388, 'page_label': '367', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4764}\n",
      "\n",
      "--- Chunk 4765 ---\n",
      "Content:\n",
      "behave as though it did. What is important here is not so much the specifics of Turing’s\n",
      "imitation \n",
      "game, but rather the proposal to judge a capacity for natural language un-\n",
      "derstanding in terms of observable behavior.\n",
      "10.2  Propositional Logic\n",
      "A logical language is designed to make reasoning formally explicit. As a result, it can\n",
      "capture aspects of natural language which determine whether a set of sentences is con-\n",
      "sistent. As part of this approach, we need to develop logical representations of a sen-\n",
      "tence φ that formally capture the truth-conditions of φ. We’ll start off with a simple\n",
      "example:\n",
      "(8) [Klaus chased Evi] and [Evi ran away].\n",
      "Let’s replace the two sub-sentences in (8) by φ and ψ respectively, and put & for the\n",
      "logical operator corresponding to the English word and: φ & ψ. This structure is the\n",
      "logical form of (8).\n",
      "Propositional logic allows us to represent just those parts of linguistic structure that\n",
      "correspond to certain sentential connectives. We have just looked at and. Other such\n",
      "connectives are not, or, and if..., then.... In the formalization of propositional logic, the\n",
      "counterparts of such connectives are sometimes called Boolean operators. The basic\n",
      "expressions of propositional logic are propositional symbols, often written as P, Q,\n",
      "R, etc. There are varying conventions for representing Boolean operators. Since we will\n",
      "be focusing on ways of exploring logic within NLTK, we will stick to the following\n",
      "ASCII versions of the operators:\n",
      ">>> nltk.boolean_ops()\n",
      "negation            -\n",
      "conjunction         &\n",
      "disjunction         |\n",
      "implication         ->\n",
      "equivalence         <->\n",
      "From the propositional symbols and the Boolean operators we can build an infinite set\n",
      "of well-formed formulas (or just formulas, for short) of propositional logic. First,\n",
      "every propositional letter is a formula. Then if φ is a formula, so is -φ. And if φ and\n",
      "ψ are formulas, then so are (φ & ψ), (φ | ψ), (φ -> ψ), and(φ <-> ψ).\n",
      "Table 10-2 specifies the truth-conditions for formulas containing these operators. As\n",
      "before we use φ and ψ as variables over sentences, and abbreviate if and only if as iff.\n",
      "Table 10-2. Truth conditions for the Boolean operators in propositional logic\n",
      "Boolean operator Truth conditions\n",
      "negation (it is not the case that ...) -φ is true in s iff φ is false in s\n",
      "conjunction (and) (φ & ψ) is true in s iff φ is true in s and ψ is true in s\n",
      "368 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 389, 'page_label': '368', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4765}\n",
      "\n",
      "--- Chunk 4766 ---\n",
      "Content:\n",
      "Boolean operator Truth conditions\n",
      "disjunction (or) (φ | ψ) is true in s iff φ is true in s or ψ is true in s\n",
      "implication (if ..., then ...) (φ -> ψ) is true in s iff φ is false in s or ψ is true in s\n",
      "equivalence (if and only if) (φ <-> ψ) is true in s iff φ and ψ are both true in s or both false in s\n",
      "These rules are generally straightforward, though the truth conditions for implication\n",
      "depart \n",
      "in many cases from our usual intuitions about the conditional in English. A\n",
      "formula of the form (P -> Q) is false only when P is true and Q is false. If P is false (say,\n",
      "P corresponds to The moon is made of green cheese) and Q is true (say, Q corresponds to\n",
      "Two plus two equals four), then P -> Q will come out true.\n",
      "NLTK’s LogicParser() parses logical expressions into various subclasses of Expression:\n",
      ">>> lp = nltk.LogicParser()\n",
      ">>> lp.parse('-(P & Q)')\n",
      "<NegatedExpression -(P & Q)>\n",
      ">>> lp.parse('P & Q')\n",
      "<AndExpression (P & Q)>\n",
      ">>> lp.parse('P | (R -> Q)')\n",
      "<OrExpression (P | (R -> Q))>\n",
      ">>> lp.parse('P <-> -- P')\n",
      "<IffExpression (P <-> --P)>\n",
      "From \n",
      "a computational perspective, logics give us an important tool for performing\n",
      "inference. Suppose you state that Freedonia is not to the north of Sylvania, and you\n",
      "give as your reasons that Sylvania is to the north of Freedonia. In this case, you have\n",
      "produced an argument. The sentence Sylvania is to the north of Freedonia  is the\n",
      "assumption of the argument, while Freedonia is not to the north of Sylvania  is the\n",
      "conclusion. The step of moving from one or more assumptions to a conclusion is called\n",
      "inference. Informally, it is common to write arguments in a format where the conclu-\n",
      "sion is preceded by therefore.\n",
      "(9) Sylvania is to the north of Freedonia.\n",
      "Therefore, Freedonia is not to the north of Sylvania.\n",
      "An argument is valid if there is no possible situation in which its premises are all true\n",
      "and its conclusion is not true.\n",
      "Now, the validity of (9) crucially depends on the meaning of the phrase to the north\n",
      "of, in particular, the fact that it is an asymmetric relation:\n",
      "(10) if x is to the north of y then y is not to the north of x.\n",
      "Unfortunately, we can’t express such rules in propositional logic: the smallest elements\n",
      "we have to play with are atomic propositions, and we cannot “look inside” these to\n",
      "talk about relations between individuals x and y. The best we can do in this case is\n",
      "capture a particular case of the asymmetry. Let’s use the propositional symbol SnF to\n",
      "10.2  Propositional Logic | 369...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 390, 'page_label': '369', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4766}\n",
      "\n",
      "--- Chunk 4767 ---\n",
      "Content:\n",
      "stand for Sylvania is to the north of Freedonia  and FnS for Freedonia is to the north of\n",
      "Sylvania. To say that Freedonia is not to the north of Sylvania , we write -FnS. That is,\n",
      "we treat not as equivalent to the phrase it is not the case that ..., and translate this as the\n",
      "one-place Boolean operator -. Replacing x and y in (10) by Sylvania and Freedonia\n",
      "respectively gives us an implication that can be written as:\n",
      "(11) SnF -> -FnS\n",
      "How about giving a version of the complete argument? We will replace the first sentence\n",
      "of (9) by two formulas of propositional logic: SnF, and also the implication in (11),\n",
      "which expresses (rather poorly) our background knowledge of the meaning of to the\n",
      "north of. We’ll write [A1, ..., An] / C to represent the argument that conclusion C\n",
      "follows from assumptions [A1, ..., An]. This leads to the following as a representation\n",
      "of argument (9):\n",
      "(12) [SnF, SnF -> -FnS] / -FnS\n",
      "This is a valid argument: if SnF and SnF -> -FnS are both true in a situation s, then\n",
      "-FnS must also be true in s. By contrast, if FnS were true, this would conflict with our\n",
      "understanding that two objects cannot both be to the north of each other in any possible\n",
      "situation. Equivalently, the list [SnF, SnF -> -FnS, FnS] is inconsistent—these sen-\n",
      "tences cannot all be true together.\n",
      "Arguments can be tested for “syntactic validity” by using a proof system. We will say\n",
      "a little bit more about this later on in Section 10.3. Logical proofs can be carried out\n",
      "with NLTK’s inference module, for example, via an interface to the third-party theo-\n",
      "rem prover Prover9. The inputs to the inference mechanism first have to be parsed into\n",
      "logical expressions by LogicParser().\n",
      ">>> lp = nltk.LogicParser()\n",
      ">>> SnF = lp.parse('SnF')\n",
      ">>> NotFnS = lp.parse('-FnS')\n",
      ">>> R = lp.parse('SnF -> -FnS')\n",
      ">>> prover = nltk.Prover9()\n",
      ">>> prover.prove(NotFnS, [SnF, R])\n",
      "True\n",
      "Here’s another way of seeing why the conclusion follows. SnF -> -FnS is semantically\n",
      "equivalent to -SnF | -FnS, where | is the two-place operator corresponding to or. In\n",
      "general, φ | ψ is true in a situation s if either φ is true in s or φ is true in s. Now, suppose\n",
      "both SnF and -SnF | -FnS are true in situation s. If SnF is true, then -SnF cannot also be\n",
      "true; a fundamental assumption of classical logic is that a sentence cannot be both true\n",
      "and false in a situation. Consequently, -FnS must be true.\n",
      "Recall that we interpret sentences of a logical language relative to a model, which is a\n",
      "very simplified version of the world. A model for propositional logic needs to assign\n",
      "the values True or False to every possible formula. We do this inductively: first, every\n",
      "propositional symbol is assigned a value, and then we compute the value of complex\n",
      "370 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 391, 'page_label': '370', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4767}\n",
      "\n",
      "--- Chunk 4768 ---\n",
      "Content:\n",
      "formulas by consulting the meanings of the Boolean operators (i.e., Table 10-2) and\n",
      "applying them to the values of the formula’s components. A Valuation is a mapping\n",
      "from basic symbols of the logic to their values. Here’s an example:\n",
      ">>> val = nltk.Valuation([('P', True), ('Q', True), ('R', False)])\n",
      "We initialize a Valuation with a list of pairs, each of which consists of a semantic symbol\n",
      "and a semantic value. The resulting object is essentially just a dictionary that maps\n",
      "logical symbols (treated as strings) to appropriate values.\n",
      ">>> val['P']\n",
      "True\n",
      "As we will see later, our models need to be somewhat more complicated in order to\n",
      "handle the more complex logical forms discussed in the next section; for the time being,\n",
      "just ignore the dom and g parameters in the following declarations.\n",
      ">>> dom = set([])\n",
      ">>> g = nltk.Assignment(dom)\n",
      "Now let’s initialize a model m that uses val:\n",
      ">>> m = nltk.Model(dom, val)\n",
      "Every model comes with an evaluate() method, which will determine the semantic\n",
      "value of logical expressions, such as formulas of propositional logic; of course, these\n",
      "values depend on the initial truth values we assigned to propositional symbols such as P,\n",
      "Q, and R.\n",
      ">>> print m.evaluate('(P & Q)', g)\n",
      "True\n",
      ">>> print m.evaluate('-(P & Q)', g)\n",
      "False\n",
      ">>> print m.evaluate('(P & R)', g)\n",
      "False\n",
      ">>> print m.evaluate('(P | R)', g)\n",
      "True\n",
      "Your Turn: Experiment with evaluating different formulas of proposi-\n",
      "tional logic. Does the model give the values that you expected?\n",
      "Up until now, we have been translating our English sentences into propositional logic.\n",
      "Because we are confined to representing atomic sentences with letters such as P and\n",
      "Q, we cannot dig into their internal structure. In effect, we are saying that there is no\n",
      "semantic benefit in dividing atomic sentences into subjects, objects, and predicates.\n",
      "However, this seems wrong: if we want to formalize arguments such as (9), we have to\n",
      "be able to “look inside” basic sentences. As a result, we will move beyond propositional\n",
      "logic to something more expressive, namely first-order logic. This is what we turn to\n",
      "in the next section.\n",
      "10.2  Propositional Logic | 371...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 392, 'page_label': '371', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4768}\n",
      "\n",
      "--- Chunk 4769 ---\n",
      "Content:\n",
      "10.3  First-Order Logic\n",
      "In \n",
      "the remainder of this chapter, we will represent the meaning of natural language\n",
      "expressions by translating them into first-order logic. Not all of natural language se-\n",
      "mantics can be expressed in first-order logic. But it is a good choice for computational\n",
      "semantics because it is expressive enough to represent many aspects of semantics, and\n",
      "on the other hand, there are excellent systems available off the shelf for carrying out\n",
      "automated inference in first-order logic.\n",
      "Our next step will be to describe how formulas of first-order logic are constructed, and\n",
      "then how such formulas can be evaluated in a model.\n",
      "Syntax\n",
      "First-order logic keeps all the Boolean operators of propositional logic, but it adds some\n",
      "important new mechanisms. To start with, propositions are analyzed into predicates\n",
      "and arguments, which takes us a step closer to the structure of natural languages. The\n",
      "standard construction rules for first-order logic recognize terms such as individual\n",
      "variables and individual constants, and predicates that take differing numbers of ar-\n",
      "guments. For example, Angus walks might be formalized as walk(angus) and Angus\n",
      "sees Bertie as see(angus, bertie). We will call walk a unary predicate, and see a binary\n",
      "predicate. The symbols used as predicates do not have intrinsic meaning, although it\n",
      "is hard to remember this. Returning to one of our earlier examples, there is no logical\n",
      "difference between (13a) and (13b).\n",
      "(13) a. love(margrietje, brunoke)\n",
      "b. houden_van(margrietje, brunoke)\n",
      "By itself, first-order logic has nothing substantive to say about lexical semantics—the\n",
      "meaning of individual words—although some theories of lexical semantics can be en-\n",
      "coded in first-order logic. Whether an atomic predication like see(angus, bertie) is true\n",
      "or false in a situation is not a matter of logic, but depends on the particular valuation\n",
      "that we have chosen for the constants see, angus, and bertie. For this reason, such\n",
      "expressions are called non-logical constants. By contrast, logical constants (such\n",
      "as the Boolean operators) always receive the same interpretation in every model for\n",
      "first-order logic.\n",
      "We should mention here that one binary predicate has special status, namely equality,\n",
      "as in formulas such as angus = aj. Equality is regarded as a logical constant, since for\n",
      "individual terms t1 and t2, the formula t1 = t2 is true if and only if t1 and t2 refer to one\n",
      "and the same entity.\n",
      "It is often helpful to inspect the syntactic structure of expressions of first-order logic,\n",
      "and the usual way of doing this is to assign types to expressions. Following the tradition\n",
      "of Montague grammar, we will use two basic types: e is the type of entities, while t is\n",
      "the type of formulas, i.e., expressions that have truth values. Given these two basic\n",
      "372 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 393, 'page_label': '372', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4769}\n",
      "\n",
      "--- Chunk 4770 ---\n",
      "Content:\n",
      "types, we can form complex types for function expressions. That is, given any types\n",
      "σ and τ, 〈σ, τ〉 is a complex type corresponding to functions from 'σ things’ to 'τ things’.\n",
      "For example, 〈e, t〉 is the type of expressions from entities to truth values, namely unary\n",
      "predicates. The LogicParser can be invoked so that it carries out type checking.\n",
      ">>> tlp = nltk.LogicParser(type_check=True)\n",
      ">>> parsed = tlp.parse('walk(angus)')\n",
      ">>> parsed.argument\n",
      "<ConstantExpression angus>\n",
      ">>> parsed.argument.type\n",
      "e\n",
      ">>> parsed.function\n",
      "<ConstantExpression walk>\n",
      ">>> parsed.function.type\n",
      "<e,?>\n",
      "Why do we see <e,?> at the end of this example? Although the type-checker will try to\n",
      "infer as many types as possible, in this case it has not managed to fully specify the type\n",
      "of walk, since its result type is unknown. Although we are intending walk to receive type\n",
      "<e, t>, as far as the type-checker knows, in this context it could be of some other type,\n",
      "such as <e, e> or <e, <e, t>. To help the type-checker, we need to specify a signa-\n",
      "ture, implemented as a dictionary that explicitly associates types with non-logical con-\n",
      "stants:\n",
      ">>> sig = {'walk': '<e, t>'}\n",
      ">>> parsed = tlp.parse('walk(angus)', sig)\n",
      ">>> parsed.function.type\n",
      "<e,t>\n",
      "A binary predicate has type 〈e, 〈e, t〉〉. Although this is the type of something which\n",
      "combines first with an argument of type e to make a unary predicate, we represent\n",
      "binary predicates as combining directly with their two arguments. For example, the\n",
      "predicate see in the translation of Angus sees Cyril will combine with its arguments to\n",
      "give the result see(angus, cyril).\n",
      "In first-order logic, arguments of predicates can also be individual variables such as x,\n",
      "y, and z. In NLTK, we adopt the convention that variables of type e are all lowercase.\n",
      "Individual variables are similar to personal pronouns like he, she, and it, in that we need\n",
      "to know about the context of use in order to figure out their denotation. One way of\n",
      "interpreting the pronoun in (14) is by pointing to a relevant individual in the local\n",
      "context.\n",
      "(14) He disappeared.\n",
      "Another way is to supply a textual antecedent for the pronoun he, for example, by\n",
      "uttering (15a) prior to (14). Here, we say that he is coreferential with the noun phrase\n",
      "Cyril. In such a context, (14) is semantically equivalent to (15b).\n",
      "(15) a. Cyril is Angus’s dog.\n",
      "b. Cyril disappeared.\n",
      "10.3  First-Order Logic | 373...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 394, 'page_label': '373', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4770}\n",
      "\n",
      "--- Chunk 4771 ---\n",
      "Content:\n",
      "Consider by contrast the occurrence of he in (16a). In this case, it is bound by the\n",
      "indefinite NP a dog, and this is a different relationship than coreference. If we replace\n",
      "the pronoun he by a dog, the result (16b) is not semantically equivalent to (16a).\n",
      "(16) a. Angus had a dog but he disappeared.\n",
      "b. Angus had a dog but a dog disappeared.\n",
      "Corresponding to (17a), we can construct an open formula (17b) with two occurrences\n",
      "of the variable x. (We ignore tense to simplify exposition.)\n",
      "(17) a. He is a dog and he disappeared.\n",
      "b. dog(x) & disappear(x)\n",
      "By placing an existential quantifier ∃x (“for some x”) in front of (17b), we can\n",
      "bind these variables, as in (18a), which means (18b) or, more idiomatically, (18c).\n",
      "(18) a. ∃x.(dog(x) & disappear(x))\n",
      "b. At least one entity is a dog and disappeared.\n",
      "c. A dog disappeared.\n",
      "Here is the NLTK counterpart of (18a):\n",
      "(19) exists x.(dog(x) & disappear(x))\n",
      "In addition to the existential quantifier, first-order logic offers us the universal quan-\n",
      "tifier ∀x (“for all x”), illustrated in (20).\n",
      "(20) a. ∀x.(dog(x) → disappear(x))\n",
      "b. Everything has the property that if it is a dog, it disappears.\n",
      "c. Every dog disappeared.\n",
      "Here is the NLTK counterpart of (20a):\n",
      "(21) all x.(dog(x) -> disappear(x))\n",
      "Although (20a) is the standard first-order logic translation of (20c), the truth conditions\n",
      "aren’t necessarily what you expect. The formula says that if some x is a dog, then x\n",
      "disappears—but it doesn’t say that there are any dogs. So in a situation where there are\n",
      "no dogs, (20a) will still come out true. (Remember that (P -> Q) is true when P is false.)\n",
      "Now you might argue that every dog disappeared does presuppose the existence of dogs,\n",
      "and that the logic formalization is simply wrong. But it is possible to find other examples\n",
      "that lack such a presupposition. For instance, we might explain that the value of the\n",
      "Python expression astring.replace('ate', '8') is the result of replacing every occur-\n",
      "rence of 'ate' in astring by '8', even though there may in fact be no such occurrences\n",
      "(Table 3-2).\n",
      "374 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 395, 'page_label': '374', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4771}\n",
      "\n",
      "--- Chunk 4772 ---\n",
      "Content:\n",
      "We have seen a number of examples where variables are bound by quantifiers. What\n",
      "happens in formulas such as the following?\n",
      "((exists x. dog(x)) -> bark(x))\n",
      "The \n",
      "scope of the exists x quantifier is dog(x), so the occurrence of x in bark(x) is\n",
      "unbound. Consequently it can become bound by some other quantifier, for example,\n",
      "all x in the next formula:\n",
      "all x.((exists x. dog(x)) -> bark(x))\n",
      "In general, an occurrence of a variable x in a formula φ is free in φ if that occurrence\n",
      "doesn’t fall within the scope of all x or some x in φ. Conversely, if x is free in formula\n",
      "φ, then it is bound in all x.φ and exists x.φ. If all variable occurrences in a formula\n",
      "are bound, the formula is said to be closed.\n",
      "We mentioned before that the parse() method of NLTK’s LogicParser returns objects\n",
      "of class Expression. Each instance expr of this class comes with a method free(), which\n",
      "returns the set of variables that are free in expr.\n",
      ">>> lp = nltk.LogicParser()\n",
      ">>> lp.parse('dog(cyril)').free()\n",
      "set([])\n",
      ">>> lp.parse('dog(x)').free()\n",
      "set([Variable('x')])\n",
      ">>> lp.parse('own(angus, cyril)').free()\n",
      "set([])\n",
      ">>> lp.parse('exists x.dog(x)').free()\n",
      "set([])\n",
      ">>> lp.parse('((some x. walk(x)) -> sing(x))').free()\n",
      "set([Variable('x')])\n",
      ">>> lp.parse('exists x.own(y, x)').free()\n",
      "set([Variable('y')])\n",
      "First-Order Theorem Proving\n",
      "Recall the constraint on to the north of, which we proposed earlier as (10):\n",
      "(22) if x is to the north of y then y is not to the north of x.\n",
      "We observed that propositional logic is not expressive enough to represent generali-\n",
      "zations about binary predicates, and as a result we did not properly capture the argu-\n",
      "ment Sylvania is to the north of Freedonia. Therefore, Freedonia is not to the north of\n",
      "Sylvania.\n",
      "You have no doubt realized that first-order logic, by contrast, is ideal for formalizing\n",
      "such rules:\n",
      "all x. all y.(north_of(x, y) -> -north_of(y, x))\n",
      "Even better, we can perform automated inference to show the validity of the argument.\n",
      "10.3  First-Order Logic | 375...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 396, 'page_label': '375', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4772}\n",
      "\n",
      "--- Chunk 4773 ---\n",
      "Content:\n",
      "The general case in theorem proving is to determine whether a formula that we want\n",
      "to \n",
      "prove (a proof goal) can be derived by a finite sequence of inference steps from a\n",
      "list of assumed formulas. We write this as A ⊢ g, where A is a (possibly empty) list of\n",
      "assumptions, and g is a proof goal. We will illustrate this with NLTK’s interface to the\n",
      "theorem prover Prover9. First, we parse the required proof goal \n",
      "  and the two as-\n",
      "sumptions \n",
      "  \n",
      " . Then we create a Prover9 instance \n",
      " , and call its prove() method on\n",
      "the goal, given the list of assumptions \n",
      " .\n",
      ">>> NotFnS = lp.parse('-north_of(f, s)')  \n",
      ">>> SnF = lp.parse('north_of(s, f)')    \n",
      ">>> R = lp.parse('all x. all y. (north_of(x, y) -> -north_of(y, x))')  \n",
      ">>> prover = nltk.Prover9()   \n",
      ">>> prover.prove(NotFnS, [SnF, R])  \n",
      "True\n",
      "Happily, \n",
      "the theorem prover agrees with us that the argument is valid. By contrast, it\n",
      "concludes that it is not possible to infer north_of(f, s) from our assumptions:\n",
      ">>> FnS = lp.parse('north_of(f, s)')\n",
      ">>> prover.prove(FnS, [SnF, R])\n",
      "False\n",
      "Summarizing the Language of First-Order Logic\n",
      "We’ll take this opportunity to restate our earlier syntactic rules for propositional logic\n",
      "and add the formation rules for quantifiers; together, these give us the syntax of first-\n",
      "order logic. In addition, we make explicit the types of the expressions involved. We’ll\n",
      "adopt the convention that 〈en, t〉 is the type of a predicate that combines with n argu-\n",
      "ments of type e to yield an expression of type t. In this case, we say that n is the arity\n",
      "of the predicate.\n",
      "1. If P is a predicate of type 〈en, t〉, and α1, ... αn are terms of type e, then\n",
      "P(α1, ... αn) is of type t.\n",
      "2. If α and β are both of type e, then (α = β) and (α != β) are of type t.\n",
      "3. If φ is of type t, then so is -φ.\n",
      "4. If φ and ψ are of type t, then so are (φ & ψ), (φ | ψ), (φ -> ψ), and (φ <-> ψ).\n",
      "5. If φ is of type t, and x is a variable of type e, then exists x.φ and all x.φ are of\n",
      "type t.\n",
      "Table 10-3 summarizes the new logical constants of the logic module, and two of the\n",
      "methods of Expressions.\n",
      "376 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 397, 'page_label': '376', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4773}\n",
      "\n",
      "--- Chunk 4774 ---\n",
      "Content:\n",
      "Table 10-3. Summary of new logical relations and operators required for first-order logic\n",
      "Example Description\n",
      "= Equality\n",
      "!= Inequality\n",
      "exists Existential quantifier\n",
      "all Universal quantifier\n",
      "Truth in Model\n",
      "We \n",
      "have looked at the syntax of first-order logic, and in Section 10.4 we will examine\n",
      "the task of translating English into first-order logic. Yet as we argued in Section 10.1,\n",
      "this gets us further forward only if we can give a meaning to sentences of first-order\n",
      "logic. In other words, we need to give a truth-conditional semantics to first-order logic.\n",
      "From the point of view of computational semantics, there are obvious limits to how far\n",
      "one can push this approach. Although we want to talk about sentences being true or\n",
      "false in situations, we only have the means of representing situations in the computer\n",
      "in a symbolic manner. Despite this limitation, it is still possible to gain a clearer picture\n",
      "of truth-conditional semantics by encoding models in NLTK.\n",
      "Given a first-order logic language L, a model M for L is a pair 〈D, Val〉, where D is an\n",
      "non-empty set called the domain of the model, and Val is a function called the valu-\n",
      "ation function, which assigns values from D to expressions of L as follows:\n",
      "1. For every individual constant c in L, Val(c) is an element of D.\n",
      "2. For every predicate symbol P of arity n ≥ 0, Val(P) is a function from Dn to\n",
      "{True, False}. (If the arity of P is 0, then Val(P) is simply a truth value, and P is\n",
      "regarded as a propositional symbol.)\n",
      "According to 2, if P is of arity 2, then Val(P) will be a function f from pairs of elements\n",
      "of D to {True, False}. In the models we shall build in NLTK, we’ll adopt a more con-\n",
      "venient alternative, in which Val(P) is a set S of pairs, defined as follows:\n",
      "(23) S = {s | f(s) = True}\n",
      "Such an f is called the characteristic function of S (as discussed in the further\n",
      "readings).\n",
      "Relations are represented semantically in NLTK in the standard set-theoretic way: as\n",
      "sets of tuples. For example, let’s suppose we have a domain of discourse consisting of\n",
      "the individuals Bertie, Olive, and Cyril, where Bertie is a boy, Olive is a girl, and Cyril\n",
      "is a dog. For mnemonic reasons, we use b, o, and c as the corresponding labels in the\n",
      "model. We can declare the domain as follows:\n",
      ">>> dom = set(['b', 'o', 'c'])\n",
      "10.3  First-Order Logic | 377...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 398, 'page_label': '377', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4774}\n",
      "\n",
      "--- Chunk 4775 ---\n",
      "Content:\n",
      "We will use the utility function parse_valuation() to convert a sequence of strings of\n",
      "the form symbol => value into a Valuation object.\n",
      ">>> v = \"\"\"\n",
      "... bertie => b\n",
      "... olive => o\n",
      "... cyril => c\n",
      "... boy => {b}\n",
      "... girl => {o}\n",
      "... dog => {c}\n",
      "... walk => {o, c}\n",
      "... see => {(b, o), (c, b), (o, c)}\n",
      "... \"\"\"\n",
      ">>> val = nltk.parse_valuation(v)\n",
      ">>> print val\n",
      "{'bertie': 'b',\n",
      " 'boy': set([('b',)]),\n",
      " 'cyril': 'c',\n",
      " 'dog': set([('c',)]),\n",
      " 'girl': set([('o',)]),\n",
      " 'olive': 'o',\n",
      " 'see': set([('o', 'c'), ('c', 'b'), ('b', 'o')]),\n",
      " 'walk': set([('c',), ('o',)])}\n",
      "So according to this valuation, the value of see is a set of tuples such that Bertie sees\n",
      "Olive, Cyril sees Bertie, and Olive sees Cyril.\n",
      "Your Turn: Draw a picture of the domain dom and the sets correspond-\n",
      "ing to each of the unary predicates, by analogy with the diagram shown\n",
      "in Figure 10-2.\n",
      "You may have noticed that our unary predicates (i.e, boy, girl, dog) also come out as\n",
      "sets of singleton tuples, rather than just sets of individuals. This is a convenience which\n",
      "allows us to have a uniform treatment of relations of any arity. A predication of the\n",
      "form P(τ1, ... τn), where P is of arity n, comes out true just in case the tuple of values\n",
      "corresponding to (τ1, ... τn) belongs to the set of tuples in the value of P.\n",
      ">>> ('o', 'c') in val['see']\n",
      "True\n",
      ">>> ('b',) in val['boy']\n",
      "True\n",
      "Individual Variables and Assignments\n",
      "In our models, the counterpart of a context of use is a variable assignment. This is a\n",
      "mapping from individual variables to entities in the domain. Assignments are created\n",
      "using the Assignment constructor, which also takes the model’s domain of discourse as\n",
      "a parameter. We are not required to actually enter any bindings, but if we do, they are\n",
      "in a (variable, value) format similar to what we saw earlier for valuations.\n",
      "378 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 399, 'page_label': '378', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4775}\n",
      "\n",
      "--- Chunk 4776 ---\n",
      "Content:\n",
      ">>> g = nltk.Assignment(dom, [('x', 'o'), ('y', 'c')])\n",
      ">>> g\n",
      "{'y': 'c', 'x': 'o'}\n",
      "In \n",
      "addition, there is a print() format for assignments which uses a notation closer to\n",
      "that often found in logic textbooks:\n",
      ">>> print g\n",
      "g[c/y][o/x]\n",
      "Let’s now look at how we can evaluate an atomic formula of first-order logic. First, we\n",
      "create a model, and then we call the evaluate() method to compute the truth value:\n",
      ">>> m = nltk.Model(dom, val)\n",
      ">>> m.evaluate('see(olive, y)', g)\n",
      "True\n",
      "What’s happening here? We are evaluating a formula which is similar to our earlier\n",
      "example, see(olive, cyril). However, when the interpretation function encounters\n",
      "the variable y, rather than checking for a value in val, it asks the variable assignment\n",
      "g to come up with a value:\n",
      ">>> g['y']\n",
      "'c'\n",
      "Since we already know that individuals o and c stand in the see relation, the value\n",
      "True is what we expected. In this case, we can say that assignment g satisfies the for-\n",
      "mula see(olive, y). By contrast, the following formula evaluates to False relative to\n",
      "g (check that you see why this is).\n",
      ">>> m.evaluate('see(y, x)', g)\n",
      "False\n",
      "In our approach (though not in standard first-order logic), variable assignments are\n",
      "partial. For example, g says nothing about any variables apart from x and y. The method\n",
      "purge() clears all bindings from an assignment.\n",
      ">>> g.purge()\n",
      ">>> g\n",
      "{}\n",
      "If we now try to evaluate a formula such as see(olive, y) relative to g, it is like trying\n",
      "to interpret a sentence containing a him when we don’t know what him refers to. In\n",
      "this case, the evaluation function fails to deliver a truth value.\n",
      ">>> m.evaluate('see(olive, y)', g)\n",
      "'Undefined'\n",
      "Since our models already contain rules for interpreting Boolean operators, arbitrarily\n",
      "complex formulas can be composed and evaluated.\n",
      ">>> m.evaluate('see(bertie, olive) & boy(bertie) & -walk(bertie)', g)\n",
      "True\n",
      "The general process of determining truth or falsity of a formula in a model is called\n",
      "model checking.\n",
      "10.3  First-Order Logic | 379...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 400, 'page_label': '379', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4776}\n",
      "\n",
      "--- Chunk 4777 ---\n",
      "Content:\n",
      "Quantification\n",
      "One \n",
      "of the crucial insights of modern logic is that the notion of variable satisfaction\n",
      "can be used to provide an interpretation for quantified formulas. Let’s use (24) as an\n",
      "example.\n",
      "(24) exists x.(girl(x) & walk(x))\n",
      "When is it true? Let’s think about all the individuals in our domain, i.e., in dom. We\n",
      "want to check whether any of these individuals has the property of being a girl and\n",
      "walking. In other words, we want to know if there is some u in dom such that g[u/x]\n",
      "satisfies the open formula (25).\n",
      "(25) girl(x) & walk(x)\n",
      "Consider the following:\n",
      ">>> m.evaluate('exists x.(girl(x) & walk(x))', g)\n",
      "True\n",
      "evaluate() returns True here because there is some u in dom such that (25) is satisfied\n",
      "by an assignment which binds x to u. In fact, o is such a u:\n",
      ">>> m.evaluate('girl(x) & walk(x)', g.add('x', 'o'))\n",
      "True\n",
      "One useful tool offered by NLTK is the satisfiers() method. This returns a set of all\n",
      "the individuals that satisfy an open formula. The method parameters are a parsed for-\n",
      "mula, a variable, and an assignment. Here are a few examples:\n",
      ">>> fmla1 = lp.parse('girl(x) | boy(x)')\n",
      ">>> m.satisfiers(fmla1, 'x', g)\n",
      "set(['b', 'o'])\n",
      ">>> fmla2 = lp.parse('girl(x) -> walk(x)')\n",
      ">>> m.satisfiers(fmla2, 'x', g)\n",
      "set(['c', 'b', 'o'])\n",
      ">>> fmla3 = lp.parse('walk(x) -> girl(x)')\n",
      ">>> m.satisfiers(fmla3, 'x', g)\n",
      "set(['b', 'o'])\n",
      "It’s useful to think about why fmla2 and fmla3 receive the values they do. The truth\n",
      "conditions for -> mean that fmla2 is equivalent to -girl(x) | walk(x), which is satisfied\n",
      "by something that either isn’t a girl or walks. Since neither b (Bertie) nor c (Cyril) are\n",
      "girls, according to model m, they both satisfy the whole formula. And of course o satisfies\n",
      "the formula because o satisfies both disjuncts. Now, since every member of the domain\n",
      "of discourse satisfies fmla2, the corresponding universally quantified formula is also\n",
      "true.\n",
      ">>> m.evaluate('all x.(girl(x) -> walk(x))', g)\n",
      "True\n",
      "380 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 401, 'page_label': '380', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4777}\n",
      "\n",
      "--- Chunk 4778 ---\n",
      "Content:\n",
      "In other words, a universally quantified formula ∀x.φ is true with respect to g just in\n",
      "case for every u, φ is true with respect to g[u/x].\n",
      "Your Turn: Try to figure out, first with pencil and paper, and then using\n",
      "m.evaluate(), what the truth values are for all x.(girl(x) &\n",
      "walk(x)) and exists x.(boy(x) -> walk(x)). Make sure you understand\n",
      "why they receive these values.\n",
      "Quantifier Scope Ambiguity\n",
      "What happens when we want to give a formal representation of a sentence with two\n",
      "quantifiers, such as the following?\n",
      "(26) Everybody admires someone.\n",
      "There are (at least) two ways of expressing (26) in first-order logic:\n",
      "(27) a. all x.(person(x) -> exists y.(person(y) & admire(x,y)))\n",
      "b. exists y.(person(y) & all x.(person(x) -> admire(x,y)))\n",
      "Can we use both of these? The answer is yes, but they have different meanings. (27b)\n",
      "is logically stronger than (27a): it claims that there is a unique person, say, Bruce, who\n",
      "is admired by everyone. (27a), on the other hand, just requires that for every person\n",
      "u, we can find some person u' whom u admires; but this could be a different person\n",
      "u' in each case. We distinguish between (27a) and (27b) in terms of the scope of the\n",
      "quantifiers. In the first, ∀ has wider scope than ∃, whereas in (27b), the scope ordering\n",
      "is reversed. So now we have two ways of representing the meaning of (26), and they\n",
      "are both quite legitimate. In other words, we are claiming that (26) is ambiguous with\n",
      "respect to quantifier scope, and the formulas in (27) give us a way to make the two\n",
      "readings explicit. However, we are not just interested in associating two distinct rep-\n",
      "resentations with (26); we also want to show in detail how the two representations lead\n",
      "to different conditions for truth in a model.\n",
      "In order to examine the ambiguity more closely, let’s fix our valuation as follows:\n",
      ">>> v2 = \"\"\"\n",
      "... bruce => b\n",
      "... cyril => c\n",
      "... elspeth => e\n",
      "... julia => j\n",
      "... matthew => m\n",
      "... person => {b, e, j, m}\n",
      "... admire => {(j, b), (b, b), (m, e), (e, m), (c, a)}\n",
      "... \"\"\"\n",
      ">>> val2 = nltk.parse_valuation(v2)\n",
      "The admire relation can be visualized using the mapping diagram shown in (28).\n",
      "10.3  First-Order Logic | 381...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 402, 'page_label': '381', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4778}\n",
      "\n",
      "--- Chunk 4779 ---\n",
      "Content:\n",
      "(28)\n",
      "In (28), an arrow between two individuals x and y indicates that x admires y. So j and\n",
      "b both admire b (Bruce is very vain), while e admires m and m admires e. In this model,\n",
      "formula (27a) is true but (27b) is false. One way of exploring these results is by using\n",
      "the satisfiers() method of Model objects.\n",
      ">>> dom2 = val2.domain\n",
      ">>> m2 = nltk.Model(dom2, val2)\n",
      ">>> g2 = nltk.Assignment(dom2)\n",
      ">>> fmla4 = lp.parse('(person(x) -> exists y.(person(y) & admire(x, y)))')\n",
      ">>> m2.satisfiers(fmla4, 'x', g2)\n",
      "set(['a', 'c', 'b', 'e', 'j', 'm'])\n",
      "This shows that fmla4 holds of every individual in the domain. By contrast, consider\n",
      "the formula fmla5; this has no satisfiers for the variable y.\n",
      ">>> fmla5 = lp.parse('(person(y) & all x.(person(x) -> admire(x, y)))')\n",
      ">>> m2.satisfiers(fmla5, 'y', g2)\n",
      "set([])\n",
      "That is, there is no person that is admired by everybody. Taking a different open for-\n",
      "mula, fmla6, we can verify that there is a person, namely Bruce, who is admired by both\n",
      "Julia and Bruce.\n",
      ">>> fmla6 = lp.parse('(person(y) & all x.((x = bruce | x = julia) -> admire(x, y)))')\n",
      ">>> m2.satisfiers(fmla6, 'y', g2)\n",
      "set(['b'])\n",
      "Your Turn: Devise a new model based on m2 such that (27a) comes out\n",
      "false in your model; similarly, devise a new model such that (27b) comes\n",
      "out true.\n",
      "382 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 403, 'page_label': '382', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4779}\n",
      "\n",
      "--- Chunk 4780 ---\n",
      "Content:\n",
      "Model Building\n",
      "We \n",
      "have been assuming that we already had a model, and wanted to check the truth\n",
      "of a sentence in the model. By contrast, model building tries to create a new model,\n",
      "given some set of sentences. If it succeeds, then we know that the set is consistent, since\n",
      "we have an existence proof of the model.\n",
      "We invoke the Mace4 model builder by creating an instance of Mace() and calling its\n",
      "build_model() method, in an analogous way to calling the Prover9 theorem prover. One\n",
      "option is to treat our candidate set of sentences as assumptions, while leaving the goal\n",
      "unspecified. The following interaction shows how both [a, c1] and [a, c2] are con-\n",
      "sistent lists, since Mace succeeds in building a model for each of them, whereas [c1,\n",
      "c2] is inconsistent.\n",
      ">>> a3 = lp.parse('exists x.(man(x) & walks(x))')\n",
      ">>> c1 = lp.parse('mortal(socrates)')\n",
      ">>> c2 = lp.parse('-mortal(socrates)')\n",
      ">>> mb = nltk.Mace(5)\n",
      ">>> print mb.build_model(None, [a3, c1])\n",
      "True\n",
      ">>> print mb.build_model(None, [a3, c2])\n",
      "True\n",
      ">>> print mb.build_model(None, [c1, c2])\n",
      "False\n",
      "We can also use the model builder as an adjunct to the theorem prover. Let’s suppose\n",
      "we are trying to prove A ⊢ g, i.e., that g is logically derivable from assumptions A = [a1,\n",
      "a2, ..., an]. We can feed this same input to Mace4, and the model builder will try to\n",
      "find a counterexample, that is, to show that g does not follow from A. So, given this\n",
      "input, Mace4 will try to find a model for the assumptions A together with the negation\n",
      "of g, namely the list A' = [a1, a2, ..., an, -g]. If g fails to follow from S, then Mace4\n",
      "may well return with a counterexample faster than Prover9 concludes that it cannot\n",
      "find the required proof. Conversely, if g is provable from S, Mace4 may take a long time\n",
      "unsuccessfully trying to find a countermodel, and will eventually give up.\n",
      "Let’s consider a concrete scenario. Our assumptions are the list [There is a woman that\n",
      "every man loves, Adam is a man, Eve is a woman]. Our conclusion is Adam loves Eve.\n",
      "Can Mace4 find a model in which the premises are true but the conclusion is false? In\n",
      "the following code, we use MaceCommand(), which will let us inspect the model that has\n",
      "been built.\n",
      ">>> a4 = lp.parse('exists y. (woman(y) & all x. (man(x) -> love(x,y)))')\n",
      ">>> a5 = lp.parse('man(adam)')\n",
      ">>> a6 = lp.parse('woman(eve)')\n",
      ">>> g = lp.parse('love(adam,eve)')\n",
      ">>> mc = nltk.MaceCommand(g, assumptions=[a4, a5, a6])\n",
      ">>> mc.build_model()\n",
      "True\n",
      "10.3  First-Order Logic | 383...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 404, 'page_label': '383', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4780}\n",
      "\n",
      "--- Chunk 4781 ---\n",
      "Content:\n",
      "So the answer is yes: Mace4 found a countermodel in which there is some woman other\n",
      "than \n",
      "Eve that Adam loves. But let’s have a closer look at Mace4’s model, converted to\n",
      "the format we use for valuations:\n",
      ">>> print mc.valuation\n",
      "{'C1': 'b',\n",
      " 'adam': 'a',\n",
      " 'eve': 'a',\n",
      " 'love': set([('a', 'b')]),\n",
      " 'man': set([('a',)]),\n",
      " 'woman': set([('a',), ('b',)])}\n",
      "The general form of this valuation should be familiar to you: it contains some individual\n",
      "constants and predicates, each with an appropriate kind of value. What might be puz-\n",
      "zling is the C1. This is a “Skolem constant” that the model builder introduces as a\n",
      "representative of the existential quantifier. That is, when the model builder encoun-\n",
      "tered the exists y part of a4, it knew that there is some individual b in the domain\n",
      "which satisfies the open formula in the body of a4. However, it doesn’t know whether\n",
      "b is also the denotation of an individual constant anywhere else in its input, so it makes\n",
      "up a new name for b on the fly, namely C1. Now, since our premises said nothing about\n",
      "the individual constants adam and eve, the model builder has decided there is no reason\n",
      "to treat them as denoting different entities, and they both get mapped to a. Moreover,\n",
      "we didn’t specify that man and woman denote disjoint sets, so the model builder lets their\n",
      "denotations overlap. This illustrates quite dramatically the implicit knowledge that we\n",
      "bring to bear in interpreting our scenario, but which the model builder knows nothing\n",
      "about. So let's add a new assumption which makes the sets of men and women disjoint.\n",
      "The model builder still produces a countermodel, but this time it is more in accord with\n",
      "our intuitions about the situation:\n",
      ">>> a7 = lp.parse('all x. (man(x) -> -woman(x))')\n",
      ">>> g = lp.parse('love(adam,eve)')\n",
      ">>> mc = nltk.MaceCommand(g, assumptions=[a4, a5, a6, a7])\n",
      ">>> mc.build_model()\n",
      "True\n",
      ">>> print mc.valuation\n",
      "{'C1': 'c',\n",
      " 'adam': 'a',\n",
      " 'eve': 'b',\n",
      " 'love': set([('a', 'c')]),\n",
      " 'man': set([('a',)]),\n",
      " 'woman': set([('b',), ('c',)])}\n",
      "On reflection, we can see that there is nothing in our premises which says that Eve is\n",
      "the only woman in the domain of discourse, so the countermodel in fact is acceptable.\n",
      "If we wanted to rule it out, we would have to add a further assumption such as exists\n",
      "y. all x. (woman(x) -> (x = y)) to ensure that there is only one woman in the model.\n",
      "384 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 405, 'page_label': '384', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4781}\n",
      "\n",
      "--- Chunk 4782 ---\n",
      "Content:\n",
      "10.4  The Semantics of English Sentences\n",
      "Compositional Semantics in Feature-Based Grammar\n",
      "At \n",
      "the beginning of the chapter we briefly illustrated a method of building semantic\n",
      "representations on the basis of a syntactic parse, using the grammar framework devel-\n",
      "oped in Chapter 9. This time, rather than constructing an SQL query, we will build a\n",
      "logical form. One of our guiding ideas for designing such grammars is the Principle of\n",
      "Compositionality. (Also known as Frege’s Principle; see [Partee, 1995] for the for-\n",
      "mulation given.)\n",
      "Principle of Compositionality: the meaning of a whole is a function of the meanings\n",
      "of the parts and of the way they are syntactically combined.\n",
      "We will assume that the semantically relevant parts of a complex expression are given\n",
      "by a theory of syntactic analysis. Within this chapter, we will take it for granted that\n",
      "expressions are parsed against a context-free grammar. However, this is not entailed\n",
      "by the Principle of Compositionality.\n",
      "Our goal now is to integrate the construction of a semantic representation in a manner\n",
      "that can be smoothly with the process of parsing. (29) illustrates a first approximation\n",
      "to the kind of analyses we would like to build.\n",
      "(29)\n",
      "In (29), the SEM value at the root node shows a semantic representation for the whole\n",
      "sentence, while the SEM values at lower nodes show semantic representations for con-\n",
      "stituents of the sentence. Since the values of SEM have to be treated in a special manner,\n",
      "they are distinguished from other feature values by being enclosed in angle brackets.\n",
      "So far, so good, but how do we write grammar rules that will give us this kind of result?\n",
      "Our approach will be similar to that adopted for the grammar sql0.fcfg at the start of\n",
      "this chapter, in that we will assign semantic representations to lexical nodes, and then\n",
      "compose the semantic representations for each phrase from those of its child nodes.\n",
      "However, in the present case we will use function application rather than string con-\n",
      "catenation as the mode of composition. To be more specific, suppose we have NP and\n",
      "VP constituents with appropriate values for their SEM nodes. Then the SEM value of an\n",
      "S is handled by a rule like (30). (Observe that in the case where the value of SEM is a\n",
      "variable, we omit the angle brackets.)\n",
      "10.4  The Semantics of English Sentences | 385...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 406, 'page_label': '385', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4782}\n",
      "\n",
      "--- Chunk 4783 ---\n",
      "Content:\n",
      "(30) S[SEM=<?vp(?np)>] -> NP[SEM=?subj] VP[SEM=?vp]\n",
      "(30) tells us that given some SEM value ?subj for the subject NP and some SEM value ?vp\n",
      "for \n",
      "the VP, the SEM value of the S parent is constructed by applying ?vp as a function\n",
      "expression to ?np. From this, we can conclude that ?vp has to denote a function which\n",
      "has the denotation of ?np in its domain. (30) is a nice example of building semantics\n",
      "using the principle of compositionality.\n",
      "To complete the grammar is very straightforward; all we require are the rules shown\n",
      "here:\n",
      "VP[SEM=?v] -> IV[SEM=?v]\n",
      "NP[SEM=<cyril>] -> 'Cyril'\n",
      "IV[SEM=<\\x.bark(x)>] -> 'barks'\n",
      "The VP rule says that the parent’s semantics is the same as the head child’s semantics.\n",
      "The two lexical rules provide non-logical constants to serve as the semantic values of\n",
      "Cyril and barks respectively. There is an additional piece of notation in the entry for\n",
      "barks which we will explain shortly.\n",
      "Before launching into compositional semantic rules in more detail, we need to add a\n",
      "new tool to our kit, namely the λ-calculus. This provides us with an invaluable tool for\n",
      "combining expressions of first-order logic as we assemble a meaning representation for\n",
      "an English sentence.\n",
      "The λ-Calculus\n",
      "In Section 1.3, we pointed out that mathematical set notation was a helpful method of\n",
      "specifying properties P of words that we wanted to select from a document. We illus-\n",
      "trated this with (31), which we glossed as “the set of all w such that w is an element of\n",
      "V (the vocabulary) and w has property P”.\n",
      "(31) {w | w ∈ V & P(w)}\n",
      "It turns out to be extremely useful to add something to first-order logic that will achieve\n",
      "the same effect. We do this with the λ-operator (pronounced “lambda”). The λ coun-\n",
      "terpart to (31) is (32). (Since we are not trying to do set theory here, we just treat V as\n",
      "a unary predicate.)\n",
      "(32) λw. (V(w) & P(w))\n",
      "λ expressions were originally designed by Alonzo Church to represent\n",
      "computable functions and to provide a foundation for mathematics and\n",
      "logic. The theory in which λ expressions are studied is known as the\n",
      "λ-calculus. Note that the λ-calculus is not part of first-order logic—both\n",
      "can be used independently of the other.\n",
      "386 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 407, 'page_label': '386', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4783}\n",
      "\n",
      "--- Chunk 4784 ---\n",
      "Content:\n",
      "λ is a binding operator, just as the first-order logic quantifiers are. If we have an open\n",
      "formula, such as (33a), then we can bind the variable x with the λ operator, as shown\n",
      "in (33b). The corresponding NLTK representation is given in (33c).\n",
      "(33) a. ( walk(x) & chew_gum(x))\n",
      "b. λx.(walk(x) & chew_gum(x))\n",
      "c. \\x.(walk(x) & chew_gum(x))\n",
      "Remember that \\ is a special character in Python strings. We must either escape it (with\n",
      "another \\), or else use “raw strings” (Section 3.4) as shown here:\n",
      ">>> lp = nltk.LogicParser()\n",
      ">>> e = lp.parse(r'\\x.(walk(x) & chew_gum(x))')\n",
      ">>> e\n",
      "<LambdaExpression \\x.(walk(x) & chew_gum(x))>\n",
      ">>> e.free()\n",
      "set([])\n",
      ">>> print lp.parse(r'\\x.(walk(x) & chew_gum(y))')\n",
      "\\x.(walk(x) & chew_gum(y))\n",
      "We have a special name for the result of binding the variables in an expression:\n",
      "λ-abstraction. When you first encounter λ-abstracts, it can be hard to get an intuitive\n",
      "sense of their meaning. A couple of English glosses for (33b) are: “be an x such that x\n",
      "walks and x chews gum” or “have the property of walking and chewing gum.” It has\n",
      "often been suggested that λ-abstracts are good representations for verb phrases (or\n",
      "subjectless clauses), particularly when these occur as arguments in their own right. This\n",
      "is illustrated in (34a) and its translation, (34b).\n",
      "(34) a. To walk and chew gum is hard\n",
      "b. hard(\\x.(walk(x) & chew_gum(x))\n",
      "So the general picture is this: given an open formula φ with free variable x, abstracting\n",
      "over x yields a property expression λx.φ—the property of being an x such that φ. Here’s\n",
      "a more official version of how abstracts are built:\n",
      "(35) If α is of type τ, and x is a variable of type e, then \\x.α is of type 〈e, τ〉.\n",
      "(34b) illustrated a case where we say something about a property, namely that it is hard.\n",
      "But what we usually do with properties is attribute them to individuals. And in fact, if\n",
      "φ is an open formula, then the abstract λx.φ can be used as a unary predicate. In (36),\n",
      "(33b) is predicated of the term gerald.\n",
      "(36) \\x.(walk(x) & chew_gum(x)) (gerald)\n",
      "Now (36) says that Gerald has the property of walking and chewing gum, which has\n",
      "the same meaning as (37).\n",
      "(37) (walk(gerald) & chew_gum(gerald))\n",
      "10.4  The Semantics of English Sentences | 387...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 408, 'page_label': '387', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4784}\n",
      "\n",
      "--- Chunk 4785 ---\n",
      "Content:\n",
      "What we have done here is remove the \\x from the beginning of \\x.(walk(x) &\n",
      "chew_gum(x)) and replaced all occurrences of x in (walk(x) & chew_gum(x)) by gerald.\n",
      "We’ll use α[β/x] as notation for the operation of replacing all free occurrences of x in\n",
      "α by the expression β. So\n",
      "(walk(x) & chew_gum(x))[gerald/x]\n",
      "represents the same expression as (37). The “reduction” of (36) to (37) is an extremely\n",
      "useful operation in simplifying semantic representations, and we shall use it a lot in the\n",
      "rest of this chapter. The operation is often called β-reduction. In order for it to be\n",
      "semantically justified, we want it to hold that λx. α(β) has the same semantic value as\n",
      "α[β/x]. This is indeed true, subject to a slight complication that we will come to shortly.\n",
      "In order to carry out β-reduction of expressions in NLTK, we can call the simplify()\n",
      "method \n",
      ".\n",
      ">>> e = lp.parse(r'\\x.(walk(x) & chew_gum(x))(gerald)')\n",
      ">>> print e\n",
      "\\x.(walk(x) & chew_gum(x))(gerald)\n",
      ">>> print e.simplify() \n",
      "(walk(gerald) & chew_gum(gerald))\n",
      "Although \n",
      "we have so far only considered cases where the body of the λ-abstract is an\n",
      "open formula, i.e., of type t, this is not a necessary restriction; the body can be any well-\n",
      "formed expression. Here’s an example with two λs:\n",
      "(38) \\x.\\y.(dog(x) & own(y, x))\n",
      "Just as (33b) plays the role of a unary predicate, (38) works like a binary predicate: it\n",
      "can be applied directly to two arguments \n",
      " . The LogicParser allows nested λs such as\n",
      "\\x.\\y. to be written in the abbreviated form \\x y. \n",
      " .\n",
      ">>> print lp.parse(r'\\x.\\y.(dog(x) & own(y, x))(cyril)').simplify()\n",
      "\\y.(dog(cyril) & own(y,cyril))\n",
      ">>> print lp.parse(r'\\x y.(dog(x) & own(y, x))(cyril, angus)').simplify() \n",
      "(dog(cyril) & own(angus,cyril))\n",
      "All \n",
      "our λ-abstracts so far have involved the familiar first-order variables: x, y, and so on\n",
      "—variables of type e. But suppose we want to treat one abstract, say, \\x.walk(x), as\n",
      "the argument of another λ-abstract? We might try this:\n",
      "\\y.y(angus)(\\x.walk(x))\n",
      "But since the variable y is stipulated to be of type e, \\y.y(angus) only applies to argu-\n",
      "ments of type e while \\x.walk(x) is of type 〈e, t〉! Instead, we need to allow abstraction\n",
      "over variables of higher type. Let’s use P and Q as variables of type 〈e, t〉, and then we\n",
      "can have an abstract such as \\P.P(angus)...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 409, 'page_label': '388', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4785}\n",
      "\n",
      "--- Chunk 4786 ---\n",
      "Content:\n",
      ". Let’s use P and Q as variables of type 〈e, t〉, and then we\n",
      "can have an abstract such as \\P.P(angus). Since P is of type 〈e, t〉, the whole abstract is\n",
      "of type 〈〈e, t〉, t〉. Then \\P.P(angus)(\\x.walk(x)) is legal, and can be simplified via β-\n",
      "reduction to \\x.walk(x)(angus) and then again to walk(angus).\n",
      "388 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 409, 'page_label': '388', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4786}\n",
      "\n",
      "--- Chunk 4787 ---\n",
      "Content:\n",
      "When carrying out β-reduction, some care has to be taken with variables. Consider,\n",
      "for example, the λ-terms (39a) and (39b), which differ only in the identity of a free\n",
      "variable.\n",
      "(39) a. \\y.see(y, x)\n",
      "b. \\y.see(y, z)\n",
      "Suppose now that we apply the λ-term \\P.exists x.P(x) to each of these terms:\n",
      "(40) a. \\P.exists x.P(x)(\\y.see(y, x))\n",
      "b. \\P.exists x.P(x)(\\y.see(y, z))\n",
      "We pointed out earlier that the results of the application should be semantically equiv-\n",
      "alent. But if we let the free variable x in (39a) fall inside the scope of the existential\n",
      "quantifier in (40a), then after reduction, the results will be different:\n",
      "(41) a. exists x.see(x, x)\n",
      "b. exists x.see(x, z)\n",
      "(41a) means there is some x that sees him/herself, whereas (41b) means that there is\n",
      "some x that sees an unspecified individual z. What has gone wrong here? Clearly, we\n",
      "want to forbid the kind of variable “capture” shown in (41a).\n",
      "In order to deal with this problem, let’s step back a moment. Does it matter what\n",
      "particular name we use for the variable bound by the existential quantifier in the func-\n",
      "tion expression of (40a)? The answer is no. In fact, given any variable-binding expres-\n",
      "sion (involving ∀, ∃, or λ), the name chosen for the bound variable is completely arbi-\n",
      "trary. For example, exists x.P(x) and exists y.P(y) are equivalent; they are called\n",
      "α-equivalents, or alphabetic variants. The process of relabeling bound variables is\n",
      "known as α-conversion. When we test for equality of VariableBinderExpressions in\n",
      "the logic module (i.e., using ==), we are in fact testing for α-equivalence:\n",
      ">>> e1 = lp.parse('exists x.P(x)')\n",
      ">>> print e1\n",
      "exists x.P(x)\n",
      ">>> e2 = e1.alpha_convert(nltk.Variable('z'))\n",
      ">>> print e2\n",
      "exists z.P(z)\n",
      ">>> e1 == e2\n",
      "True\n",
      "When β-reduction is carried out on an application f(a), we check whether there are\n",
      "free variables in a that also occur as bound variables in any subterms of f. Suppose, as\n",
      "in the example just discussed, that x is free in a, and that f contains the subterm exists\n",
      "x.P(x). In this case, we produce an alphabetic variant of exists x.P(x), say, exists\n",
      "z1.P(z1), and then carry on with the reduction. This relabeling is carried out automat-\n",
      "ically by the β-reduction code in logic, and the results can be seen in the following\n",
      "example:\n",
      "10.4  The Semantics of English Sentences | 389...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 410, 'page_label': '389', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4787}\n",
      "\n",
      "--- Chunk 4788 ---\n",
      "Content:\n",
      ">>> e3 = lp.parse('\\P.exists x.P(x)(\\y.see(y, x))')\n",
      ">>> print e3\n",
      "(\\P.exists x.P(x))(\\y.see(y,x))\n",
      ">>> print e3.simplify()\n",
      "exists z1.see(z1,x)\n",
      "As you work through examples like these in the following sections, you\n",
      "may find that the logical expressions which are returned have different\n",
      "variable \n",
      "names; for example, you might see z14 in place of z1 in the\n",
      "preceding formula. This change in labeling is innocuous—in fact, it is\n",
      "just an illustration of alphabetic variants.\n",
      "After this excursus, let’s return to the task of building logical forms for English\n",
      "sentences.\n",
      "Quantified NPs\n",
      "At the start of this section, we briefly described how to build a semantic representation\n",
      "for Cyril barks. You would be forgiven for thinking this was all too easy—surely there\n",
      "is a bit more to building compositional semantics. What about quantifiers, for instance?\n",
      "Right, this is a crucial issue. For example, we want (42a) to be given the logical form\n",
      "in (42b). How can this be accomplished?\n",
      "(42) a. A dog barks.\n",
      "b. exists x.(dog(x) & bark(x))\n",
      "Let’s make the assumption that our only operation for building complex semantic rep-\n",
      "resentations is function application. Then our problem is this: how do we give a se-\n",
      "mantic representation to the quantified NPs a dog  so that it can be combined with\n",
      "bark to give the result in (42b)? As a first step, let’s make the subject’s SEM value act as\n",
      "the function expression rather than the argument. (This is sometimes called type-\n",
      "raising.) Now we are looking for a way of instantiating ?np so that\n",
      "[SEM=<?np(\\x.bark(x))>] is equivalent to [SEM=<exists x.(dog(x) & bark(x))>].\n",
      "Doesn’t this look a bit reminiscent of carrying out β-reduction in the λ-calculus? In\n",
      "other words, we want a λ-term M to replace ?np so that applying M to \\x.bark(x) yields\n",
      "(42b). To do this, we replace the occurrence of \\x.bark(x) in (42b) by a predicate\n",
      "variable P, and bind the variable with λ, as shown in (43).\n",
      "(43) \\P.exists x.(dog(x) & P(x))\n",
      "We have used a different style of variable in (43)—that is, 'P' rather than 'x' or 'y'—\n",
      "to signal that we are abstracting over a different kind of object—not an individual, but\n",
      "a function expression of type 〈e, t〉. So the type of (43) as a whole is 〈〈e, t〉, t〉. We will\n",
      "take this to be the type of NPs in general. To illustrate further, a universally quantified\n",
      "NP will look like (44).\n",
      "390 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 411, 'page_label': '390', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4788}\n",
      "\n",
      "--- Chunk 4789 ---\n",
      "Content:\n",
      "(44) \\P.all x.(dog(x) -> P(x))\n",
      "We \n",
      "are pretty much done now, except that we also want to carry out a further abstrac-\n",
      "tion plus application for the process of combining the semantics of the determiner a,\n",
      "namely (45), with the semantics of dog.\n",
      "(45) \\Q P.exists x.(Q(x) & P(x))\n",
      "Applying (45) as a function expression to \\x.dog(x)yields (43), and applying that to\n",
      "\\x.bark(x) gives us \\P.exists x.(dog(x) & P(x))(\\x.bark(x)). Finally, carrying out β-\n",
      "reduction yields just what we wanted, namely (42b).\n",
      "Transitive Verbs\n",
      "Our next challenge is to deal with sentences containing transitive verbs, such as (46).\n",
      "(46) Angus chases a dog.\n",
      "The output semantics that we want to build is exists x.(dog(x) & chase(angus, x)).\n",
      "Let’s look at how we can use λ-abstraction to get this result. A significant constraint\n",
      "on possible solutions is to require that the semantic representation of a dog be inde-\n",
      "pendent of whether the NP acts as subject or object of the sentence. In other words, we\n",
      "want to get the formula just shown as our output while sticking to (43) as the NP se-\n",
      "mantics. A second constraint is that VPs should have a uniform type of interpretation,\n",
      "regardless of whether they consist of just an intransitive verb or a transitive verb plus\n",
      "object. More specifically, we stipulate that VPs are always of type 〈e, t〉. Given these\n",
      "constraints, here’s a semantic representation for chases a dog that does the trick.\n",
      "(47) \\y.exists x.(dog(x) & chase(y, x))\n",
      "Think of (47) as the property of being a y such that for some dog x, y chases x; or more\n",
      "colloquially, being a y who chases a dog. Our task now resolves to designing a semantic\n",
      "representation for chases which can combine with (43) so as to allow (47) to be derived.\n",
      "Let’s carry out the inverse of β-reduction on (47), giving rise to (48).\n",
      "(48) \\P.exists x.(dog(x) & P(x))(\\z.chase(y, z))\n",
      "(48) may be slightly hard to read at first; you need to see that it involves applying the\n",
      "quantified NP representation from (43) to \\z.chase(y,z). (48) is equivalent via β-\n",
      "reduction to exists x.(dog(x) & chase(y, x)).\n",
      "Now let’s replace the function expression in (48) by a variable X of the same type as an\n",
      "NP, that is, of type 〈〈e, t〉, t〉.\n",
      "(49) X(\\z.chase(y, z))\n",
      "10.4  The Semantics of English Sentences | 391...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 412, 'page_label': '391', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4789}\n",
      "\n",
      "--- Chunk 4790 ---\n",
      "Content:\n",
      "The representation of a transitive verb will have to apply to an argument of the type of\n",
      "X \n",
      "to yield a function expression of the type of VPs, that is, of type 〈e, t〉. We can ensure\n",
      "this by abstracting over both the X variable in (49) and also the subject variable y. So\n",
      "the full solution is reached by giving chases the semantic representation shown in (50).\n",
      "(50) \\X y.X(\\x.chase(y, x))\n",
      "If (50) is applied to (43), the result after β-reduction is equivalent to (47), which is what\n",
      "we wanted all along:\n",
      ">>> lp = nltk.LogicParser()\n",
      ">>> tvp = lp.parse(r'\\X x.X(\\y.chase(x,y))')\n",
      ">>> np = lp.parse(r'(\\P.exists x.(dog(x) & P(x)))')\n",
      ">>> vp = nltk.ApplicationExpression(tvp, np)\n",
      ">>> print vp\n",
      "(\\X x.X(\\y.chase(x,y)))(\\P.exists x.(dog(x) & P(x)))\n",
      ">>> print vp.simplify()\n",
      "\\x.exists z2.(dog(z2) & chase(x,z2))\n",
      "In order to build a semantic representation for a sentence, we also need to combine in\n",
      "the semantics of the subject NP. If the latter is a quantified expression, such as every\n",
      "girl, everything proceeds in the same way as we showed for a dog barks earlier on; the\n",
      "subject is translated as a function expression which is applied to the semantic repre-\n",
      "sentation of the VP. However, we now seem to have created another problem for our-\n",
      "selves with proper names. So far, these have been treated semantically as individual\n",
      "constants, and these cannot be applied as functions to expressions like (47). Conse-\n",
      "quently, we need to come up with a different semantic representation for them. What\n",
      "we do in this case is reinterpret proper names so that they too are function expressions,\n",
      "like quantified NPs. Here is the required λ-expression for Angus:\n",
      "(51) \\P.P(angus)\n",
      "(51) denotes the characteristic function corresponding to the set of all properties which\n",
      "are true of Angus. Converting from an individual constant angus to \\P.P(angus) is an-\n",
      "other example of type-raising, briefly mentioned earlier, and allows us to replace a\n",
      "Boolean-valued application such as \\x.walk(x)(angus) with an equivalent function ap-\n",
      "plication \\P.P(angus)(\\x.walk(x)). By β-reduction, both expressions reduce to\n",
      "walk(angus).\n",
      "The grammar simple-sem.fcfg contains a small set of rules for parsing and translating\n",
      "simple examples of the kind that we have been looking at. Here’s a slightly more com-\n",
      "plicated example:\n",
      ">>> from nltk import load_parser\n",
      ">>> parser = load_parser('grammars/book_grammars/simple-sem.fcfg', trace=0)\n",
      ">>> sentence = 'Angus gives a bone to every dog'\n",
      ">>> tokens = sentence.split()\n",
      ">>> trees = parser.nbest_parse(tokens)\n",
      " \n",
      "392 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 413, 'page_label': '392', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4790}\n",
      "\n",
      "--- Chunk 4791 ---\n",
      "Content:\n",
      ">>> for tree in trees:\n",
      "...     print tree.node['SEM']\n",
      "all z2.(dog(z2) -> exists z1.(bone(z1) & give(angus,z1,z2)))\n",
      "NLTK \n",
      "provides some utilities to make it easier to derive and inspect semantic inter-\n",
      "pretations. The function batch_interpret() is intended for batch interpretation of a list\n",
      "of input sentences. It builds a dictionary d where for each sentence sent in the input,\n",
      "d[sent] is a list of pairs (synrep, semrep) consisting of trees and semantic representations\n",
      "for sent. The value is a list since sent may be syntactically ambiguous; in the following\n",
      "example, however, there is only one parse tree per sentence in the list.\n",
      "(S[SEM=<walk(irene)>]\n",
      "  (NP[-LOC, NUM='sg', SEM=<\\P.P(irene)>]\n",
      "    (PropN[-LOC, NUM='sg', SEM=<\\P.P(irene)>] Irene))\n",
      "  (VP[NUM='sg', SEM=<\\x.walk(x)>]\n",
      "    (IV[NUM='sg', SEM=<\\x.walk(x)>, TNS='pres'] walks)))\n",
      "(S[SEM=<exists z1.(ankle(z1) & bite(cyril,z1))>]\n",
      "  (NP[-LOC, NUM='sg', SEM=<\\P.P(cyril)>]\n",
      "    (PropN[-LOC, NUM='sg', SEM=<\\P.P(cyril)>] Cyril))\n",
      "  (VP[NUM='sg', SEM=<\\x.exists z1.(ankle(z1) & bite(x,z1))>]\n",
      "    (TV[NUM='sg', SEM=<\\X x.X(\\y.bite(x,y))>, TNS='pres'] bites)\n",
      "    (NP[NUM='sg', SEM=<\\Q.exists x.(ankle(x) & Q(x))>]\n",
      "      (Det[NUM='sg', SEM=<\\P Q.exists x.(P(x) & Q(x))>] an)\n",
      "      (Nom[NUM='sg', SEM=<\\x.ankle(x)>]\n",
      "        (N[NUM='sg', SEM=<\\x.ankle(x)>] ankle)))))\n",
      "We have seen now how to convert English sentences into logical forms, and earlier we\n",
      "saw how logical forms could be checked as true or false in a model. Putting these two\n",
      "mappings together, we can check the truth value of English sentences in a given model.\n",
      "Let’s take model m as defined earlier. The utility batch_evaluate() resembles\n",
      "batch_interpret(), except that we need to pass a model and a variable assignment as\n",
      "parameters. The output is a triple (synrep, semrep, value), where synrep, semrep are as\n",
      "before, and value is a truth value. For simplicity, the following example only processes\n",
      "a single sentence.\n",
      ">>> v = \"\"\"\n",
      "... bertie => b\n",
      "... olive => o\n",
      "... cyril => c\n",
      "... boy => {b}\n",
      "... girl => {o}\n",
      "... dog => {c}\n",
      "... walk => {o, c}\n",
      "... see => {(b, o), (c, b), (o, c)}\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 414, 'page_label': '393', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4791}\n",
      "\n",
      "--- Chunk 4792 ---\n",
      "Content:\n",
      ". The output is a triple (synrep, semrep, value), where synrep, semrep are as\n",
      "before, and value is a truth value. For simplicity, the following example only processes\n",
      "a single sentence.\n",
      ">>> v = \"\"\"\n",
      "... bertie => b\n",
      "... olive => o\n",
      "... cyril => c\n",
      "... boy => {b}\n",
      "... girl => {o}\n",
      "... dog => {c}\n",
      "... walk => {o, c}\n",
      "... see => {(b, o), (c, b), (o, c)}\n",
      "... \"\"\"\n",
      ">>> val = nltk.parse_valuation(v)\n",
      ">>> g = nltk.Assignment(val.domain)\n",
      ">>> m = nltk.Model(val.domain, val)\n",
      ">>> sent = 'Cyril sees every boy'\n",
      ">>> grammar_file = 'grammars/book_grammars/simple-sem.fcfg'\n",
      ">>> results = nltk.batch_evaluate([sent], grammar_file, m, g)[0]\n",
      ">>> for (syntree, semrel, value) in results:\n",
      "...     print semrep\n",
      "10.4  The Semantics of English Sentences | 393...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 414, 'page_label': '393', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4792}\n",
      "\n",
      "--- Chunk 4793 ---\n",
      "Content:\n",
      "...     print value\n",
      "exists z3.(ankle(z3) & bite(cyril,z3))\n",
      "True\n",
      "Quantifier Ambiguity Revisited\n",
      "One \n",
      "important limitation of the methods described earlier is that they do not deal with\n",
      "scope ambiguity. Our translation method is syntax-driven, in the sense that the se-\n",
      "mantic representation is closely coupled with the syntactic analysis, and the scope of\n",
      "the quantifiers in the semantics therefore reflects the relative scope of the corresponding\n",
      "NPs in the syntactic parse tree. Consequently, a sentence like (26), repeated here, will\n",
      "always be translated as (53a), not (53b).\n",
      "(52) Every girl chases a dog.\n",
      "(53) a. all x.(girl(x) -> exists y.(dog(y) & chase(x,y)))\n",
      "b. exists y.(dog(y) & all x.(girl(x) -> chase(x,y)))\n",
      "There are numerous approaches to dealing with scope ambiguity, and we will look very\n",
      "briefly at one of the simplest. To start with, let’s briefly consider the structure of scoped\n",
      "formulas. Figure 10-3 depicts the way in which the two readings of (52) differ.\n",
      "Figure 10-3. Quantifier scopings.\n",
      "Let’s \n",
      "consider the lefthand structure first. At the top, we have the quantifier corre-\n",
      "sponding to every girl. The φ can be thought of as a placeholder for whatever is inside\n",
      "the scope of the quantifier. Moving downward, we see that we can plug in the quantifier\n",
      "corresponding to a dog as an instantiation of φ. This gives a new placeholder ψ, rep-\n",
      "resenting the scope of a dog, and into this we can plug the “core” of the semantics,\n",
      "namely the open sentence corresponding to x chases y. The structure on the righthand\n",
      "side is identical, except we have swapped round the order of the two quantifiers.\n",
      "In the method known as Cooper storage, a semantic representation is no longer an\n",
      "expression of first-order logic, but instead a pair consisting of a “core” semantic rep-\n",
      "resentation plus a list of binding operators. For the moment, think of a binding op-\n",
      "erator as being identical to the semantic representation of a quantified NP such as (44) or\n",
      "394 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 415, 'page_label': '394', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4793}\n",
      "\n",
      "--- Chunk 4794 ---\n",
      "Content:\n",
      "(45). Following along the lines indicated in Figure 10-3, let’s assume that we have\n",
      "constructed a Cooper-storage-style semantic representation of sentence (52), and let’s\n",
      "take our core to be the open formula chase(x,y). Given a list of binding operators\n",
      "corresponding to the two NPs in (52), we pick a binding operator off the list, and com-\n",
      "bine it with the core.\n",
      "\\P.exists y.(dog(y) & P(y))(\\z2.chase(z1,z2))\n",
      "Then we take the result, and apply the next binding operator from the list to it.\n",
      "\\P.all x.(girl(x) -> P(x))(\\z1.exists x.(dog(x) & chase(z1,x)))\n",
      "Once the list is empty, we have a conventional logical form for the sentence. Combining\n",
      "binding operators with the core in this way is called S-Retrieval. If we are careful to\n",
      "allow every possible order of binding operators (for example, by taking all permutations\n",
      "of the list; see Section 4.5), then we will be able to generate every possible scope ordering\n",
      "of quantifiers.\n",
      "The next question to address is how we build up a core+store representation compo-\n",
      "sitionally. As before, each phrasal and lexical rule in the grammar will have a SEM feature,\n",
      "but now there will be embedded features CORE and STORE. To illustrate the machinery,\n",
      "let’s consider a simpler example, namely Cyril smiles. Here’s a lexical rule for the verb\n",
      "smiles (taken from the grammar storage.fcfg), which looks pretty innocuous:\n",
      "IV[SEM=[CORE=<\\x.smile(x)>, STORE=(/)]] -> 'smiles'\n",
      "The rule for the proper name Cyril is more complex.\n",
      "NP[SEM=[CORE=<@x>, STORE=(<bo(\\P.P(cyril),@x)>)]] -> 'Cyril'\n",
      "The bo predicate has two subparts: the standard (type-raised) representation of a proper\n",
      "name, and the expression @x, which is called the address of the binding operator. (We’ll\n",
      "explain the need for the address variable shortly.) @x is a metavariable, that is, a variable\n",
      "that ranges over individual variables of the logic and, as you will see, also provides the\n",
      "value of core. The rule for VP just percolates up the semantics of the IV, and the inter-\n",
      "esting work is done by the S rule.\n",
      "VP[SEM=?s] -> IV[SEM=?s]\n",
      "S[SEM=[CORE=<?vp(?subj)>, STORE=(?b1+?b2)]] ->\n",
      "   NP[SEM=[CORE=?subj, STORE=?b1]] VP[SEM=[core=?vp, store=?b2]]\n",
      "The core value at the S node is the result of applying the VP’s core value, namely\n",
      "\\x.smile(x), to the subject NP’s value. The latter will not be @x, but rather an instan-\n",
      "tiation of @x, say, z3. After β-reduction, <?vp(?subj)> will be unified with\n",
      "<smile(z3)>. Now, when @x is instantiated as part of the parsing process, it will be\n",
      "instantiated uniformly...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 416, 'page_label': '395', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4794}\n",
      "\n",
      "--- Chunk 4795 ---\n",
      "Content:\n",
      ". The latter will not be @x, but rather an instan-\n",
      "tiation of @x, say, z3. After β-reduction, <?vp(?subj)> will be unified with\n",
      "<smile(z3)>. Now, when @x is instantiated as part of the parsing process, it will be\n",
      "instantiated uniformly. In particular, the occurrence of @x in the subject NP’s STORE will\n",
      "also be mapped to z3, yielding the element bo(\\P.P(cyril),z3). These steps can be seen\n",
      "in the following parse tree.\n",
      "10.4  The Semantics of English Sentences | 395...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 416, 'page_label': '395', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4795}\n",
      "\n",
      "--- Chunk 4796 ---\n",
      "Content:\n",
      "(S[SEM=[CORE=<smile(z3)>, STORE=(bo(\\P.P(cyril),z3))]]\n",
      "  (NP[SEM=[CORE=<z3>, STORE=(bo(\\P.P(cyril),z3))]] Cyril)\n",
      "  (VP[SEM=[CORE=<\\x.smile(x)>, STORE=()]]\n",
      "    (IV[SEM=[CORE=<\\x.smile(x)>, STORE=()]] smiles)))\n",
      "Let’s \n",
      "return to our more complex example, (52), and see what the storage style SEM\n",
      "value is, after parsing with grammar storage.fcfg.\n",
      "CORE  = <chase(z1,z2)>\n",
      "STORE = (bo(\\P.all x.(girl(x) -> P(x)),z1), bo(\\P.exists x.(dog(x) & P(x)),z2))\n",
      "It should be clearer now why the address variables are an important part of the binding\n",
      "operator. Recall that during S-retrieval, we will be taking binding operators off the\n",
      "STORE list and applying them successively to the CORE. Suppose we start with bo(\\P.all\n",
      "x.(girl(x) -> P(x)),z1), which we want to combine with chase(z1,z2). The quantifier\n",
      "part of the binding operator is \\P.all x.(girl(x) -> P(x)), and to combine this with\n",
      "chase(z1,z2), the latter needs to first be turned into a λ-abstract. How do we know\n",
      "which variable to abstract over? This is what the address z1 tells us, i.e., that every\n",
      "girl has the role of chaser rather than chasee.\n",
      "The module nltk.sem.cooper_storage deals with the task of turning storage-style se-\n",
      "mantic representations into standard logical forms. First, we construct a CooperStore\n",
      "instance, and inspect its STORE and CORE.\n",
      ">>> from nltk.sem import cooper_storage as cs\n",
      ">>> sentence = 'every girl chases a dog'\n",
      ">>> trees = cs.parse_with_bindops(sentence, grammar='grammars/book_grammars/storage.fcfg')\n",
      ">>> semrep = trees[0].node['sem']\n",
      ">>> cs_semrep = cs.CooperStore(semrep)\n",
      ">>> print cs_semrep.core\n",
      "chase(z1,z2)\n",
      ">>> for bo in cs_semrep.store:\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 417, 'page_label': '396', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4796}\n",
      "\n",
      "--- Chunk 4797 ---\n",
      "Content:\n",
      ". First, we construct a CooperStore\n",
      "instance, and inspect its STORE and CORE.\n",
      ">>> from nltk.sem import cooper_storage as cs\n",
      ">>> sentence = 'every girl chases a dog'\n",
      ">>> trees = cs.parse_with_bindops(sentence, grammar='grammars/book_grammars/storage.fcfg')\n",
      ">>> semrep = trees[0].node['sem']\n",
      ">>> cs_semrep = cs.CooperStore(semrep)\n",
      ">>> print cs_semrep.core\n",
      "chase(z1,z2)\n",
      ">>> for bo in cs_semrep.store:\n",
      "...     print bo\n",
      "bo(\\P.all x.(girl(x) -> P(x)),z1)\n",
      "bo(\\P.exists x.(dog(x) & P(x)),z2)\n",
      "Finally, we call s_retrieve() and check the readings.\n",
      ">>> cs_semrep.s_retrieve(trace=True)\n",
      "Permutation 1\n",
      "   (\\P.all x.(girl(x) -> P(x)))(\\z1.chase(z1,z2))\n",
      "   (\\P.exists x.(dog(x) & P(x)))(\\z2.all x.(girl(x) -> chase(x,z2)))\n",
      "Permutation 2\n",
      "   (\\P.exists x.(dog(x) & P(x)))(\\z2.chase(z1,z2))\n",
      "   (\\P.all x.(girl(x) -> P(x)))(\\z1.exists x.(dog(x) & chase(z1,x)))\n",
      ">>> for reading in cs_semrep.readings:\n",
      "...     print reading\n",
      "exists x.(dog(x) & all z3.(girl(z3) -> chase(z3,x)))\n",
      "all x.(girl(x) -> exists z4.(dog(z4) & chase(x,z4)))\n",
      "396 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 417, 'page_label': '396', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4797}\n",
      "\n",
      "--- Chunk 4798 ---\n",
      "Content:\n",
      "10.5  Discourse Semantics\n",
      "A discourse \n",
      "is a sequence of sentences. Very often, the interpretation of a sentence in\n",
      "a discourse depends on what preceded it. A clear example of this comes from anaphoric\n",
      "pronouns, such as he, she, and it. Given a discourse such as Angus used to have a dog.\n",
      "But he recently disappeared., you will probably interpret he as referring to Angus’s dog.\n",
      "However, in Angus used to have a dog. He took him for walks in New Town. , you are\n",
      "more likely to interpret he as referring to Angus himself.\n",
      "Discourse Representation Theory\n",
      "The standard approach to quantification in first-order logic is limited to single senten-\n",
      "ces. Yet there seem to be examples where the scope of a quantifier can extend over two\n",
      "or more sentences. We saw one earlier, and here’s a second example, together with a\n",
      "translation.\n",
      "(54) a. Angus owns a dog. It bit Irene.\n",
      "b. ∃x.(dog(x) & own(Angus, x) & bite(x, Irene))\n",
      "That is, the NP a dog acts like a quantifier which binds the it in the second sentence.\n",
      "Discourse Representation Theory (DRT) was developed with the specific goal of pro-\n",
      "viding a means for handling this and other semantic phenomena which seem to be\n",
      "characteristic of discourse. A discourse representation structure (DRS) presents the\n",
      "meaning of discourse in terms of a list of discourse referents and a list of conditions.\n",
      "The discourse referents are the things under discussion in the discourse, and they\n",
      "correspond to the individual variables of first-order logic. The DRS conditions apply\n",
      "to those discourse referents, and correspond to atomic open formulas of first-order\n",
      "logic. Figure 10-4 illustrates how a DRS for the first sentence in (54a) is augmented to\n",
      "become a DRS for both sentences.\n",
      "When the second sentence of (54a) is processed, it is interpreted in the context of what\n",
      "is already present in the lefthand side of Figure 10-4. The pronoun it triggers the addi-\n",
      "tion of a new discourse referent, say, u, and we need to find an anaphoric\n",
      "antecedent for it—that is, we want to work out what it refers to. In DRT, the task of\n",
      "finding the antecedent for an anaphoric pronoun involves linking it to a discourse ref-\n",
      "erent already within the current DRS, and y is the obvious choice. (We will say more\n",
      "about anaphora resolution shortly.) This processing step gives rise to a new condition\n",
      "u = y. The remaining content contributed by the second sentence is also merged with\n",
      "the content of the first, and this is shown on the righthand side of Figure 10-4.\n",
      "Figure 10-4 illustrates how a DRS can represent more than just a single sentence. In\n",
      "this case, it is a two-sentence discourse, but in principle a single DRS could correspond\n",
      "to the interpretation of a whole text. We can inquire into the truth conditions of the\n",
      "righthand DRS in Figure 10-4. Informally, it is true in some situation s if there are\n",
      "entities a, c, and i in s corresponding to the discourse referents in the DRS such that\n",
      "10.5  Discourse Semantics | 397...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 418, 'page_label': '397', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4798}\n",
      "\n",
      "--- Chunk 4799 ---\n",
      "Content:\n",
      "all the conditions are true in s; that is, a is named Angus, c is a dog, a owns c, i is named\n",
      "Irene, and c bit i.\n",
      "In order to process DRSs computationally, we need to convert them into a linear format.\n",
      "Here’s an example, where the DRS is a pair consisting of a list of discourse referents\n",
      "and a list of DRS conditions:\n",
      "([x, y], [angus(x), dog(y), own(x,y)])\n",
      "The easiest way to build a DRS object in NLTK is by parsing a string representation \n",
      ".\n",
      ">>> dp = nltk.DrtParser()\n",
      ">>> drs1 = dp.parse('([x, y], [angus(x), dog(y), own(x, y)])') \n",
      ">>> print drs1\n",
      "([x,y],[angus(x), dog(y), own(x,y)])\n",
      "We can use the draw() method \n",
      "  to visualize the result, as shown in Figure 10-5.\n",
      ">>> drs1.draw() \n",
      "Figure 10-5. DRS screenshot.\n",
      "When we discussed the truth conditions of the DRSs in Figure 10-4, we assumed that\n",
      "the \n",
      "topmost discourse referents were interpreted as existential quantifiers, while the\n",
      "Figure 10-4. Building a DRS: The DRS on the lefthand side represents the result of processing the first\n",
      "sentence \n",
      "in the discourse, while the DRS on the righthand side shows the effect of processing the second\n",
      "sentence and integrating its content.\n",
      "398 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 419, 'page_label': '398', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4799}\n",
      "\n",
      "--- Chunk 4800 ---\n",
      "Content:\n",
      "conditions were interpreted as though they are conjoined. In fact, every DRS can be\n",
      "translated \n",
      "into a formula of first-order logic, and the fol() method implements this\n",
      "translation.\n",
      ">>> print drs1.fol()\n",
      "exists x y.((angus(x) & dog(y)) & own(x,y))\n",
      "In addition to the functionality available for first-order logic expressions, DRT\n",
      "Expressions have a DRS-concatenation operator, represented as the + symbol. The\n",
      "concatenation of two DRSs is a single DRS containing the merged discourse referents\n",
      "and the conditions from both arguments. DRS-concatenation automatically α-converts\n",
      "bound variables to avoid name-clashes.\n",
      ">>> drs2 = dp.parse('([x], [walk(x)]) + ([y], [run(y)])')\n",
      ">>> print drs2\n",
      "(([x],[walk(x)]) + ([y],[run(y)]))\n",
      ">>> print drs2.simplify()\n",
      "([x,y],[walk(x), run(y)])\n",
      "While all the conditions seen so far have been atomic, it is possible to embed one DRS\n",
      "within another, and this is how universal quantification is handled. In drs3, there are\n",
      "no top-level discourse referents, and the sole condition is made up of two sub-DRSs,\n",
      "connected by an implication. Again, we can use fol() to get a handle on the truth\n",
      "conditions.\n",
      ">>> drs3 = dp.parse('([], [(([x], [dog(x)]) -> ([y],[ankle(y), bite(x, y)]))])')\n",
      ">>> print drs3.fol()\n",
      "all x.(dog(x) -> exists y.(ankle(y) & bite(x,y)))\n",
      "We pointed out earlier that DRT is designed to allow anaphoric pronouns to be inter-\n",
      "preted by linking to existing discourse referents. DRT sets constraints on which dis-\n",
      "course referents are “accessible” as possible antecedents, but is not intended to explain\n",
      "how a particular antecedent is chosen from the set of candidates...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 420, 'page_label': '399', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4800}\n",
      "\n",
      "--- Chunk 4801 ---\n",
      "Content:\n",
      ". DRT sets constraints on which dis-\n",
      "course referents are “accessible” as possible antecedents, but is not intended to explain\n",
      "how a particular antecedent is chosen from the set of candidates. The module\n",
      "nltk.sem.drt_resolve_anaphora adopts a similarly conservative strategy: if the DRS\n",
      "contains a condition of the form PRO(x), the method resolve_anaphora() replaces this\n",
      "with a condition of the form x = [...], where [...] is a list of possible antecedents.\n",
      ">>> drs4 = dp.parse('([x, y], [angus(x), dog(y), own(x, y)])')\n",
      ">>> drs5 = dp.parse('([u, z], [PRO(u), irene(z), bite(u, z)])')\n",
      ">>> drs6 = drs4 + drs5\n",
      ">>> print drs6.simplify()\n",
      "([x,y,u,z],[angus(x), dog(y), own(x,y), PRO(u), irene(z), bite(u,z)])\n",
      ">>> print drs6.simplify().resolve_anaphora()\n",
      "([x,y,u,z],[angus(x), dog(y), own(x,y), (u = [x,y,z]), irene(z), bite(u,z)])\n",
      "Since the algorithm for anaphora resolution has been separated into its own module,\n",
      "this facilitates swapping in alternative procedures that try to make more intelligent\n",
      "guesses about the correct antecedent.\n",
      "Our treatment of DRSs is fully compatible with the existing machinery for handling λ-\n",
      "abstraction, and consequently it is straightforward to build compositional semantic\n",
      "representations that are based on DRT rather than first-order logic. This technique is\n",
      "10.5  Discourse Semantics | 399...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 420, 'page_label': '399', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4801}\n",
      "\n",
      "--- Chunk 4802 ---\n",
      "Content:\n",
      "illustrated in the following rule for indefinites (which is part of the grammar drt.fcfg).\n",
      "For \n",
      "ease of comparison, we have added the parallel rule for indefinites from simple-\n",
      "sem.fcfg.\n",
      "Det[NUM=sg,SEM=<\\P Q.([x],[]) + P(x) + Q(x)>] -> 'a'\n",
      "Det[NUM=sg,SEM=<\\P Q. exists x.(P(x) & Q(x))>] -> 'a'\n",
      "To get a better idea of how the DRT rule works, look at this subtree for the NP a dog:\n",
      "(NP[NUM='sg', SEM=<\\Q.(([x],[dog(x)]) + Q(x))>]\n",
      "  (Det[NUM'sg', SEM=<\\P Q.((([x],[]) + P(x)) + Q(x))>] a)\n",
      "  (Nom[NUM='sg', SEM=<\\x.([],[dog(x)])>]\n",
      "    (N[NUM='sg', SEM=<\\x.([],[dog(x)])>] dog)))))\n",
      "The λ-abstract for the indefinite is applied as a function expression to \\x.([],\n",
      "[dog(x)]) which leads to \\Q.(([x],[]) + ([],[dog(x)]) + Q(x)); after simplification,\n",
      "we get \\Q.(([x],[dog(x)]) + Q(x)) as the representation for the NP as a whole.\n",
      "In order to parse with grammar drt.fcfg, we specify in the call to load_earley() that\n",
      "SEM values in feature structures are to be parsed using DrtParser in place of the default\n",
      "LogicParser.\n",
      ">>> from nltk import load_parser\n",
      ">>> parser = load_parser('grammars/book_grammars/drt.fcfg', logic_parser=nltk.DrtParser())\n",
      ">>> trees = parser.nbest_parse('Angus owns a dog'.split())\n",
      ">>> print trees[0].node['sem'].simplify()\n",
      "([x,z2],[Angus(x), dog(z2), own(x,z2)])\n",
      "Discourse Processing\n",
      "When we interpret a sentence, we use a rich context for interpretation, determined in\n",
      "part by the preceding context and in part by our background assumptions. DRT pro-\n",
      "vides a theory of how the meaning of a sentence is integrated into a representation of\n",
      "the prior discourse, but two things have been glaringly absent from the processing\n",
      "approach just discussed. First, there has been no attempt to incorporate any kind of\n",
      "inference; and second, we have only processed individual sentences. These omissions\n",
      "are redressed by the module nltk.inference.discourse.\n",
      "Whereas a discourse is a sequence s1, ... sn of sentences, a discourse thread is a sequence\n",
      "s1-ri, ... sn-rj of readings, one for each sentence in the discourse...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 421, 'page_label': '400', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4802}\n",
      "\n",
      "--- Chunk 4803 ---\n",
      "Content:\n",
      ". DRT pro-\n",
      "vides a theory of how the meaning of a sentence is integrated into a representation of\n",
      "the prior discourse, but two things have been glaringly absent from the processing\n",
      "approach just discussed. First, there has been no attempt to incorporate any kind of\n",
      "inference; and second, we have only processed individual sentences. These omissions\n",
      "are redressed by the module nltk.inference.discourse.\n",
      "Whereas a discourse is a sequence s1, ... sn of sentences, a discourse thread is a sequence\n",
      "s1-ri, ... sn-rj of readings, one for each sentence in the discourse. The module processes\n",
      "sentences incrementally, keeping track of all possible threads when there is ambiguity.\n",
      "For simplicity, the following example ignores scope ambiguity:\n",
      ">>> dt = nltk.DiscourseTester(['A student dances', 'Every student is a person'])\n",
      ">>> dt.readings()\n",
      "s0 readings: s0-r0: exists x.(student(x) & dance(x))\n",
      "s1 readings: s1-r0: all x.(student(x) -> person(x))\n",
      "When a new sentence is added to the current discourse, setting the parameter\n",
      "consistchk=True causes consistency to be checked by invoking the model checker for\n",
      "each thread, i.e., each sequence of admissible readings. In this case, the user has the\n",
      "option of retracting the sentence in question.\n",
      "400 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 421, 'page_label': '400', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4803}\n",
      "\n",
      "--- Chunk 4804 ---\n",
      "Content:\n",
      ">>> dt.add_sentence('No person dances', consistchk=True)\n",
      "Inconsistent discourse d0 ['s0-r0', 's1-r0', 's2-r0']:\n",
      "s0-r0: exists x.(student(x) & dance(x))\n",
      "s1-r0: all x.(student(x) -> person(x))\n",
      "s2-r0: -exists x.(person(x) & dance(x))\n",
      ">>> dt.retract_sentence('No person dances', verbose=True)\n",
      "Current sentences are\n",
      "s0: A student dances\n",
      "s1: Every student is a person\n",
      "In \n",
      "a similar manner, we use informchk=True to check whether a new sentence φ is\n",
      "informative relative to the current discourse. The theorem prover treats existing sen-\n",
      "tences in the thread as assumptions and attempts to prove φ; it is informative if no such\n",
      "proof can be found.\n",
      ">>> dt.add_sentence('A person dances', informchk=True)\n",
      "Sentence 'A person dances' under reading 'exists x.(person(x) & dance(x))':\n",
      "Not informative relative to thread 'd0'\n",
      "It is also possible to pass in an additional set of assumptions as background knowledge\n",
      "and use these to filter out inconsistent readings; see the Discourse HOWTO at http://\n",
      "www.nltk.org/howto for more details.\n",
      "The discourse module can accommodate semantic ambiguity and filter out readings\n",
      "that are not admissible. The following example invokes both Glue Semantics as well\n",
      "as DRT. Since the Glue Semantics module is configured to use the wide-coverage Malt\n",
      "dependency parser, the input (Every dog chases a boy. He runs.) needs to be tagged as\n",
      "well as tokenized.\n",
      ">>> from nltk.tag import RegexpTagger\n",
      ">>> tagger = RegexpTagger(\n",
      "...     [('^(chases|runs)$', 'VB'),\n",
      "...      ('^(a)$', 'ex_quant'),\n",
      "...      ('^(every)$', 'univ_quant'),\n",
      "...      ('^(dog|boy)$', 'NN'),\n",
      "...      ('^(He)$', 'PRP')\n",
      "... ])\n",
      ">>> rc = nltk.DrtGlueReadingCommand(depparser=nltk.MaltParser(tagger=tagger))\n",
      ">>> dt = nltk.DiscourseTester(['Every dog chases a boy', 'He runs'], rc)\n",
      ">>> dt.readings()\n",
      "s0 readings:\n",
      "s0-r0: ([],[(([x],[dog(x)]) -> ([z3],[boy(z3), chases(x,z3)]))]) \n",
      "s0-r1: ([z4],[boy(z4), (([x],[dog(x)]) -> ([],[chases(x,z4)]))])\n",
      "s1 readings:\n",
      "s1-r0: ([x],[PRO(x), runs(x)])\n",
      "The first sentence of the discourse has two possible readings, depending on the quan-\n",
      "tifier scoping. The unique reading of the second sentence represents the pronoun He\n",
      "via the condition PRO(x)...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 422, 'page_label': '401', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4804}\n",
      "\n",
      "--- Chunk 4805 ---\n",
      "Content:\n",
      ". The unique reading of the second sentence represents the pronoun He\n",
      "via the condition PRO(x). Now let’s look at the discourse threads that result:\n",
      ">>> dt.readings(show_thread_readings=True)\n",
      "d0: ['s0-r0', 's1-r0'] : INVALID: AnaphoraResolutionException\n",
      "10.5  Discourse Semantics | 401...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 422, 'page_label': '401', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4805}\n",
      "\n",
      "--- Chunk 4806 ---\n",
      "Content:\n",
      "d1: ['s0-r1', 's1-r0'] : ([z6,z10],[boy(z6), (([x],[dog(x)]) ->\n",
      "([],[chases(x,z6)])), (z10 = z6), runs(z10)])\n",
      "When \n",
      "we examine threads d0 and d1, we see that reading s0-r0, where every dog out-\n",
      "scopes a boy, is deemed inadmissible because the pronoun in the second sentence\n",
      "cannot be resolved. By contrast, in thread d1 the pronoun (relettered to z10) has been\n",
      "bound via the equation (z10 = z6).\n",
      "Inadmissible readings can be filtered out by passing the parameter filter=True.\n",
      ">>> dt.readings(show_thread_readings=True, filter=True)\n",
      "d1: ['s0-r1', 's1-r0'] : ([z12,z15],[boy(z12), (([x],[dog(x)]) ->\n",
      "([],[chases(x,z12)])), (z17 = z15), runs(z15)])\n",
      "Although this little discourse is extremely limited, it should give you a feel for the kind\n",
      "of semantic processing issues that arise when we go beyond single sentences, and also\n",
      "a feel for the techniques that can be deployed to address them.\n",
      "10.6  Summary\n",
      "• First-order logic is a suitable language for representing natural language meaning\n",
      "in a computational setting since it is flexible enough to represent many useful as-\n",
      "pects of natural meaning, and there are efficient theorem provers for reasoning with\n",
      "first-order logic. (Equally, there are a variety of phenomena in natural language\n",
      "semantics which are believed to require more powerful logical mechanisms.)\n",
      "• As well as translating natural language sentences into first-order logic, we can state\n",
      "the truth conditions of these sentences by examining models of first-order formu-\n",
      "las.\n",
      "• In order to build meaning representations compositionally, we supplement first-\n",
      "order logic with the λ-calculus.\n",
      "• β-reduction in the λ-calculus corresponds semantically to application of a function\n",
      "to an argument. Syntactically, it involves replacing a variable bound by λ in the\n",
      "function expression with the expression that provides the argument in the function\n",
      "application.\n",
      "• A key part of constructing a model lies in building a valuation which assigns in-\n",
      "terpretations to non-logical constants. These are interpreted as either n-ary predi-\n",
      "cates or as individual constants.\n",
      "• An open expression is an expression containing one or more free variables. Open\n",
      "expressions receive an interpretation only when their free variables receive values\n",
      "from a variable assignment.\n",
      "• Quantifiers are interpreted by constructing, for a formula φ[x] open in variable x,\n",
      "the set of individuals which make φ[x] true when an assignment g assigns them as\n",
      "the value of x. The quantifier then places constraints on that set.\n",
      "402 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 423, 'page_label': '402', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4806}\n",
      "\n",
      "--- Chunk 4807 ---\n",
      "Content:\n",
      "• A closed expression is one that has no free variables; that is, the variables are all\n",
      "bound. A closed sentence is true or false with respect to all variable assignments.\n",
      "• If two formulas differ only in the label of the variable bound by binding operator\n",
      "(i.e., λ or a quantifier) , they are said to be α-equivalents. The result of relabeling\n",
      "a bound variable in a formula is called α-conversion.\n",
      "• Given a formula with two nested quantifiers Q1 and Q2, the outermost quantifier\n",
      "Q1 is said to have wide scope (or scope over Q2). English sentences are frequently\n",
      "ambiguous with respect to the scope of the quantifiers they contain.\n",
      "• English sentences can be associated with a semantic representation by treating\n",
      "SEM as a feature in a feature-based grammar. The SEM value of a complex expressions,\n",
      "typically involves functional application of the SEM values of the component\n",
      "expressions.\n",
      "10.7  Further Reading\n",
      "Consult http://www.nltk.org/ for further materials on this chapter and on how to install\n",
      "the Prover9 theorem prover and Mace4 model builder. General information about these\n",
      "two inference tools is given by (McCune, 2008).\n",
      "For more examples of semantic analysis with NLTK, please see the semantics and logic\n",
      "HOWTOs at http://www.nltk.org/howto. Note that there are implementations of two\n",
      "other approaches to scope ambiguity, namely Hole semantics as described in (Black-\n",
      "burn & Bos, 2005), and Glue semantics, as described in (Dalrymple et al., 1999).\n",
      "There are many phenomena in natural language semantics that have not been touched\n",
      "on in this chapter, most notably:\n",
      "1. Events, tense, and aspect\n",
      "2. Semantic roles\n",
      "3. Generalized quantifiers, such as most\n",
      "4. Intensional constructions involving, for example, verbs such as may and believe\n",
      "While (1) and (2) can be dealt with using first-order logic, (3) and (4) require different\n",
      "logics. These issues are covered by many of the references in the following readings.\n",
      "A comprehensive overview of results and techniques in building natural language front-\n",
      "ends to databases can be found in (Androutsopoulos, Ritchie & Thanisch, 1995).\n",
      "Any introductory book to modern logic will present propositional and first-order logic.\n",
      "(Hodges, 1977) is highly recommended as an entertaining and insightful text with many\n",
      "illustrations from natural language.\n",
      "For a wide-ranging, two-volume textbook on logic that also presents contemporary\n",
      "material on the formal semantics of natural language, including Montague Grammar\n",
      "and intensional logic, see (Gamut, 1991a, 1991b). (Kamp & Reyle, 1993) provides the\n",
      "10.7  Further Reading | 403...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 424, 'page_label': '403', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4807}\n",
      "\n",
      "--- Chunk 4808 ---\n",
      "Content:\n",
      "definitive account of Discourse Representation Theory, and covers a large and inter-\n",
      "esting \n",
      "fragment of natural language, including tense, aspect, and modality. Another\n",
      "comprehensive study of the semantics of many natural language constructions is (Car-\n",
      "penter, 1997).\n",
      "There are numerous works that introduce logical semantics within the framework of\n",
      "linguistic theory. (Chierchia & McConnell-Ginet, 1990) is relatively agnostic about\n",
      "syntax, while (Heim & Kratzer, 1998) and (Larson & Segal, 1995) are both more ex-\n",
      "plicitly oriented toward integrating truth-conditional semantics into a Chomskyan\n",
      "framework.\n",
      "(Blackburn & Bos, 2005) is the first textbook devoted to computational semantics, and\n",
      "provides an excellent introduction to the area. It expands on many of the topics covered\n",
      "in this chapter, including underspecification of quantifier scope ambiguity, first-order\n",
      "inference, and discourse processing.\n",
      "To gain an overview of more advanced contemporary approaches to semantics, in-\n",
      "cluding treatments of tense and generalized quantifiers, try consulting (Lappin, 1996)\n",
      "or (van Benthem & ter Meulen, 1997).\n",
      "10.8  Exercises\n",
      "1. ○ Translate the following sentences into propositional logic and verify that they\n",
      "parse with LogicParser. Provide a key that shows how the propositional variables\n",
      "in your translation correspond to expressions of English.\n",
      "a. If Angus sings, it is not the case that Bertie sulks.\n",
      "b. Cyril runs and barks.\n",
      "c. It will snow if it doesn’t rain.\n",
      "d. It’s not the case that Irene will be happy if Olive or Tofu comes.\n",
      "e. Pat didn’t cough or sneeze.\n",
      "f. If you don’t come if I call, I won’t come if you call.\n",
      "2. ○ Translate the following sentences into predicate-argument formulas of first-order\n",
      "logic.\n",
      "a. Angus likes Cyril and Irene hates Cyril.\n",
      "b. Tofu is taller than Bertie.\n",
      "c. Bruce loves himself and Pat does too.\n",
      "d. Cyril saw Bertie, but Angus didn’t.\n",
      "e. Cyril is a four-legged friend.\n",
      "f. Tofu and Olive are near each other.\n",
      "3. ○ Translate the following sentences into quantified formulas of first-order logic.\n",
      "a. Angus likes someone and someone likes Julia.\n",
      "404 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 425, 'page_label': '404', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4808}\n",
      "\n",
      "--- Chunk 4809 ---\n",
      "Content:\n",
      "b. Angus loves a dog who loves him.\n",
      "c.\n",
      "Nobody smiles at Pat.\n",
      "d. Somebody coughs and sneezes.\n",
      "e. Nobody coughed or sneezed.\n",
      "f. Bruce loves somebody other than Bruce.\n",
      "g. Nobody other than Matthew loves Pat.\n",
      "h. Cyril likes everyone except for Irene.\n",
      "i. Exactly one person is asleep.\n",
      "4. ○ Translate the following verb phrases using λ-abstracts and quantified formulas\n",
      "of first-order logic.\n",
      "a. feed Cyril and give a capuccino to Angus\n",
      "b. be given ‘War and Peace’ by Pat\n",
      "c. be loved by everyone\n",
      "d. be loved or detested by everyone\n",
      "e. be loved by everyone and detested by no-one\n",
      "5. ○ Consider the following statements:\n",
      ">>> lp = nltk.LogicParser()\n",
      ">>> e2 = lp.parse('pat')\n",
      ">>> e3 = nltk.ApplicationExpression(e1, e2)\n",
      ">>> print e3.simplify()\n",
      "exists y.love(pat, y)\n",
      "Clearly something is missing here, namely a declaration of the value of e1. In order\n",
      "for ApplicationExpression(e1, e2) to be β-convertible to exists y.love(pat, y),\n",
      "e1 must be a λ-abstract which can take pat as an argument. Your task is to construct\n",
      "such an abstract, bind it to e1, and satisfy yourself that these statements are all\n",
      "satisfied (up to alphabetic variance). In addition, provide an informal English\n",
      "translation of e3.simplify().\n",
      "Now carry on doing this same task for the further cases of e3.simplify() shown\n",
      "here:\n",
      ">>> print e3.simplify()\n",
      "exists y.(love(pat,y) | love(y,pat))\n",
      ">>> print e3.simplify()\n",
      "exists y.(love(pat,y) | love(y,pat))\n",
      ">>> print e3.simplify()\n",
      "walk(fido)\n",
      "6. ○ As in the preceding exercise, find a λ-abstract e1 that yields results equivalent to\n",
      "those shown here:\n",
      ">>> e2 = lp.parse('chase')\n",
      ">>> e3 = nltk.ApplicationExpression(e1, e2)\n",
      "10.8  Exercises | 405...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 426, 'page_label': '405', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4809}\n",
      "\n",
      "--- Chunk 4810 ---\n",
      "Content:\n",
      ">>> print e3.simplify()\n",
      "\\x.all y.(dog(y) -> chase(x,pat))\n",
      ">>> e2 = lp.parse('chase')\n",
      ">>> e3 = nltk.ApplicationExpression(e1, e2)\n",
      ">>> print e3.simplify()\n",
      "\\x.exists y.(dog(y) & chase(pat,x))\n",
      ">>> e2 = lp.parse('give')\n",
      ">>> e3 = nltk.ApplicationExpression(e1, e2)\n",
      ">>> print e3.simplify()\n",
      "\\x0 x1.exists y.(present(y) & give(x1,y,x0))\n",
      "7. ○ As \n",
      "in the preceding exercise, find a λ-abstract e1 that yields results equivalent to\n",
      "those shown here:\n",
      ">>> e2 = lp.parse('bark')\n",
      ">>> e3 = nltk.ApplicationExpression(e1, e2)\n",
      ">>> print e3.simplify()\n",
      "exists y.(dog(x) & bark(x))\n",
      ">>> e2 = lp.parse('bark')\n",
      ">>> e3 = nltk.ApplicationExpression(e1, e2)\n",
      ">>> print e3.simplify()\n",
      "bark(fido)\n",
      ">>> e2 = lp.parse('\\\\P. all x. (dog(x) -> P(x))')\n",
      ">>> e3 = nltk.ApplicationExpression(e1, e2)\n",
      ">>> print e3.simplify()\n",
      "all x.(dog(x) -> bark(x))\n",
      "8. ◑ Develop a method for translating English sentences into formulas with binary\n",
      "generalized quantifiers. In such an approach, given a generalized quantifier Q, a\n",
      "quantified formula is of the form Q(A, B), where both A and B are expressions of\n",
      "type 〈e, t〉. Then, for example, all(A, B) is true iff A denotes a subset of what B\n",
      "denotes.\n",
      "9. ◑ Extend the approach in the preceding exercise so that the truth conditions for\n",
      "quantifiers such as most and exactly three can be computed in a model.\n",
      "10. ◑ Modify the sem.evaluate code so that it will give a helpful error message if an\n",
      "expression is not in the domain of a model’s valuation function.\n",
      "11. ● Select three or four contiguous sentences from a book for children. A possible\n",
      "source of examples are the collections of stories in nltk.corpus.gutenberg: bryant-\n",
      "stories.txt, burgess-busterbrown.txt, and edgeworth-parents.txt. Develop a\n",
      "grammar that will allow your sentences to be translated into first-order logic, and\n",
      "build a model that will allow those translations to be checked for truth or falsity.\n",
      "12. ● Carry out the preceding exercise, but use DRT as the meaning representation.\n",
      "13. ● Taking (Warren & Pereira, 1982) as a starting point, develop a technique for\n",
      "converting a natural language query into a form that can be evaluated more effi-\n",
      "ciently in a model. For example, given a query of the form (P(x) & Q(x)), convert\n",
      "it to (Q(x) & P(x)) if the extension of Q is smaller than the extension of P.\n",
      "406 | Chapter 10:  Analyzing the Meaning of Sentences...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 427, 'page_label': '406', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4810}\n",
      "\n",
      "--- Chunk 4811 ---\n",
      "Content:\n",
      "CHAPTER 11\n",
      "Managing Linguistic Data\n",
      "Structured collections of annotated linguistic data are essential in most areas of NLP;\n",
      "however, \n",
      "we still face many obstacles in using them. The goal of this chapter is to answer\n",
      "the following questions:\n",
      "1. How do we design a new language resource and ensure that its coverage, balance,\n",
      "and documentation support a wide range of uses?\n",
      "2. When existing data is in the wrong format for some analysis tool, how can we\n",
      "convert it to a suitable format?\n",
      "3. What is a good way to document the existence of a resource we have created so\n",
      "that others can easily find it?\n",
      "Along the way, we will study the design of existing corpora, the typical workflow for\n",
      "creating a corpus, and the life cycle of a corpus. As in other chapters, there will be many\n",
      "examples drawn from practical experience managing linguistic data, including data\n",
      "that has been collected in the course of linguistic fieldwork, laboratory work, and web\n",
      "crawling.\n",
      "11.1  Corpus Structure: A Case Study\n",
      "The TIMIT Corpus was the first annotated speech database to be widely distributed,\n",
      "and it has an especially clear organization. TIMIT was developed by a consortium in-\n",
      "cluding Texas Instruments and MIT, from which it derives its name. It was designed\n",
      "to provide data for the acquisition of acoustic-phonetic knowledge and to support the\n",
      "development and evaluation of automatic speech recognition systems.\n",
      "The Structure of TIMIT\n",
      "Like the Brown Corpus, which displays a balanced selection of text genres and sources,\n",
      "TIMIT includes a balanced selection of dialects, speakers, and materials. For each of\n",
      "eight dialect regions, 50 male and female speakers having a range of ages and educa-\n",
      "tional backgrounds each read 10 carefully chosen sentences. Two sentences, read by\n",
      "all speakers, were designed to bring out dialect variation:\n",
      "407...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 428, 'page_label': '407', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4811}\n",
      "\n",
      "--- Chunk 4812 ---\n",
      "Content:\n",
      "(1) a. she had your dark suit in greasy wash water all year\n",
      "b.\n",
      "don’t ask me to carry an oily rag like that\n",
      "The remaining sentences were chosen to be phonetically rich, involving all phones\n",
      "(sounds) and a comprehensive range of diphones (phone bigrams). Additionally, the\n",
      "design strikes a balance between multiple speakers saying the same sentence in order\n",
      "to permit comparison across speakers, and having a large range of sentences covered\n",
      "by the corpus to get maximal coverage of diphones. Five of the sentences read by each\n",
      "speaker are also read by six other speakers (for comparability). The remaining three\n",
      "sentences read by each speaker were unique to that speaker (for coverage).\n",
      "NLTK includes a sample from the TIMIT Corpus. You can access its documentation\n",
      "in the usual way, using help(nltk.corpus.timit). Print nltk.corpus.timit.fileids()\n",
      "to see a list of the 160 recorded utterances in the corpus sample. Each filename has\n",
      "internal structure, as shown in Figure 11-1.\n",
      "Figure 11-1. Structure of a TIMIT identifier: Each recording is labeled using a string made up of the\n",
      "speaker’s dialect region, gender, speaker identifier, sentence type, and sentence identifier.\n",
      "Each \n",
      "item has a phonetic transcription which can be accessed using the phones() meth-\n",
      "od. We can access the corresponding word tokens in the customary way. Both access\n",
      "methods permit an optional argument offset=True, which includes the start and end\n",
      "offsets of the corresponding span in the audio file.\n",
      ">>> phonetic = nltk.corpus.timit.phones('dr1-fvmh0/sa1')\n",
      ">>> phonetic\n",
      "['h#', 'sh', 'iy', 'hv', 'ae', 'dcl', 'y', 'ix', 'dcl', 'd', 'aa', 'kcl',\n",
      "408 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 429, 'page_label': '408', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4812}\n",
      "\n",
      "--- Chunk 4813 ---\n",
      "Content:\n",
      "'s', 'ux', 'tcl', 'en', 'gcl', 'g', 'r', 'iy', 's', 'iy', 'w', 'aa',\n",
      "'sh', 'epi', 'w', 'aa', 'dx', 'ax', 'q', 'ao', 'l', 'y', 'ih', 'ax', 'h#']\n",
      ">>> nltk.corpus.timit.word_times('dr1-fvmh0/sa1')\n",
      "[('she', 7812, 10610), ('had', 10610, 14496), ('your', 14496, 15791),\n",
      "('dark', 15791, 20720), ('suit', 20720, 25647), ('in', 25647, 26906),\n",
      "('greasy', 26906, 32668), ('wash', 32668, 37890), ('water', 38531, 42417),\n",
      "('all', 43091, 46052), ('year', 46052, 50522)]\n",
      "In \n",
      "addition to this text data, TIMIT includes a lexicon that provides the canonical\n",
      "pronunciation of every word, which can be compared with a particular utterance:\n",
      ">>> timitdict = nltk.corpus.timit.transcription_dict()\n",
      ">>> timitdict['greasy'] + timitdict['wash'] + timitdict['water']\n",
      "['g', 'r', 'iy1', 's', 'iy', 'w', 'ao1', 'sh', 'w', 'ao1', 't', 'axr']\n",
      ">>> phonetic[17:30]\n",
      "['g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'ax']\n",
      "This gives us a sense of what a speech processing system would have to do in producing\n",
      "or recognizing speech in this particular dialect (New England). Finally, TIMIT includes\n",
      "demographic data about the speakers, permitting fine-grained study of vocal, social,\n",
      "and gender characteristics.\n",
      ">>> nltk.corpus.timit.spkrinfo('dr1-fvmh0')\n",
      "SpeakerInfo(id='VMH0', sex='F', dr='1', use='TRN', recdate='03/11/86',\n",
      "birthdate='01/08/60', ht='5\\'05\"', race='WHT', edu='BS',\n",
      "comments='BEST NEW ENGLAND ACCENT SO FAR')\n",
      "Notable Design Features\n",
      "TIMIT illustrates several key features of corpus design. First, the corpus contains two\n",
      "layers of annotation, at the phonetic and orthographic levels. In general, a text or speech\n",
      "corpus may be annotated at many different linguistic levels, including morphological,\n",
      "syntactic, and discourse levels. Moreover, even at a given level there may be different\n",
      "labeling schemes or even disagreement among annotators, such that we want to rep-\n",
      "resent multiple versions. A second property of TIMIT is its balance across multiple\n",
      "dimensions of variation, for coverage of dialect regions and diphones. The inclusion of\n",
      "speaker demographics brings in many more independent variables that may help to\n",
      "account for variation in the data, and which facilitate later uses of the corpus for pur-\n",
      "poses that were not envisaged when the corpus was created, such as sociolinguistics.\n",
      "A third property is that there is a sharp division between the original linguistic event\n",
      "captured as an audio recording and the annotations of that event. The same holds true\n",
      "of text corpora, in the sense that the original text usually has an external source, and\n",
      "is considered to be an immutable artifact...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 430, 'page_label': '409', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4813}\n",
      "\n",
      "--- Chunk 4814 ---\n",
      "Content:\n",
      ". A second property of TIMIT is its balance across multiple\n",
      "dimensions of variation, for coverage of dialect regions and diphones. The inclusion of\n",
      "speaker demographics brings in many more independent variables that may help to\n",
      "account for variation in the data, and which facilitate later uses of the corpus for pur-\n",
      "poses that were not envisaged when the corpus was created, such as sociolinguistics.\n",
      "A third property is that there is a sharp division between the original linguistic event\n",
      "captured as an audio recording and the annotations of that event. The same holds true\n",
      "of text corpora, in the sense that the original text usually has an external source, and\n",
      "is considered to be an immutable artifact. Any transformations of that artifact which\n",
      "involve human judgment—even something as simple as tokenization—are subject to\n",
      "later revision; thus it is important to retain the source material in a form that is as close\n",
      "to the original as possible.\n",
      "A fourth feature of TIMIT is the hierarchical structure of the corpus. With 4 files per\n",
      "sentence, and 10 sentences for each of 500 speakers, there are 20,000 files. These are\n",
      "organized into a tree structure, shown schematically in Figure 11-2. At the top level\n",
      "11.1  Corpus Structure: A Case Study | 409...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 430, 'page_label': '409', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4814}\n",
      "\n",
      "--- Chunk 4815 ---\n",
      "Content:\n",
      "there is a split between training and testing sets, which gives away its intended use for\n",
      "developing and evaluating statistical models.\n",
      "Finally, \n",
      "notice that even though TIMIT is a speech corpus, its transcriptions and asso-\n",
      "ciated data are just text, and can be processed using programs just like any other text\n",
      "corpus. Therefore, many of the computational methods described in this book are ap-\n",
      "plicable. Moreover, notice that all of the data types included in the TIMIT Corpus fall\n",
      "into the two basic categories of lexicon and text, which we will discuss later. Even the\n",
      "speaker demographics data is just another instance of the lexicon data type.\n",
      "This last observation is less surprising when we consider that text and record structures\n",
      "are the primary domains for the two subfields of computer science that focus on data\n",
      "management, namely text retrieval and databases. A notable feature of linguistic data\n",
      "management is that it usually brings both data types together, and that it can draw on\n",
      "results and techniques from both fields.\n",
      "Figure 11-2. Structure of the published TIMIT Corpus: The CD-ROM contains doc, train, and test\n",
      "directories at the top level; the train and test directories both have eight sub-directories, one per dialect\n",
      "region; each of these contains further subdirectories, one per speaker; the contents of the directory for\n",
      "female speaker aks0 are listed, showing 10 wav files accompanied by a text transcription, a word-\n",
      "aligned transcription, and a phonetic transcription.\n",
      "410 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 431, 'page_label': '410', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4815}\n",
      "\n",
      "--- Chunk 4816 ---\n",
      "Content:\n",
      "Fundamental Data Types\n",
      "Despite \n",
      "its complexity, the TIMIT Corpus contains only two fundamental data types,\n",
      "namely lexicons and texts. As we saw in Chapter 2, most lexical resources can be rep-\n",
      "resented using a record structure, i.e., a key plus one or more fields, as shown in\n",
      "Figure 11-3. A lexical resource could be a conventional dictionary or comparative\n",
      "wordlist, as illustrated. It could also be a phrasal lexicon, where the key field is a phrase\n",
      "rather than a single word. A thesaurus also consists of record-structured data, where\n",
      "we look up entries via non-key fields that correspond to topics. We can also construct\n",
      "special tabulations (known as paradigms) to illustrate contrasts and systematic varia-\n",
      "tion, as shown in Figure 11-3 for three verbs. TIMIT’s speaker table is also a kind of\n",
      "lexicon.\n",
      "Figure 11-3. Basic linguistic data types—lexicons and texts: Amid their diversity, lexicons have a\n",
      "record structure, whereas annotated texts have a temporal organization.\n",
      "At the most abstract level, a text is a representation of a real or fictional speech event,\n",
      "and \n",
      "the time-course of that event carries over into the text itself. A text could be a small\n",
      "unit, such as a word or sentence, or a complete narrative or dialogue. It may come with\n",
      "annotations such as part-of-speech tags, morphological analysis, discourse structure,\n",
      "and so forth. As we saw in the IOB tagging technique (Chapter 7), it is possible to\n",
      "represent higher-level constituents using tags on individual words. Thus the abstraction\n",
      "of text shown in Figure 11-3 is sufficient.\n",
      "11.1  Corpus Structure: A Case Study | 411...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 432, 'page_label': '411', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4816}\n",
      "\n",
      "--- Chunk 4817 ---\n",
      "Content:\n",
      "Despite the complexities and idiosyncrasies of individual corpora, at base they are col-\n",
      "lections \n",
      "of texts together with record-structured data. The contents of a corpus are\n",
      "often biased toward one or the other of these types. For example, the Brown Corpus\n",
      "contains 500 text files, but we still use a table to relate the files to 15 different genres.\n",
      "At the other end of the spectrum, WordNet contains 117,659 synset records, yet it\n",
      "incorporates many example sentences (mini-texts) to illustrate word usages. TIMIT is\n",
      "an interesting midpoint on this spectrum, containing substantial free-standing material\n",
      "of both the text and lexicon types.\n",
      "11.2  The Life Cycle of a Corpus\n",
      "Corpora are not born fully formed, but involve careful preparation and input from\n",
      "many people over an extended period. Raw data needs to be collected, cleaned up,\n",
      "documented, and stored in a systematic structure. Various layers of annotation might\n",
      "be applied, some requiring specialized knowledge of the morphology or syntax of the\n",
      "language. Success at this stage depends on creating an efficient workflow involving\n",
      "appropriate tools and format converters. Quality control procedures can be put in place\n",
      "to find inconsistencies in the annotations, and to ensure the highest possible level of\n",
      "inter-annotator agreement. Because of the scale and complexity of the task, large cor-\n",
      "pora may take years to prepare, and involve tens or hundreds of person-years of effort.\n",
      "In this section, we briefly review the various stages in the life cycle of a corpus.\n",
      "Three Corpus Creation Scenarios\n",
      "In one type of corpus, the design unfolds over in the course of the creator’s explorations.\n",
      "This is the pattern typical of traditional “field linguistics,” in which material from elic-\n",
      "itation sessions is analyzed as it is gathered, with tomorrow’s elicitation often based on\n",
      "questions that arise in analyzing today’s. The resulting corpus is then used during sub-\n",
      "sequent years of research, and may serve as an archival resource indefinitely. Comput-\n",
      "erization is an obvious boon to work of this type, as exemplified by the popular program\n",
      "Shoebox, now over two decades old and re-released as Toolbox (see Section 2.4). Other\n",
      "software tools, even simple word processors and spreadsheets, are routinely used to\n",
      "acquire the data. In the next section, we will look at how to extract data from these\n",
      "sources.\n",
      "Another corpus creation scenario is typical of experimental research where a body of\n",
      "carefully designed material is collected from a range of human subjects, then analyzed\n",
      "to evaluate a hypothesis or develop a technology. It has become common for such\n",
      "databases to be shared and reused within a laboratory or company, and often to be\n",
      "published more widely. Corpora of this type are the basis of the “common task” method\n",
      "of research management, which over the past two decades has become the norm in\n",
      "government-funded research programs in language technology. We have already en-\n",
      "countered many such corpora in the earlier chapters; we will see how to write Python\n",
      "412 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 433, 'page_label': '412', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4817}\n",
      "\n",
      "--- Chunk 4818 ---\n",
      "Content:\n",
      "programs to implement the kinds of curation tasks that are necessary before such cor-\n",
      "pora are published.\n",
      "Finally, there are efforts to gather a “reference corpus” for a particular language, such\n",
      "as the American National Corpus \n",
      "(ANC) and the British National Corpus (BNC). Here\n",
      "the goal has been to produce a comprehensive record of the many forms, styles, and\n",
      "uses of a language. Apart from the sheer challenge of scale, there is a heavy reliance on\n",
      "automatic annotation tools together with post-editing to fix any errors. However, we\n",
      "can write programs to locate and repair the errors, and also to analyze the corpus for\n",
      "balance.\n",
      "Quality Control\n",
      "Good tools for automatic and manual preparation of data are essential. However, the\n",
      "creation of a high-quality corpus depends just as much on such mundane things as\n",
      "documentation, training, and workflow. Annotation guidelines define the task and\n",
      "document the markup conventions. They may be regularly updated to cover difficult\n",
      "cases, along with new rules that are devised to achieve more consistent annotations.\n",
      "Annotators need to be trained in the procedures, including methods for resolving cases\n",
      "not covered in the guidelines. A workflow needs to be established, possibly with sup-\n",
      "porting software, to keep track of which files have been initialized, annotated, validated,\n",
      "manually checked, and so on. There may be multiple layers of annotation, provided by\n",
      "different specialists. Cases of uncertainty or disagreement may require adjudication.\n",
      "Large annotation tasks require multiple annotators, which raises the problem of\n",
      "achieving consistency. How consistently can a group of annotators perform? We can\n",
      "easily measure consistency by having a portion of the source material independently\n",
      "annotated by two people. This may reveal shortcomings in the guidelines or differing\n",
      "abilities with the annotation task. In cases where quality is paramount, the entire corpus\n",
      "can be annotated twice, and any inconsistencies adjudicated by an expert.\n",
      "It is considered best practice to report the inter-annotator agreement that was achieved\n",
      "for a corpus (e.g., by double-annotating 10% of the corpus). This score serves as a\n",
      "helpful upper bound on the expected performance of any automatic system that is\n",
      "trained on this corpus.\n",
      "Caution!\n",
      "Care \n",
      "should be exercised when interpreting an inter-annotator agree-\n",
      "ment score, since annotation tasks vary greatly in their difficulty. For\n",
      "example, 90% agreement would be a terrible score for part-of-speech\n",
      "tagging, but an exceptional score for semantic role labeling.\n",
      "The Kappa coefficient κ measures agreement between two people making category\n",
      "judgments, correcting for expected chance agreement. For example, suppose an item\n",
      "is to be annotated, and four coding options are equally likely. In this case, two people\n",
      "coding randomly would be expected to agree 25% of the time. Thus, an agreement of\n",
      "11.2  The Life Cycle of a Corpus | 413...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 434, 'page_label': '413', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4818}\n",
      "\n",
      "--- Chunk 4819 ---\n",
      "Content:\n",
      "25% will be assigned κ = 0, and better levels of agreement will be scaled accordingly.\n",
      "For an agreement of 50%, we would get κ = 0.333, as 50 is a third of the way from 25\n",
      "to 100. Many other agreement measures exist; see help(nltk.metrics.agreement) for\n",
      "details.\n",
      "We can also measure the agreement between two independent segmentations of lan-\n",
      "guage input, e.g., for tokenization, sentence segmentation, and named entity recogni-\n",
      "tion. In Figure 11-4 we see three possible segmentations of a sequence of items which\n",
      "might have been produced by annotators (or programs). Although none of them agree\n",
      "exactly, S1 and S2 are in close agreement, and we would like a suitable measure. Win-\n",
      "dowdiff is a simple algorithm for evaluating the agreement of two segmentations by\n",
      "running a sliding window over the data and awarding partial credit for near misses. If\n",
      "we preprocess our tokens into a sequence of zeros and ones, to record when a token is\n",
      "followed by a boundary, we can represent the segmentations as strings and apply the\n",
      "windowdiff scorer.\n",
      ">>> s1 = \"00000010000000001000000\"\n",
      ">>> s2 = \"00000001000000010000000\"\n",
      ">>> s3 = \"00010000000000000001000\"\n",
      ">>> nltk.windowdiff(s1, s1, 3)\n",
      "0\n",
      ">>> nltk.windowdiff(s1, s2, 3)\n",
      "4\n",
      ">>> nltk.windowdiff(s2, s3, 3)\n",
      "16\n",
      "In this example, the window had a size of 3. The windowdiff computation slides this\n",
      "window across a pair of strings. At each position it totals up the number of boundaries\n",
      "found inside this window, for both strings, then computes the difference. These dif-\n",
      "ferences are then summed. We can increase or shrink the window size to control the\n",
      "sensitivity of the measure.\n",
      "Curation Versus Evolution\n",
      "As large corpora are published, researchers are increasingly likely to base their inves-\n",
      "tigations on balanced, focused subsets that were derived from corpora produced for\n",
      "Figure 11-4. Three segmentations of a sequence: The small rectangles represent characters, words,\n",
      "sentences, \n",
      "in short, any sequence which might be divided into linguistic units; S 1 and S2 are in close\n",
      "agreement, but both differ significantly from S3.\n",
      "414 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 435, 'page_label': '414', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4819}\n",
      "\n",
      "--- Chunk 4820 ---\n",
      "Content:\n",
      "entirely different reasons. For instance, the Switchboard database, originally collected\n",
      "for \n",
      "speaker identification research, has since been used as the basis for published studies\n",
      "in speech recognition, word pronunciation, disfluency, syntax, intonation, and dis-\n",
      "course structure. The motivations for recycling linguistic corpora include the desire to\n",
      "save time and effort, the desire to work on material available to others for replication,\n",
      "and sometimes a desire to study more naturalistic forms of linguistic behavior than\n",
      "would be possible otherwise. The process of choosing a subset for such a study may\n",
      "count as a non-trivial contribution in itself.\n",
      "In addition to selecting an appropriate subset of a corpus, this new work could involve\n",
      "reformatting a text file (e.g., converting to XML), renaming files, retokenizing the text,\n",
      "selecting a subset of the data to enrich, and so forth. Multiple research groups might\n",
      "do this work independently, as illustrated in Figure 11-5. At a later date, should some-\n",
      "one want to combine sources of information from different versions, the task will\n",
      "probably be extremely onerous.\n",
      "Figure 11-5. Evolution of a corpus over time: After a corpus is published, research groups will use it\n",
      "independently, \n",
      "selecting and enriching different pieces; later research that seeks to integrate separate\n",
      "annotations confronts the difficult challenge of aligning the annotations.\n",
      "The task of using derived corpora is made even more difficult by the lack of any record\n",
      "about how the derived version was created, and which version is the most up-to-date.\n",
      "An alternative to this chaotic situation is for a corpus to be centrally curated, and for\n",
      "committees of experts to revise and extend it at periodic intervals, considering sub-\n",
      "missions from third parties and publishing new releases from time to time. Print dic-\n",
      "tionaries and national corpora may be centrally curated in this way. However, for most\n",
      "corpora this model is simply impractical.\n",
      "A middle course is for the original corpus publication to have a scheme for identifying\n",
      "any sub-part. Each sentence, tree, or lexical entry could have a globally unique identi-\n",
      "fier, and each token, node, or field (respectively) could have a relative offset. Annota-\n",
      "tions, including segmentations, could reference the source using this identifier scheme\n",
      "(a method which is known as standoff annotation). This way, new annotations could\n",
      "be distributed independently of the source, and multiple independent annotations of\n",
      "the same source could be compared and updated without touching the source.\n",
      "If the corpus publication is provided in multiple versions, the version number or date\n",
      "could be part of the identification scheme. A table of correspondences between\n",
      "11.2  The Life Cycle of a Corpus | 415...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 436, 'page_label': '415', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4820}\n",
      "\n",
      "--- Chunk 4821 ---\n",
      "Content:\n",
      "identifiers across editions of the corpus would permit any standoff annotations to be\n",
      "updated easily.\n",
      "Caution!\n",
      "Sometimes \n",
      "an updated corpus contains revisions of base material that\n",
      "has been externally annotated. Tokens might be split or merged, and\n",
      "constituents may have been rearranged. There may not be a one-to-one\n",
      "correspondence between old and new identifiers. It is better to cause\n",
      "standoff annotations to break on such components of the new version\n",
      "than to silently allow their identifiers to refer to incorrect locations.\n",
      "11.3  Acquiring Data\n",
      "Obtaining Data from the Web\n",
      "The Web is a rich source of data for language analysis purposes. We have already\n",
      "discussed methods for accessing individual files, RSS feeds, and search engine results\n",
      "(see Section 3.1). However, in some cases we want to obtain large quantities of web text.\n",
      "The simplest approach is to obtain a published corpus of web text. The ACL Special\n",
      "Interest Group on Web as Corpus (SIGWAC) maintains a list of resources at http://\n",
      "www.sigwac.org.uk/. The advantage of using a well-defined web corpus is that they are\n",
      "documented, stable, and permit reproducible experimentation.\n",
      "If the desired content is localized to a particular website, there are many utilities for\n",
      "capturing all the accessible contents of a site, such as GNU Wget (http://www.gnu.org/\n",
      "software/wget/). For maximal flexibility and control, a web crawler can be used, such\n",
      "as Heritrix ( http://crawler.archive.org/). Crawlers permit fine-grained control over\n",
      "where to look, which links to follow, and how to organize the results. For example, if\n",
      "we want to compile a bilingual text collection having corresponding pairs of documents\n",
      "in each language, the crawler needs to detect the structure of the site in order to extract\n",
      "the correspondence between the documents, and it needs to organize the downloaded\n",
      "pages in such a way that the correspondence is captured. It might be tempting to write\n",
      "your own web crawler, but there are dozens of pitfalls having to do with detecting\n",
      "MIME types, converting relative to absolute URLs, avoiding getting trapped in cyclic\n",
      "link structures, dealing with network latencies, avoiding overloading the site or being\n",
      "banned from accessing the site, and so on.\n",
      "Obtaining Data from Word Processor Files\n",
      "Word processing software is often used in the manual preparation of texts and lexicons\n",
      "in projects that have limited computational infrastructure. Such projects often provide\n",
      "templates for data entry, though the word processing software does not ensure that the\n",
      "data is correctly structured. For example, each text may be required to have a title and\n",
      "date. Similarly, each lexical entry may have certain obligatory fields. As the data grows\n",
      "416 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 437, 'page_label': '416', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4821}\n",
      "\n",
      "--- Chunk 4822 ---\n",
      "Content:\n",
      "in size and complexity, a larger proportion of time may be spent maintaining its con-\n",
      "sistency.\n",
      "How \n",
      "can we extract the content of such files so that we can manipulate it in external\n",
      "programs? Moreover, how can we validate the content of these files to help authors\n",
      "create well-structured data, so that the quality of the data can be maximized in the\n",
      "context of the original authoring process?\n",
      "Consider a dictionary in which each entry has a part-of-speech field, drawn from a set\n",
      "of 20 possibilities, displayed after the pronunciation field, and rendered in 11-point\n",
      "bold type. No conventional word processor has search or macro functions capable of\n",
      "verifying that all part-of-speech fields have been correctly entered and displayed. This\n",
      "task requires exhaustive manual checking. If the word processor permits the document\n",
      "to be saved in a non-proprietary format, such as text, HTML, or XML, we can some-\n",
      "times write programs to do this checking automatically.\n",
      "Consider the following fragment of a lexical entry: “sleep [sli:p] v.i. condition of body\n",
      "and mind...”. We can key in such text using MSWord, then “Save as Web Page,” then\n",
      "inspect the resulting HTML file:\n",
      "<p class=MsoNormal>sleep\n",
      "  <span style='mso-spacerun:yes'> </span>\n",
      "  [<span class=SpellE>sli:p</span>]\n",
      "  <span style='mso-spacerun:yes'> </span>\n",
      "  <b><span style='font-size:11.0pt'>v.i.</span></b>\n",
      "  <span style='mso-spacerun:yes'> </span>\n",
      "  <i>a condition of body and mind ...<o:p></o:p></i>\n",
      "</p>\n",
      "Observe that the entry is represented as an HTML paragraph, using the <p> element,\n",
      "and that the part of speech appears inside a <span style='font-size:11.0pt'> element.\n",
      "The following program defines the set of legal parts-of-speech, legal_pos. Then it ex-\n",
      "tracts all 11-point content from the dict.htm file and stores it in the set used_pos. Observe\n",
      "that the search pattern contains a parenthesized sub-expression; only the material that\n",
      "matches this subexpression is returned by re.findall. Finally, the program constructs\n",
      "the set of illegal parts-of-speech as the set difference between used_pos and legal_pos:\n",
      ">>> legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])\n",
      ">>> pattern = re.compile(r\"'font-size:11.0pt'>([a-z.]+)<\")\n",
      ">>> document = open(\"dict.htm\").read()\n",
      ">>> used_pos = set(re.findall(pattern, document))\n",
      ">>> illegal_pos = used_pos.difference(legal_pos)\n",
      ">>> print list(illegal_pos)\n",
      "['v.i', 'intrans']\n",
      "This simple program represents the tip of the iceberg. We can develop sophisticated\n",
      "tools to check the consistency of word processor files, and report errors so that the\n",
      "maintainer of the dictionary can correct the original file using the original word\n",
      "processor.\n",
      "11.3  Acquiring Data | 417...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 438, 'page_label': '417', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4822}\n",
      "\n",
      "--- Chunk 4823 ---\n",
      "Content:\n",
      "Once we know the data is correctly formatted, we can write other programs to convert\n",
      "the \n",
      "data into a different format. The program in Example 11-1 strips out the HTML\n",
      "markup using nltk.clean_html(), extracts the words and their pronunciations, and\n",
      "generates output in “comma-separated value” (CSV) format.\n",
      "Example 11-1. Converting HTML created by Microsoft Word into comma-separated values.\n",
      "def lexical_data(html_file):\n",
      "    SEP = '_ENTRY'\n",
      "    html = open(html_file).read()\n",
      "    html = re.sub(r'<p', SEP + '<p', html)\n",
      "    text = nltk.clean_html(html)\n",
      "    text = ' '.join(text.split())\n",
      "    for entry in text.split(SEP):\n",
      "        if entry.count(' ') > 2:\n",
      "            yield entry.split(' ', 3)\n",
      ">>> import csv\n",
      ">>> writer = csv.writer(open(\"dict1.csv\", \"wb\"))\n",
      ">>> writer.writerows(lexical_data(\"dict.htm\"))\n",
      "Obtaining Data from Spreadsheets and Databases\n",
      "Spreadsheets are often used for acquiring wordlists or paradigms. For example, a com-\n",
      "parative wordlist may be created using a spreadsheet, with a row for each cognate set\n",
      "and a column for each language (see nltk.corpus.swadesh and www.rosettapro\n",
      "ject.org). Most spreadsheet software can export their data in CSV format. As we will\n",
      "see later, it is easy for Python programs to access these using the csv module.\n",
      "Sometimes lexicons are stored in a full-fledged relational database. When properly\n",
      "normalized, these databases can ensure the validity of the data. For example, we can\n",
      "require that all parts-of-speech come from a specified vocabulary by declaring that the\n",
      "part-of-speech field is an enumerated type or a foreign key that references a separate\n",
      "part-of-speech table. However, the relational model requires the structure of the data\n",
      "(the schema) be declared in advance, and this runs counter to the dominant approach\n",
      "to structuring linguistic data, which is highly exploratory. Fields which were assumed\n",
      "to be obligatory and unique often turn out to be optional and repeatable. A relational\n",
      "database can accommodate this when it is fully known in advance; however, if it is not,\n",
      "or if just about every property turns out to be optional or repeatable, the relational\n",
      "approach is unworkable.\n",
      "Nevertheless, when our goal is simply to extract the contents from a database, it is\n",
      "enough to dump out the tables (or SQL query results) in CSV format and load them\n",
      "into our program. Our program might perform a linguistically motivated query that\n",
      "cannot easily be expressed in SQL, e.g., select all words that appear in example sentences\n",
      "for which no dictionary entry is provided. For this task, we would need to extract enough\n",
      "information from a record for it to be uniquely identified, along with the headwords\n",
      "and example sentences. Let’s suppose this information was now available in a CSV file\n",
      "dict.csv:\n",
      "418 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 439, 'page_label': '418', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4823}\n",
      "\n",
      "--- Chunk 4824 ---\n",
      "Content:\n",
      "\"sleep\",\"sli:p\",\"v.i\",\"a condition of body and mind ...\"\n",
      "\"walk\",\"wo:k\",\"v.intr\",\"progress by lifting and setting down each foot ...\"\n",
      "\"wake\",\"weik\",\"intrans\",\"cease to sleep\"\n",
      "Now we can express this query as shown here:\n",
      ">>> import csv\n",
      ">>> lexicon = csv.reader(open('dict.csv'))\n",
      ">>> pairs = [(lexeme, defn) for (lexeme, _, _, defn) in lexicon]\n",
      ">>> lexemes, defns = zip(*pairs)\n",
      ">>> defn_words = set(w for defn in defns for w in defn.split())\n",
      ">>> sorted(defn_words.difference(lexemes))\n",
      "['...', 'a', 'and', 'body', 'by', 'cease', 'condition', 'down', 'each',\n",
      "'foot', 'lifting', 'mind', 'of', 'progress', 'setting', 'to']\n",
      "This \n",
      "information would then guide the ongoing work to enrich the lexicon, work that\n",
      "updates the content of the relational database.\n",
      "Converting Data Formats\n",
      "Annotated linguistic data rarely arrives in the most convenient format, and it is often\n",
      "necessary to perform various kinds of format conversion. Converting between character\n",
      "encodings has already been discussed (see Section 3.3). Here we focus on the structure\n",
      "of the data.\n",
      "In the simplest case, the input and output formats are isomorphic. For instance, we\n",
      "might be converting lexical data from Toolbox format to XML, and it is straightforward\n",
      "to transliterate the entries one at a time ( Section 11.4). The structure of the data is\n",
      "reflected in the structure of the required program: a for loop whose body takes care of\n",
      "a single entry.\n",
      "In another common case, the output is a digested form of the input, such as an inverted\n",
      "file index. Here it is necessary to build an index structure in memory (see Example 4.8),\n",
      "then write it to a file in the desired format. The following example constructs an index\n",
      "that maps the words of a dictionary definition to the corresponding lexeme \n",
      " for each\n",
      "lexical entry \n",
      " , having tokenized the definition text \n",
      " , and discarded short words \n",
      " .\n",
      "Once \n",
      "the index has been constructed, we open a file and then iterate over the index\n",
      "entries, to write out the lines in the required format \n",
      " .\n",
      ">>> idx = nltk.Index((defn_word, lexeme) \n",
      "...                  for (lexeme, defn) in pairs \n",
      "...                  for defn_word in nltk.word_tokenize(defn) \n",
      "...                  if len(defn_word) > 3) \n",
      ">>> idx_file = open(\"dict.idx\", \"w\")\n",
      ">>> for word in sorted(idx):\n",
      "...     idx_words = ', '.join(idx[word])\n",
      "...     idx_line = \"%s: %s\\n\" % (word, idx_words) \n",
      "...     idx_file.write(idx_line)\n",
      ">>> idx_file.close()\n",
      "The \n",
      "resulting file dict.idx contains the following lines...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 440, 'page_label': '419', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4824}\n",
      "\n",
      "--- Chunk 4825 ---\n",
      "Content:\n",
      ".     idx_words = ', '.join(idx[word])\n",
      "...     idx_line = \"%s: %s\\n\" % (word, idx_words) \n",
      "...     idx_file.write(idx_line)\n",
      ">>> idx_file.close()\n",
      "The \n",
      "resulting file dict.idx contains the following lines. (With a larger dictionary, we\n",
      "would expect to find multiple lexemes listed for each index entry.)\n",
      "11.3  Acquiring Data | 419...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 440, 'page_label': '419', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4825}\n",
      "\n",
      "--- Chunk 4826 ---\n",
      "Content:\n",
      "body: sleep\n",
      "cease: wake\n",
      "condition: sleep\n",
      "down: walk\n",
      "each: walk\n",
      "foot: walk\n",
      "lifting: walk\n",
      "mind: sleep\n",
      "progress: walk\n",
      "setting: walk\n",
      "sleep: wake\n",
      "In \n",
      "some cases, the input and output data both consist of two or more dimensions. For\n",
      "instance, the input might be a set of files, each containing a single column of word\n",
      "frequency data. The required output might be a two-dimensional table in which the\n",
      "original columns appear as rows. In such cases we populate an internal data structure\n",
      "by filling up one column at a time, then read off the data one row at a time as we write\n",
      "data to the output file.\n",
      "In the most vexing cases, the source and target formats have slightly different coverage\n",
      "of the domain, and information is unavoidably lost when translating between them.\n",
      "For example, we could combine multiple Toolbox files to create a single CSV file con-\n",
      "taining a comparative wordlist, losing all but the \\lx field of the input files. If the CSV\n",
      "file was later modified, it would be a labor-intensive process to inject the changes into\n",
      "the original Toolbox files. A partial solution to this “round-tripping” problem is to\n",
      "associate explicit identifiers with each linguistic object, and to propagate the identifiers\n",
      "with the objects.\n",
      "Deciding Which Layers of Annotation to Include\n",
      "Published corpora vary greatly in the richness of the information they contain. At a\n",
      "minimum, a corpus will typically contain at least a sequence of sound or orthographic\n",
      "symbols. At the other end of the spectrum, a corpus could contain a large amount of\n",
      "information about the syntactic structure, morphology, prosody, and semantic content\n",
      "of every sentence, plus annotation of discourse relations or dialogue acts. These extra\n",
      "layers of annotation may be just what someone needs for performing a particular data\n",
      "analysis task. For example, it may be much easier to find a given linguistic pattern if\n",
      "we can search for specific syntactic structures; and it may be easier to categorize a\n",
      "linguistic pattern if every word has been tagged with its sense. Here are some commonly\n",
      "provided annotation layers:\n",
      "Word tokenization\n",
      "The orthographic form of text does not unambiguously identify its tokens. A to-\n",
      "kenized and normalized version, in addition to the conventional orthographic ver-\n",
      "sion, may be a very convenient resource.\n",
      "Sentence segmentation\n",
      "As we saw in Chapter 3, sentence segmentation can be more difficult than it seems.\n",
      "Some corpora therefore use explicit annotations to mark sentence segmentation.\n",
      "420 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 441, 'page_label': '420', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4826}\n",
      "\n",
      "--- Chunk 4827 ---\n",
      "Content:\n",
      "Paragraph segmentation\n",
      "Paragraphs \n",
      "and other structural elements (headings, chapters, etc.) may be explic-\n",
      "itly annotated.\n",
      "Part-of-speech\n",
      "The syntactic category of each word in a document.\n",
      "Syntactic structure\n",
      "A tree structure showing the constituent structure of a sentence.\n",
      "Shallow semantics\n",
      "Named entity and coreference annotations, and semantic role labels.\n",
      "Dialogue and discourse\n",
      "Dialogue act tags and rhetorical structure.\n",
      "Unfortunately, there is not much consistency between existing corpora in how they\n",
      "represent their annotations. However, two general classes of annotation representation\n",
      "should be distinguished. Inline annotation modifies the original document by insert-\n",
      "ing special symbols or control sequences that carry the annotated information. For\n",
      "example, when part-of-speech tagging a document, the string \"fly\" might be replaced\n",
      "with the string \"fly/NN\", to indicate that the word fly is a noun in this context. In\n",
      "contrast, standoff annotation does not modify the original document, but instead\n",
      "creates a new file that adds annotation information using pointers that reference the\n",
      "original document. For example, this new document might contain the string \"<token\n",
      "id=8 pos='NN'/>\", to indicate that token 8 is a noun.\n",
      "Standards and Tools\n",
      "For a corpus to be widely useful, it needs to be available in a widely supported format.\n",
      "However, the cutting edge of NLP research depends on new kinds of annotations,\n",
      "which by definition are not widely supported. In general, adequate tools for creation,\n",
      "publication, and use of linguistic data are not widely available. Most projects must\n",
      "develop their own set of tools for internal use, which is no help to others who lack the\n",
      "necessary resources. Furthermore, we do not have adequate, generally accepted stand-\n",
      "ards for expressing the structure and content of corpora. Without such standards, gen-\n",
      "eral-purpose tools are impossible—though at the same time, without available tools,\n",
      "adequate standards are unlikely to be developed, used, and accepted.\n",
      "One response to this situation has been to forge ahead with developing a generic format\n",
      "that is sufficiently expressive to capture a wide variety of annotation types (see Sec-\n",
      "tion 11.8 for examples). The challenge for NLP is to write programs that cope with the\n",
      "generality of such formats. For example, if the programming task involves tree data,\n",
      "and the file format permits arbitrary directed graphs, then input data must be validated\n",
      "to check for tree properties such as rootedness, connectedness, and acyclicity. If the\n",
      "input files contain other layers of annotation, the program would need to know how\n",
      "to ignore them when the data was loaded, but not invalidate or obliterate those layers\n",
      "when the tree data was saved back to the file.\n",
      "11.3  Acquiring Data | 421...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 442, 'page_label': '421', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4827}\n",
      "\n",
      "--- Chunk 4828 ---\n",
      "Content:\n",
      "Another response has been to write one-off scripts to manipulate corpus formats; such\n",
      "scripts \n",
      "litter the filespaces of many NLP researchers. NLTK’s corpus readers are a more\n",
      "systematic approach, founded on the premise that the work of parsing a corpus format\n",
      "should be done only once (per programming language).\n",
      "Instead of focusing on a common format, we believe it is more promising to develop a\n",
      "common interface (see nltk.corpus). Consider the case of treebanks, an important\n",
      "corpus type for work in NLP. There are many ways to store a phrase structure tree in\n",
      "a file. We can use nested parentheses, or nested XML elements, or a dependency no-\n",
      "tation with a (child-id, parent-id) pair on each line, or an XML version of the dependency\n",
      "notation, etc. However, in each case the logical structure is almost the same. It is much\n",
      "easier to devise a common interface that allows application programmers to write code\n",
      "to access tree data using methods such as children(), leaves(), depth(), and so forth.\n",
      "Note that this approach follows accepted practice within computer science, viz. ab-\n",
      "stract data types, object-oriented design, and the three-layer architecture ( Fig-\n",
      "ure 11-6). The last of these—from the world of relational databases—allows end-user\n",
      "applications to use a common model (the “relational model”) and a common language\n",
      "(SQL) to abstract away from the idiosyncrasies of file storage. It also allows innovations\n",
      "in filesystem technologies to occur without disturbing end-user applications. In the\n",
      "same way, a common corpus interface insulates application programs from data\n",
      "formats.\n",
      "Figure 11-6. A common format versus a common interface.\n",
      "In this context, when creating a new corpus for dissemination, it is expedient to use a\n",
      "widely \n",
      "used format wherever possible. When this is not possible, the corpus could be\n",
      "accompanied with software—such as an nltk.corpus module—that supports existing\n",
      "interface methods.\n",
      "Special Considerations When Working with Endangered Languages\n",
      "The importance of language to science and the arts is matched in significance by the\n",
      "cultural treasure embodied in language. Each of the world’s ~7,000 human languages\n",
      "422 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 443, 'page_label': '422', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4828}\n",
      "\n",
      "--- Chunk 4829 ---\n",
      "Content:\n",
      "is rich in unique respects, in its oral histories and creation legends, down to its gram-\n",
      "matical \n",
      "constructions and its very words and their nuances of meaning. Threatened\n",
      "remnant cultures have words to distinguish plant subspecies according to therapeutic\n",
      "uses that are unknown to science. Languages evolve over time as they come into contact\n",
      "with each other, and each one provides a unique window onto human pre-history. In\n",
      "many parts of the world, small linguistic variations from one town to the next add up\n",
      "to a completely different language in the space of a half-hour drive. For its breathtaking\n",
      "complexity and diversity, human language is as a colorful tapestry stretching through\n",
      "time and space.\n",
      "However, most of the world’s languages face extinction. In response to this, many\n",
      "linguists are hard at work documenting the languages, constructing rich records of this\n",
      "important facet of the world’s linguistic heritage. What can the field of NLP offer to\n",
      "help with this effort? Developing taggers, parsers, named entity recognizers, etc., is not\n",
      "an early priority, and there is usually insufficient data for developing such tools in any\n",
      "case. Instead, the most frequently voiced need is to have better tools for collecting and\n",
      "curating data, with a focus on texts and lexicons.\n",
      "On the face of things, it should be a straightforward matter to start collecting texts in\n",
      "an endangered language. Even if we ignore vexed issues such as who owns the texts,\n",
      "and sensitivities surrounding cultural knowledge contained in the texts, there is the\n",
      "obvious practical issue of transcription. Most languages lack a standard orthography.\n",
      "When a language has no literary tradition, the conventions of spelling and punctuation\n",
      "are not well established. Therefore it is common practice to create a lexicon in tandem\n",
      "with a text collection, continually updating the lexicon as new words appear in the\n",
      "texts. This work could be done using a text processor (for the texts) and a spreadsheet\n",
      "(for the lexicon). Better still, SIL’s free linguistic software Toolbox and Fieldworks\n",
      "provide sophisticated support for integrated creation of texts and lexicons.\n",
      "When speakers of the language in question are trained to enter texts themselves, a\n",
      "common obstacle is an overriding concern for correct spelling. Having a lexicon greatly\n",
      "helps this process, but we need to have lookup methods that do not assume someone\n",
      "can determine the citation form of an arbitrary word. The problem may be acute for\n",
      "languages having a complex morphology that includes prefixes. In such cases it helps\n",
      "to tag lexical items with semantic domains, and to permit lookup by semantic domain\n",
      "or by gloss.\n",
      "Permitting lookup by pronunciation similarity is also a big help. Here’s a simple dem-\n",
      "onstration of how to do this. The first step is to identify confusible letter sequences,\n",
      "and map complex versions to simpler versions. We might also notice that the relative\n",
      "order of letters within a cluster of consonants is a source of spelling errors, and so we\n",
      "normalize the order of consonants.\n",
      "11.3  Acquiring Data | 423...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 444, 'page_label': '423', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4829}\n",
      "\n",
      "--- Chunk 4830 ---\n",
      "Content:\n",
      ">>> mappings = [('ph', 'f'), ('ght', 't'), ('^kn', 'n'), ('qu', 'kw'),\n",
      "...             ('[aeiou]+', 'a'), (r'(.)\\1', r'\\1')]\n",
      ">>> def signature(word):\n",
      "...     for patt, repl in mappings:\n",
      "...         word = re.sub(patt, repl, word)\n",
      "...     pieces = re.findall('[^aeiou]+', word)\n",
      "...     return ''.join(char for piece in pieces for char in sorted(piece))[:8]\n",
      ">>> signature('illefent')\n",
      "'lfnt'\n",
      ">>> signature('ebsekwieous')\n",
      "'bskws'\n",
      ">>> signature('nuculerr')\n",
      "'nclr'\n",
      "Next, \n",
      "we create a mapping from signatures to words, for all the words in our lexicon.\n",
      "We can use this to get candidate corrections for a given input word (but we must first\n",
      "compute that word’s signature).\n",
      ">>> signatures = nltk.Index((signature(w), w) for w in nltk.corpus.words.words())\n",
      ">>> signatures[signature('nuculerr')]\n",
      "['anicular', 'inocular', 'nucellar', 'nuclear', 'unicolor', 'uniocular', 'unocular']\n",
      "Finally, we should rank the results in terms of similarity with the original word. This\n",
      "is done by the function rank(). The only remaining function provides a simple interface\n",
      "to the user:\n",
      ">>> def rank(word, wordlist):\n",
      "...     ranked = sorted((nltk.edit_dist(word, w), w) for w in wordlist)\n",
      "...     return [word for (_, word) in ranked]\n",
      ">>> def fuzzy_spell(word):\n",
      "...     sig = signature(word)\n",
      "...     if sig in signatures:\n",
      "...         return rank(word, signatures[sig])\n",
      "...     else:\n",
      "...         return []\n",
      ">>> fuzzy_spell('illefent')\n",
      "['olefiant', 'elephant', 'oliphant', 'elephanta']\n",
      ">>> fuzzy_spell('ebsekwieous')\n",
      "['obsequious']\n",
      ">>> fuzzy_spell('nucular')\n",
      "['nuclear', 'nucellar', 'anicular', 'inocular', 'unocular', 'unicolor', 'uniocular']\n",
      "This is just one illustration where a simple program can facilitate access to lexical data\n",
      "in a context where the writing system of a language may not be standardized, or where\n",
      "users of the language may not have a good command of spellings. Other simple appli-\n",
      "cations of NLP in this area include building indexes to facilitate access to data, gleaning\n",
      "wordlists from texts, locating examples of word usage in constructing a lexicon, de-\n",
      "tecting prevalent or exceptional patterns in poorly understood data, and performing\n",
      "specialized validation on data created using various linguistic software tools. We will\n",
      "return to the last of these in Section 11.5.\n",
      "424 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 445, 'page_label': '424', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4830}\n",
      "\n",
      "--- Chunk 4831 ---\n",
      "Content:\n",
      "11.4  Working with XML\n",
      "The \n",
      "Extensible Markup Language (XML) provides a framework for designing domain-\n",
      "specific markup languages. It is sometimes used for representing annotated text and\n",
      "for lexical resources. Unlike HTML with its predefined tags, XML permits us to make\n",
      "up our own tags. Unlike a database, XML permits us to create data without first spec-\n",
      "ifying its structure, and it permits us to have optional and repeatable elements. In this\n",
      "section, we briefly review some features of XML that are relevant for representing lin-\n",
      "guistic data, and show how to access data stored in XML files using Python programs.\n",
      "Using XML for Linguistic Structures\n",
      "Thanks to its flexibility and extensibility, XML is a natural choice for representing\n",
      "linguistic structures. Here’s an example of a simple lexical entry.\n",
      "(2) <entry>\n",
      "  <headword>whale</headword>\n",
      "  <pos>noun</pos>\n",
      "  <gloss>any of the larger cetacean mammals having a streamlined\n",
      "    body and breathing through a blowhole on the head</gloss>\n",
      "</entry>\n",
      "It \n",
      "consists of a series of XML tags enclosed in angle brackets. Each opening tag, such\n",
      "as <gloss>, is matched with a closing tag, </gloss>; together they constitute an XML\n",
      "element. The preceding example has been laid out nicely using whitespace, but it could\n",
      "equally have been put on a single long line. Our approach to processing XML will\n",
      "usually not be sensitive to whitespace. In order for XML to be well formed, all opening\n",
      "tags must have corresponding closing tags, at the same level of nesting (i.e., the XML\n",
      "document must be a well-formed tree).\n",
      "XML permits us to repeat elements, e.g., to add another gloss field, as we see next. We\n",
      "will use different whitespace to underscore the point that layout does not matter.\n",
      "(3) <entry><headword>whale</headword><pos>noun</pos><gloss>any of the\n",
      "larger cetacean mammals having a streamlined body and breathing\n",
      "through a blowhole on the head</gloss><gloss>a very large person;\n",
      "impressive in size or qualities</gloss></entry>\n",
      "A further step might be to link our lexicon to some external resource, such as WordNet,\n",
      "using external identifiers. In (4) we group the gloss and a synset identifier inside a new\n",
      "element, which we have called “sense.”\n",
      "(4) <entry>\n",
      "  <headword>whale</headword>\n",
      "  <pos>noun</pos>\n",
      "  <sense>\n",
      "    <gloss>any of the larger cetacean mammals having a streamlined\n",
      "      body and breathing through a blowhole on the head</gloss>\n",
      "    <synset>whale.n.02</synset>\n",
      "11.4  Working with XML | 425...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 446, 'page_label': '425', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4831}\n",
      "\n",
      "--- Chunk 4832 ---\n",
      "Content:\n",
      "</sense>\n",
      "    <gloss>a very large person; impressive in size or qualities</gloss>\n",
      "    <synset>giant.n.04</synset>\n",
      "  </sense>\n",
      "</entry>\n",
      "Alternatively, \n",
      "we could have represented the synset identifier using an XML\n",
      "attribute, without the need for any nested structure, as in (5).\n",
      "(5) <entry>\n",
      "  <headword>whale</headword>\n",
      "  <pos>noun</pos>\n",
      "  <gloss synset=\"whale.n.02\">any of the larger cetacean mammals having\n",
      "      a streamlined body and breathing through a blowhole on the head</gloss>\n",
      "  <gloss synset=\"giant.n.04\">a very large person; impressive in size or\n",
      "      qualities</gloss>\n",
      "</entry>\n",
      "This illustrates some of the flexibility of XML. If it seems somewhat arbitrary, that’s\n",
      "because it is! Following the rules of XML, we can invent new attribute names, and nest\n",
      "them as deeply as we like. We can repeat elements, leave them out, and put them in a\n",
      "different order each time. We can have fields whose presence depends on the value of\n",
      "some other field; e.g., if the part of speech is verb, then the entry can have a\n",
      "past_tense element to hold the past tense of the verb, but if the part of speech is noun,\n",
      "no past_tense element is permitted. To impose some order over all this freedom, we\n",
      "can constrain the structure of an XML file using a “schema,” which is a declaration\n",
      "akin to a context-free grammar. Tools exist for testing the validity of an XML file with\n",
      "respect to a schema.\n",
      "The Role of XML\n",
      "We can use XML to represent many kinds of linguistic information. However, the\n",
      "flexibility comes at a price. Each time we introduce a complication, such as by permit-\n",
      "ting an element to be optional or repeated, we make more work for any program that\n",
      "accesses the data. We also make it more difficult to check the validity of the data, or to\n",
      "interrogate the data using one of the XML query languages.\n",
      "Thus, using XML to represent linguistic structures does not magically solve the data\n",
      "modeling problem. We still have to work out how to structure the data, then define\n",
      "that structure with a schema, and then write programs to read and write the format\n",
      "and convert it to other formats. Similarly, we still need to follow some standard prin-\n",
      "ciples concerning data normalization. It is wise to avoid making duplicate copies of the\n",
      "same information, so that we don’t end up with inconsistent data when only one copy\n",
      "is changed. For example, a cross-reference that was represented as <xref>headword</\n",
      "xref> would duplicate the storage of the headword of some other lexical entry, and the\n",
      "link would break if the copy of the string at the other location was modified. Existential\n",
      "dependencies between information types need to be modeled, so that we can’t create\n",
      "elements without a home. For example, if sense definitions cannot exist independently\n",
      "426 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 447, 'page_label': '426', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4832}\n",
      "\n",
      "--- Chunk 4833 ---\n",
      "Content:\n",
      "of a lexical entry, the sense element can be nested inside the entry element. Many-to-\n",
      "many relations need to be abstracted out of hierarchical structures. For example, if a\n",
      "word can have many corresponding senses, and a sense can have several corresponding\n",
      "words, then both words and senses must be enumerated separately, as must the list of\n",
      "(word, sense) pairings. This complex structure might even be split across three separate\n",
      "XML files.\n",
      "As we can see, although XML provides us with a convenient format accompanied by\n",
      "an extensive collection of tools, it offers no panacea.\n",
      "The ElementTree Interface\n",
      "Python’s ElementTree module provides a convenient way to access data stored in XML\n",
      "files. ElementTree is part of Python’s standard library (since Python 2.5), and is also\n",
      "provided as part of NLTK in case you are using Python 2.4.\n",
      "We will illustrate the use of ElementTree using a collection of Shakespeare plays that\n",
      "have been formatted using XML. Let’s load the XML file and inspect the raw data, first\n",
      "at the top of the file \n",
      ", where we see some XML headers and the name of a schema\n",
      "called play.dtd, \n",
      "followed by the root element PLAY. We pick it up again at the start of\n",
      "Act 1 \n",
      " . (Some blank lines have been omitted from the output.)\n",
      ">>> merchant_file = nltk.data.find('corpora/shakespeare/merchant.xml')\n",
      ">>> raw = open(merchant_file).read()\n",
      ">>> print raw[0:168] \n",
      "<?xml version=\"1.0\"?>\n",
      "<?xml-stylesheet type=\"text/css\" href=\"shakes.css\"?>\n",
      "<!-- <!DOCTYPE PLAY SYSTEM \"play.dtd\"> -->\n",
      "<PLAY>\n",
      "<TITLE>The Merchant of Venice</TITLE>\n",
      ">>> print raw[1850:2075] \n",
      "<TITLE>ACT I</TITLE>\n",
      "<SCENE><TITLE>SCENE I.  Venice. A street.</TITLE>\n",
      "<STAGEDIR>Enter ANTONIO, SALARINO, and SALANIO</STAGEDIR>\n",
      "<SPEECH>\n",
      "<SPEAKER>ANTONIO</SPEAKER>\n",
      "<LINE>In sooth, I know not why I am so sad:</LINE>\n",
      "We \n",
      "have just accessed the XML data as a string. As we can see, the string at the start\n",
      "of Act 1 contains XML tags for title, scene, stage directions, and so forth.\n",
      "The next step is to process the file contents as structured XML data, using Element\n",
      "Tree. We are processing a file (a multiline string) and building a tree, so it’s not sur-\n",
      "prising that the method name is parse \n",
      " . The variable merchant contains an XML ele-\n",
      "ment PLAY \n",
      " . This element has internal structure; we can use an index to get its first\n",
      "child, a TITLE element \n",
      " . We can also see the text content of this element, the title of\n",
      "the play \n",
      " . To get a list of all the child elements, we use the getchildren() method \n",
      " .\n",
      ">>> from nltk.etree.ElementTree import ElementTree\n",
      ">>> merchant = ElementTree().parse(merchant_file) \n",
      ">>> merchant\n",
      "11.4  Working with XML | 427...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 448, 'page_label': '427', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4833}\n",
      "\n",
      "--- Chunk 4834 ---\n",
      "Content:\n",
      "<Element PLAY at 22fa800> \n",
      ">>> merchant[0]\n",
      "<Element TITLE at 22fa828> \n",
      ">>> merchant[0].text\n",
      "'The Merchant of Venice' \n",
      ">>> merchant.getchildren() \n",
      "[<Element TITLE at 22fa828>, <Element PERSONAE at 22fa7b0>, <Element SCNDESCR at 2300170>,\n",
      "<Element PLAYSUBT at 2300198>, <Element ACT at 23001e8>, <Element ACT at 234ec88>,\n",
      "<Element ACT at 23c87d8>, <Element ACT at 2439198>, <Element ACT at 24923c8>]\n",
      "The \n",
      "play consists of a title, the personae, a scene description, a subtitle, and five acts.\n",
      "Each act has a title and some scenes, and each scene consists of speeches which are\n",
      "made up of lines, a structure with four levels of nesting. Let’s dig down into Act IV:\n",
      ">>> merchant[-2][0].text\n",
      "'ACT IV'\n",
      ">>> merchant[-2][1]\n",
      "<Element SCENE at 224cf80>\n",
      ">>> merchant[-2][1][0].text\n",
      "'SCENE I.  Venice. A court of justice.'\n",
      ">>> merchant[-2][1][54]\n",
      "<Element SPEECH at 226ee40>\n",
      ">>> merchant[-2][1][54][0]\n",
      "<Element SPEAKER at 226ee90>\n",
      ">>> merchant[-2][1][54][0].text\n",
      "'PORTIA'\n",
      ">>> merchant[-2][1][54][1]\n",
      "<Element LINE at 226eee0>\n",
      ">>> merchant[-2][1][54][1].text\n",
      "\"The quality of mercy is not strain'd,\"\n",
      "Your Turn: Repeat some of the methods just shown, for one of the\n",
      "other Shakespeare plays included in the corpus, such as Romeo and Ju-\n",
      "liet or Macbeth. For a list, see nltk.corpus.shakespeare.fileids().\n",
      "Although we can access the entire tree this way, it is more convenient to search for sub-\n",
      "elements with particular names. Recall that the elements at the top level have several\n",
      "types. We can iterate over just the types we are interested in (such as the acts), using\n",
      "merchant.findall('ACT'). Here’s an example of doing such tag-specific searches at ev-\n",
      "ery level of nesting:\n",
      ">>> for i, act in enumerate(merchant.findall('ACT')):\n",
      "...     for j, scene in enumerate(act.findall('SCENE')):\n",
      "...         for k, speech in enumerate(scene.findall('SPEECH')):\n",
      "...             for line in speech.findall('LINE'):\n",
      "...                 if 'music' in str(line.text):\n",
      ".....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 449, 'page_label': '428', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4834}\n",
      "\n",
      "--- Chunk 4835 ---\n",
      "Content:\n",
      ".     for j, scene in enumerate(act.findall('SCENE')):\n",
      "...         for k, speech in enumerate(scene.findall('SPEECH')):\n",
      "...             for line in speech.findall('LINE'):\n",
      "...                 if 'music' in str(line.text):\n",
      "...                     print \"Act %d Scene %d Speech %d: %s\" % (i+1, j+1, k+1, line.text)\n",
      "Act 3 Scene 2 Speech 9: Let music sound while he doth make his choice;\n",
      "Act 3 Scene 2 Speech 9: Fading in music: that the comparison\n",
      "Act 3 Scene 2 Speech 9: And what is music then? Then music is\n",
      "Act 5 Scene 1 Speech 23: And bring your music forth into the air.\n",
      "Act 5 Scene 1 Speech 23: Here will we sit and let the sounds of music\n",
      "428 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 449, 'page_label': '428', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4835}\n",
      "\n",
      "--- Chunk 4836 ---\n",
      "Content:\n",
      "Act 5 Scene 1 Speech 23: And draw her home with music.\n",
      "Act 5 Scene 1 Speech 24: I am never merry when I hear sweet music.\n",
      "Act 5 Scene 1 Speech 25: Or any air of music touch their ears,\n",
      "Act 5 Scene 1 Speech 25: By the sweet power of music: therefore the poet\n",
      "Act 5 Scene 1 Speech 25: But music for the time doth change his nature.\n",
      "Act 5 Scene 1 Speech 25: The man that hath no music in himself,\n",
      "Act 5 Scene 1 Speech 25: Let no such man be trusted. Mark the music.\n",
      "Act 5 Scene 1 Speech 29: It is your music, madam, of the house.\n",
      "Act 5 Scene 1 Speech 32: No better a musician than the wren.\n",
      "Instead \n",
      "of navigating each step of the way down the hierarchy, we can search for par-\n",
      "ticular embedded elements. For example, let’s examine the sequence of speakers. We\n",
      "can use a frequency distribution to see who has the most to say:\n",
      ">>> speaker_seq = [s.text for s in merchant.findall('ACT/SCENE/SPEECH/SPEAKER')]\n",
      ">>> speaker_freq = nltk.FreqDist(speaker_seq)\n",
      ">>> top5 = speaker_freq.keys()[:5]\n",
      ">>> top5\n",
      "['PORTIA', 'SHYLOCK', 'BASSANIO', 'GRATIANO', 'ANTONIO']\n",
      "We can also look for patterns in who follows whom in the dialogues. Since there are\n",
      "23 speakers, we need to reduce the “vocabulary” to a manageable size first, using the\n",
      "method described in Section 5.3.\n",
      ">>> mapping = nltk.defaultdict(lambda: 'OTH')\n",
      ">>> for s in top5:\n",
      "...     mapping[s] = s[:4]\n",
      "...\n",
      ">>> speaker_seq2 = [mapping[s] for s in speaker_seq]\n",
      ">>> cfd = nltk.ConditionalFreqDist(nltk.ibigrams(speaker_seq2))\n",
      ">>> cfd.tabulate()\n",
      "     ANTO BASS GRAT  OTH PORT SHYL\n",
      "ANTO    0   11    4   11    9   12\n",
      "BASS   10    0   11   10   26   16\n",
      "GRAT    6    8    0   19    9    5\n",
      " OTH    8   16   18  153   52   25\n",
      "PORT    7   23   13   53    0   21\n",
      "SHYL   15   15    2   26   21    0\n",
      "Ignoring the entry of 153 for exchanges between people other than the top five, the\n",
      "largest values suggest that Othello and Portia have the most significant interactions.\n",
      "Using ElementTree for Accessing Toolbox Data\n",
      "In Section 2.4, we saw a simple interface for accessing Toolbox data, a popular and\n",
      "well-established format used by linguists for managing data. In this section, we discuss\n",
      "a variety of techniques for manipulating Toolbox data in ways that are not supported\n",
      "by the Toolbox software. The methods we discuss could be applied to other record-\n",
      "structured data, regardless of the actual file format.\n",
      "We can use the toolbox.xml() method to access a Toolbox file and load it into an\n",
      "ElementTree object...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 450, 'page_label': '429', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4836}\n",
      "\n",
      "--- Chunk 4837 ---\n",
      "Content:\n",
      ". In this section, we discuss\n",
      "a variety of techniques for manipulating Toolbox data in ways that are not supported\n",
      "by the Toolbox software. The methods we discuss could be applied to other record-\n",
      "structured data, regardless of the actual file format.\n",
      "We can use the toolbox.xml() method to access a Toolbox file and load it into an\n",
      "ElementTree object. This file contains a lexicon for the Rotokas language of Papua New\n",
      "Guinea.\n",
      "11.4  Working with XML | 429...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 450, 'page_label': '429', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4837}\n",
      "\n",
      "--- Chunk 4838 ---\n",
      "Content:\n",
      ">>> from nltk.corpus import toolbox\n",
      ">>> lexicon = toolbox.xml('rotokas.dic')\n",
      "There \n",
      "are two ways to access the contents of the lexicon object: by indexes and by\n",
      "paths. Indexes use the familiar syntax; thus lexicon[3] returns entry number 3 (which\n",
      "is actually the fourth entry counting from zero) and lexicon[3][0] returns its first field:\n",
      ">>> lexicon[3][0]\n",
      "<Element lx at 77bd28>\n",
      ">>> lexicon[3][0].tag\n",
      "'lx'\n",
      ">>> lexicon[3][0].text\n",
      "'kaa'\n",
      "The second way to access the contents of the lexicon object uses paths. The lexicon is\n",
      "a series of record objects, each containing a series of field objects, such as lx and ps.\n",
      "We can conveniently address all of the lexemes using the path record/lx. Here we use\n",
      "the findall() function to search for any matches to the path record/lx, and we access\n",
      "the text content of the element, normalizing it to lowercase:\n",
      ">>> [lexeme.text.lower() for lexeme in lexicon.findall('record/lx')]\n",
      "['kaa', 'kaa', 'kaa', 'kaakaaro', 'kaakaaviko', 'kaakaavo', 'kaakaoko',\n",
      "'kaakasi', 'kaakau', 'kaakauko', 'kaakito', 'kaakuupato', ..., 'kuvuto']\n",
      "Let’s view the Toolbox data in XML format. The write() method of ElementTree ex-\n",
      "pects a file object. We usually create one of these using Python’s built-in open() func-\n",
      "tion. In order to see the output displayed on the screen, we can use a special predefined\n",
      "file object called stdout \n",
      " (standard output), defined in Python’s sys module.\n",
      ">>> import sys\n",
      ">>> from nltk.etree.ElementTree import ElementTree\n",
      ">>> tree = ElementTree(lexicon[3])\n",
      ">>> tree.write(sys.stdout) \n",
      "<record>\n",
      "  <lx>kaa</lx>\n",
      "  <ps>N</ps>\n",
      "  <pt>MASC</pt>\n",
      "  <cl>isi</cl>\n",
      "  <ge>cooking banana</ge>\n",
      "  <tkp>banana bilong kukim</tkp>\n",
      "  <pt>itoo</pt>\n",
      "  <sf>FLORA</sf>\n",
      "  <dt>12/Aug/2005</dt>\n",
      "  <ex>Taeavi iria kaa isi kovopaueva kaparapasia.</ex>\n",
      "  <xp>Taeavi i bin planim gaden banana bilong kukim tasol long paia.</xp>\n",
      "  <xe>Taeavi planted banana in order to cook it.</xe>\n",
      "</record>\n",
      "Formatting Entries\n",
      "We \n",
      "can use the same idea we saw in the previous section to generate HTML tables\n",
      "instead of plain text. This would be useful for publishing a Toolbox lexicon on the\n",
      "Web. It produces HTML elements <table>, <tr> (table row), and <td> (table data).\n",
      "430 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 451, 'page_label': '430', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4838}\n",
      "\n",
      "--- Chunk 4839 ---\n",
      "Content:\n",
      ">>> html = \"<table>\\n\"\n",
      ">>> for entry in lexicon[70:80]:\n",
      "...     lx = entry.findtext('lx')\n",
      "...     ps = entry.findtext('ps')\n",
      "...     ge = entry.findtext('ge')\n",
      "...     html += \"  <tr><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" % (lx, ps, ge)\n",
      ">>> html += \"</table>\"\n",
      ">>> print html\n",
      "<table>\n",
      "  <tr><td>kakae</td><td>???</td><td>small</td></tr>\n",
      "  <tr><td>kakae</td><td>CLASS</td><td>child</td></tr>\n",
      "  <tr><td>kakaevira</td><td>ADV</td><td>small-like</td></tr>\n",
      "  <tr><td>kakapikoa</td><td>???</td><td>small</td></tr>\n",
      "  <tr><td>kakapikoto</td><td>N</td><td>newborn baby</td></tr>\n",
      "  <tr><td>kakapu</td><td>V</td><td>place in sling for purpose of carrying</td></tr>\n",
      "  <tr><td>kakapua</td><td>N</td><td>sling for lifting</td></tr>\n",
      "  <tr><td>kakara</td><td>N</td><td>arm band</td></tr>\n",
      "  <tr><td>Kakarapaia</td><td>N</td><td>village name</td></tr>\n",
      "  <tr><td>kakarau</td><td>N</td><td>frog</td></tr>\n",
      "</table>\n",
      "11.5  Working with Toolbox Data\n",
      "Given \n",
      "the popularity of Toolbox among linguists, we will discuss some further methods\n",
      "for working with Toolbox data. Many of the methods discussed in previous chapters,\n",
      "such as counting, building frequency distributions, and tabulating co-occurrences, can\n",
      "be applied to the content of Toolbox entries. For example, we can trivially compute\n",
      "the average number of fields for each entry:\n",
      ">>> from nltk.corpus import toolbox\n",
      ">>> lexicon = toolbox.xml('rotokas.dic')\n",
      ">>> sum(len(entry) for entry in lexicon) / len(lexicon)\n",
      "13.635955056179775\n",
      "In this section, we will discuss two tasks that arise in the context of documentary lin-\n",
      "guistics, neither of which is supported by the Toolbox software.\n",
      "Adding a Field to Each Entry\n",
      "It is often convenient to add new fields that are derived automatically from existing\n",
      "ones. Such fields often facilitate search and analysis. For instance, in Example 11-2 we\n",
      "define a function cv(), which maps a string of consonants and vowels to the corre-\n",
      "sponding CV sequence, e.g., kakapua would map to CVCVCVV. This mapping has four\n",
      "steps. First, the string is converted to lowercase, then we replace any non-alphabetic\n",
      "characters [^a-z] with an underscore. Next, we replace all vowels with V...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 452, 'page_label': '431', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4839}\n",
      "\n",
      "--- Chunk 4840 ---\n",
      "Content:\n",
      ". Such fields often facilitate search and analysis. For instance, in Example 11-2 we\n",
      "define a function cv(), which maps a string of consonants and vowels to the corre-\n",
      "sponding CV sequence, e.g., kakapua would map to CVCVCVV. This mapping has four\n",
      "steps. First, the string is converted to lowercase, then we replace any non-alphabetic\n",
      "characters [^a-z] with an underscore. Next, we replace all vowels with V. Finally, any-\n",
      "thing that is not a V or an underscore must be a consonant, so we replace it with a C.\n",
      "Now, we can scan the lexicon and add a new cv field after every lx field. Exam-\n",
      "ple 11-2 shows what this does to a particular entry; note the last line of output, which\n",
      "shows the new cv field.\n",
      "11.5  Working with Toolbox Data | 431...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 452, 'page_label': '431', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4840}\n",
      "\n",
      "--- Chunk 4841 ---\n",
      "Content:\n",
      "Example 11-2. Adding a new cv field to a lexical entry.\n",
      "from nltk.etree.ElementTree import SubElement\n",
      "def cv(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub(r'[^a-z]',     r'_', s)\n",
      "    s = re.sub(r'[aeiou]',    r'V', s)\n",
      "    s = re.sub(r'[^V_]',      r'C', s)\n",
      "    return (s)\n",
      "def add_cv_field(entry):\n",
      "    for field in entry:\n",
      "        if field.tag == 'lx':\n",
      "            cv_field = SubElement(entry, 'cv')\n",
      "            cv_field.text = cv(field.text)\n",
      ">>> lexicon = toolbox.xml('rotokas.dic')\n",
      ">>> add_cv_field(lexicon[53])\n",
      ">>> print nltk.to_sfm_string(lexicon[53])\n",
      "\\lx kaeviro\n",
      "\\ps V\n",
      "\\pt A\n",
      "\\ge lift off\n",
      "\\ge take off\n",
      "\\tkp go antap\n",
      "\\sc MOTION\n",
      "\\vx 1\n",
      "\\nt used to describe action of plane\n",
      "\\dt 03/Jun/2005\n",
      "\\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.\n",
      "\\xp Pita i go antap na lukim haus win i bagarapim.\n",
      "\\xe Peter went to look at the house that the wind destroyed.\n",
      "\\cv CVVCVCV\n",
      "If a Toolbox file is being continually updated, the program in Exam-\n",
      "ple \n",
      "11-2 will need to be run more than once. It would be possible to\n",
      "modify add_cv_field() to modify the contents of an existing entry.\n",
      "However, it is a safer practice to use such programs to create enriched\n",
      "files for the purpose of data analysis, without replacing the manually\n",
      "curated source files.\n",
      "Validating a Toolbox Lexicon\n",
      "Many lexicons in Toolbox format do not conform to any particular schema. Some\n",
      "entries may include extra fields, or may order existing fields in a new way. Manually\n",
      "inspecting thousands of lexical entries is not practicable. However, we can easily iden-\n",
      "tify frequent versus exceptional field sequences, with the help of a FreqDist:\n",
      ">>> fd = nltk.FreqDist(':'.join(field.tag for field in entry) for entry in lexicon)\n",
      ">>> fd.items()\n",
      "[('lx:ps:pt:ge:tkp:dt:ex:xp:xe', 41), ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe', 37),\n",
      "432 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 453, 'page_label': '432', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4841}\n",
      "\n",
      "--- Chunk 4842 ---\n",
      "Content:\n",
      "('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 27), ('lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe', 20),\n",
      "..., ('lx:alt:rt:ps:pt:ge:eng:eng:eng:tkp:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe', 1)]\n",
      "After \n",
      "inspecting the high-frequency field sequences, we could devise a context-free\n",
      "grammar for lexical entries. The grammar in Example 11-3 uses the CFG format we\n",
      "saw in Chapter 8. Such a grammar models the implicit nested structure of Toolbox\n",
      "entries, building a tree structure, where the leaves of the tree are individual field names.\n",
      "We iterate over the entries and report their conformance with the grammar, as shown\n",
      "in Example 11-3. Those that are accepted by the grammar are prefixed with a '+' \n",
      ",\n",
      "and \n",
      "those that are rejected are prefixed with a '-' \n",
      " . During the process of developing\n",
      "such a grammar, it helps to filter out some of the tags \n",
      " .\n",
      "Example 11-3...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 454, 'page_label': '433', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4842}\n",
      "\n",
      "--- Chunk 4843 ---\n",
      "Content:\n",
      ". Those that are accepted by the grammar are prefixed with a '+' \n",
      ",\n",
      "and \n",
      "those that are rejected are prefixed with a '-' \n",
      " . During the process of developing\n",
      "such a grammar, it helps to filter out some of the tags \n",
      " .\n",
      "Example 11-3. Validating Toolbox entries using a context-free grammar.\n",
      "grammar = nltk.parse_cfg('''\n",
      "  S -> Head PS Glosses Comment Date Sem_Field Examples\n",
      "  Head -> Lexeme Root\n",
      "  Lexeme -> \"lx\"\n",
      "  Root -> \"rt\" |\n",
      "  PS -> \"ps\"\n",
      "  Glosses -> Gloss Glosses |\n",
      "  Gloss -> \"ge\" | \"tkp\" | \"eng\"\n",
      "  Date -> \"dt\"\n",
      "  Sem_Field -> \"sf\"\n",
      "  Examples -> Example Ex_Pidgin Ex_English Examples |\n",
      "  Example -> \"ex\"\n",
      "  Ex_Pidgin -> \"xp\"\n",
      "  Ex_English -> \"xe\"\n",
      "  Comment -> \"cmt\" | \"nt\" |\n",
      "  ''')\n",
      "def validate_lexicon(grammar, lexicon, ignored_tags):\n",
      "    rd_parser = nltk.RecursiveDescentParser(grammar)\n",
      "    for entry in lexicon:\n",
      "        marker_list = [field.tag for field in entry if field.tag not in ignored_tags]\n",
      "        if rd_parser.nbest_parse(marker_list):\n",
      "            print \"+\", ':'.join(marker_list) \n",
      "        else:\n",
      "            print \"-\", ':'.join(marker_list) \n",
      ">>> lexicon = toolbox.xml('rotokas.dic')[10:20]\n",
      ">>> ignored_tags = ['arg', 'dcsv', 'pt', 'vx'] \n",
      ">>> validate_lexicon(grammar, lexicon, ignored_tags)\n",
      "- lx:ps:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe\n",
      "- lx:rt:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\n",
      "- lx:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\n",
      "- lx:ps:ge:tkp:nt:sf:dt\n",
      "- lx:ps:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe\n",
      "- lx:ps:ge:ge:ge:tkp:cmt:dt:ex:xp:xe\n",
      "- lx:rt:ps:ge:ge:tkp:dt\n",
      "- lx:rt:ps:ge:eng:eng:eng:ge:tkp:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe\n",
      "- lx:rt:ps:ge:tkp:dt:ex:xp:xe\n",
      "- lx:ps:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe\n",
      "11.5  Working with Toolbox Data | 433...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 454, 'page_label': '433', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4843}\n",
      "\n",
      "--- Chunk 4844 ---\n",
      "Content:\n",
      "Another approach would be to use a chunk parser ( Chapter 7), since these are much\n",
      "more effective at identifying partial structures and can report the partial structures that\n",
      "have been identified. In Example 11-4 we set up a chunk grammar for the entries of a\n",
      "lexicon, then parse each entry. A sample of the output from this program is shown in\n",
      "Figure 11-7.\n",
      "Figure 11-7. XML representation of a lexical entry, resulting from chunk parsing a Toolbox record.\n",
      "Example 11-4. Chunking a Toolbox lexicon: A chunk grammar describing the structure of entries for\n",
      "a lexicon for Iu Mien, a language of China.\n",
      "from nltk_contrib import toolbox\n",
      "grammar = r\"\"\"\n",
      "      lexfunc: {<lf>(<lv><ln|le>*)*}\n",
      "      example: {<rf|xv><xn|xe>*}\n",
      "      sense:   {<sn><ps><pn|gv|dv|gn|gp|dn|rn|ge|de|re>*<example>*<lexfunc>*}\n",
      "      record:   {<lx><hm><sense>+<dt>}\n",
      "    \"\"\"\n",
      ">>> from nltk.etree.ElementTree import ElementTree\n",
      ">>> db = toolbox.ToolboxData()\n",
      ">>> db.open(nltk.data.find('corpora/toolbox/iu_mien_samp.db'))\n",
      ">>> lexicon = db.parse(grammar, encoding='utf8')\n",
      ">>> toolbox.data.indent(lexicon)\n",
      ">>> tree = ElementTree(lexicon)\n",
      ">>> output = open(\"iu_mien_samp.xml\", \"w\")\n",
      ">>> tree.write(output, encoding='utf8')\n",
      ">>> output.close()\n",
      "434 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 455, 'page_label': '434', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4844}\n",
      "\n",
      "--- Chunk 4845 ---\n",
      "Content:\n",
      "11.6  Describing Language Resources Using OLAC Metadata\n",
      "Members \n",
      "of the NLP community have a common need for discovering language re-\n",
      "sources with high precision and recall. The solution which has been developed by the\n",
      "Digital Libraries community involves metadata aggregation.\n",
      "What Is Metadata?\n",
      "The simplest definition of metadata is “structured data about data.” Metadata is de-\n",
      "scriptive information about an object or resource, whether it be physical or electronic.\n",
      "Although the term “metadata” itself is relatively new, the underlying concepts behind\n",
      "metadata have been in use for as long as collections of information have been organized.\n",
      "Library catalogs represent a well-established type of metadata; they have served as col-\n",
      "lection management and resource discovery tools for decades. Metadata can be gen-\n",
      "erated either “by hand” or automatically using software.\n",
      "The Dublin Core Metadata Initiative began in 1995 to develop conventions for finding,\n",
      "sharing, and managing information. The Dublin Core metadata elements represent a\n",
      "broad, interdisciplinary consensus about the core set of elements that are likely to be\n",
      "widely useful to support resource discovery. The Dublin Core consists of 15 metadata\n",
      "elements, where each element is optional and repeatable: Title, Creator, Subject, De-\n",
      "scription, Publisher, Contributor, Date, Type, Format, Identifier, Source, Language,\n",
      "Relation, Coverage, and Rights. This metadata set can be used to describe resources\n",
      "that exist in digital or traditional formats.\n",
      "The Open Archives Initiative (OAI) provides a common framework across digital re-\n",
      "positories of scholarly materials, regardless of their type, including documents, data,\n",
      "software, recordings, physical artifacts, digital surrogates, and so forth. Each repository\n",
      "consists of a network-accessible server offering public access to archived items. Each\n",
      "item has a unique identifier, and is associated with a Dublin Core metadata record (and\n",
      "possibly additional records in other formats). The OAI defines a protocol for metadata\n",
      "search services to “harvest” the contents of repositories.\n",
      "OLAC: Open Language Archives Community\n",
      "The Open Language Archives Community, or OLAC, is an international partnership\n",
      "of institutions and individuals who are creating a worldwide virtual library of language\n",
      "resources by: (i) developing consensus on best current practices for the digital archiving\n",
      "of language resources, and (ii) developing a network of interoperating repositories and\n",
      "services for housing and accessing such resources. OLAC’s home on the Web is at http:\n",
      "//www.language-archives.org/.\n",
      "OLAC Metadata is a standard for describing language resources. Uniform description\n",
      "across repositories is ensured by limiting the values of certain metadata elements to the\n",
      "use of terms from controlled vocabularies. OLAC metadata can be used to describe\n",
      "data and tools, in both physical and digital formats. OLAC metadata extends the\n",
      "11.6  Describing Language Resources Using OLAC Metadata | 435...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 456, 'page_label': '435', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4845}\n",
      "\n",
      "--- Chunk 4846 ---\n",
      "Content:\n",
      "Dublin Core Metadata Set, a widely accepted standard for describing resources of all\n",
      "types. To this core set, OLAC adds descriptors to cover fundamental properties of\n",
      "language resources, such as subject language and linguistic type. Here’s an example of\n",
      "a complete OLAC record:\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<olac:olac xmlns:olac=\"http://www.language-archives.org/OLAC/1.1/\"\n",
      "           xmlns=\"http://purl.org/dc/elements/1.1/\"\n",
      "           xmlns:dcterms=\"http://purl.org/dc/terms/\"\n",
      "           xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
      "           xsi:schemaLocation=\"http://www.language-archives.org/OLAC/1.1/\n",
      "                http://www.language-archives.org/OLAC/1.1/olac.xsd\">\n",
      "  <title>A grammar of Kayardild. With comparative notes on Tangkic.</title>\n",
      "  <creator>Evans, Nicholas D.</creator>\n",
      "  <subject>Kayardild grammar</subject>\n",
      "  <subject xsi:type=\"olac:language\" olac:code=\"gyd\">Kayardild</subject>\n",
      "  <language xsi:type=\"olac:language\" olac:code=\"en\">English</language>\n",
      "  <description>Kayardild Grammar (ISBN 3110127954)</description>\n",
      "  <publisher>Berlin - Mouton de Gruyter</publisher>\n",
      "  <contributor xsi:type=\"olac:role\" olac:code=\"author\">Nicholas Evans</contributor>\n",
      "  <format>hardcover, 837 pages</format>\n",
      "  <relation>related to ISBN 0646119966</relation>\n",
      "  <coverage>Australia</coverage>\n",
      "  <type xsi:type=\"olac:linguistic-type\" olac:code=\"language_description\"/>\n",
      "  <type xsi:type=\"dcterms:DCMIType\">Text</type>\n",
      "</olac:olac>\n",
      "Participating language archives publish their catalogs in an XML format, and these\n",
      "records are regularly “harvested” by OLAC services using the OAI protocol. In addition\n",
      "to this software infrastructure, OLAC has documented a series of best practices for\n",
      "describing language resources, through a process that involved extended consultation\n",
      "with the language resources community (e.g., see http://www.language-archives.org/\n",
      "REC/bpr.html).\n",
      "OLAC repositories can be searched using a query engine on the OLAC website...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 457, 'page_label': '436', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4846}\n",
      "\n",
      "--- Chunk 4847 ---\n",
      "Content:\n",
      ". In addition\n",
      "to this software infrastructure, OLAC has documented a series of best practices for\n",
      "describing language resources, through a process that involved extended consultation\n",
      "with the language resources community (e.g., see http://www.language-archives.org/\n",
      "REC/bpr.html).\n",
      "OLAC repositories can be searched using a query engine on the OLAC website. Search-\n",
      "ing for “German lexicon” finds the following resources, among others:\n",
      "• CALLHOME German Lexicon, at http://www.language-archives.org/item/oai:\n",
      "www.ldc.upenn.edu:LDC97L18\n",
      "• MULTILEX multilingual lexicon, at http://www.language-archives.org/item/oai:el\n",
      "ra.icp.inpg.fr:M0001\n",
      "• Slelex Siemens Phonetic lexicon, at http://www.language-archives.org/item/oai:elra\n",
      ".icp.inpg.fr:S0048\n",
      "Searching for “Korean” finds a newswire corpus, and a treebank, a lexicon, a child-\n",
      "language corpus, and interlinear glossed texts. It also finds software, including a syn-\n",
      "tactic analyzer and a morphological analyzer.\n",
      "Observe that the previous URLs include a substring of the form:\n",
      "oai:www.ldc.upenn.edu:LDC97L18. This is an OAI identifier, using a URI scheme regis-\n",
      "tered with ICANN (the Internet Corporation for Assigned Names and Numbers). These\n",
      "436 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 457, 'page_label': '436', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4847}\n",
      "\n",
      "--- Chunk 4848 ---\n",
      "Content:\n",
      "identifiers have the format oai:archive:local_id, where oai is the name of the URI\n",
      "scheme, archive is an archive identifier, such as www.ldc.upenn.edu, and local_id is the\n",
      "resource identifier assigned by the archive, e.g., LDC97L18.\n",
      "Given an OAI identifier for an OLAC resource, it is possible to retrieve the complete\n",
      "XML record for the resource using a URL of the following form: http://www.language-\n",
      "archives.org/static-records/oai:archive:local_id.\n",
      "11.7  Summary\n",
      "• Fundamental data types, present in most corpora, are annotated texts and lexicons.\n",
      "Texts have a temporal structure, whereas lexicons have a record structure.\n",
      "• The life cycle of a corpus includes data collection, annotation, quality control, and\n",
      "publication. The life cycle continues after publication as the corpus is modified\n",
      "and enriched during the course of research.\n",
      "• Corpus development involves a balance between capturing a representative sample\n",
      "of language usage, and capturing enough material from any one source or genre to\n",
      "be useful; multiplying out the dimensions of variability is usually not feasible be-\n",
      "cause of resource limitations.\n",
      "• XML provides a useful format for the storage and interchange of linguistic data,\n",
      "but provides no shortcuts for solving pervasive data modeling problems.\n",
      "• Toolbox format is widely used in language documentation projects; we can write\n",
      "programs to support the curation of Toolbox files, and to convert them to XML.\n",
      "• The Open Language Archives Community (OLAC) provides an infrastructure for\n",
      "documenting and discovering language resources.\n",
      "11.8  Further Reading\n",
      "Extra materials for this chapter are posted at http://www.nltk.org/, including links to\n",
      "freely available resources on the Web.\n",
      "The primary sources of linguistic corpora are the Linguistic Data Consortium and the\n",
      "European Language Resources Agency, both with extensive online catalogs. More de-\n",
      "tails concerning the major corpora mentioned in the chapter are available: American\n",
      "National Corpus (Reppen, Ide & Suderman, 2005), British National Corpus (BNC,\n",
      "1999), Thesaurus Linguae Graecae (TLG, 1999), Child Language Data Exchange Sys-\n",
      "tem (CHILDES) (MacWhinney, 1995), and TIMIT (Garofolo et al., 1986).\n",
      "Two special interest groups of the Association for Computational Linguistics that or-\n",
      "ganize regular workshops with published proceedings are SIGWAC, which promotes\n",
      "the use of the Web as a corpus and has sponsored the CLEANEVAL task for removing\n",
      "HTML markup, and SIGANN, which is encouraging efforts toward interoperability of\n",
      "11.8  Further Reading | 437...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 458, 'page_label': '437', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4848}\n",
      "\n",
      "--- Chunk 4849 ---\n",
      "Content:\n",
      "linguistic annotations. An extended discussion of web crawling is provided by (Croft,\n",
      "Metzler & Strohman, 2009).\n",
      "Full \n",
      "details of the Toolbox data format are provided with the distribution (Buseman,\n",
      "Buseman & Early, 1996), and with the latest distribution freely available from http://\n",
      "www.sil.org/computing/toolbox/. For guidelines on the process of constructing a Tool-\n",
      "box lexicon, see http://www.sil.org/computing/ddp/. More examples of our efforts with\n",
      "the Toolbox are documented in (Bird, 1999) and (Robinson, Aumann & Bird, 2007).\n",
      "Dozens of other tools for linguistic data management are available, some surveyed by\n",
      "(Bird & Simons, 2003). See also the proceedings of the LaTeCH workshops on language\n",
      "technology for cultural heritage data.\n",
      "There are many excellent resources for XML (e.g., http://zvon.org/) and for writing\n",
      "Python programs to work with XML http://www.python.org/doc/lib/markup.html.\n",
      "Many editors have XML modes. XML formats for lexical information include OLIF\n",
      "(http://www.olif.net/) and LIFT (http://code.google.com/p/lift-standard/).\n",
      "For a survey of linguistic annotation software, see the Linguistic Annotation Page at\n",
      "http://www.ldc.upenn.edu/annotation/. The initial proposal for standoff annotation was\n",
      "(Thompson & McKelvie, 1997). An abstract data model for linguistic annotations,\n",
      "called “annotation graphs,” was proposed in (Bird & Liberman, 2001). A general-\n",
      "purpose ontology for linguistic description (GOLD) is documented at http://www.lin\n",
      "guistics-ontology.org/.\n",
      "For guidance on planning and constructing a corpus, see (Meyer, 2002) and (Farghaly,\n",
      "2003). More details of methods for scoring inter-annotator agreement are available in\n",
      "(Artstein & Poesio, 2008) and (Pevzner & Hearst, 2002).\n",
      "Rotokas data was provided by Stuart Robinson, and Iu Mien data was provided by Greg\n",
      "Aumann.\n",
      "For more information about the Open Language Archives Community, visit http://www\n",
      ".language-archives.org/, or see (Simons & Bird, 2003).\n",
      "11.9  Exercises\n",
      "1. ◑ In Example 11-2 the new field appeared at the bottom of the entry. Modify this\n",
      "program so that it inserts the new subelement right after the lx field. (Hint: create\n",
      "the new cv field using Element('cv'), assign a text value to it, then use the\n",
      "insert() method of the parent element.)\n",
      "2. ◑ Write a function that deletes a specified field from a lexical entry. (We could use\n",
      "this to sanitize our lexical data before giving it to others, e.g., by removing fields\n",
      "containing irrelevant or uncertain content.)\n",
      "3. ◑ Write a program that scans an HTML dictionary file to find entries having an\n",
      "illegal part-of-speech field, and then reports the headword for each entry.\n",
      "438 | Chapter 11:  Managing Linguistic Data...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 459, 'page_label': '438', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4849}\n",
      "\n",
      "--- Chunk 4850 ---\n",
      "Content:\n",
      "4. ◑ Write a program to find any parts-of-speech ( ps field) that occurred less than 10\n",
      "times. Perhaps these are typing mistakes?\n",
      "5. ◑ We saw a method for adding a cv field (Section 11.5). There is an interesting\n",
      "issue with keeping this up-to-date when someone modifies the content of the lx\n",
      "field on which it is based. Write a version of this program to add a cv field, replacing\n",
      "any existing cv field.\n",
      "6. ◑ Write a function to add a new field syl which gives a count of the number of\n",
      "syllables in the word.\n",
      "7. ◑ Write a function which displays the complete entry for a lexeme. When the\n",
      "lexeme is incorrectly spelled, it should display the entry for the most similarly\n",
      "spelled lexeme.\n",
      "8. ◑ Write a function that takes a lexicon and finds which pairs of consecutive fields\n",
      "are most frequent (e.g., ps is often followed by pt). (This might help us to discover\n",
      "some of the structure of a lexical entry.)\n",
      "9. ◑ Create a spreadsheet using office software, containing one lexical entry per row,\n",
      "consisting of a headword, a part of speech, and a gloss. Save the spreadsheet in\n",
      "CSV format. Write Python code to read the CSV file and print it in Toolbox format,\n",
      "using lx for the headword, ps for the part of speech, and gl for the gloss.\n",
      "10. ◑ Index the words of Shakespeare’s plays, with the help of nltk.Index. The result-\n",
      "ing data structure should permit lookup on individual words, such as music, re-\n",
      "turning a list of references to acts, scenes, and speeches, of the form [(3, 2, 9),\n",
      "(5, 1, 23), ...], where (3, 2, 9) indicates Act 3 Scene 2 Speech 9.\n",
      "11. ◑ Construct a conditional frequency distribution which records the word length\n",
      "for each speech in The Merchant of Venice, conditioned on the name of the char-\n",
      "acter; e.g., cfd['PORTIA'][12] would give us the number of speeches by Portia\n",
      "consisting of 12 words.\n",
      "12. ◑ Write a recursive function to convert an arbitrary NLTK tree into an XML coun-\n",
      "terpart, with non-terminals represented as XML elements, and leaves represented\n",
      "as text content, e.g.:\n",
      "<S>\n",
      "  <NP type=\"SBJ\">\n",
      "    <NP>\n",
      "      <NNP>Pierre</NNP>\n",
      "      <NNP>Vinken</NNP>\n",
      "    </NP>\n",
      "    <COMMA>,</COMMA>\n",
      "13. ● Obtain a comparative wordlist in CSV format, and write a program that prints\n",
      "those cognates having an edit-distance of at least three from each other.\n",
      "14. ● Build an index of those lexemes which appear in example sentences. Suppose\n",
      "the lexeme for a given entry is w. Then, add a single cross-reference field xrf to this\n",
      "entry, referencing the headwords of other entries having example sentences con-\n",
      "taining w. Do this for all entries and save the result as a Toolbox-format file.\n",
      "11.9  Exercises | 439...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 460, 'page_label': '439', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4850}\n",
      "\n",
      "--- Chunk 4851 ---\n",
      "Content:\n",
      "Afterword: The Language Challenge\n",
      "Natural language throws up some interesting computational challenges. We’ve ex-\n",
      "plored \n",
      "many of these in the preceding chapters, including tokenization, tagging, clas-\n",
      "sification, information extraction, and building syntactic and semantic representations.\n",
      "You should now be equipped to work with large datasets, to create robust models of\n",
      "linguistic phenomena, and to extend them into components for practical language\n",
      "technologies. We hope that the Natural Language Toolkit (NLTK) has served to open\n",
      "up the exciting endeavor of practical natural language processing to a broader audience\n",
      "than before.\n",
      "In spite of all that has come before, language presents us with far more than a temporary\n",
      "challenge for computation. Consider the following sentences which attest to the riches\n",
      "of language:\n",
      "1. Overhead the day drives level and grey, hiding the sun by a flight of grey spears.\n",
      "(William Faulkner, As I Lay Dying, 1935)\n",
      "2. When using the toaster please ensure that the exhaust fan is turned on. (sign in\n",
      "dormitory kitchen)\n",
      "3. Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated activi-\n",
      "ties with Ki values of 45.1-271.6 μM (Medline, PMID: 10718780)\n",
      "4. Iraqi Head Seeks Arms (spoof news headline)\n",
      "5. The earnest prayer of a righteous man has great power and wonderful results.\n",
      "(James 5:16b)\n",
      "6. Twas brillig, and the slithy toves did gyre and gimble in the wabe (Lewis Carroll,\n",
      "Jabberwocky, 1872)\n",
      "7. There are two ways to do this, AFAIK :smile: (Internet discussion archive)\n",
      "Other evidence for the riches of language is the vast array of disciplines whose work\n",
      "centers on language. Some obvious disciplines include translation, literary criticism,\n",
      "philosophy, anthropology, and psychology. Many less obvious disciplines investigate\n",
      "language use, including law, hermeneutics, forensics, telephony, pedagogy, archaeol-\n",
      "ogy, cryptanalysis, and speech pathology. Each applies distinct methodologies to gather\n",
      "441...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 462, 'page_label': '441', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4851}\n",
      "\n",
      "--- Chunk 4852 ---\n",
      "Content:\n",
      "observations, develop theories, and test hypotheses. All serve to deepen our under-\n",
      "standing of language and of the intellect that is manifested in language.\n",
      "In \n",
      "view of the complexity of language and the broad range of interest in studying it\n",
      "from different angles, it’s clear that we have barely scratched the surface here. Addi-\n",
      "tionally, within NLP itself, there are many important methods and applications that\n",
      "we haven’t mentioned.\n",
      "In our closing remarks we will take a broader view of NLP, including its foundations\n",
      "and the further directions you might want to explore. Some of the topics are not well\n",
      "supported by NLTK, and you might like to rectify that problem by contributing new\n",
      "software and data to the toolkit.\n",
      "Language Processing Versus Symbol Processing\n",
      "The very notion that natural language could be treated in a computational manner grew\n",
      "out of a research program, dating back to the early 1900s, to reconstruct mathematical\n",
      "reasoning using logic, most clearly manifested in work by Frege, Russell, Wittgenstein,\n",
      "Tarski, Lambek, and Carnap. This work led to the notion of language as a formal system\n",
      "amenable to automatic processing. Three later developments laid the foundation for\n",
      "natural language processing. The first was formal language theory . This defined a\n",
      "language as a set of strings accepted by a class of automata, such as context-free lan-\n",
      "guages and pushdown automata, and provided the underpinnings for computational\n",
      "syntax.\n",
      "The second development was symbolic logic. This provided a formal method for cap-\n",
      "turing selected aspects of natural language that are relevant for expressing logical\n",
      "proofs. A formal calculus in symbolic logic provides the syntax of a language, together\n",
      "with rules of inference and, possibly, rules of interpretation in a set-theoretic model;\n",
      "examples are propositional logic and first-order logic. Given such a calculus, with a\n",
      "well-defined syntax and semantics, it becomes possible to associate meanings with\n",
      "expressions of natural language by translating them into expressions of the formal cal-\n",
      "culus. For example, if we translate John saw Mary into a formula saw(j, m), we (im-\n",
      "plicitly or explicitly) interpret the English verb saw as a binary relation, and John and\n",
      "Mary as denoting individuals. More general statements like All birds fly require quan-\n",
      "tifiers, in this case ∀, meaning for all: ∀x (bird(x) \n",
      "→ fly(x)). This use of logic provided\n",
      "the technical machinery to perform inferences that are an important part of language\n",
      "understanding.\n",
      "A closely related development was the principle of compositionality , namely that\n",
      "the meaning of a complex expression is composed from the meaning of its parts and\n",
      "their mode of combination ( Chapter 10). This principle provided a useful corre-\n",
      "spondence between syntax and semantics, namely that the meaning of a complex ex-\n",
      "pression could be computed recursively. Consider the sentence It is not true that  p,\n",
      "where p is a proposition. We can represent the meaning of this sentence as not(p).\n",
      "442 | Afterword: The Language Challenge...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 463, 'page_label': '442', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4852}\n",
      "\n",
      "--- Chunk 4853 ---\n",
      "Content:\n",
      "Similarly, we can represent the meaning of John saw Mary as saw(j, m). Now we can\n",
      "compute the interpretation of It is not true that John saw Mary  recursively, using the\n",
      "foregoing information, to get not(saw(j,m)).\n",
      "The approaches just outlined share the premise that computing with natural language\n",
      "crucially relies on rules for manipulating symbolic representations. For a certain period\n",
      "in the development of NLP, particularly during the 1980s, this premise provided a\n",
      "common starting point for both linguists and practitioners of NLP, leading to a family\n",
      "of grammar formalisms known as unification-based (or feature-based) grammar (see\n",
      "Chapter 9), and to NLP applications implemented in the Prolog programming lan-\n",
      "guage. Although grammar-based NLP is still a significant area of research, it has become\n",
      "somewhat eclipsed in the last 15–20 years due to a variety of factors. One significant\n",
      "influence came from automatic speech recognition. Although early work in speech\n",
      "processing adopted a model that emulated the kind of rule-based phonological pho-\n",
      "nology processing typified by the Sound Pattern of English (Chomsky & Halle, 1968),\n",
      "this turned out to be hopelessly inadequate in dealing with the hard problem of rec-\n",
      "ognizing actual speech in anything like real time. By contrast, systems which involved\n",
      "learning patterns from large bodies of speech data were significantly more accurate,\n",
      "efficient, and robust. In addition, the speech community found that progress in building\n",
      "better systems was hugely assisted by the construction of shared resources for quanti-\n",
      "tatively measuring performance against common test data. Eventually, much of the\n",
      "NLP community embraced a data-intensive orientation to language processing, cou-\n",
      "pled with a growing use of machine-learning techniques and evaluation-led\n",
      "methodology.\n",
      "Contemporary Philosophical Divides\n",
      "The contrasting approaches to NLP described in the preceding section relate back to\n",
      "early metaphysical debates about rationalism versus empiricism and realism versus\n",
      "idealism that occurred in the Enlightenment period of Western philosophy. These\n",
      "debates took place against a backdrop of orthodox thinking in which the source of all\n",
      "knowledge was believed to be divine revelation. During this period of the 17th and 18th\n",
      "centuries, philosophers argued that human reason or sensory experience has priority\n",
      "over revelation. Descartes and Leibniz, among others, took the rationalist position,\n",
      "asserting that all truth has its origins in human thought, and in the existence of “innate\n",
      "ideas” implanted in our minds from birth. For example, they argued that the principles\n",
      "of Euclidean geometry were developed using human reason, and were not the result of\n",
      "supernatural revelation or sensory experience. In contrast, Locke and others took the\n",
      "empiricist view, that our primary source of knowledge is the experience of our faculties,\n",
      "and that human reason plays a secondary role in reflecting on that experience. Often-\n",
      "cited evidence for this position was Galileo’s discovery—based on careful observation\n",
      "of the motion of the planets—that the solar system is heliocentric and not geocentric.\n",
      "In the context of linguistics, this debate leads to the following question: to what extent\n",
      "does human linguistic experience, versus our innate “language faculty,” provide the\n",
      "Afterword: The Language Challenge | 443...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 464, 'page_label': '443', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4853}\n",
      "\n",
      "--- Chunk 4854 ---\n",
      "Content:\n",
      "basis for our knowledge of language? In NLP this issue surfaces in debates about the\n",
      "priority of corpus data versus linguistic introspection in the construction of computa-\n",
      "tional models.\n",
      "A \n",
      "further concern, enshrined in the debate between realism and idealism, was the\n",
      "metaphysical status of the constructs of a theory. Kant argued for a distinction between\n",
      "phenomena, the manifestations we can experience, and “things in themselves” which\n",
      "can never been known directly. A linguistic realist would take a theoretical construct\n",
      "like noun phrase to be a real-world entity that exists independently of human percep-\n",
      "tion and reason, and which actually causes the observed linguistic phenomena. A lin-\n",
      "guistic idealist, on the other hand, would argue that noun phrases, along with more\n",
      "abstract constructs, like semantic representations, are intrinsically unobservable, and\n",
      "simply play the role of useful fictions. The way linguists write about theories often\n",
      "betrays a realist position, whereas NLP practitioners occupy neutral territory or else\n",
      "lean toward the idealist position. Thus, in NLP, it is often enough if a theoretical ab-\n",
      "straction leads to a useful result; it does not matter whether this result sheds any light\n",
      "on human linguistic processing.\n",
      "These issues are still alive today, and show up in the distinctions between symbolic\n",
      "versus statistical methods, deep versus shallow processing, binary versus gradient clas-\n",
      "sifications, and scientific versus engineering goals. However, such contrasts are now\n",
      "highly nuanced, and the debate is no longer as polarized as it once was. In fact, most\n",
      "of the discussions—and most of the advances, even—involve a “balancing act.” For\n",
      "example, one intermediate position is to assume that humans are innately endowed\n",
      "with analogical and memory-based learning methods (weak rationalism), and use these\n",
      "methods to identify meaningful patterns in their sensory language experience (empiri-\n",
      "cism).\n",
      "We have seen many examples of this methodology throughout this book. Statistical\n",
      "methods inform symbolic models anytime corpus statistics guide the selection of pro-\n",
      "ductions in a context-free grammar, i.e., “grammar engineering.” Symbolic methods\n",
      "inform statistical models anytime a corpus that was created using rule-based methods\n",
      "is used as a source of features for training a statistical language model, i.e., “grammatical\n",
      "inference.” The circle is closed.\n",
      "NLTK Roadmap\n",
      "The Natural Language Toolkit is a work in progress, and is being continually expanded\n",
      "as people contribute code. Some areas of NLP and linguistics are not (yet) well sup-\n",
      "ported in NLTK, and contributions in these areas are especially welcome. Check http:\n",
      "//www.nltk.org/ for news about developments after the publication date of this book.\n",
      "Contributions in the following areas are particularly encouraged:\n",
      "444 | Afterword: The Language Challenge...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 465, 'page_label': '444', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4854}\n",
      "\n",
      "--- Chunk 4855 ---\n",
      "Content:\n",
      "Phonology and morphology\n",
      "Computational \n",
      "approaches to the study of sound patterns and word structures\n",
      "typically use a finite-state toolkit. Phenomena such as suppletion and non-concat-\n",
      "enative morphology are difficult to address using the string-processing methods\n",
      "we have been studying. The technical challenge is not only to link NLTK to a high-\n",
      "performance finite-state toolkit, but to avoid duplication of lexical data and to link\n",
      "the morphosyntactic features needed by morph analyzers and syntactic parsers.\n",
      "High-performance components\n",
      "Some NLP tasks are too computationally intensive for pure Python implementa-\n",
      "tions to be feasible. However, in some cases the expense arises only when training\n",
      "models, not when using them to label inputs. NLTK’s package system provides a\n",
      "convenient way to distribute trained models, even models trained using corpora\n",
      "that cannot be freely distributed. Alternatives are to develop Python interfaces to\n",
      "high-performance machine learning tools, or to expand the reach of Python by\n",
      "using parallel programming techniques such as MapReduce.\n",
      "Lexical semantics\n",
      "This is a vibrant area of current research, encompassing inheritance models of the\n",
      "lexicon, ontologies, multiword expressions, etc., mostly outside the scope of NLTK\n",
      "as it stands. A conservative goal would be to access lexical information from rich\n",
      "external stores in support of tasks in word sense disambiguation, parsing, and\n",
      "semantic interpretation.\n",
      "Natural language generation\n",
      "Producing coherent text from underlying representations of meaning is an impor-\n",
      "tant part of NLP; a unification-based approach to NLG has been developed in\n",
      "NLTK, and there is scope for more contributions in this area.\n",
      "Linguistic fieldwork\n",
      "A major challenge faced by linguists is to document thousands of endangered lan-\n",
      "guages, work which generates heterogeneous and rapidly evolving data in large\n",
      "quantities. More fieldwork data formats, including interlinear text formats and\n",
      "lexicon interchange formats, could be supported in NLTK, helping linguists to\n",
      "curate and analyze this data, while liberating them to spend as much time as pos-\n",
      "sible on data elicitation.\n",
      "Other languages\n",
      "Improved support for NLP in languages other than English could involve work in\n",
      "two areas: obtaining permission to distribute more corpora with NLTK’s data col-\n",
      "lection; and writing language-specific HOWTOs for posting at http://www.nltk\n",
      ".org/howto, illustrating the use of NLTK and discussing language-specific problems\n",
      "for NLP, including character encodings, word segmentation, and morphology.\n",
      "NLP researchers with expertise in a particular language could arrange to translate\n",
      "this book and host a copy on the NLTK website; this would go beyond translating\n",
      "the discussions to providing equivalent worked examples using data in the target\n",
      "language, a non-trivial undertaking.\n",
      "Afterword: The Language Challenge | 445...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 466, 'page_label': '445', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4855}\n",
      "\n",
      "--- Chunk 4856 ---\n",
      "Content:\n",
      "NLTK-Contrib\n",
      "Many \n",
      "of NLTK’s core components were contributed by members of the NLP com-\n",
      "munity, and were initially housed in NLTK’s “Contrib” package, nltk_contrib.\n",
      "The only requirement for software to be added to this package is that it must be\n",
      "written in Python, relevant to NLP, and given the same open source license as the\n",
      "rest of NLTK. Imperfect software is welcome, and will probably be improved over\n",
      "time by other members of the NLP community.\n",
      "Teaching materials\n",
      "Since the earliest days of NLTK development, teaching materials have accompa-\n",
      "nied the software, materials that have gradually expanded to fill this book, plus a\n",
      "substantial quantity of online materials as well. We hope that instructors who\n",
      "supplement these materials with presentation slides, problem sets, solution sets,\n",
      "and more detailed treatments of the topics we have covered will make them avail-\n",
      "able, and will notify the authors so we can link them from http://www.nltk.org/. Of\n",
      "particular value are materials that help NLP become a mainstream course in the\n",
      "undergraduate programs of computer science and linguistics departments, or that\n",
      "make NLP accessible at the secondary level, where there is significant scope for\n",
      "including computational content in the language, literature, computer science, and\n",
      "information technology curricula.\n",
      "Only a toolkit\n",
      "As stated in the preface, NLTK is a toolkit, not a system. Many problems will be\n",
      "tackled with a combination of NLTK, Python, other Python libraries, and interfaces\n",
      "to external NLP tools and formats.\n",
      "446 | Afterword: The Language Challenge...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 467, 'page_label': '446', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4856}\n",
      "\n",
      "--- Chunk 4857 ---\n",
      "Content:\n",
      "Envoi...\n",
      "Linguists \n",
      "are sometimes asked how many languages they speak, and have to explain\n",
      "that this field actually concerns the study of abstract structures that are shared by lan-\n",
      "guages, a study which is more profound and elusive than learning to speak as many\n",
      "languages as possible. Similarly, computer scientists are sometimes asked how many\n",
      "programming languages they know, and have to explain that computer science actually\n",
      "concerns the study of data structures and algorithms that can be implemented in any\n",
      "programming language, a study which is more profound and elusive than striving for\n",
      "fluency in as many programming languages as possible.\n",
      "This book has covered many topics in the field of Natural Language Processing. Most\n",
      "of the examples have used Python and English. However, it would be unfortunate if\n",
      "readers concluded that NLP is about how to write Python programs to manipulate\n",
      "English text, or more broadly, about how to write programs (in any programming lan-\n",
      "guage) to manipulate text (in any natural language). Our selection of Python and Eng-\n",
      "lish was expedient, nothing more. Even our focus on programming itself was only a\n",
      "means to an end: as a way to understand data structures and algorithms for representing\n",
      "and manipulating collections of linguistically annotated text, as a way to build new\n",
      "language technologies to better serve the needs of the information society, and ulti-\n",
      "mately as a pathway into deeper understanding of the vast riches of human language.\n",
      "But for the present: happy hacking!\n",
      "Afterword: The Language Challenge | 447...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 468, 'page_label': '447', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4857}\n",
      "\n",
      "--- Chunk 4858 ---\n",
      "Content:\n",
      "Bibliography\n",
      "[Abney, 1989] Steven P. Abney. A computational model of human parsing. Journal of\n",
      "Psycholinguistic Research, 18:129–144, 1989.\n",
      "[Abney, \n",
      "1991] Steven P. Abney. Parsing by chunks. In Robert C. Berwick, Steven P.\n",
      "Abney, and Carol Tenny, editors, Principle-Based Parsing: Computation and Psycho-\n",
      "linguistics, volume 44 of Studies in Linguistics and Philosophy. Kluwer Academic Pub-\n",
      "lishers, Dordrecht, 1991.\n",
      "[Abney, 1996a] Steven Abney. Part-of-speech tagging and partial parsing. In Ken\n",
      "Church, Steve Young, and Gerrit Bloothooft, editors, Corpus-Based Methods in Lan-\n",
      "guage and Speech. Kluwer Academic Publishers, Dordrecht, 1996.\n",
      "[Abney, 1996b] Steven Abney. Statistical methods and linguistics . In Judith Klavans\n",
      "and Philip Resnik, editors, The Balancing Act: Combining Symbolic and Statistical Ap-\n",
      "proaches to Language. MIT Press, 1996.\n",
      "[Abney, 2008] Steven Abney. Semisupervised Learning for Computational Linguistics .\n",
      "Chapman and Hall, 2008.\n",
      "[Agirre and Edmonds, 2007] Eneko Agirre and Philip Edmonds. Word Sense Disam-\n",
      "biguation: Algorithms and Applications. Springer, 2007.\n",
      "[Alpaydin, 2004] Ethem Alpaydin. Introduction to Machine Learning. MIT Press, 2004.\n",
      "[Ananiadou and McNaught, 2006] Sophia Ananiadou and John McNaught, editors.\n",
      "Text Mining for Biology and Biomedicine. Artech House, 2006.\n",
      "[Androutsopoulos et al., 1995] Ion Androutsopoulos, Graeme Ritchie, and Peter Tha-\n",
      "nisch. Natural language interfaces to databases—an introduction. Journal of Natural\n",
      "Language Engineering, 1:29–81, 1995.\n",
      "[Artstein and Poesio, 2008] Ron Artstein and Massimo Poesio. Inter-coder agreement\n",
      "for computational linguistics. Computational Linguistics, pages 555–596, 2008.\n",
      "[Baayen, 2008] Harald Baayen. Analyzing Linguistic Data: A Practical Introduction to\n",
      "Statistics Using R. Cambridge University Press, 2008.\n",
      "449...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 470, 'page_label': '449', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4858}\n",
      "\n",
      "--- Chunk 4859 ---\n",
      "Content:\n",
      "[Bachenko and Fitzpatrick, 1990] J. Bachenko and E. Fitzpatrick. A computational\n",
      "grammar of discourse-neutral prosodic phrasing in English. Computational Linguis-\n",
      "tics, 16:155–170, 1990.\n",
      "[Baldwin & Kim, 2010] Timothy Baldwin and Su Nam Kim. Multiword Expressions.\n",
      "In Nitin Indurkhya and Fred J. Damerau, editors, Handbook of Natural Language Pro-\n",
      "cessing, second edition. Morgan and Claypool, 2010.\n",
      "[Beazley, 2006] David M. Beazley. Python Essential Reference . Developer’s Library.\n",
      "Sams Publishing, third edition, 2006.\n",
      "[Biber et al., 1998] Douglas Biber, Susan Conrad, and Randi Reppen. Corpus Linguis-\n",
      "tics: Investigating Language Structure and Use. Cambridge University Press, 1998.\n",
      "[Bird, 1999] Steven Bird. Multidimensional exploration of online linguistic field data.\n",
      "In Pius Tamanji, Masako Hirotani, and Nancy Hall, editors, Proceedings of the 29th\n",
      "Annual Meeting of the Northeast Linguistics Society, pages 33–47. GLSA, University of\n",
      "Massachussetts at Amherst, 1999.\n",
      "[Bird and Liberman, 2001] Steven Bird and Mark Liberman. A formal framework for\n",
      "linguistic annotation. Speech Communication, 33:23–60, 2001.\n",
      "[Bird and Simons, 2003] Steven Bird and Gary Simons. Seven dimensions of portability\n",
      "for language documentation and description. Language, 79:557–582, 2003.\n",
      "[Blackburn and Bos, 2005] Patrick Blackburn and Johan Bos. Representation and In-\n",
      "ference for Natural Language: A First Course in Computational Semantics. CSLI Publi-\n",
      "cations, Stanford, CA, 2005.\n",
      "[BNC, 1999] BNC. British National Corpus, 1999. [http://info.ox.ac.uk/bnc/].\n",
      "[Brent and Cartwright, 1995] Michael Brent and Timothy Cartwright. Distributional\n",
      "regularity and phonotactic constraints are useful for segmentation. In Michael Brent,\n",
      "editor, Computational Approaches to Language Acquisition. MIT Press, 1995.\n",
      "[Bresnan and Hay, 2006] Joan Bresnan and Jennifer Hay. Gradient grammar: An effect\n",
      "of animacy on the syntax of give in New Zealand and American English. Lingua 118:\n",
      "254–59, 2008.\n",
      "[Budanitsky and Hirst, 2006] Alexander Budanitsky and Graeme Hirst. Evaluating\n",
      "wordnet-based measures of lexical semantic relatedness. Computational Linguistics,\n",
      "32:13–48, 2006.\n",
      "[Burton-Roberts, 1997] Noel Burton-Roberts. Analysing Sentences. Longman, 1997.\n",
      "[Buseman et al., 1996] Alan Buseman, Karen Buseman, and Rod Early. The Linguist’s\n",
      "Shoebox: Integrated Data Management and Analysis for the Field Linguist. Waxhaw NC:\n",
      "SIL, 1996.\n",
      "[Carpenter, 1992] Bob Carpenter. The Logic of Typed Feature Structures . Cambridge\n",
      "University Press, 1992.\n",
      "450 | Bibliography...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 471, 'page_label': '450', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4859}\n",
      "\n",
      "--- Chunk 4860 ---\n",
      "Content:\n",
      "[Carpenter, 1997] Bob Carpenter. Type-Logical Semantics. MIT Press, 1997.\n",
      "[Chierchia and McConnell-Ginet, 1990] Gennaro Chierchia and Sally McConnell-Gi-\n",
      "net. Meaning \n",
      "and Grammar: An Introduction to Meaning. MIT Press, Cambridge, MA,\n",
      "1990.\n",
      "[Chomsky, 1965] Noam Chomsky. Aspects of the Theory of Syntax. MIT Press, Cam-\n",
      "bridge, MA, 1965.\n",
      "[Chomsky, 1970] Noam Chomsky. Remarks on nominalization. In R. Jacobs and P.\n",
      "Rosenbaum, editors, Readings in English Transformational Grammar. Blaisdell, Wal-\n",
      "tham, MA, 1970.\n",
      "[Chomsky and Halle, 1968] Noam Chomsky and Morris Halle. The Sound Pattern of\n",
      "English. New York: Harper and Row, 1968.\n",
      "[Church and Patil, 1982] Kenneth Church and Ramesh Patil. Coping with syntactic\n",
      "ambiguity or how to put the block in the box on the table. American Journal of Com-\n",
      "putational Linguistics, 8:139–149, 1982.\n",
      "[Cohen and Hunter, 2004] K. Bretonnel Cohen and Lawrence Hunter. Natural lan-\n",
      "guage processing and systems biology. In Werner Dubitzky and Francisco Azuaje, ed-\n",
      "itors, Artificial Intelligence Methods and Tools for Systems Biology , page 147–174\n",
      "Springer Verlag, 2004.\n",
      "[Cole, 1997] Ronald Cole, editor. Survey of the State of the Art in Human Language\n",
      "Technology. Studies in Natural Language Processing. Cambridge University Press,\n",
      "1997.\n",
      "[Copestake, 2002] Ann Copestake. Implementing Typed Feature Structure Grammars.\n",
      "CSLI Publications, Stanford, CA, 2002.\n",
      "[Corbett, 2006] Greville G. Corbett. Agreement. Cambridge University Press, 2006.\n",
      "[Croft et al., 2009] Bruce Croft, Donald Metzler, and Trevor Strohman. Search Engines:\n",
      "Information Retrieval in Practice. Addison Wesley, 2009.\n",
      "[Daelemans and van den Bosch, 2005] Walter Daelemans and Antal van den Bosch.\n",
      "Memory-Based Language Processing. Cambridge University Press, 2005.\n",
      "[Dagan et al., 2006] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL\n",
      "recognising textual entailment challenge. In J. Quinonero-Candela, I. Dagan, B. Mag-\n",
      "nini, and F. d’Alché Buc, editors, Machine Learning Challenges, volume 3944 of Lecture\n",
      "Notes in Computer Science, pages 177–190. Springer, 2006.\n",
      "[Dale et al., 2000] Robert Dale, Hermann Moisl, and Harold Somers, editors. Handbook\n",
      "of Natural Language Processing. Marcel Dekker, 2000.\n",
      "[Dalrymple, 2001] Mary Dalrymple. Lexical Functional Grammar, volume 34 of Syntax\n",
      "and Semantics. Academic Press, New York, 2001.\n",
      "Bibliography | 451...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 472, 'page_label': '451', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4860}\n",
      "\n",
      "--- Chunk 4861 ---\n",
      "Content:\n",
      "[Dalrymple et al., 1999] Mary Dalrymple, V. Gupta, John Lamping, and V. Saraswat.\n",
      "Relating resource-based semantics to categorial semantics. In Mary Dalrymple, editor,\n",
      "Semantics and Syntax in Lexical Functional Grammar: The Resource Logic Approach ,\n",
      "pages 261–280. MIT Press, Cambridge, MA, 1999.\n",
      "[Dowty et al., 1981] David R. Dowty, Robert E. Wall, and Stanley Peters. Introduction\n",
      "to Montague Semantics. Kluwer Academic Publishers, 1981.\n",
      "[Earley, 1970] Jay Earley. An efficient context-free parsing algorithm. Communications\n",
      "of the Association for Computing Machinery, 13:94–102, 1970.\n",
      "[Emele and Zajac, 1990] Martin C. Emele and Rémi Zajac. Typed unification gram-\n",
      "mars. In Proceedings of the 13th Conference on Computational Linguistics, pages 293–\n",
      "298. Association for Computational Linguistics, Morristown, NJ, 1990.\n",
      "[Farghaly, 2003] Ali Farghaly, editor. Handbook for Language Engineers. CSLI Publi-\n",
      "cations, Stanford, CA, 2003.\n",
      "[Feldman and Sanger, 2007] Ronen Feldman and James Sanger. The Text Mining\n",
      "Handbook: Advanced Approaches in Analyzing Unstructured Data. Cambridge Univer-\n",
      "sity Press, 2007.\n",
      "[Fellbaum, 1998] Christiane Fellbaum, editor. WordNet: An Electronic Lexical Data-\n",
      "base. MIT Press, 1998. http://wordnet.princeton.edu/.\n",
      "[Finegan, 2007] Edward Finegan. Language: Its Structure and Use . Wadsworth, Fifth\n",
      "edition, 2007.\n",
      "[Forsyth and Martell, 2007] Eric N. Forsyth and Craig H. Martell. Lexical and discourse\n",
      "analysis of online chat dialog. In Proceedings of the First IEEE International Conference\n",
      "on Semantic Computing, pages 19–26, 2007.\n",
      "[Friedl, 2002] Jeffrey E. F. Friedl. Mastering Regular Expressions. O’Reilly, second ed-\n",
      "ition, 2002.\n",
      "[Gamut, 1991a] L. T. F. Gamut. Intensional Logic and Logical Grammar, volume 2 of\n",
      "Logic, Language and Meaning. University of Chicago Press, Chicago, 1991.\n",
      "[Gamut, 1991b] L. T. F. Gamut. Introduction to Logic, volume 1 of Logic, Language\n",
      "and Meaning. University of Chicago Press, 1991.\n",
      "[Garofolo et al., 1986] John S. Garofolo, Lori F. Lamel, William M. Fisher, Jonathon\n",
      "G. Fiscus, David S. Pallett, and Nancy L. Dahlgren. The DARPA TIMIT Acoustic-\n",
      "Phonetic Continuous Speech Corpus CDROM. NIST, 1986.\n",
      "[Gazdar et al., 1985] Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and Ivan Sag (1985).\n",
      "Generalized Phrase Structure Grammar. Basil Blackwell, 1985.\n",
      "[Gomes et al., 2006] Bruce Gomes, William Hayes, and Raf Podowski. Text mining.\n",
      "In Darryl Leon and Scott Markel, editors, In Silico Technologies in Drug Target Identi-\n",
      "fication and Validation, Taylor & Francis, 2006.\n",
      "452 | Bibliography...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 473, 'page_label': '452', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4861}\n",
      "\n",
      "--- Chunk 4862 ---\n",
      "Content:\n",
      "[Gries, 2009] Stefan Gries. Quantitative Corpus Linguistics with R: A Practical Intro-\n",
      "duction. Routledge, 2009.\n",
      "[Guzdial, 2005] Mark Guzdial. Introduction to Computing and Programming in Python:\n",
      "A Multimedia Approach. Prentice Hall, 2005.\n",
      "[Harel, 2004] David Harel. Algorithmics: The Spirit of Computing. Addison Wesley,\n",
      "2004.\n",
      "[Hastie et al., 2009] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The El-\n",
      "ements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, second\n",
      "edition, 2009.\n",
      "[Hearst, 1992] Marti Hearst. Automatic acquisition of hyponyms from large text cor-\n",
      "pora. In Proceedings of the 14th Conference on Computational Linguistics (COLING) ,\n",
      "pages 539–545, 1992.\n",
      "[Heim and Kratzer, 1998] Irene Heim and Angelika Kratzer. Semantics in Generative\n",
      "Grammar. Blackwell, 1998.\n",
      "[Hirschman et al., 2005] Lynette Hirschman, Alexander Yeh, Christian Blaschke, and\n",
      "Alfonso Valencia. Overview of BioCreAtIvE: critical assessment of information extrac\n",
      "tion for biology. BMC Bioinformatics, 6, May 2005. Supplement 1.\n",
      "[Hodges, 1977] Wilfred Hodges. Logic. Penguin Books, Harmondsworth, 1977.\n",
      "[Huddleston and Pullum, 2002] Rodney D. Huddleston and Geoffrey K. Pullum. The\n",
      "Cambridge Grammar of the English Language. Cambridge University Press, 2002.\n",
      "[Hunt and Thomas, 2000] Andrew Hunt and David Thomas. The Pragmatic Program-\n",
      "mer: From Journeyman to Master. Addison Wesley, 2000.\n",
      "[Indurkhya and Damerau, 2010] Nitin Indurkhya and Fred Damerau, editors. Hand-\n",
      "book of Natural Language Processing . CRC Press, Taylor and Francis Group, second\n",
      "edition, 2010.\n",
      "[Jackendoff, 1977] Ray Jackendoff. X-Syntax: a Study of Phrase Strucure. Number 2 in\n",
      "Linguistic Inquiry Monograph. MIT Press, Cambridge, MA, 1977.\n",
      "[Johnson, 1988] Mark Johnson. Attribute Value Logic and Theory of Grammar. CSLI\n",
      "Lecture Notes Series. University of Chicago Press, 1988.\n",
      "[Jurafsky and Martin, 2008] Daniel Jurafsky and James H. Martin. Speech and\n",
      "Language Processing. Prentice Hall, second edition, 2008.\n",
      "[Kamp and Reyle, 1993] Hans Kamp and Uwe Reyle. From Discourse to the Lexicon:\n",
      "Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Dis-\n",
      "course Representation Theory. Kluwer Academic Publishers, 1993.\n",
      "[Kaplan, 1989] Ronald Kaplan. The formal architecture of lexical-functional grammar.\n",
      "In Chu-Ren Huang and Keh-Jiann Chen, editors, Proceedings of ROCLING II, pages\n",
      "1–18. CSLI, 1989. Reprinted in Dalrymple, Kaplan, Maxwell, and Zaenen (eds), Formal\n",
      "Bibliography | 453...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 474, 'page_label': '453', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4862}\n",
      "\n",
      "--- Chunk 4863 ---\n",
      "Content:\n",
      "Issues in Lexical-Functional Grammar , pages 7–27. CSLI Publications, Stanford, CA,\n",
      "1995.\n",
      "[Kaplan and Bresnan, 1982] Ronald Kaplan and Joan Bresnan. Lexical-functional\n",
      "grammar: A formal system for grammatical representation. In Joan Bresnan, editor,\n",
      "The Mental Representation of Grammatical Relations, pages 173–281. MIT Press, Cam-\n",
      "bridge, MA, 1982.\n",
      "[Kasper and Rounds, 1986] Robert T. Kasper and William C. Rounds. A logical se-\n",
      "mantics for feature structures. In Proceedings of the 24th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics, pages 257–266. Association for Computational\n",
      "Linguistics, 1986.\n",
      "[Kathol, 1999] Andreas Kathol. Agreement and the syntax-morphology interface in\n",
      "HPSG. In Robert D. Levine and Georgia M. Green, editors, Studies in Contemporary\n",
      "Phrase Structure Grammar, pages 223–274. Cambridge University Press, 1999.\n",
      "[Kay, 1985] Martin Kay. Unification in grammar. In Verónica Dahl and Patrick Saint-\n",
      "Dizier, editors, Natural Language Understanding and Logic Programming, pages 233–\n",
      "240. North-Holland, 1985. Proceedings of the First International Workshop on Natural\n",
      "Language Understanding and Logic Programming.\n",
      "[Kiss and Strunk, 2006] Tibor Kiss and Jan Strunk. Unsupervised multilingual sentence\n",
      "boundary detection. Computational Linguistics, 32: 485–525, 2006.\n",
      "[Kiusalaas, 2005] Jaan Kiusalaas. Numerical Methods in Engineering with Python. Cam-\n",
      "bridge University Press, 2005.\n",
      "[Klein and Manning, 2003] Dan Klein and Christopher D. Manning. A* parsing: Fast\n",
      "exact viterbi parse selection. In Proceedings of HLT-NAACL 03, 2003.\n",
      "[Knuth, 2006] Donald E. Knuth. The Art of Computer Programming, Volume 4: Gen-\n",
      "erating All Trees. Addison Wesley, 2006.\n",
      "[Lappin, 1996] Shalom Lappin, editor. The Handbook of Contemporary Semantic\n",
      "Theory. Blackwell Publishers, Oxford, 1996.\n",
      "[Larson and Segal, 1995] Richard Larson and Gabriel Segal. Knowledge of Meaning: An\n",
      "Introduction to Semantic Theory. MIT Press, Cambridge, MA, 1995.\n",
      "[Levin, 1993] Beth Levin. English Verb Classes and Alternations. University of Chicago\n",
      "Press, 1993.\n",
      "[Levitin, 2004] Anany Levitin. The Design and Analysis of Algorithms. Addison Wesley,\n",
      "2004.\n",
      "[Lutz and Ascher, 2003] Mark Lutz and David Ascher. Learning Python. O’Reilly, sec-\n",
      "ond edition, 2003.\n",
      "[MacWhinney, 1995] Brian MacWhinney. The CHILDES Project: Tools for Analyzing\n",
      "Talk. Mahwah, NJ: Lawrence Erlbaum, second edition, 1995. [ http://childes.psy.cmu\n",
      ".edu/].\n",
      "454 | Bibliography...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 475, 'page_label': '454', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4863}\n",
      "\n",
      "--- Chunk 4864 ---\n",
      "Content:\n",
      "[Madnani, 2007] Nitin Madnani. Getting started on natural language processing with\n",
      "Python. ACM Crossroads, 13(4), 2007.\n",
      "[Manning, 2003] Christopher Manning. Probabilistic syntax. In Probabilistic Linguis-\n",
      "tics, pages 289–341. MIT Press, Cambridge, MA, 2003.\n",
      "[Manning and Schütze, 1999] Christopher Manning and Hinrich Schütze. Foundations\n",
      "of Statistical Natural Language Processing. MIT Press, Cambridge, MA, 1999.\n",
      "[Manning et al., 2008] Christopher Manning, Prabhakar Raghavan, and Hinrich Schü-\n",
      "tze. Introduction to Information Retrieval. Cambridge University Press, 2008.\n",
      "[McCawley, 1998] James McCawley. The Syntactic Phenomena of English. University\n",
      "of Chicago Press, 1998.\n",
      "[McConnell, 2004] Steve McConnell. Code Complete: A Practical Handbook of Software\n",
      "Construction. Microsoft Press, 2004.\n",
      "[McCune, 2008] William McCune. Prover9: Automated theorem prover for first-order\n",
      "and equational logic, 2008.\n",
      "[McEnery, 2006] Anthony McEnery. Corpus-Based Language Studies: An Advanced\n",
      "Resource Book. Routledge, 2006.\n",
      "[Melamed, 2001] Dan Melamed. Empirical Methods for Exploiting Parallel Texts. MIT\n",
      "Press, 2001.\n",
      "[Mertz, 2003] David Mertz. Text Processing in Python. Addison-Wesley, Boston, MA,\n",
      "2003.\n",
      "[Meyer, 2002] Charles Meyer. English Corpus Linguistics: An Introduction. Cambridge\n",
      "University Press, 2002.\n",
      "[Miller and Charles, 1998] George Miller and Walter Charles. Contextual correlates of\n",
      "semantic similarity. Language and Cognitive Processes, 6:1–28, 1998.\n",
      "[Mitkov, 2002a] Ruslan Mitkov. Anaphora Resolution. Longman, 2002.\n",
      "[Mitkov, 2002b] Ruslan Mitkov, editor. Oxford Handbook of Computational Linguis-\n",
      "tics. Oxford University Press, 2002.\n",
      "[Müller, 2002] Stefan Müller. Complex Predicates: Verbal Complexes, Resultative Con-\n",
      "structions, and Particle Verbs in German . Number 13 in Studies in Constraint-Based\n",
      "Lexicalism. Center for the Study of Language and Information, Stanford, 2002. http://\n",
      "www.dfki.de/~stefan/Pub/complex.html.\n",
      "[Nerbonne et al., 1994] John Nerbonne, Klaus Netter, and Carl Pollard. German in\n",
      "Head-Driven Phrase Structure Grammar. CSLI Publications, Stanford, CA, 1994.\n",
      "[Nespor and Vogel, 1986] Marina Nespor and Irene Vogel. Prosodic Phonology. Num-\n",
      "ber 28 in Studies in Generative Grammar. Foris Publications, Dordrecht, 1986.\n",
      "Bibliography | 455...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 476, 'page_label': '455', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4864}\n",
      "\n",
      "--- Chunk 4865 ---\n",
      "Content:\n",
      "[Nivre et al., 2006] J. Nivre, J. Hall, and J. Nilsson. Maltparser: A data-driven parser-\n",
      "generator for dependency parsing. In Proceedings of LREC, pages 2216–2219, 2006.\n",
      "[Niyogi, 2006] Partha Niyogi. The Computational Nature of Language Learning and\n",
      "Evolution. MIT Press, 2006.\n",
      "[O’Grady et al., 2004] William O’Grady, John Archibald, Mark Aronoff, and Janie\n",
      "Rees-Miller. Contemporary Linguistics: An Introduction. St. Martin’s Press, fifth edition,\n",
      "2004.\n",
      "[OSU, 2007] OSU, editor. Language Files: Materials for an Introduction to Language\n",
      "and Linguistics. Ohio State University Press, tenth edition, 2007.\n",
      "[Partee, 1995] Barbara Partee. Lexical semantics and compositionality. In L. R. Gleit-\n",
      "man and M. Liberman, editors, An Invitation to Cognitive Science: Language , volume\n",
      "1, pages 311–360. MIT Press, 1995.\n",
      "[Pasca, 2003] Marius Pasca. Open-Domain Question Answering from Large Text Col-\n",
      "lections. CSLI Publications, Stanford, CA, 2003.\n",
      "[Pevzner and Hearst, 2002] L. Pevzner and M. Hearst. A critique and improvement of\n",
      "an evaluation metric for text segmentation. Computational Linguistics, 28:19–36, 2002.\n",
      "[Pullum, 2005] Geoffrey K. Pullum. Fossilized prejudices about “however”, 2005.\n",
      "[Radford, 1988] Andrew Radford. Transformational Grammar: An Introduction. Cam-\n",
      "bridge University Press, 1988.\n",
      "[Ramshaw and Marcus, 1995] Lance A. Ramshaw and Mitchell P. Marcus. Text chunk-\n",
      "ing using transformation-based learning. In Proceedings of the Third ACL Workshop on\n",
      "Very Large Corpora, pages 82–94, 1995.\n",
      "[Reppen et al., 2005] Randi Reppen, Nancy Ide, and Keith Suderman. American Na\n",
      "tional Corpus. Linguistic Data Consortium, 2005.\n",
      "[Robinson et al., 2007] Stuart Robinson, Greg Aumann, and Steven Bird. Managing\n",
      "fieldwork data with toolbox and the natural language toolkit. Language Documentation\n",
      "and Conservation, 1:44–57, 2007.\n",
      "[Sag and Wasow, 1999] Ivan A. Sag and Thomas Wasow. Syntactic Theory: A Formal\n",
      "Introduction. CSLI Publications, Stanford, CA, 1999.\n",
      "[Sampson and McCarthy, 2005] Geoffrey Sampson and Diana McCarthy. Corpus Lin-\n",
      "guistics: Readings in a Widening Discipline. Continuum, 2005.\n",
      "[Scott and Tribble, 2006] Mike Scott and Christopher Tribble. Textual Patterns: Key\n",
      "Words and Corpus Analysis in Language Education. John Benjamins, 2006.\n",
      "[Segaran, 2007] Toby Segaran. Collective Intelligence. O’Reilly Media, 2007.\n",
      "[Shatkay and Feldman, 2004] Hagit Shatkay and R. Feldman. Mining the biomedical\n",
      "literature in the genomic era: An overview. Journal of Computational Biology, 10:821–\n",
      "855, 2004.\n",
      "456 | Bibliography...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 477, 'page_label': '456', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4865}\n",
      "\n",
      "--- Chunk 4866 ---\n",
      "Content:\n",
      "[Shieber, 1986] Stuart M. Shieber. An Introduction to Unification-Based Approaches to\n",
      "Grammar, volume 4 of CSLI Lecture Notes Series .CSLI Publications, Stanford, CA,\n",
      "1986.\n",
      "[Shieber et al., 1983] Stuart Shieber, Hans Uszkoreit, Fernando Pereira, Jane Robinson,\n",
      "and Mabry Tyson. The formalism and implementation of PATR-II. In Barbara J. Grosz\n",
      "and Mark Stickel, editors, Research on Interactive Acquisition and Use of Knowledge ,\n",
      "techreport 4, pages 39–79. SRI International, Menlo Park, CA, November 1983. (http:\n",
      "//www.eecs.harvard.edu/ shieber/Biblio/Papers/Shieber-83-FIP.pdf)\n",
      "[Simons and Bird, 2003] Gary Simons and Steven Bird. The Open Language Archives\n",
      "Community: An infrastructure for distributed archiving of language resources. Literary\n",
      "and Linguistic Computing, 18:117–128, 2003.\n",
      "[Sproat et al., 2001] Richard Sproat, Alan Black, Stanley Chen, Shankar Kumar, Mari\n",
      "Ostendorf, and Christopher Richards. Normalization of non-standard words. Com-\n",
      "puter Speech and Language, 15:287–333, 2001.\n",
      "[Strunk and White, 1999] William Strunk and E. B. White. The Elements of Style. Bos-\n",
      "ton, Allyn and Bacon, 1999.\n",
      "[Thompson and McKelvie, 1997] Henry S. Thompson and David McKelvie. Hyperlink\n",
      "semantics for standoff markup of read-only documents. In SGML Europe ’97 , 1997.\n",
      "http://www.ltg.ed.ac.uk/~ht/sgmleu97.html.\n",
      "[TLG, 1999] TLG. Thesaurus Linguae Graecae, 1999.\n",
      "[Turing, 1950] Alan M. Turing. Computing machinery and intelligence. Mind, 59(236):\n",
      "433–460, 1950.\n",
      "[van Benthem and ter Meulen, 1997] Johan van Benthem and Alice ter Meulen, editors.\n",
      "Handbook of Logic and Language. MIT Press, Cambridge, MA, 1997.\n",
      "[van Rossum and Drake, 2006a] Guido van Rossum and Fred L. Drake. An Introduction\n",
      "to Python—The Python Tutorial. Network Theory Ltd, Bristol, 2006.\n",
      "[van Rossum and Drake, 2006b] Guido van Rossum and Fred L. Drake. The Python\n",
      "Language Reference Manual. Network Theory Ltd, Bristol, 2006.\n",
      "[Warren and Pereira, 1982] David H. D. Warren and Fernando C. N. Pereira. An effi-\n",
      "cient easily adaptable system for interpreting natural language queries. American Jour-\n",
      "nal of Computational Linguistics, 8(3-4):110–122, 1982.\n",
      "[Wechsler and Zlatic, 2003] Stephen Mark Wechsler and Larisa Zlatic. The Many Faces\n",
      "of Agreement. Stanford Monographs in Linguistics. CSLI Publications, Stanford, CA,\n",
      "2003.\n",
      "[Weiss et al., 2004] Sholom Weiss, Nitin Indurkhya, Tong Zhang, and Fred Damerau.\n",
      "Text Mining: Predictive Methods for Analyzing Unstructured Information . Springer,\n",
      "2004.\n",
      "Bibliography | 457...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 478, 'page_label': '457', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4866}\n",
      "\n",
      "--- Chunk 4867 ---\n",
      "Content:\n",
      "[Woods et al., 1986] Anthony Woods, Paul Fletcher, and Arthur Hughes. Statistics in\n",
      "Language Studies. Cambridge University Press, 1986.\n",
      "[Zhao and Zobel, 2007] Y. Zhao and J. Zobel. Search with style: Authorship attribution\n",
      "in classic literature. In Proceedings of the Thirtieth Australasian Computer Science Con-\n",
      "ference. Association for Computing Machinery, 2007.\n",
      "458 | Bibliography...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 479, 'page_label': '458', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4867}\n",
      "\n",
      "--- Chunk 4868 ---\n",
      "Content:\n",
      "NLTK Index\n",
      "Symbols\n",
      "A\n",
      "abspath, 50\n",
      "accuracy, 119, 149, 217\n",
      "AnaphoraResolutionException, 401\n",
      "AndExpression, 369\n",
      "append, 11, 86, 127, 197\n",
      "ApplicationExpression, 405\n",
      "apply, 10\n",
      "apply_features, 224\n",
      "Assignment, 378\n",
      "assumptions, 383\n",
      "B\n",
      "babelize_shell, 30\n",
      "background, 21\n",
      "backoff, 200, 201, 205, 208\n",
      "batch_evaluate, 393\n",
      "batch_interpret, 393\n",
      "bigrams, 20, 55, 56, 141\n",
      "BigramTagger, 274\n",
      "BracketParseCorpusReader, 51\n",
      "build_model, 383\n",
      "C\n",
      "chart, 168\n",
      "Chat, 4\n",
      "chat, 105, 163, 215\n",
      "chat80, 363\n",
      "chat80.sql_query, 363\n",
      "child, 82, 162, 170, 180, 281, 316, 334, 431\n",
      "children, 187, 334, 335, 422\n",
      "chunk, 267, 273, 275, 277\n",
      "ChunkParserI, 273\n",
      "classifier, 223, 224, 225, 226, 227, 228, 229,\n",
      "231, 234, 235, 239\n",
      "classify, 228\n",
      "collocations, 20, 21\n",
      "common_contexts, 5, 6\n",
      "concordance, 4, 108\n",
      "ConditionalFreqDist, 52, 53, 56\n",
      "conditions, 44, 53, 54, 55, 56\n",
      "conlltags2tree, 273\n",
      "ConstantExpression, 373\n",
      "context, 5, 108, 180, 210\n",
      "CooperStore, 396\n",
      "cooper_storage, 396\n",
      "corpora, 51, 85, 94, 270, 363, 427, 434\n",
      "corpus, 40, 51, 241, 284, 285, 315\n",
      "correct, 210, 226\n",
      "count, 9, 119, 139, 224, 225\n",
      "D\n",
      "data, 46, 147, 188\n",
      "default, 193, 199\n",
      "display, 200, 201, 308, 309\n",
      "distance, 155\n",
      "draw, 265, 280, 323, 398, 429\n",
      "draw_trees, 324\n",
      "DRS, 398\n",
      "DrtParser, 400\n",
      "E\n",
      "edit_distance, 155\n",
      "Element, 427, 428, 430, 438\n",
      "ElementTree, 427, 430, 434\n",
      "ellipsis, 111\n",
      "em, 67\n",
      "encoding, 50, 95, 96, 434, 436\n",
      "entries, 63, 64, 66, 316, 433\n",
      "entropy, 244\n",
      "We’d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.\n",
      "459...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 480, 'page_label': '459', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4868}\n",
      "\n",
      "--- Chunk 4869 ---\n",
      "Content:\n",
      "entry, 63, 316, 418, 419, 425, 426, 427, 431,\n",
      "432, 433\n",
      "error, 14, 65\n",
      "evaluate, 115, 216, 217, 371, 379, 380\n",
      "Expression, 369, 375, 376, 399\n",
      "extract_property, 149, 150, 152\n",
      "F\n",
      "FeatStruct, 337\n",
      "feed, 83\n",
      "fileid, 40, 41, 42, 45, 46, 50, 54, 62, 227, 288\n",
      "filename, 125, 289\n",
      "findall, 105, 127, 430\n",
      "fol, 399\n",
      "format, 117, 120, 121, 157, 419, 436\n",
      "freq, 17, 21, 213\n",
      "FreqDist, \n",
      "17, 18, 21, 22, 36, 52, 53, 56, 59, 61,\n",
      "135, 147, 153, 177, 185, 432\n",
      "freqdist, 61, 147, 148, 153, 244\n",
      "G\n",
      "generate, 6, 7\n",
      "get, 68, 185, 194\n",
      "getchildren, 427, 428\n",
      "grammar, 265, 267, 269, 272, 278, 308, 311, 317,\n",
      "320, 321, 396, 433, 434, 436\n",
      "Grammar, 320, 334, 351, 354, 436\n",
      "H\n",
      "hole, 99\n",
      "hyp_extra, 236\n",
      "I\n",
      "ic, 176\n",
      "ieer, 284\n",
      "IffExpression, 369\n",
      "index, 13, 14, 16, 24, 90, 127, 134, 308\n",
      "inference, 370\n",
      "J\n",
      "jaccard_distance, 155\n",
      "K\n",
      "keys, 17, 192\n",
      "L\n",
      "LambdaExpression, 387\n",
      "lancaster, 107\n",
      "leaves, 51, 71, 422\n",
      "Lemma, 68, 71\n",
      "lemma, 68, 69, 214\n",
      "lemmas, 68\n",
      "length, 25, 61, 136, 149\n",
      "load, 124, 206\n",
      "load_corpus, 147\n",
      "load_earley, 335, 352, 355, 363, 392, 400\n",
      "load_parser, 334\n",
      "logic, 376, 389\n",
      "LogicParser, 369, 370, 373, 375, 388, 400,\n",
      "404\n",
      "M\n",
      "Mace, 383\n",
      "MaceCommand, 383\n",
      "maxent, 275\n",
      "megam, 275\n",
      "member_holonyms, 70, 74\n",
      "member_meronyms, 74\n",
      "metrics, 154, 155\n",
      "model, 200, 201\n",
      "Model, 201, 382\n",
      "N\n",
      "nbest_parse, 334\n",
      "ne, 236, 237, 283\n",
      "NegatedExpression, 369\n",
      "ngrams, 141\n",
      "NgramTagger, 204\n",
      "nltk.chat.chatbots, 31\n",
      "nltk.classify, 224\n",
      "nltk.classify.rte_classify, 237\n",
      "nltk.cluster, 172\n",
      "nltk.corpus, 40, 42, 43, 44, 45, 48, 51, 53, 54,\n",
      "60, 65, 67, 90, 105, 106, 119, 162,\n",
      "170, 184, 188, 195, 198, 203, 223,\n",
      "227, 258, 259, 271, 272, 285, 315,\n",
      "316, 422, 430, 431\n",
      "nltk.data.find, 85, 94, 427, 434\n",
      "nltk.data.load, 112, 300, 334\n",
      "nltk.data.show_cfg, 334, 351, 354, 363\n",
      "nltk.downloader, 316\n",
      "nltk.draw.tree, 324\n",
      "nltk.etree.ElementTree, 427, 430, 432, 434\n",
      "nltk.grammar, 298\n",
      "nltk.help.brown_tagset, 214...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 481, 'page_label': '460', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4869}\n",
      "\n",
      "--- Chunk 4870 ---\n",
      "Content:\n",
      "170, 184, 188, 195, 198, 203, 223,\n",
      "227, 258, 259, 271, 272, 285, 315,\n",
      "316, 422, 430, 431\n",
      "nltk.data.find, 85, 94, 427, 434\n",
      "nltk.data.load, 112, 300, 334\n",
      "nltk.data.show_cfg, 334, 351, 354, 363\n",
      "nltk.downloader, 316\n",
      "nltk.draw.tree, 324\n",
      "nltk.etree.ElementTree, 427, 430, 432, 434\n",
      "nltk.grammar, 298\n",
      "nltk.help.brown_tagset, 214\n",
      "nltk.help.upenn_tagset, 180, 214\n",
      "nltk.inference.discourse, 400\n",
      "nltk.metrics.agreement, 414\n",
      "nltk.metrics.distance, 155\n",
      "nltk.parse, 335, 352, 363, 392, 400\n",
      "nltk.probability, 219\n",
      "nltk.sem, 363, 396\n",
      "nltk.sem.cooper_storage, 396\n",
      "460 | NLTK Index...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 481, 'page_label': '460', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4870}\n",
      "\n",
      "--- Chunk 4871 ---\n",
      "Content:\n",
      "nltk.sem.drt_resolve_anaphora, 399\n",
      "nltk.tag, 401\n",
      "nltk.tag.brill.demo, 210, 218\n",
      "nltk.text.Text, 81\n",
      "node, 170\n",
      "nps_chat, 42, 105, 235\n",
      "O\n",
      "olac, 436\n",
      "OrExpression, 369\n",
      "P\n",
      "packages, 154\n",
      "parse, 273, 275, 320, 375, 398, 427\n",
      "parsed, 51, 373\n",
      "ParseI, 326\n",
      "parse_valuation, 378\n",
      "part_holonyms, 74\n",
      "part_meronyms, 70, 74\n",
      "path, 85, 94, 95, 96\n",
      "path_similarity, 72\n",
      "phones, 408\n",
      "phonetic, 408, 409\n",
      "PlaintextCorpusReader, 51\n",
      "porter, 107, 108\n",
      "posts, 65, 235\n",
      "ppattach, 259\n",
      "PPAttachment, 258, 259\n",
      "productions, 308, 311, 320, 334\n",
      "prove, 376\n",
      "Prover9, 376\n",
      "punkt, 112\n",
      "R\n",
      "RecursiveDescentParser, 302, 304\n",
      "regexp, 102, 103, 105, 122\n",
      "RegexpChunk, 287\n",
      "RegexpParser, 266, 286\n",
      "RegexpTagger, 217, 219, 401\n",
      "regexp_tokenize, 111\n",
      "resolve_anaphora, 399\n",
      "reverse, 195\n",
      "rte_features, 236\n",
      "S\n",
      "samples, 22, 44, 54, 55, 56\n",
      "satisfiers, 380, 382\n",
      "satisfy, 155\n",
      "score, 115, 272, 273, 274, 276, 277\n",
      "search, 177\n",
      "SEM, 362, 363, 385, 386, 390, 393, 395, 396,\n",
      "403\n",
      "sem, 363, 396, 400\n",
      "sem.evaluate, 406\n",
      "Senseval, 257\n",
      "senseval, 258\n",
      "ShiftReduceParser, 305\n",
      "show_clause, 285\n",
      "show_most_informative_features, 228\n",
      "show_raw_rtuple, 285\n",
      "similar, 5, 6, 21, 319\n",
      "simplify, 388\n",
      "sort, 12, 136, 192\n",
      "SpeakerInfo, 409\n",
      "sr, 65\n",
      "State, 20, 187\n",
      "stem, 104, 105\n",
      "str2tuple, 181\n",
      "SubElement, 432\n",
      "substance_holonyms, 74\n",
      "substance_meronyms, 70, 74\n",
      "Synset, 67, 68, 69, 70, 71, 72\n",
      "synset, 68, 69, 70, 71, 425, 426\n",
      "s_retrieve, 396\n",
      "T\n",
      "tabulate, 54, 55, 119\n",
      "tag, 146, 164, 181, 184, 185, 186, 187, 188, 189,\n",
      "195, 196, 198, 207, 210, 226, 231,\n",
      "232, 233, 241, 273, 275\n",
      "tagged_sents, 183, 231, 233, 238, 241, 275\n",
      "tagged_words, 182, 187, 229\n",
      "tags, 135, 164, 188, 198, 210, 277, 433\n",
      "Text, 4, 284, 436\n",
      "token, 26, 105, 139, 319, 421\n",
      "tokenize, 263\n",
      "tokens, 16, 80, 81, 82, 86, 105, 107, 108, 111,\n",
      "139, 140, 153, 198, 206, 234, 308,\n",
      "309, 317, 328, 335, 352, 353, 355,\n",
      "392\n",
      "toolbox, 66, 67, 430, 431, 434, 438\n",
      "toolbox.ToolboxData, 434\n",
      "train, 112, 225\n",
      "translate, 66, 74\n",
      "tree, 268, 294, 298, 300, 301, 311, 316, 317,...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 482, 'page_label': '461', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4871}\n",
      "\n",
      "--- Chunk 4872 ---\n",
      "Content:\n",
      "Text, 4, 284, 436\n",
      "token, 26, 105, 139, 319, 421\n",
      "tokenize, 263\n",
      "tokens, 16, 80, 81, 82, 86, 105, 107, 108, 111,\n",
      "139, 140, 153, 198, 206, 234, 308,\n",
      "309, 317, 328, 335, 352, 353, 355,\n",
      "392\n",
      "toolbox, 66, 67, 430, 431, 434, 438\n",
      "toolbox.ToolboxData, 434\n",
      "train, 112, 225\n",
      "translate, 66, 74\n",
      "tree, 268, 294, 298, 300, 301, 311, 316, 317,\n",
      "319, 335, 352, 353, 355, 393, 430,\n",
      "434\n",
      "Tree, 315, 322\n",
      "Tree.productions, 325\n",
      "tree2conlltags, 273\n",
      "treebank, 51, 315, 316\n",
      "trees, 294, 311, 334, 335, 363, 392, 393, 396,\n",
      "400\n",
      "trigrams, 141\n",
      "TrigramTagger, 205\n",
      "tuples, 192\n",
      "NLTK Index | 461...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 482, 'page_label': '461', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4872}\n",
      "\n",
      "--- Chunk 4873 ---\n",
      "Content:\n",
      "turns, 12\n",
      "Type, 2, 4, 169\n",
      "U\n",
      "Undefined, 379\n",
      "unify, 342\n",
      "UnigramTagger, 200, 203, 219, 274\n",
      "url, 80, 82, 147, 148\n",
      "V\n",
      "Valuation, 371, 378\n",
      "values, 149, 192\n",
      "Variable, 375\n",
      "VariableBinderExpression, 389\n",
      "W\n",
      "wordlist, 61, 64, 98, 99, 111, 201, 424\n",
      "wordnet, 67, 162, 170\n",
      "X\n",
      "xml, 427, 436\n",
      "xml_posts, 235\n",
      "462 | NLTK Index...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 483, 'page_label': '462', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4873}\n",
      "\n",
      "--- Chunk 4874 ---\n",
      "Content:\n",
      "General Index\n",
      "Symbols\n",
      "! (exclamation mark)\n",
      "!= (not equal to) operator, 22, 376\n",
      "\" \" (quotation marks, double), in strings, 87\n",
      "$ (dollar sign) in regular expressions, 98, 101\n",
      "% (percent sign)\n",
      "%% in string formatting, 119\n",
      "%*s formatting string, 107, 119\n",
      "%s and %d conversion specifiers, 118\n",
      "& (ampersand), and operator, 368\n",
      "' ' (quotation marks, single) in strings, 88\n",
      "' ' (quotation marks, single), in strings, 87\n",
      "' (apostrophe) in tokenization, 110\n",
      "( ) (parentheses)\n",
      "adding extra to break lines of code, 139\n",
      "enclosing expressions in Python, 2\n",
      "in function names, 9\n",
      "in regular expressions, 100, 104\n",
      "in tuples, 134\n",
      "use with strings, 88\n",
      "* (asterisk)\n",
      "*? non-greedy matching in regular\n",
      "expressions, 104\n",
      "multiplication operator, 2\n",
      "multiplying strings, 88\n",
      "in regular expressions, 100, 101\n",
      "+ (plus sign)\n",
      "+= (addition and assignment) operator,\n",
      "195\n",
      "concatenating lists, 11\n",
      "concatenating strings, 16, 88\n",
      "in regular expressions, 100, 101\n",
      ", (comma) operator, 133\n",
      "- (hyphen) in tokenization, 110\n",
      "- (minus sign), negation operator, 368\n",
      "-> (implication) operator, 368\n",
      ". (dot) wildcard character in regular\n",
      "expressions, 98, 101\n",
      "/ (slash),\n",
      "division operator, 2\n",
      ": (colon), ending Python statements, 26\n",
      "< (less than) operator, 22\n",
      "<-> (equivalence) operator, 368\n",
      "<= (less than or equal to) operator, 22\n",
      "= (equals sign)\n",
      "== (equal to) operator, 22\n",
      "== (identity) operator, 132\n",
      "assignment operator, 14, 130\n",
      "equality operator, 376\n",
      "> (greater than) operator, 22\n",
      ">= (greater than or equal to) operator, 22\n",
      "? (question mark) in regular expressions, 99,\n",
      "101\n",
      "[ ] (brackets)\n",
      "enclosing keys in dictionary, 65\n",
      "indexing lists, 12\n",
      "omitting in list comprehension used as\n",
      "function parameter, 55\n",
      "regular expression character classes, 99\n",
      "\\ (backslash)\n",
      "ending broken line of code, 139\n",
      "escaping string literals, 87\n",
      "in regular expressions, 100, 101\n",
      "use with multiline strings, 88\n",
      "^ (caret)\n",
      "character class negation in regular\n",
      "expressions, 100\n",
      "end of string matching in regular\n",
      "expressions, 99\n",
      "We’d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.\n",
      "463...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 484, 'page_label': '463', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4874}\n",
      "\n",
      "--- Chunk 4875 ---\n",
      "Content:\n",
      "regular expression metacharacter, 101\n",
      "{ } (curly braces) in regular expressions, 100\n",
      "| (pipe character)\n",
      "alternation in regular expressions, 100, 101\n",
      "or operator, 368\n",
      "α-conversion, 389\n",
      "α-equivalents, 389\n",
      "β-reduction, 388\n",
      "λ (lambda operator), 386–390\n",
      "A\n",
      "accumulative functions, 150\n",
      "accuracy of classification, 239\n",
      "ACL (Association for Computational\n",
      "Linguistics), 34\n",
      "Special Interest Group on Web as Corpus\n",
      "(SIGWAC), 416\n",
      "adjectives, categorizing and tagging, 186\n",
      "adjuncts of lexical head, 347\n",
      "adverbs, categorizing and tagging, 186\n",
      "agreement, 329–331\n",
      "resources for further reading, 357\n",
      "algorithm design, 160–167\n",
      "dynamic programming, 165\n",
      "recursion, 161\n",
      "resources for further information, 173\n",
      "all operator, 376\n",
      "alphabetic variants, 389\n",
      "ambiguity\n",
      "broad-coverage grammars and, 317\n",
      "capturing structural ambiguity with\n",
      "dependency parser, 311\n",
      "quantifier scope, 381, 394–397\n",
      "scope of modifier, 314\n",
      "structurally ambiguous sentences, 300\n",
      "ubiquitous ambiguity in sentence structure\n",
      ",\n",
      "293\n",
      "anagram dictionary, creating, 196\n",
      "anaphora resolution, 29\n",
      "anaphoric antecedent, 397\n",
      "AND (in SQL), 365\n",
      "and operator, 24\n",
      "annotated text corpora, 46–48\n",
      "annotation layers\n",
      "creating, 412\n",
      "deciding which to include when acquiring\n",
      "data, 420\n",
      "quality control for, 413\n",
      "survey of annotation software, 438\n",
      "annotation, inline, 421\n",
      "antecedent, 28\n",
      "antonymy, 71\n",
      "apostrophes in tokenization, 110\n",
      "appending, 11\n",
      "arguments\n",
      "functions as, 149\n",
      "named, 152\n",
      "passing to functions (example), 143\n",
      "arguments in logic, 369, 372\n",
      "arity, 378\n",
      "articles, 186\n",
      "assert statements\n",
      "using in defensive programming, 159\n",
      "using to find logical errors, 146\n",
      "assignment, 130, 378\n",
      "defined, 14\n",
      "to list index values, 13\n",
      "Association \n",
      "for Computational Linguistics (see\n",
      "ACL)\n",
      "associative arrays, 189\n",
      "assumptions, 369\n",
      "atomic values, 336\n",
      "attribute value matrix, 336\n",
      "attribute-value pairs (Toolbox lexicon), 67\n",
      "attributes, XML, 426\n",
      "auxiliaries, 348\n",
      "auxiliary verbs, 336\n",
      "inversion and, 348\n",
      "B\n",
      "\\b word boundary in regular expressions, 110\n",
      "backoff, 200\n",
      "backtracking, 303\n",
      "bar charts, 168\n",
      "base case, 161\n",
      "basic types, 373\n",
      "Bayes classifier (see naive Bayes classifier)\n",
      "bigram taggers, 204\n",
      "bigrams, 20\n",
      "generating random text with, 55\n",
      "binary formats, text, 85\n",
      "binary predicate, 372\n",
      "binary search, 160\n",
      "binding variables, 374\n",
      "binning, 249\n",
      "BIO Format, 286\n",
      "book module (NLTK), downloading, 3\n",
      "Boolean operators, 368\n",
      "464 | General Index...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 485, 'page_label': '464', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4875}\n",
      "\n",
      "--- Chunk 4876 ---\n",
      "Content:\n",
      "in propositional logic, truth conditions for,\n",
      "368\n",
      "Boolean values, 336\n",
      "bottom-up approach to dynamic\n",
      "programming, 167\n",
      "bottom-up parsing, 304\n",
      "bound, 374, 375\n",
      "breakpoints, 158\n",
      "Brill tagging, 208\n",
      "demonstration of NLTK Brill tagger, 209\n",
      "steps in, 209\n",
      "Brown Corpus, 42–44\n",
      "bugs, 156\n",
      "C\n",
      "call structure, 165\n",
      "call-by-value, 144\n",
      "carriage return and linefeed characters, 80\n",
      "case in German, 353–356\n",
      "Catalan numbers, 317\n",
      "categorical grammar, 346\n",
      "categorizing and tagging words, 179–219\n",
      "adjectives and adverbs, 186\n",
      "automatically \n",
      "adding POS tags to text, 198–\n",
      "203\n",
      "determining word category, 210–213\n",
      "differences in POS tagsets, 213\n",
      "exploring tagged corpora using POS tags,\n",
      "187–189\n",
      "mapping words to properties using Python\n",
      "dictionaries, 189–198\n",
      "n-gram tagging, 203–208\n",
      "nouns, 184\n",
      "resources for further reading, 214\n",
      "tagged corpora, 181–189\n",
      "transformation-based tagging, 208–210\n",
      "using POS (part-of-speech) tagger, 179\n",
      "using unsimplified POS tags, 187\n",
      "verbs, 185\n",
      "character class symbols in regular expressions,\n",
      "110\n",
      "character encodings, 48, 54, 94\n",
      "(see also Unicode)\n",
      "using your local encoding in Python, 97\n",
      "characteristic function, 377\n",
      "chart, 307\n",
      "chart parsing, 307\n",
      "Earley chart parser, 334\n",
      "charts, displaying information in, 168\n",
      "chat text, 42\n",
      "chatbots, 31\n",
      "child nodes, 279\n",
      "chink, 268, 286\n",
      "chinking, 268\n",
      "chunk grammar, 265\n",
      "chunking, 214, 264–270\n",
      "building nested structure with cascaded\n",
      "chunkers, 278–279\n",
      "chinking, 268\n",
      "developing and evaluating chunkers, 270–\n",
      "278\n",
      "reading IOB format and CoNLL 2000\n",
      "corpus, 270–272\n",
      "simple evaluation and baselines, 272–\n",
      "274\n",
      "training classifier-based chunkers, 274–\n",
      "278\n",
      "exploring text corpora with NP chunker,\n",
      "267\n",
      "noun phrase (NP), 264\n",
      "representing chunks, tags versus trees, 269\n",
      "resources for further reading, 286\n",
      "tag patterns, 266\n",
      "Toolbox lexicon, 434\n",
      "using regular expressions, 266\n",
      "chunks, 264\n",
      "class labels, 221\n",
      "classification, 221–259\n",
      "classifier trained to recognize named\n",
      "entities, 283\n",
      "decision trees, 242–245\n",
      "defined, 221\n",
      "evaluating models, 237–241\n",
      "accuracy, 239\n",
      "confusion matrices, 240\n",
      "cross-validation, 241\n",
      "precision and recall, 239\n",
      "test set, 238\n",
      "generative versus conditional, 254\n",
      "Maximum Entropy classifiers, 251–254\n",
      "modelling linguistic patterns, 255\n",
      "naive Bayes classifiers, 246–250\n",
      "supervised (see supervised classification)\n",
      "classifier-based chunkers, 274–278\n",
      "closed class, 212\n",
      "closed formula, 375\n",
      "closures (+ and *), 100\n",
      "clustering package (nltk.cluster), 172\n",
      "General Index | 465...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 486, 'page_label': '465', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4876}\n",
      "\n",
      "--- Chunk 4877 ---\n",
      "Content:\n",
      "CMU Pronouncing Dictionary for U.S.\n",
      "English, 63\n",
      "code blocks, nested, 25\n",
      "code examples, downloading, 57\n",
      "code points, 94\n",
      "codecs module, 95\n",
      "coindex (in feature structure), 340\n",
      "collocations, 20, 81\n",
      "comma operator (,), 133\n",
      "comparative wordlists, 65\n",
      "comparison operators\n",
      "numerical, 22\n",
      "for words, 23\n",
      "complements of lexical head, 347\n",
      "complements of verbs, 313\n",
      "complex types, 373\n",
      "complex values, 336\n",
      "components, language understanding, 31\n",
      "computational \n",
      "linguistics, challenges of natural\n",
      "language, 441\n",
      "computer understanding of sentence meaning,\n",
      "368\n",
      "concatenation, 11, 88\n",
      "lists and strings, 87\n",
      "strings, 16\n",
      "conclusions in logic, 369\n",
      "concordances\n",
      "creating, 40\n",
      "graphical POS-concordance tool, 184\n",
      "conditional classifiers, 254\n",
      "conditional expressions, 25\n",
      "conditional frequency distributions, 44, 52–56\n",
      "combining with regular expressions, 103\n",
      "condition and event pairs, 52\n",
      "counting words by genre, 52\n",
      "generating random text with bigrams, 55\n",
      "male and female names ending in each\n",
      "alphabet letter, 62\n",
      "plotting and tabulating distributions, 53\n",
      "using to find minimally contrasting set of\n",
      "words, 64\n",
      "ConditionalFreqDist, 52\n",
      "commonly used methods, 56\n",
      "conditionals, 22, 133\n",
      "confusion matrix, 207, 240\n",
      "consecutive classification, 232\n",
      "non phrase chunking with consecutive\n",
      "classifier, 275\n",
      "consistent, 366\n",
      "constituent structure, 296\n",
      "constituents, 297\n",
      "context\n",
      "exploiting in part-of-speech classifier, 230\n",
      "for taggers, 203\n",
      "context-free grammar, 298, 300\n",
      "(see also grammars)\n",
      "probabilistic context-free grammar, 320\n",
      "contractions in tokenization, 112\n",
      "control, 22\n",
      "control structures, 26\n",
      "conversion specifiers, 118\n",
      "conversions of data formats, 419\n",
      "coordinate structures, 295\n",
      "coreferential, 373\n",
      "corpora, 39–52\n",
      "annotated text corpora, 46–48\n",
      "Brown Corpus, 42–44\n",
      "creating \n",
      "and accessing, resources for further\n",
      "reading, 438\n",
      "defined, 39\n",
      "differences in corpus access methods, 50\n",
      "exploring text corpora using a chunker,\n",
      "267\n",
      "Gutenberg Corpus, 39–42\n",
      "Inaugural Address Corpus, 45\n",
      "from languages other than English, 48\n",
      "loading your own corpus, 51\n",
      "obtaining from Web, 416\n",
      "Reuters Corpus, 44\n",
      "sources of, 73\n",
      "tagged, 181–189\n",
      "text corpus structure, 49–51\n",
      "web and chat text, 42\n",
      "wordlists, 60–63\n",
      "corpora, included with NLTK, 46\n",
      "corpus\n",
      "case study, structure of TIMIT, 407–412\n",
      "corpus HOWTOs, 122\n",
      "life cycle of, 412–416\n",
      "creation scenarios, 412\n",
      "curation versus evolution, 415\n",
      "quality control, 413\n",
      "widely-used format for, 421\n",
      "counters, legitimate uses of, 141\n",
      "cross-validation, 241\n",
      "CSV (comma-separated value) format, 418\n",
      "CSV (comma-separated-value) format, 170\n",
      "466 | General Index...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 487, 'page_label': '466', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4877}\n",
      "\n",
      "--- Chunk 4878 ---\n",
      "Content:\n",
      "D\n",
      "\\d decimal digits in regular expressions, 110\n",
      "\\D nondigit characters in regular expressions,\n",
      "111\n",
      "data formats, converting, 419\n",
      "data types\n",
      "dictionary, 190\n",
      "documentation for Python standard types,\n",
      "173\n",
      "finding type of Python objects, 86\n",
      "function parameter, 146\n",
      "operations on objects, 86\n",
      "database query via natural language, 361–365\n",
      "databases, obtaining data from, 418\n",
      "debugger (Python), 158\n",
      "debugging techniques, 158\n",
      "decimal integers, formatting, 119\n",
      "decision nodes, 242\n",
      "decision stumps, 243\n",
      "decision trees, 242–245\n",
      "entropy and information gain, 243\n",
      "decision-tree classifier, 229\n",
      "declarative style, 140\n",
      "decoding, 94\n",
      "def keyword, 9\n",
      "defaultdict, 193\n",
      "defensive programming, 159\n",
      "demonstratives, agreement with noun, 329\n",
      "dependencies, 310\n",
      "criteria for, 312\n",
      "existential dependencies, modeling in\n",
      "XML, 427\n",
      "non-projective, 312\n",
      "projective, 311\n",
      "unbounded dependency constructions,\n",
      "349–353\n",
      "dependency grammars, 310–315\n",
      "valency and the lexicon, 312\n",
      "dependents, 310\n",
      "descriptive models, 255\n",
      "determiners, 186\n",
      "agreement with nouns, 333\n",
      "deve-test set, 225\n",
      "development set, 225\n",
      "similarity to test set, 238\n",
      "dialogue act tagging, 214\n",
      "dialogue acts, identifying types, 235\n",
      "dialogue \n",
      "systems (see spoken dialogue systems)\n",
      "dictionaries\n",
      "feature set, 223\n",
      "feature structures as, 337\n",
      "pronouncing dictionary, 63–65\n",
      "Python, 189–198\n",
      "default, 193\n",
      "defining, 193\n",
      "dictionary data type, 190\n",
      "finding key given a value, 197\n",
      "indexing lists versus, 189\n",
      "summary of dictionary methods, 197\n",
      "updating incrementally, 195\n",
      "storing features and values, 327\n",
      "translation, 66\n",
      "dictionary\n",
      "methods, 197\n",
      "dictionary data structure (Python), 65\n",
      "directed acyclic graphs (DAGs), 338\n",
      "discourse module, 401\n",
      "discourse semantics, 397–402\n",
      "discourse processing, 400–402\n",
      "discourse referents, 397\n",
      "discourse representation structure (DRS),\n",
      "397\n",
      "Discourse Representation Theory (DRT),\n",
      "397–400\n",
      "dispersion plot, 6\n",
      "divide-and-conquer strategy, 160\n",
      "docstrings, 143\n",
      "contents and structure of, 148\n",
      "example of complete docstring, 148\n",
      "module-level, 155\n",
      "doctest block, 148\n",
      "doctest module, 160\n",
      "document classification, 227\n",
      "documentation\n",
      "functions, 148\n",
      "online Python documentation, versions\n",
      "and, 173\n",
      "Python, resources for further information,\n",
      "173\n",
      "docutils module, 148\n",
      "domain (of a model), 377\n",
      "DRS (discourse representation structure), 397\n",
      "DRS conditions, 397\n",
      "DRT \n",
      "(Discourse Representation Theory), 397–\n",
      "400\n",
      "Dublin Core Metadata initiative, 435\n",
      "duck typing, 281\n",
      "dynamic programming, 165\n",
      "General Index | 467...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 488, 'page_label': '467', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4878}\n",
      "\n",
      "--- Chunk 4879 ---\n",
      "Content:\n",
      "application to parsing with context-free\n",
      "grammar, 307\n",
      "different approaches to, 167\n",
      "E\n",
      "Earley chart parser, 334\n",
      "electronic books, 80\n",
      "elements, XML, 425\n",
      "ElementTree interface, 427–429\n",
      "using to access Toolbox data, 429\n",
      "elif clause, if . . . elif statement, 133\n",
      "elif statements, 26\n",
      "else statements, 26\n",
      "encoding, 94\n",
      "encoding features, 223\n",
      "encoding parameters, codecs module, 95\n",
      "endangered languages, special considerations\n",
      "with, 423–424\n",
      "entities, 373\n",
      "entity detection, using chunking, 264–270\n",
      "entries\n",
      "adding field to, in Toolbox, 431\n",
      "contents of, 60\n",
      "converting data formats, 419\n",
      "formatting in XML, 430\n",
      "entropy, 251\n",
      "(see also Maximum Entropy classifiers)\n",
      "calculating for gender prediction task, 243\n",
      "maximizing in Maximum Entropy\n",
      "classifier, 252\n",
      "epytext markup language, 148\n",
      "equality, 132, 372\n",
      "equivalence (<->) operator, 368\n",
      "equivalent, 340\n",
      "error analysis, 225\n",
      "errors\n",
      "runtime, 13\n",
      "sources of, 156\n",
      "syntax, 3\n",
      "evaluation sets, 238\n",
      "events, pairing with conditions in conditional\n",
      "frequency distribution, 52\n",
      "exceptions, 158\n",
      "existential quantifier, 374\n",
      "exists operator, 376\n",
      "Expected Likelihood Estimation, 249\n",
      "exporting data, 117\n",
      "F\n",
      "f-structure, 357\n",
      "feature extractors\n",
      "defining for dialogue acts, 235\n",
      "defining for document classification, 228\n",
      "defining for noun phrase (NP) chunker,\n",
      "276–278\n",
      "defining for punctuation, 234\n",
      "defining for suffix checking, 229\n",
      "Recognizing Textual Entailment (RTE),\n",
      "236\n",
      "selecting relevant features, 224–227\n",
      "feature paths, 339\n",
      "feature sets, 223\n",
      "feature structures, 328\n",
      "order of features, 337\n",
      "resources for further reading, 357\n",
      "feature-based grammars, 327–360\n",
      "auxiliary verbs and inversion, 348\n",
      "case and gender in German, 353\n",
      "example grammar, 333\n",
      "extending, 344–356\n",
      "lexical heads, 347\n",
      "parsing using Earley chart parser, 334\n",
      "processing feature structures, 337–344\n",
      "subsumption and unification, 341–344\n",
      "resources for further reading, 357\n",
      "subcategorization, 344–347\n",
      "syntactic agreement, 329–331\n",
      "terminology, 336\n",
      "translating from English to SQL, 362\n",
      "unbounded dependency constructions,\n",
      "349–353\n",
      "using attributes and constraints, 331–336\n",
      "features, 223\n",
      "non-binary features in naive Bayes\n",
      "classifier, 249\n",
      "fields, 136\n",
      "file formats, libraries for, 172\n",
      "files\n",
      "opening and reading local files, 84\n",
      "writing program output to, 120\n",
      "fillers, 349\n",
      "first-order logic, 372–385\n",
      "individual variables and assignments, 378\n",
      "model building, 383\n",
      "quantifier scope ambiguity, 381\n",
      "summary of language, 376\n",
      "syntax, 372–375\n",
      "468 | General Index...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 489, 'page_label': '468', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4879}\n",
      "\n",
      "--- Chunk 4880 ---\n",
      "Content:\n",
      "theorem proving, 375\n",
      "truth in model, 377\n",
      "floating-point numbers, formatting, 119\n",
      "folds, 241\n",
      "for statements, 26\n",
      "combining with if statements, 26\n",
      "inside a list comprehension, 63\n",
      "iterating over characters in strings, 90\n",
      "format strings, 118\n",
      "formatting program output, 116–121\n",
      "converting from lists to strings, 116\n",
      "strings and formats, 117–118\n",
      "text wrapping, 120\n",
      "writing results to file, 120\n",
      "formulas of propositional logic, 368\n",
      "formulas, type (t), 373\n",
      "free, 375\n",
      "Frege’s Principle, 385\n",
      "frequency distributions, 17, 22\n",
      "conditional (see conditional frequency\n",
      "distributions)\n",
      "functions defined for, 22\n",
      "letters, occurrence in strings, 90\n",
      "functions, 142–154\n",
      "abstraction provided by, 147\n",
      "accumulative, 150\n",
      "as arguments to another function, 149\n",
      "call-by-value parameter passing, 144\n",
      "checking parameter types, 146\n",
      "defined, 9, 57\n",
      "documentation for Python built-in\n",
      "functions, 173\n",
      "documenting, 148\n",
      "errors from, 157\n",
      "for frequency distributions, 22\n",
      "for iteration over sequences, 134\n",
      "generating plurals of nouns (example), 58\n",
      "higher-order, 151\n",
      "inputs and outputs, 143\n",
      "named arguments, 152\n",
      "naming, 142\n",
      "poorly-designed, 147\n",
      "recursive, call structure, 165\n",
      "saving in modules, 59\n",
      "variable scope, 145\n",
      "well-designed, 147\n",
      "G\n",
      "gaps, 349\n",
      "gazetteer, 282\n",
      "gender identification, 222\n",
      "Decision Tree model for, 242\n",
      "gender in German, 353–356\n",
      "Generalized Phrase Structure Grammar\n",
      "(GPSG), 345\n",
      "generate_model ( ) function, 55\n",
      "generation of language output, 29\n",
      "generative classifiers, 254\n",
      "generator expressions, 138\n",
      "functions exemplifying, 151\n",
      "genres, systematic differences between, 42–44\n",
      "German, case and gender in, 353–356\n",
      "gerunds, 211\n",
      "glyphs, 94\n",
      "gold standard, 201\n",
      "government-sponsored challenges to machine\n",
      "learning application in NLP, 257\n",
      "gradient (grammaticality), 318\n",
      "grammars, 327\n",
      "(see also feature-based grammars)\n",
      "chunk grammar, 265\n",
      "context-free, 298–302\n",
      "parsing with, 302–310\n",
      "validating Toolbox entries with, 433\n",
      "writing your own, 300\n",
      "dependency, 310–315\n",
      "development, 315–321\n",
      "problems with ambiguity, 317\n",
      "treebanks and grammars, 315–317\n",
      "weighted grammar, 318–321\n",
      "dilemmas in sentence structure analysis,\n",
      "292–295\n",
      "resources for further reading, 322\n",
      "scaling up, 315\n",
      "grammatical category, 328\n",
      "graphical displays of data\n",
      "conditional frequency distributions, 56\n",
      "Matplotlib, 168–170\n",
      "graphs\n",
      "defining and manipulating, 170\n",
      "directed acyclic graphs, 338\n",
      "greedy sequence classification, 232\n",
      "Gutenberg Corpus, 40–42, 80\n",
      "H\n",
      "hapaxes, 19\n",
      "hash arrays, 189, 190\n",
      "(see also dictionaries)\n",
      "General Index | 469...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 490, 'page_label': '469', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4880}\n",
      "\n",
      "--- Chunk 4881 ---\n",
      "Content:\n",
      "head of a sentence, 310\n",
      "criteria for head and dependencies, 312\n",
      "heads, lexical, 347\n",
      "headword (lemma), 60\n",
      "Heldout Estimation, 249\n",
      "hexadecimal notation for Unicode string\n",
      "literal, 95\n",
      "Hidden Markov Models, 233\n",
      "higher-order functions, 151\n",
      "holonyms, 70\n",
      "homonyms, 60\n",
      "HTML documents, 82\n",
      "HTML markup, stripping out, 418\n",
      "hypernyms, 70\n",
      "searching corpora for, 106\n",
      "semantic similarity and, 72\n",
      "hyphens in tokenization, 110\n",
      "hyponyms, 69\n",
      "I\n",
      "identifiers for variables, 15\n",
      "idioms, Python, 24\n",
      "IDLE (Interactive DeveLopment\n",
      "Environment), 2\n",
      "if . . . elif statements, 133\n",
      "if statements, 25\n",
      "combining with for statements, 26\n",
      "conditions in, 133\n",
      "immediate constituents, 297\n",
      "immutable, 93\n",
      "implication (->) operator, 368\n",
      "in operator, 91\n",
      "Inaugural Address Corpus, 45\n",
      "inconsistent, 366\n",
      "indenting code, 138\n",
      "independence assumption, 248\n",
      "naivete of, 249\n",
      "indexes\n",
      "counting from zero (0), 12\n",
      "list, 12–14\n",
      "mapping dictionary definition to lexeme,\n",
      "419\n",
      "speeding up program by using, 163\n",
      "string, 15, 89, 91\n",
      "text index created using a stemmer, 107\n",
      "words containing a given consonant-vowel\n",
      "pair, 103\n",
      "inference, 369\n",
      "information extraction, 261–289\n",
      "architecture of system, 263\n",
      "chunking, 264–270\n",
      "defined, 262\n",
      "developing and evaluating chunkers, 270–\n",
      "278\n",
      "named entity recognition, 281–284\n",
      "recursion in linguistic structure, 278–281\n",
      "relation extraction, 284\n",
      "resources for further reading, 286\n",
      "information gain, 243\n",
      "inside, outside, begin tags (see IOB tags)\n",
      "integer ordinal, finding for character, 95\n",
      "interpreter\n",
      ">>> prompt, 2\n",
      "accessing, 2\n",
      "using text editor instead of to write\n",
      "programs, 56\n",
      "inverted clauses, 348\n",
      "IOB tags, 269, 286\n",
      "reading, 270–272\n",
      "is operator, 145\n",
      "testing for object identity, 132\n",
      "ISO 639 language codes, 65\n",
      "iterative optimization techniques, 251\n",
      "J\n",
      "joint classifier models, 231\n",
      "joint-features (maximum entropy model), 252\n",
      "K\n",
      "Kappa coefficient (k), 414\n",
      "keys, 65, 191\n",
      "complex, 196\n",
      "keyword arguments, 153\n",
      "Kleene closures, 100\n",
      "L\n",
      "lambda expressions, 150, 386–390\n",
      "example, 152\n",
      "lambda operator (λ), 386\n",
      "Lancaster stemmer, 107\n",
      "language codes, 65\n",
      "language output, generating, 29\n",
      "language processing, symbol processing\n",
      "versus, 442\n",
      "language resources\n",
      "describing using OLAC metadata, 435–437\n",
      "LanguageLog (linguistics blog), 35\n",
      "470 | General Index...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 491, 'page_label': '470', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4881}\n",
      "\n",
      "--- Chunk 4882 ---\n",
      "Content:\n",
      "latent semantic analysis, 171\n",
      "Latin-2 character encoding, 94\n",
      "leaf nodes, 242\n",
      "left-corner parser, 306\n",
      "left-recursive, 302\n",
      "lemmas, 60\n",
      "lexical relationships between, 71\n",
      "pairing of synset with a word, 68\n",
      "lemmatization, 107\n",
      "example of, 108\n",
      "length of a text, 7\n",
      "letter trie, 162\n",
      "lexical categories, 179\n",
      "lexical entry, 60\n",
      "lexical relations, 70\n",
      "lexical resources\n",
      "comparative wordlists, 65\n",
      "pronouncing dictionary, 63–65\n",
      "Shoebox and Toolbox lexicons, 66\n",
      "wordlist corpora, 60–63\n",
      "lexicon, 60\n",
      "(see also lexical resources)\n",
      "chunking Toolbox lexicon, 434\n",
      "defined, 60\n",
      "validating in Toolbox, 432–435\n",
      "LGB rule of name resolution, 145\n",
      "licensed, 350\n",
      "likelihood ratios, 224\n",
      "Linear-Chain Conditional Random Field\n",
      "Models, 233\n",
      "linguistic objects, mappings from keys to\n",
      "values, 190\n",
      "linguistic patterns, modeling, 255\n",
      "linguistics \n",
      "and NLP-related concepts, resources\n",
      "for, 34\n",
      "list comprehensions, 24\n",
      "for statement in, 63\n",
      "function invoked in, 64\n",
      "used as function parameters, 55\n",
      "lists, 10\n",
      "appending item to, 11\n",
      "concatenating, using + operator, 11\n",
      "converting to strings, 116\n",
      "indexing, 12–14\n",
      "indexing, dictionaries versus, 189\n",
      "normalizing and sorting, 86\n",
      "Python list type, 86\n",
      "sorted, 14\n",
      "strings versus, 92\n",
      "tuples versus, 136\n",
      "local variables, 58\n",
      "logic\n",
      "first-order, 372–385\n",
      "natural language, semantics, and, 365–368\n",
      "propositional, 368–371\n",
      "resources for further reading, 404\n",
      "logical constants, 372\n",
      "logical form, 368\n",
      "logical proofs, 370\n",
      "loops, 26\n",
      "looping with conditions, 26\n",
      "lowercase, converting text to, 45, 107\n",
      "M\n",
      "machine learning\n",
      "application to NLP, web pages for\n",
      "government challenges, 257\n",
      "decision trees, 242–245\n",
      "Maximum Entropy classifiers, 251–254\n",
      "naive Bayes classifiers, 246–250\n",
      "packages, 237\n",
      "resources for further reading, 257\n",
      "supervised classification, 221–237\n",
      "machine translation (MT)\n",
      "limitations of, 30\n",
      "using NLTK’s babelizer, 30\n",
      "mapping, 189\n",
      "Matplotlib package, 168–170\n",
      "maximal projection, 347\n",
      "Maximum Entropy classifiers, 251–254\n",
      "Maximum Entropy Markov Models, 233\n",
      "Maximum Entropy principle, 253\n",
      "memoization, 167\n",
      "meronyms, 70\n",
      "metadata, 435\n",
      "OLAC (Open Language Archives\n",
      "Community), 435\n",
      "modals, 186\n",
      "model building, 383\n",
      "model checking, 379\n",
      "models\n",
      "interpretation of sentences of logical\n",
      "language, 371\n",
      "of linguistic patterns, 255\n",
      "representation using set theory, 367\n",
      "truth-conditional semantics in first-order\n",
      "logic, 377\n",
      "General Index | 471...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 492, 'page_label': '471', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4882}\n",
      "\n",
      "--- Chunk 4883 ---\n",
      "Content:\n",
      "what can be learned from models of\n",
      "language, 255\n",
      "modifiers, 314\n",
      "modules\n",
      "defined, 59\n",
      "multimodule programs, 156\n",
      "structure of Python module, 154\n",
      "morphological analysis, 213\n",
      "morphological cues to word category, 211\n",
      "morphological tagging, 214\n",
      "morphosyntactic information in tagsets, 212\n",
      "MSWord, text from, 85\n",
      "mutable, 93\n",
      "N\n",
      "\\n newline character in regular expressions,\n",
      "111\n",
      "n-gram tagging, 203–208\n",
      "across sentence boundaries, 208\n",
      "combining taggers, 205\n",
      "n-gram tagger as generalization of unigram\n",
      "tagger, 203\n",
      "performance limitations, 206\n",
      "separating training and test data, 203\n",
      "storing taggers, 206\n",
      "unigram tagging, 203\n",
      "unknown words, 206\n",
      "naive Bayes assumption, 248\n",
      "naive Bayes classifier, 246–250\n",
      "developing for gender identification task,\n",
      "223\n",
      "double-counting problem, 250\n",
      "as generative classifier, 254\n",
      "naivete of independence assumption, 249\n",
      "non-binary features, 249\n",
      "underlying probabilistic model, 248\n",
      "zero counts and smoothing, 248\n",
      "name resolution, LGB rule for, 145\n",
      "named arguments, 152\n",
      "named entities\n",
      "commonly used types of, 281\n",
      "relations between, 284\n",
      "named entity recognition (NER), 281–284\n",
      "Names Corpus, 61\n",
      "negative lookahead assertion, 284\n",
      "NER (see named entity recognition)\n",
      "nested code blocks, 25\n",
      "NetworkX package, 170\n",
      "new words in languages, 212\n",
      "newlines, 84\n",
      "matching in regular expressions, 109\n",
      "printing with print statement, 90\n",
      "resources for further information, 122\n",
      "non-logical constants, 372\n",
      "non-standard words, 108\n",
      "normalizing text, 107–108\n",
      "lemmatization, 108\n",
      "using stemmers, 107\n",
      "noun phrase (NP), 297\n",
      "noun phrase (NP) chunking, 264\n",
      "regular expression–based NP chunker, 267\n",
      "using unigram tagger, 272\n",
      "noun phrases, quantified, 390\n",
      "nouns\n",
      "categorizing and tagging, 184\n",
      "program to find most frequent noun tags,\n",
      "187\n",
      "syntactic agreement, 329\n",
      "numerically intense algorithms in Python,\n",
      "increasing efficiency of, 257\n",
      "NumPy package, 171\n",
      "O\n",
      "object references, 130\n",
      "copying, 132\n",
      "objective function, 114\n",
      "objects, finding data type for, 86\n",
      "OLAC metadata, 74, 435\n",
      "definition of metadata, 435\n",
      "Open Language Archives Community, 435\n",
      "Open Archives Initiative (OAI), 435\n",
      "open class, 212\n",
      "open formula, 374\n",
      "Open Language Archives Community\n",
      "(OLAC), 435\n",
      "operators, 369\n",
      "(see also names of individual operators)\n",
      "addition and multiplication, 88\n",
      "Boolean, 368\n",
      "numerical comparison, 22\n",
      "scope of, 157\n",
      "word comparison, 23\n",
      "or operator, 24\n",
      "orthography, 328\n",
      "out-of-vocabulary items, 206\n",
      "overfitting, 225, 245\n",
      "472 | General Index...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 493, 'page_label': '472', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4883}\n",
      "\n",
      "--- Chunk 4884 ---\n",
      "Content:\n",
      "P\n",
      "packages, 59\n",
      "parameters, 57\n",
      "call-by-value parameter passing, 144\n",
      "checking types of, 146\n",
      "defined, 9\n",
      "defining for functions, 143\n",
      "parent nodes, 279\n",
      "parsing, 318\n",
      "(see also grammars)\n",
      "with context-free grammar\n",
      "left-corner parser, 306\n",
      "recursive descent parsing, 303\n",
      "shift-reduce parsing, 304\n",
      "well-formed substring tables, 307–310\n",
      "Earley chart parser, parsing feature-based\n",
      "grammars, 334\n",
      "parsers, 302\n",
      "projective dependency parser, 311\n",
      "part-of-speech tagging (see POS tagging)\n",
      "partial information, 341\n",
      "parts of speech, 179\n",
      "PDF text, 85\n",
      "Penn Treebank Corpus, 51, 315\n",
      "personal pronouns, 186\n",
      "philosophical divides in contemporary NLP,\n",
      "444\n",
      "phonetics\n",
      "computer-readable phonetic alphabet\n",
      "(SAMPA), 137\n",
      "phones, 63\n",
      "resources for further information, 74\n",
      "phrasal level, 347\n",
      "phrasal projections, 347\n",
      "pipeline for NLP, 31\n",
      "pixel images, 169\n",
      "plotting functions, Matplotlib, 168\n",
      "Porter stemmer, 107\n",
      "POS (part-of-speech) tagging, 179, 208, 229\n",
      "(see also tagging)\n",
      "differences in POS tagsets, 213\n",
      "examining word context, 230\n",
      "finding IOB chunk tag for word's POS tag,\n",
      "272\n",
      "in information retrieval, 263\n",
      "morphology in POS tagsets, 212\n",
      "resources for further reading, 214\n",
      "simplified tagset, 183\n",
      "storing POS tags in tagged corpora, 181\n",
      "tagged data from four Indian languages,\n",
      "182\n",
      "unsimplifed tags, 187\n",
      "use in noun phrase chunking, 265\n",
      "using consecutive classifier, 231\n",
      "pre-sorting, 160\n",
      "precision, evaluating search tasks for, 239\n",
      "precision/recall trade-off in information\n",
      "retrieval, 205\n",
      "predicates (first-order logic), 372\n",
      "prepositional phrase (PP), 297\n",
      "prepositional phrase attachment ambiguity,\n",
      "300\n",
      "Prepositional Phrase Attachment Corpus, 316\n",
      "prepositions, 186\n",
      "present participles, 211\n",
      "Principle of Compositionality, 385, 443\n",
      "print statements, 89\n",
      "newline at end, 90\n",
      "string formats and, 117\n",
      "prior probability, 246\n",
      "probabilistic context-free grammar (PCFG),\n",
      "320\n",
      "probabilistic model, naive Bayes classifier, 248\n",
      "probabilistic parsing, 318\n",
      "procedural style, 139\n",
      "processing pipeline (NLP), 86\n",
      "productions in grammars, 293\n",
      "rules for writing CFGs for parsing in\n",
      "NLTK, 301\n",
      "program development, 154–160\n",
      "debugging techniques, 158\n",
      "defensive programming, 159\n",
      "multimodule programs, 156\n",
      "Python module structure, 154\n",
      "sources of error, 156\n",
      "programming style, 139\n",
      "programs, writing, 129–177\n",
      "advanced features of functions, 149–154\n",
      "algorithm design, 160–167\n",
      "assignment, 130\n",
      "conditionals, 133\n",
      "equality, 132\n",
      "functions, 142–149\n",
      "resources for further reading, 173\n",
      "sequences, 133–138\n",
      "style considerations, 138–142\n",
      "legitimate uses for counters, 141\n",
      "procedural versus declarative style, 139\n",
      "General Index | 473...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 494, 'page_label': '473', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4884}\n",
      "\n",
      "--- Chunk 4885 ---\n",
      "Content:\n",
      "Python coding style, 138\n",
      "summary of important points, 172\n",
      "using Python libraries, 167–172\n",
      "Project Gutenberg, 80\n",
      "projections, 347\n",
      "projective, 311\n",
      "pronouncing dictionary, 63–65\n",
      "pronouns\n",
      "anaphoric antecedents, 397\n",
      "interpreting in first-order logic, 373\n",
      "resolving in discourse processing, 401\n",
      "proof goal, 376\n",
      "properties of linguistic categories, 331\n",
      "propositional logic, 368–371\n",
      "Boolean operators, 368\n",
      "propositional symbols, 368\n",
      "pruning decision nodes, 245\n",
      "punctuation, classifier for, 233\n",
      "Python\n",
      "carriage return and linefeed characters, 80\n",
      "codecs module, 95\n",
      "dictionary data structure, 65\n",
      "dictionary methods, summary of, 197\n",
      "documentation, 173\n",
      "documentation and information resources,\n",
      "34\n",
      "ElementTree module, 427\n",
      "errors in understanding semantics of, 157\n",
      "finding type of any object, 86\n",
      "getting started, 2\n",
      "increasing efficiency of numerically intense\n",
      "algorithms, 257\n",
      "libraries, 167–172\n",
      "CSV, 170\n",
      "Matplotlib, 168–170\n",
      "NetworkX, 170\n",
      "NumPy, 171\n",
      "other, 172\n",
      "reference materials, 122\n",
      "style guide for Python code, 138\n",
      "textwrap module, 120\n",
      "Python Package Index, 172\n",
      "Q\n",
      "quality control in corpus creation, 413\n",
      "quantification\n",
      "first-order logic, 373, 380\n",
      "quantified noun phrases, 390\n",
      "scope ambiguity, 381, 394–397\n",
      "quantified formulas, interpretation of, 380\n",
      "questions, answering, 29\n",
      "quotation marks in strings, 87\n",
      "R\n",
      "random text\n",
      "generating in various styles, 6\n",
      "generating using bigrams, 55\n",
      "raster (pixel) images, 169\n",
      "raw strings, 101\n",
      "raw text, processing, 79–128\n",
      "capturing user input, 85\n",
      "detecting word patterns with regular\n",
      "expressions, 97–101\n",
      "formatting from lists to strings, 116–121\n",
      "HTML documents, 82\n",
      "NLP pipeline, 86\n",
      "normalizing text, 107–108\n",
      "reading local files, 84\n",
      "regular \n",
      "expressions for tokenizing text, 109–\n",
      "112\n",
      "resources for further reading, 122\n",
      "RSS feeds, 83\n",
      "search engine results, 82\n",
      "segmentation, 112–116\n",
      "strings, lowest level text processing, 87–93\n",
      "summary of important points, 121\n",
      "text from web and from disk, 80\n",
      "text in binary formats, 85\n",
      "useful applications of regular expressions,\n",
      "102–106\n",
      "using Unicode, 93–97\n",
      "raw( ) function, 41\n",
      "re module, 101, 110\n",
      "recall, evaluating search tasks for, 240\n",
      "Recognizing Textual Entailment (RTE), 32,\n",
      "235\n",
      "exploiting word context, 230\n",
      "records, 136\n",
      "recursion, 161\n",
      "function to compute Sanskrit meter\n",
      "(example), 165\n",
      "in linguistic structure, 278–281\n",
      "tree traversal, 280\n",
      "trees, 279–280\n",
      "performance and, 163\n",
      "in syntactic structure, 301\n",
      "recursive, 301\n",
      "recursive descent parsing, 303\n",
      "474 | General Index...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 495, 'page_label': '474', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4885}\n",
      "\n",
      "--- Chunk 4886 ---\n",
      "Content:\n",
      "reentrancy, 340\n",
      "references (see object references)\n",
      "regression testing framework, 160\n",
      "regular expressions, 97–106\n",
      "character class and other symbols, 110\n",
      "chunker based on, evaluating, 272\n",
      "extracting word pieces, 102\n",
      "finding word stems, 104\n",
      "matching initial and final vowel sequences\n",
      "and all consonants, 102\n",
      "metacharacters, 101\n",
      "metacharacters, summary of, 101\n",
      "noun phrase (NP) chunker based on, 265\n",
      "ranges and closures, 99\n",
      "resources for further information, 122\n",
      "searching tokenized text, 105\n",
      "symbols, 110\n",
      "tagger, 199\n",
      "tokenizing text, 109–112\n",
      "use in PlaintextCorpusReader, 51\n",
      "using basic metacharacters, 98\n",
      "using for relation extraction, 284\n",
      "using with conditional frequency\n",
      "distributions, 103\n",
      "relation detection, 263\n",
      "relation extraction, 284\n",
      "relational operators, 22\n",
      "reserved words, 15\n",
      "return statements, 144\n",
      "return value, 57\n",
      "reusing code, 56–59\n",
      "creating programs using a text editor, 56\n",
      "functions, 57\n",
      "modules, 59\n",
      "Reuters Corpus, 44\n",
      "root element (XML), 427\n",
      "root hypernyms, 70\n",
      "root node, 242\n",
      "root synsets, 69\n",
      "Rotokas language, 66\n",
      "extracting all consonant-vowel sequences\n",
      "from words, 103\n",
      "Toolbox file containing lexicon, 429\n",
      "RSS feeds, 83\n",
      "feedparser library, 172\n",
      "RTE (Recognizing Textual Entailment), 32,\n",
      "235\n",
      "exploiting word context, 230\n",
      "runtime errors, 13\n",
      "S\n",
      "\\s whitespace characters in regular\n",
      "expressions, 111\n",
      "\\S nonwhitespace characters in regular\n",
      "expressions, 111\n",
      "SAMPA \n",
      "computer-readable phonetic alphabet,\n",
      "137\n",
      "Sanskrit meter, computing, 165\n",
      "satisfies, 379\n",
      "scope of quantifiers, 381\n",
      "scope of variables, 145\n",
      "searches\n",
      "binary search, 160\n",
      "evaluating for precision and recall, 239\n",
      "processing search engine results, 82\n",
      "using POS tags, 187\n",
      "segmentation, 112–116\n",
      "in chunking and tokenization, 264\n",
      "sentence, 112\n",
      "word, 113–116\n",
      "semantic cues to word category, 211\n",
      "semantic interpretations, NLTK functions for,\n",
      "393\n",
      "semantic role labeling, 29\n",
      "semantics\n",
      "natural language, logic and, 365–368\n",
      "natural language, resources for\n",
      "information, 403\n",
      "semantics of English sentences, 385–397\n",
      "quantifier ambiguity, 394–397\n",
      "transitive verbs, 391–394\n",
      "⋏-calculus, 386–390\n",
      "SemCor tagging, 214\n",
      "sentence boundaries, tagging across, 208\n",
      "sentence segmentation, 112, 233\n",
      "in chunking, 264\n",
      "in information retrieval process, 263\n",
      "sentence structure, analyzing, 291–326\n",
      "context-free grammar, 298–302\n",
      "dependencies and dependency grammar,\n",
      "310–315\n",
      "grammar development, 315–321\n",
      "grammatical dilemmas, 292\n",
      "parsing with context-free grammar, 302–\n",
      "310\n",
      "resources for further reading, 322\n",
      "summary of important points, 321\n",
      "syntax, 295–298\n",
      "sents( ) function, 41\n",
      "General Index | 475...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 496, 'page_label': '475', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4886}\n",
      "\n",
      "--- Chunk 4887 ---\n",
      "Content:\n",
      "sequence classification, 231–233\n",
      "other methods, 233\n",
      "POS tagging with consecutive classifier,\n",
      "232\n",
      "sequence iteration, 134\n",
      "sequences, 133–138\n",
      "combining different sequence types, 136\n",
      "converting between sequence types, 135\n",
      "operations on sequence types, 134\n",
      "processing using generator expressions,\n",
      "137\n",
      "strings and lists as, 92\n",
      "shift operation, 305\n",
      "shift-reduce parsing, 304\n",
      "Shoebox, 66, 412\n",
      "sibling nodes, 279\n",
      "signature, 373\n",
      "similarity, semantic, 71\n",
      "Sinica Treebank Corpus, 316\n",
      "slash categories, 350\n",
      "slicing\n",
      "lists, 12, 13\n",
      "strings, 15, 90\n",
      "smoothing, 249\n",
      "space-time trade-offs in algorihm design, 163\n",
      "spaces, matching in regular expressions, 109\n",
      "Speech Synthesis Markup Language (W3C\n",
      "SSML), 214\n",
      "spellcheckers, Words Corpus used by, 60\n",
      "spoken dialogue systems, 31\n",
      "spreadsheets, obtaining data from, 418\n",
      "SQL (Structured Query Language), 362\n",
      "translating English sentence to, 362\n",
      "stack trace, 158\n",
      "standards for linguistic data creation, 421\n",
      "standoff annotation, 415, 421\n",
      "start symbol for grammars, 298, 334\n",
      "startswith( ) function, 45\n",
      "stemming, 107\n",
      "NLTK HOWTO, 122\n",
      "stemmers, 107\n",
      "using regular expressions, 104\n",
      "using stem( ) fuinction, 105\n",
      "stopwords, 60\n",
      "stress (in pronunciation), 64\n",
      "string formatting expressions, 117\n",
      "string literals, Unicode string literal in Python,\n",
      "95\n",
      "strings, 15, 87–93\n",
      "accessing individual characters, 89\n",
      "accessing substrings, 90\n",
      "basic operations with, 87–89\n",
      "converting lists to, 116\n",
      "formats, 117–118\n",
      "formatting\n",
      "lining things up, 118\n",
      "tabulating data, 119\n",
      "immutability of, 93\n",
      "lists versus, 92\n",
      "methods, 92\n",
      "more operations on, useful string methods,\n",
      "92\n",
      "printing, 89\n",
      "Python’s str data type, 86\n",
      "regular expressions as, 101\n",
      "tokenizing, 86\n",
      "structurally ambiguous sentences, 300\n",
      "structure sharing, 340\n",
      "interaction with unification, 343\n",
      "structured data, 261\n",
      "style guide for Python code, 138\n",
      "stylistics, 43\n",
      "subcategories of verbs, 314\n",
      "subcategorization, 344–347\n",
      "substrings (WFST), 307\n",
      "substrings, accessing, 90\n",
      "subsumes, 341\n",
      "subsumption, 341–344\n",
      "suffixes, classifier for, 229\n",
      "supervised classification, 222–237\n",
      "choosing features, 224–227\n",
      "documents, 227\n",
      "exploiting context, 230\n",
      "gender identification, 222\n",
      "identifying dialogue act types, 235\n",
      "part-of-speech tagging, 229\n",
      "Recognizing Textual Entailment (RTE),\n",
      "235\n",
      "scaling up to large datasets, 237\n",
      "sentence segmentation, 233\n",
      "sequence classification, 231–233\n",
      "Swadesh wordlists, 65\n",
      "symbol processing, language processing\n",
      "versus, 442\n",
      "synonyms, 67\n",
      "synsets, 67\n",
      "semantic similarity, 71\n",
      "in WordNet concept hierarchy, 69\n",
      "476 | General Index...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 497, 'page_label': '476', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4887}\n",
      "\n",
      "--- Chunk 4888 ---\n",
      "Content:\n",
      "syntactic agreement, 329–331\n",
      "syntactic cues to word category, 211\n",
      "syntactic structure, recursion in, 301\n",
      "syntax, 295–298\n",
      "syntax errors, 3\n",
      "T\n",
      "\\t tab character in regular expressions, 111\n",
      "T9 system, entering text on mobile phones, 99\n",
      "tabs\n",
      "avoiding in code indentation, 138\n",
      "matching in regular expressions, 109\n",
      "tag patterns, 266\n",
      "matching, precedence in, 267\n",
      "tagging, 179–219\n",
      "adjectives and adverbs, 186\n",
      "combining taggers, 205\n",
      "default tagger, 198\n",
      "evaluating tagger performance, 201\n",
      "exploring tagged corpora, 187–189\n",
      "lookup tagger, 200–201\n",
      "mapping words to tags using Python\n",
      "dictionaries, 189–198\n",
      "nouns, 184\n",
      "part-of-speech (POS) tagging, 229\n",
      "performance limitations, 206\n",
      "reading tagged corpora, 181\n",
      "regular expression tagger, 199\n",
      "representing tagged tokens, 181\n",
      "resources for further reading, 214\n",
      "across sentence boundaries, 208\n",
      "separating training and testing data, 203\n",
      "simplified part-of-speech tagset, 183\n",
      "storing taggers, 206\n",
      "transformation-based, 208–210\n",
      "unigram tagging, 202\n",
      "unknown words, 206\n",
      "unsimplified POS tags, 187\n",
      "using POS (part-of-speech) tagger, 179\n",
      "verbs, 185\n",
      "tags\n",
      "in feature structures, 340\n",
      "IOB tags representing chunk structures,\n",
      "269\n",
      "XML, 425\n",
      "tagsets, 179\n",
      "morphosyntactic information in POS\n",
      "tagsets, 212\n",
      "simplified POS tagset, 183\n",
      "terms (first-order logic), 372\n",
      "test sets, 44, 223\n",
      "choosing for classification models, 238\n",
      "testing classifier for document classification,\n",
      "228\n",
      "text, 1\n",
      "computing statistics from, 16–22\n",
      "counting vocabulary, 7–10\n",
      "entering on mobile phones (T9 system), 99\n",
      "as lists of words, 10–16\n",
      "searching, 4–7\n",
      "examining common contexts, 5\n",
      "text alignment, 30\n",
      "text editor, creating programs with, 56\n",
      "textonyms, 99\n",
      "textual entailment, 32\n",
      "textwrap module, 120\n",
      "theorem proving in first order logic, 375\n",
      "timeit module, 164\n",
      "TIMIT Corpus, 407–412\n",
      "tokenization, 80\n",
      "chunking and, 264\n",
      "in information retrieval, 263\n",
      "issues with, 111\n",
      "list produced from tokenizing string, 86\n",
      "regular expressions for, 109–112\n",
      "representing tagged tokens, 181\n",
      "segmentation and, 112\n",
      "with Unicode strings as input and output,\n",
      "97\n",
      "tokenized text, searching, 105\n",
      "tokens, 8\n",
      "Toolbox, 66, 412, 431–435\n",
      "accessing data from XML, using\n",
      "ElementTree, 429\n",
      "adding field to each entry, 431\n",
      "resources for further reading, 438\n",
      "validating lexicon, 432–435\n",
      "tools for creation, publication, and use of\n",
      "linguistic data, 421\n",
      "top-down approach to dynamic programming,\n",
      "167\n",
      "top-down parsing, 304\n",
      "total likelihood, 251\n",
      "training\n",
      "classifier, 223\n",
      "classifier for document classification, 228\n",
      "classifier-based chunkers, 274–278\n",
      "taggers, 203\n",
      "General Index | 477...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 498, 'page_label': '477', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4888}\n",
      "\n",
      "--- Chunk 4889 ---\n",
      "Content:\n",
      "unigram chunker using CoNLL 2000\n",
      "Chunking Corpus, 273\n",
      "training sets, 223, 225\n",
      "transformation-based tagging, 208–210\n",
      "transitive verbs, 314, 391–394\n",
      "translations\n",
      "comparative wordlists, 66\n",
      "machine (see machine translation)\n",
      "treebanks, 315–317\n",
      "trees, 279–281\n",
      "representing chunks, 270\n",
      "traversal of, 280\n",
      "trie, 162\n",
      "trigram taggers, 204\n",
      "truth conditions, 368\n",
      "truth-conditional \n",
      "semantics in first-order logic,\n",
      "377\n",
      "tuples, 133\n",
      "lists versus, 136\n",
      "parentheses with, 134\n",
      "representing tagged tokens, 181\n",
      "Turing Test, 31, 368\n",
      "type-raising, 390\n",
      "type-token distinction, 8\n",
      "TypeError, 157\n",
      "types, 8, 86\n",
      "(see also data types)\n",
      "types (first-order logic), 373\n",
      "U\n",
      "unary predicate, 372\n",
      "unbounded dependency constructions, 349–\n",
      "353\n",
      "defined, 350\n",
      "underspecified, 333\n",
      "Unicode, 93–97\n",
      "decoding and encoding, 94\n",
      "definition and description of, 94\n",
      "extracting gfrom files, 94\n",
      "resources for further information, 122\n",
      "using your local encoding in Python, 97\n",
      "unicodedata module, 96\n",
      "unification, 342–344\n",
      "unigram taggers\n",
      "confusion matrix for, 240\n",
      "noun phrase chunking with, 272\n",
      "unigram tagging, 202\n",
      "lookup tagger (example), 200\n",
      "separating training and test data, 203\n",
      "unique beginners, 69\n",
      "Universal Feed Parser, 83\n",
      "universal quantifier, 374\n",
      "unknown words, tagging, 206\n",
      "updating dictionary incrementally, 195\n",
      "US Presidential Inaugural Addresses Corpus,\n",
      "45\n",
      "user input, capturing, 85\n",
      "V\n",
      "valencies, 313\n",
      "validity of arguments, 369\n",
      "validity of XML documents, 426\n",
      "valuation, 377\n",
      "examining quantifier scope ambiguity, 381\n",
      "Mace4 model converted to, 384\n",
      "valuation function, 377\n",
      "values, 191\n",
      "complex, 196\n",
      "variables\n",
      "arguments of predicates in first-order logic,\n",
      "373\n",
      "assignment, 378\n",
      "bound by quantifiers in first-order logic,\n",
      "373\n",
      "defining, 14\n",
      "local, 58\n",
      "naming, 15\n",
      "relabeling bound variables, 389\n",
      "satisfaction of, using to interpret quantified\n",
      "formulas, 380\n",
      "scope of, 145\n",
      "verb phrase (VP), 297\n",
      "verbs\n",
      "agreement paradigm for English regular\n",
      "verbs, 329\n",
      "auxiliary, 336\n",
      "auxiliary verbs and inversion of subject and\n",
      "verb, 348\n",
      "categorizing and tagging, 185\n",
      "examining for dependency grammar, 312\n",
      "head of sentence and dependencies, 310\n",
      "present participle, 211\n",
      "transitive, 391–394\n",
      "W\n",
      "\\W non-word characters in Python, 110, 111\n",
      "\\w word characters in Python, 110, 111\n",
      "478 | General Index...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 499, 'page_label': '478', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4889}\n",
      "\n",
      "--- Chunk 4890 ---\n",
      "Content:\n",
      "web text, 42\n",
      "Web, obtaining data from, 416\n",
      "websites, obtaining corpora from, 416\n",
      "weighted grammars, 318–321\n",
      "probabilistic \n",
      "context-free grammar (PCFG),\n",
      "320\n",
      "well-formed (XML), 425\n",
      "well-formed formulas, 368\n",
      "well-formed substring tables (WFST), 307–\n",
      "310\n",
      "whitespace\n",
      "regular expression characters for, 109\n",
      "tokenizing text on, 109\n",
      "wildcard symbol (.), 98\n",
      "windowdiff scorer, 414\n",
      "word classes, 179\n",
      "word comparison operators, 23\n",
      "word occurrence, counting in text, 8\n",
      "word offset, 45\n",
      "word processor files, obtaining data from, 417\n",
      "word segmentation, 113–116\n",
      "word sense disambiguation, 28\n",
      "word sequences, 7\n",
      "wordlist corpora, 60–63\n",
      "WordNet, 67–73\n",
      "concept hierarchy, 69\n",
      "lemmatizer, 108\n",
      "more lexical relations, 70\n",
      "semantic similarity, 71\n",
      "visualization of hypernym hierarchy using\n",
      "Matplotlib and NetworkX, 170\n",
      "Words Corpus, 60\n",
      "words( ) function, 40\n",
      "wrapping text, 120\n",
      "X\n",
      "XML, 425–431\n",
      "ElementTree interface, 427–429\n",
      "formatting entries, 430\n",
      "representation of lexical entry from chunk\n",
      "parsing Toolbox record, 434\n",
      "resources for further reading, 438\n",
      "role of, in using to represent linguistic\n",
      "structures, 426\n",
      "using ElementTree to access Toolbox data,\n",
      "429\n",
      "using for linguistic structures, 425\n",
      "validity of documents, 426\n",
      "Z\n",
      "zero counts (naive Bayes classifier), 249\n",
      "zero projection, 347\n",
      "General Index | 479...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 500, 'page_label': '479', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4890}\n",
      "\n",
      "--- Chunk 4891 ---\n",
      "Content:\n",
      "About the Authors\n",
      "Steven Bird is Associate Professor in the Department of Computer Science and Soft-\n",
      "ware Engineering at the University of Melbourne, and Senior Research Associate in the\n",
      "Linguistic Data Consortium at the University of Pennsylvania. He completed a Ph.D.\n",
      "on computational phonology at the University of Edinburgh in 1990, supervised by\n",
      "Ewan Klein. He later moved to Cameroon to conduct linguistic fieldwork on the Grass-\n",
      "fields Bantu languages under the auspices of the Summer Institute of Linguistics. More\n",
      "recently, he spent several years as Associate Director of the Linguistic Data Consortium,\n",
      "where he led an R&D team to create models and tools for large databases of annotated\n",
      "text. At Melbourne University, he established a language technology research group\n",
      "and has taught at all levels of the undergraduate computer science curriculum. In 2009,\n",
      "Steven is President of the Association for Computational Linguistics.\n",
      "Ewan Klein is Professor of Language Technology in the School of Informatics at the\n",
      "University of Edinburgh. He completed a Ph.D. on formal semantics at the University\n",
      "of Cambridge in 1978. After some years working at the Universities of Sussex and\n",
      "Newcastle upon Tyne, Ewan took up a teaching position at Edinburgh. He was involved\n",
      "in the establishment of Edinburgh’s Language Technology Group in 1993, and has\n",
      "been closely associated with it ever since. From 2000 to 2002, he took leave from the\n",
      "University to act as Research Manager for the Edinburgh-based Natural Language Re-\n",
      "search Group of Edify Corporation, Santa Clara, and was responsible for spoken dia-\n",
      "logue processing. Ewan is a past President of the European Chapter of the Association\n",
      "for Computational Linguistics and was a founding member and Coordinator of the\n",
      "European Network of Excellence in Human Language Technologies (ELSNET).\n",
      "Edward Loper has recently completed a Ph.D. on machine learning for natural lan-\n",
      "guage processing at the University of Pennsylvania. Edward was a student in Steven’s\n",
      "graduate course on computational linguistics in the fall of 2000, and went on to be a\n",
      "Teacher’s Assistant and share in the development of NLTK. In addition to NLTK, he\n",
      "has helped develop two packages for documenting and testing Python software,\n",
      "epydoc and doctest.\n",
      "Colophon\n",
      "The animal on the cover of Natural Language Processing with Python is a right whale,\n",
      "the \n",
      "rarest of all large whales. It is identifiable by its enormous head, which can measure\n",
      "up to one-third of its total body length. It lives in temperate and cool seas in both\n",
      "hemispheres at the surface of the ocean. It’s believed that the right whale may have\n",
      "gotten its name from whalers who thought that it was the “right” whale to kill for oil.\n",
      "Even though it has been protected since the 1930s, the right whale is still the most\n",
      "endangered of all the great whales.\n",
      "The large and bulky right whale is easily distinguished from other whales by the calluses\n",
      "on its head. It has a broad back without a dorsal fin and a long arching mouth that...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 502, 'page_label': '481', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4891}\n",
      "\n",
      "--- Chunk 4892 ---\n",
      "Content:\n",
      "begins above the eye. Its body is black, except for a white patch on its belly. Wounds\n",
      "and \n",
      "scars may appear bright orange, often becoming infested with whale lice or\n",
      "cyamids. The calluses—which are also found near the blowholes, above the eyes, and\n",
      "on the chin, and upper lip—are black or gray. It has large flippers that are shaped like\n",
      "paddles, and a distinctive V-shaped blow, caused by the widely spaced blowholes on\n",
      "the top of its head, which rises to 16 feet above the ocean’s surface.\n",
      "The right whale feeds on planktonic organisms, including shrimp-like krill and cope-\n",
      "pods. As baleen whales, they have a series of 225–250 fringed overlapping plates hang-\n",
      "ing from each side of the upper jaw, where teeth would otherwise be located. The plates\n",
      "are black and can be as long as 7.2 feet. Right whales are “grazers of the sea,” often\n",
      "swimming slowly with their mouths open. As water flows into the mouth and through\n",
      "the baleen, prey is trapped near the tongue.\n",
      "Because females are not sexually mature until 10 years of age and they give birth to a\n",
      "single calf after a year-long pregnancy, populations grow slowly. The young right whale\n",
      "stays with its mother for one year.\n",
      "Right whales are found worldwide but in very small numbers. A right whale is com-\n",
      "monly found alone or in small groups of 1 to 3, but when courting, they may form\n",
      "groups of up to 30. Like most baleen whales, they are seasonally migratory. They inhabit\n",
      "colder waters for feeding and then migrate to warmer waters for breeding and calving.\n",
      "Although they may move far out to sea during feeding seasons, right whales give birth\n",
      "in coastal areas. Interestingly, many of the females do not return to these coastal breed-\n",
      "ing areas every year, but visit the area only in calving years. Where they go in other\n",
      "years remains a mystery.\n",
      "The right whale’s only predators are orcas and humans. When danger lurks, a group\n",
      "of right whales may come together in a circle, with their tails pointing outward, to deter\n",
      "a predator. This defense is not always successful and calves are occasionally separated\n",
      "from their mother and killed.\n",
      "Right whales are among the slowest swimming whales, although they may reach speeds\n",
      "up to 10 mph in short spurts. They can dive to at least 1,000 feet and can stay submerged\n",
      "for up to 40 minutes. The right whale is extremely endangered, even after years of\n",
      "protected status. Only in the past 15 years is there evidence of a population recovery\n",
      "in the Southern Hemisphere, and it is still not known if the right whale will survive at\n",
      "all in the Northern Hemisphere. Although not presently hunted, current conservation\n",
      "problems include collisions with ships, conflicts with fishing activities, habitat de-\n",
      "struction, oil drilling, and possible competition from other whale species. Right whales\n",
      "have no teeth, so ear bones and, in some cases, eye lenses can be used to estimate the\n",
      "age of a right whale at death. It is believed that right whales live at least 50 years, but\n",
      "there is little data on their longevity.\n",
      "The cover image is from the Dover Pictorial Archive. The cover font is Adobe ITC\n",
      "Garamond. The text font is Linotype Birka; the heading font is Adobe Myriad Con-\n",
      "densed; and the code font is LucasFont’s TheSansMonoCondensed....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 2.6.0 (Linux)', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2009-06-11T09:25:02-05:00', 'author': 'Steven Bird', 'moddate': '2009-06-11T13:29:29-04:00', 'title': 'Natural Language Processing with Python', 'trapped': '/False', 'source': '/home/bengtegard/github/data-science-rag/data/python/Natural Language Processing with Python.pdf', 'total_pages': 504, 'page': 503, 'page_label': '482', 'rel_path': 'python/Natural Language Processing with Python.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4892}\n",
      "\n",
      "--- Chunk 4893 ---\n",
      "Content:\n",
      "Jake VanderPlas\n",
      "Python  \n",
      "Data Science \n",
      "Handbook\n",
      "ESSENTIAL TOOLS FOR WORKING WITH DATA\n",
      "powered by...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 0, 'page_label': 'Cover', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4893}\n",
      "\n",
      "--- Chunk 4894 ---\n",
      "Content:\n",
      "Jake VanderPlas\n",
      "Python Data Science Handbook\n",
      "Essential Tools for Working with Data\n",
      "Boston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 2, 'page_label': 'i', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4894}\n",
      "\n",
      "--- Chunk 4895 ---\n",
      "Content:\n",
      "978-1-491-91205-8\n",
      "[LSI]\n",
      "Python Data Science Handbook\n",
      "by Jake VanderPlas\n",
      "Copyright © 2017 Jake VanderPlas. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles ( http://oreilly.com/safari). For more information, contact our corporate/insti‐\n",
      "tutional sales department: 800-998-9938 or corporate@oreilly.com.\n",
      "Editor: Dawn Schanafelt\n",
      "Production Editor: Kristen Brown\n",
      "Copyeditor: Jasmine Kwityn\n",
      "Proofreader: Rachel Monaghan\n",
      "Indexer: WordCo Indexing Services, Inc.\n",
      "Interior Designer: David Futato\n",
      "Cover Designer: Karen Montgomery\n",
      "Illustrator: Rebecca Demarest\n",
      "December 2016:  First Edition\n",
      "Revision History for the First Edition\n",
      "2016-11-17: First Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781491912058 for release details.\n",
      "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Python Data Science Handbook , the\n",
      "cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n",
      "While the publisher and the author have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk. If any code samples or other technology this work contains or describes is subject to open source\n",
      "licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
      "thereof complies with such licenses and/or rights....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 3, 'page_label': 'ii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4895}\n",
      "\n",
      "--- Chunk 4896 ---\n",
      "Content:\n",
      "Table of Contents\n",
      "Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\n",
      "1. IPython: Beyond Normal Python. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 4, 'page_label': 'iii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4896}\n",
      "\n",
      "--- Chunk 4897 ---\n",
      "Content:\n",
      ".  1\n",
      "Shell or Notebook?                                                                                                             2\n",
      "Launching the IPython Shell                                                                                        2\n",
      "Launching the Jupyter Notebook                                                                                 2\n",
      "Help and Documentation in IPython                                                                             3\n",
      "Accessing Documentation with ?                                                                                 3\n",
      "Accessing Source Code with ?...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 4, 'page_label': 'iii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4897}\n",
      "\n",
      "--- Chunk 4898 ---\n",
      "Content:\n",
      "?                                                                                     5\n",
      "Exploring Modules with Tab Completion                                                                  6\n",
      "Keyboard Shortcuts in the IPython Shell                                                                       8\n",
      "Navigation Shortcuts                                                                                                      8\n",
      "Text Entry Shortcuts                                                                                                      9\n",
      "Command History Shortcuts                                                                                       9\n",
      "Miscellaneous Shortcuts                                                                                              10\n",
      "IPython Magic Commands                                                                                            10...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 4, 'page_label': 'iii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4898}\n",
      "\n",
      "--- Chunk 4899 ---\n",
      "Content:\n",
      "IPython Magic Commands                                                                                            10\n",
      "Pasting Code Blocks: %paste and %cpaste                                                               11\n",
      "Running External Code: %run                                                                                   12\n",
      "Timing Code Execution: %timeit                                                                               12\n",
      "Help on Magic Functions: ?, %magic, and %lsmagic                                              13\n",
      "Input and Output History                                                                                              13\n",
      "IPython’s In and Out Objects                                                                                      13\n",
      "Underscore Shortcuts and Previous Outputs                                                           15\n",
      "Suppressing Output                                                                                                     15...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 4, 'page_label': 'iii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4899}\n",
      "\n",
      "--- Chunk 4900 ---\n",
      "Content:\n",
      "Suppressing Output                                                                                                     15\n",
      "Related Magic Commands                                                                                          16\n",
      "IPython and Shell Commands                                                                                       16\n",
      "Quick Introduction to the Shell                                                                                 16\n",
      "Shell Commands in IPython                                                                                      18\n",
      "iii...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 4, 'page_label': 'iii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4900}\n",
      "\n",
      "--- Chunk 4901 ---\n",
      "Content:\n",
      "Passing Values to and from the Shell                                                                         18\n",
      "Shell-Related Magic Commands                                                                                   19\n",
      "Errors and Debugging                                                                                                     20\n",
      "Controlling Exceptions: %xmode                                                                              20\n",
      "Debugging: When Reading Tracebacks Is Not Enough                                          22\n",
      "Profiling and Timing Code                                                                                             25\n",
      "Timing Code Snippets: %timeit and %time                                                             25\n",
      "Profiling Full Scripts: %prun                                                                                      27\n",
      "Line-by-Line Profiling with %lprun                                                                          28...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 5, 'page_label': 'iv', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4901}\n",
      "\n",
      "--- Chunk 4902 ---\n",
      "Content:\n",
      "Line-by-Line Profiling with %lprun                                                                          28\n",
      "Profiling Memory Use: %memit and %mprun                                                        29\n",
      "More IPython Resources                                                                                                 30\n",
      "Web Resources                                                                                                              30\n",
      "Books                                                                                                                              31\n",
      "2...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 5, 'page_label': 'iv', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4902}\n",
      "\n",
      "--- Chunk 4903 ---\n",
      "Content:\n",
      ". Introduction to NumPy. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 5, 'page_label': 'iv', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4903}\n",
      "\n",
      "--- Chunk 4904 ---\n",
      "Content:\n",
      ".  33\n",
      "Understanding Data Types in Python                                                                           34\n",
      "A Python Integer Is More Than Just an Integer                                                       35\n",
      "A Python List Is More Than Just a List                                                                     37\n",
      "Fixed-Type Arrays in Python                                                                                     38\n",
      "Creating Arrays from Python Lists                                                                            39\n",
      "Creating Arrays from Scratch                                                                                     39\n",
      "NumPy Standard Data Types                                                                                     41\n",
      "The Basics of NumPy Arrays                                                                                          42\n",
      "NumPy Array Attributes                                                                                             42...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 5, 'page_label': 'iv', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4904}\n",
      "\n",
      "--- Chunk 4905 ---\n",
      "Content:\n",
      "NumPy Array Attributes                                                                                             42\n",
      "Array Indexing: Accessing Single Elements                                                             43\n",
      "Array Slicing: Accessing Subarrays                                                                            44\n",
      "Reshaping of Arrays                                                                                                     47\n",
      "Array Concatenation and Splitting                                                                            48\n",
      "Computation on NumPy Arrays: Universal Functions                                              50\n",
      "The Slowness of Loops                                                                                                50\n",
      "Introducing UFuncs                                                                                                     51...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 5, 'page_label': 'iv', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4905}\n",
      "\n",
      "--- Chunk 4906 ---\n",
      "Content:\n",
      "Introducing UFuncs                                                                                                     51\n",
      "Exploring NumPy’s UFuncs                                                                                        52\n",
      "Advanced Ufunc Features                                                                                           56\n",
      "Ufuncs: Learning More                                                                                                58\n",
      "Aggregations: Min, Max, and Everything in Between                                                58\n",
      "Summing the Values in an Array                                                                               59\n",
      "Minimum and Maximum                                                                                           59\n",
      "Example: What Is the Average Height of US Presidents...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 5, 'page_label': 'iv', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4906}\n",
      "\n",
      "--- Chunk 4907 ---\n",
      "Content:\n",
      "?                                       61\n",
      "Computation on Arrays: Broadcasting                                                                         63\n",
      "Introducing Broadcasting                                                                                           63\n",
      "Rules of Broadcasting                                                                                                  65\n",
      "Broadcasting in Practice                                                                                              68\n",
      "iv | Table of Contents...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 5, 'page_label': 'iv', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4907}\n",
      "\n",
      "--- Chunk 4908 ---\n",
      "Content:\n",
      "Comparisons, Masks, and Boolean Logic                                                                     70\n",
      "Example: Counting Rainy Days                                                                                 70\n",
      "Comparison Operators as ufuncs                                                                              71\n",
      "Working with Boolean Arrays                                                                                    73\n",
      "Boolean Arrays as Masks                                                                                             75\n",
      "Fancy Indexing                                                                                                                 78\n",
      "Exploring Fancy Indexing                                                                                           79\n",
      "Combined Indexing                                                                                                     80...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 6, 'page_label': 'v', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4908}\n",
      "\n",
      "--- Chunk 4909 ---\n",
      "Content:\n",
      "Combined Indexing                                                                                                     80\n",
      "Example: Selecting Random Points                                                                           81\n",
      "Modifying Values with Fancy Indexing                                                                    82\n",
      "Example: Binning Data                                                                                                83\n",
      "Sorting Arrays                                                                                                                  85\n",
      "Fast Sorting in NumPy: np.sort and np.argsort                                                       86\n",
      "Partial Sorts: Partitioning                                                                                            88\n",
      "Example: k-Nearest Neighbors                                                                                   88...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 6, 'page_label': 'v', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4909}\n",
      "\n",
      "--- Chunk 4910 ---\n",
      "Content:\n",
      "Example: k-Nearest Neighbors                                                                                   88\n",
      "Structured Data: NumPy’s Structured Arrays                                                              92\n",
      "Creating Structured Arrays                                                                                         94\n",
      "More Advanced Compound Types                                                                            95\n",
      "RecordArrays: Structured Arrays with a Twist                                                        96\n",
      "On to Pandas                                                                                                                 96\n",
      "3...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 6, 'page_label': 'v', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4910}\n",
      "\n",
      "--- Chunk 4911 ---\n",
      "Content:\n",
      ". Data Manipulation with Pandas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 6, 'page_label': 'v', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4911}\n",
      "\n",
      "--- Chunk 4912 ---\n",
      "Content:\n",
      ".  97\n",
      "Installing and Using Pandas                                                                                           97\n",
      "Introducing Pandas Objects                                                                                           98\n",
      "The Pandas Series Object                                                                                            99\n",
      "The Pandas DataFrame Object                                                                                 102\n",
      "The Pandas Index Object                                                                                          105\n",
      "Data Indexing and Selection                                                                                        107\n",
      "Data Selection in Series                                                                                             107\n",
      "Data Selection in DataFrame                                                                                    110...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 6, 'page_label': 'v', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4912}\n",
      "\n",
      "--- Chunk 4913 ---\n",
      "Content:\n",
      "Data Selection in DataFrame                                                                                    110\n",
      "Operating on Data in Pandas                                                                                       115\n",
      "Ufuncs: Index Preservation                                                                                      115\n",
      "UFuncs: Index Alignment                                                                                         116\n",
      "Ufuncs: Operations Between DataFrame and Series                                            118\n",
      "Handling Missing Data                                                                                                 119\n",
      "Trade-Offs in Missing Data Conventions                                                               120\n",
      "Missing Data in Pandas                                                                                             120\n",
      "Operating on Null Values                                                                                          124...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 6, 'page_label': 'v', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4913}\n",
      "\n",
      "--- Chunk 4914 ---\n",
      "Content:\n",
      "Operating on Null Values                                                                                          124\n",
      "Hierarchical Indexing                                                                                                   128\n",
      "A Multiply Indexed Series                                                                                         128\n",
      "Methods of MultiIndex Creation                                                                             131\n",
      "Indexing and Slicing a MultiIndex                                                                          134\n",
      "Table of Contents | v...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 6, 'page_label': 'v', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4914}\n",
      "\n",
      "--- Chunk 4915 ---\n",
      "Content:\n",
      "Rearranging Multi-Indices                                                                                        137\n",
      "Data Aggregations on Multi-Indices                                                                       140\n",
      "Combining Datasets: Concat and Append                                                                 141\n",
      "Recall: Concatenation of NumPy Arrays                                                                142\n",
      "Simple Concatenation with pd.concat                                                                    142\n",
      "Combining Datasets: Merge and Join                                                                         146\n",
      "Relational Algebra                                                                                                      146\n",
      "Categories of Joins                                                                                                      147\n",
      "Specification of the Merge Key                                                                                 149...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 7, 'page_label': 'vi', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4915}\n",
      "\n",
      "--- Chunk 4916 ---\n",
      "Content:\n",
      "Specification of the Merge Key                                                                                 149\n",
      "Specifying Set Arithmetic for Joins                                                                         152\n",
      "Overlapping Column Names: The suffixes Keyword                                           153\n",
      "Example: US States Data                                                                                           154\n",
      "Aggregation and Grouping                                                                                          158\n",
      "Planets Data                                                                                                                159\n",
      "Simple Aggregation in Pandas                                                                                 159\n",
      "GroupBy: Split, Apply, Combine                                                                             161...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 7, 'page_label': 'vi', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4916}\n",
      "\n",
      "--- Chunk 4917 ---\n",
      "Content:\n",
      "GroupBy: Split, Apply, Combine                                                                             161\n",
      "Pivot Tables                                                                                                                     170\n",
      "Motivating Pivot Tables                                                                                             170\n",
      "Pivot Tables by Hand                                                                                                 171\n",
      "Pivot Table Syntax                                                                                                      171\n",
      "Example: Birthrate Data                                                                                            174\n",
      "Vectorized String Operations                                                                                       178\n",
      "Introducing Pandas String Operations                                                                   178...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 7, 'page_label': 'vi', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4917}\n",
      "\n",
      "--- Chunk 4918 ---\n",
      "Content:\n",
      "Introducing Pandas String Operations                                                                   178\n",
      "Tables of Pandas String Methods                                                                             180\n",
      "Example: Recipe Database                                                                                        184\n",
      "Working with Time Series                                                                                            188\n",
      "Dates and Times in Python                                                                                       188\n",
      "Pandas Time Series: Indexing by Time                                                                   192\n",
      "Pandas Time Series Data Structures                                                                        192\n",
      "Frequencies and Offsets                                                                                            195\n",
      "Resampling, Shifting, and Windowing                                                                   196...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 7, 'page_label': 'vi', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4918}\n",
      "\n",
      "--- Chunk 4919 ---\n",
      "Content:\n",
      "Resampling, Shifting, and Windowing                                                                   196\n",
      "Where to Learn More                                                                                                202\n",
      "Example: Visualizing Seattle Bicycle Counts                                                          202\n",
      "High-Performance Pandas: eval() and query()                                                         208\n",
      "Motivating query() and eval(): Compound Expressions                                      209\n",
      "pandas.eval() for Efficient Operations                                                                    210\n",
      "DataFrame.eval() for Column-Wise Operations                                                   211\n",
      "DataFrame.query() Method                                                                                      213\n",
      "Performance: When to Use These Functions                                                         214\n",
      "Further Resources                                                                                                          215\n",
      "vi | Table of Contents...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 7, 'page_label': 'vi', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4919}\n",
      "\n",
      "--- Chunk 4920 ---\n",
      "Content:\n",
      "4. Visualization with Matplotlib. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 8, 'page_label': 'vii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4920}\n",
      "\n",
      "--- Chunk 4921 ---\n",
      "Content:\n",
      ".  217\n",
      "General Matplotlib Tips                                                                                                218\n",
      "Importing matplotlib                                                                                                 218\n",
      "Setting Styles                                                                                                               218\n",
      "show() or No show()...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 8, 'page_label': 'vii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4921}\n",
      "\n",
      "--- Chunk 4922 ---\n",
      "Content:\n",
      "? How to Display Y our Plots                                                218\n",
      "Saving Figures to File                                                                                                 221\n",
      "Two Interfaces for the Price of One                                                                            222\n",
      "Simple Line Plots                                                                                                           224\n",
      "Adjusting the Plot: Line Colors and Styles                                                             226\n",
      "Adjusting the Plot: Axes Limits                                                                                228\n",
      "Labeling Plots                                                                                                              230\n",
      "Simple Scatter Plots                                                                                                       233...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 8, 'page_label': 'vii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4922}\n",
      "\n",
      "--- Chunk 4923 ---\n",
      "Content:\n",
      "Simple Scatter Plots                                                                                                       233\n",
      "Scatter Plots with plt.plot                                                                                          233\n",
      "Scatter Plots with plt.scatter                                                                                      235\n",
      "plot Versus scatter: A Note on Efficiency                                                               237\n",
      "Visualizing Errors                                                                                                          237\n",
      "Basic Errorbars                                                                                                           238\n",
      "Continuous Errors                                                                                                     239...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 8, 'page_label': 'vii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4923}\n",
      "\n",
      "--- Chunk 4924 ---\n",
      "Content:\n",
      "Continuous Errors                                                                                                     239\n",
      "Density and Contour Plots                                                                                           241\n",
      "Visualizing a Three-Dimensional Function                                                           241\n",
      "Histograms, Binnings, and Density                                                                            245\n",
      "Two-Dimensional Histograms and Binnings                                                        247\n",
      "Customizing Plot Legends                                                                                            249\n",
      "Choosing Elements for the Legend                                                                          251\n",
      "Legend for Size of Points                                                                                           252...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 8, 'page_label': 'vii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4924}\n",
      "\n",
      "--- Chunk 4925 ---\n",
      "Content:\n",
      "Legend for Size of Points                                                                                           252\n",
      "Multiple Legends                                                                                                        254\n",
      "Customizing Colorbars                                                                                                 255\n",
      "Customizing Colorbars                                                                                             256\n",
      "Example: Handwritten Digits                                                                                   261\n",
      "Multiple Subplots                                                                                                           262\n",
      "plt.axes: Subplots by Hand                                                                                        263\n",
      "plt.subplot: Simple Grids of Subplots                                                                      264 plt.subplot: Simple Grids of Subplots             ...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 8, 'page_label': 'vii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4925}\n",
      "\n",
      "--- Chunk 4926 ---\n",
      "Content:\n",
      "                                                         264\n",
      "plt.subplots: The Whole Grid in One Go                                                               265\n",
      "plt.GridSpec: More Complicated Arrangements                                                   266\n",
      "Text and Annotation                                                                                                     268\n",
      "Example: Effect of Holidays on US Births                                                              269\n",
      "Transforms and Text Position                                                                                  270\n",
      "Arrows and Annotation                                                                                            272\n",
      "Customizing Ticks                                                                                                         275\n",
      "Major and Minor Ticks                                                                                             276...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 8, 'page_label': 'vii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4926}\n",
      "\n",
      "--- Chunk 4927 ---\n",
      "Content:\n",
      "Major and Minor Ticks                                                                                             276\n",
      "Hiding Ticks or Labels                                                                                               277\n",
      "Reducing or Increasing the Number of Ticks                                                        278\n",
      "Table of Contents | vii...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 8, 'page_label': 'vii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4927}\n",
      "\n",
      "--- Chunk 4928 ---\n",
      "Content:\n",
      "Fancy Tick Formats                                                                                                    279\n",
      "Summary of Formatters and Locators                                                                    281\n",
      "Customizing Matplotlib: Configurations and Stylesheets                                       282\n",
      "Plot Customization by Hand                                                                                    282\n",
      "Changing the Defaults: rcParams                                                                            284\n",
      "Stylesheets                                                                                                                   285\n",
      "Three-Dimensional Plotting in Matplotlib                                                                290\n",
      "Three-Dimensional Points and Lines                                                                      291\n",
      "Three-Dimensional Contour Plots                                                                          292...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 9, 'page_label': 'viii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4928}\n",
      "\n",
      "--- Chunk 4929 ---\n",
      "Content:\n",
      "Three-Dimensional Contour Plots                                                                          292\n",
      "Wireframes and Surface Plots                                                                                  293\n",
      "Surface Triangulations                                                                                               295\n",
      "Geographic Data with Basemap                                                                                   298\n",
      "Map Projections                                                                                                         300\n",
      "Drawing a Map Background                                                                                     304\n",
      "Plotting Data on Maps                                                                                               307\n",
      "Example: California Cities                                                                                        308...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 9, 'page_label': 'viii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4929}\n",
      "\n",
      "--- Chunk 4930 ---\n",
      "Content:\n",
      "Example: California Cities                                                                                        308\n",
      "Example: Surface Temperature Data                                                                       309\n",
      "Visualization with Seaborn                                                                                           311\n",
      "Seaborn Versus Matplotlib                                                                                        312\n",
      "Exploring Seaborn Plots                                                                                            313\n",
      "Example: Exploring Marathon Finishing Times                                                   322\n",
      "Further Resources                                                                                                          329\n",
      "Matplotlib Resources                                                                                                 329...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 9, 'page_label': 'viii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4930}\n",
      "\n",
      "--- Chunk 4931 ---\n",
      "Content:\n",
      "Matplotlib Resources                                                                                                 329\n",
      "Other Python Graphics Libraries                                                                             330\n",
      "5...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 9, 'page_label': 'viii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4931}\n",
      "\n",
      "--- Chunk 4932 ---\n",
      "Content:\n",
      ". Machine Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ....\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 9, 'page_label': 'viii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4932}\n",
      "\n",
      "--- Chunk 4933 ---\n",
      "Content:\n",
      ".  331\n",
      "What Is Machine Learning...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 9, 'page_label': 'viii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4933}\n",
      "\n",
      "--- Chunk 4934 ---\n",
      "Content:\n",
      "?                                                                                         332\n",
      "Categories of Machine Learning                                                                              332\n",
      "Qualitative Examples of Machine Learning Applications                                    333\n",
      "Summary                                                                                                                     342\n",
      "Introducing Scikit-Learn                                                                                              343\n",
      "Data Representation in Scikit-Learn                                                                       343\n",
      "Scikit-Learn’s Estimator API                                                                                     346\n",
      "Application: Exploring Handwritten Digits                                                           354...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 9, 'page_label': 'viii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4934}\n",
      "\n",
      "--- Chunk 4935 ---\n",
      "Content:\n",
      "Application: Exploring Handwritten Digits                                                           354\n",
      "Summary                                                                                                                     359\n",
      "Hyperparameters and Model Validation                                                                    359\n",
      "Thinking About Model Validation                                                                          359\n",
      "Selecting the Best Model                                                                                           363\n",
      "Learning Curves                                                                                                         370\n",
      "Validation in Practice: Grid Search                                                                         373\n",
      "Summary                                                                                                                     375...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 9, 'page_label': 'viii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4935}\n",
      "\n",
      "--- Chunk 4936 ---\n",
      "Content:\n",
      "Summary                                                                                                                     375\n",
      "Feature Engineering                                                                                                      375\n",
      "viii | Table of Contents...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 9, 'page_label': 'viii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4936}\n",
      "\n",
      "--- Chunk 4937 ---\n",
      "Content:\n",
      "Categorical Features                                                                                                   376\n",
      "Text Features                                                                                                               377\n",
      "Image Features                                                                                                            378\n",
      "Derived Features                                                                                                         378\n",
      "Imputation of Missing Data                                                                                      381\n",
      "Feature Pipelines                                                                                                        381\n",
      "In Depth: Naive Bayes Classification                                                                          382...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 10, 'page_label': 'ix', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4937}\n",
      "\n",
      "--- Chunk 4938 ---\n",
      "Content:\n",
      "In Depth: Naive Bayes Classification                                                                          382\n",
      "Bayesian Classification                                                                                              383\n",
      "Gaussian Naive Bayes                                                                                                383\n",
      "Multinomial Naive Bayes                                                                                          386\n",
      "When to Use Naive Bayes                                                                                         389\n",
      "In Depth: Linear Regression                                                                                        390\n",
      "Simple Linear Regression                                                                                          390\n",
      "Basis Function Regression                                                                                        392...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 10, 'page_label': 'ix', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4938}\n",
      "\n",
      "--- Chunk 4939 ---\n",
      "Content:\n",
      "Basis Function Regression                                                                                        392\n",
      "Regularization                                                                                                             396\n",
      "Example: Predicting Bicycle Traffic                                                                         400\n",
      "In-Depth: Support Vector Machines                                                                           405\n",
      "Motivating Support Vector Machines                                                                     405\n",
      "Support Vector Machines: Maximizing the Margin                                              407\n",
      "Example: Face Recognition                                                                                       416\n",
      "Support Vector Machine Summary                                                                         420\n",
      "In-Depth: Decision Trees and Random Forests                                                        421...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 10, 'page_label': 'ix', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4939}\n",
      "\n",
      "--- Chunk 4940 ---\n",
      "Content:\n",
      "Support Vector Machine Summary                                                                         420\n",
      "In-Depth: Decision Trees and Random Forests                                                        421\n",
      "Motivating Random Forests: Decision Trees                                                         421\n",
      "Ensembles of Estimators: Random Forests                                                            426\n",
      "Random Forest Regression                                                                                       428\n",
      "Example: Random Forest for Classifying Digits                                                    430\n",
      "Summary of Random Forests                                                                                   432\n",
      "In Depth: Principal Component Analysis                                                                  433\n",
      "Introducing Principal Component Analysis                                                          433\n",
      "PCA as Noise Filtering                                                                                              440...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 10, 'page_label': 'ix', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4940}\n",
      "\n",
      "--- Chunk 4941 ---\n",
      "Content:\n",
      "PCA as Noise Filtering                                                                                              440\n",
      "Example: Eigenfaces                                                                                                   442\n",
      "Principal Component Analysis Summary                                                              445\n",
      "In-Depth: Manifold Learning                                                                                      445\n",
      "Manifold Learning: “HELLO”                                                                                  446\n",
      "Multidimensional Scaling (MDS)                                                                            447\n",
      "MDS as Manifold Learning                                                                                      450\n",
      "Nonlinear Embeddings: Where MDS Fails                                                            452\n",
      "Nonlinear Manifolds: Locally Linear Embedding                                                 453...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 10, 'page_label': 'ix', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4941}\n",
      "\n",
      "--- Chunk 4942 ---\n",
      "Content:\n",
      "Nonlinear Embeddings: Where MDS Fails                                                            452\n",
      "Nonlinear Manifolds: Locally Linear Embedding                                                 453\n",
      "Some Thoughts on Manifold Methods                                                                   455\n",
      "Example: Isomap on Faces                                                                                        456\n",
      "Example: Visualizing Structure in Digits                                                                460\n",
      "In Depth: k-Means Clustering                                                                                     462\n",
      "Table of Contents | ix...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 10, 'page_label': 'ix', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4942}\n",
      "\n",
      "--- Chunk 4943 ---\n",
      "Content:\n",
      "Introducing k-Means                                                                                                 463\n",
      "k-Means Algorithm: Expectation–Maximization                                                  465\n",
      "Examples                                                                                                                      470\n",
      "In Depth: Gaussian Mixture Models                                                                           476\n",
      "Motivating GMM: Weaknesses of k-Means                                                           477\n",
      "Generalizing E–M: Gaussian Mixture Models                                                       480\n",
      "GMM as Density Estimation                                                                                    484\n",
      "Example: GMM for Generating New Data                                                             488\n",
      "In-Depth: Kernel Density Estimation                                                                         491...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 11, 'page_label': 'x', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4943}\n",
      "\n",
      "--- Chunk 4944 ---\n",
      "Content:\n",
      "In-Depth: Kernel Density Estimation                                                                         491\n",
      "Motivating KDE: Histograms                                                                                   491\n",
      "Kernel Density Estimation in Practice                                                                    496\n",
      "Example: KDE on a Sphere                                                                                       498\n",
      "Example: Not-So-Naive Bayes                                                                                  501\n",
      "Application: A Face Detection Pipeline                                                                      506\n",
      "HOG Features                                                                                                             506\n",
      "HOG in Action: A Simple Face Detector                                                                507...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 11, 'page_label': 'x', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4944}\n",
      "\n",
      "--- Chunk 4945 ---\n",
      "Content:\n",
      "HOG in Action: A Simple Face Detector                                                                507\n",
      "Caveats and Improvements                                                                                      512\n",
      "Further Machine Learning Resources                                                                         514\n",
      "Machine Learning in Python                                                                                    514\n",
      "General Machine Learning                                                                                       515\n",
      "Index...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 11, 'page_label': 'x', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4945}\n",
      "\n",
      "--- Chunk 4946 ---\n",
      "Content:\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  517\n",
      "x | Table of Contents...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 11, 'page_label': 'x', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'content_category': 'python', 'chunk_id': 4946}\n",
      "\n",
      "--- Chunk 4947 ---\n",
      "Content:\n",
      "Preface\n",
      "What Is Data Science?\n",
      "This is a book about doing data science with Python, which immediately begs the\n",
      "question: what is data science? It’s a surprisingly hard definition to nail down, espe‐\n",
      "cially given how ubiquitous the term has become. Vocal critics have variously dis‐\n",
      "missed the term as a superfluous label (after all, what science doesn’t involve data?) or\n",
      "a simple buzzword that only exists to salt résumés and catch the eye of overzealous\n",
      "tech recruiters.\n",
      "In my mind, these critiques miss something important. Data science, despite its hype-\n",
      "laden veneer, is perhaps the best label we have for the cross-disciplinary set of skills\n",
      "that are becoming increasingly important in many applications across industry and\n",
      "academia. This cross-disciplinary piece is key: in my mind, the best existing defini‐\n",
      "tion of data science is illustrated by Drew Conway’s Data Science Venn Diagram, first\n",
      "published on his blog in September 2010 (see Figure P-1).\n",
      "Figure P-1. Drew Conway’s Data Science Venn Diagram\n",
      "xi...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 12, 'page_label': 'xi', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4947}\n",
      "\n",
      "--- Chunk 4948 ---\n",
      "Content:\n",
      "While some of the intersection labels are a bit tongue-in-cheek, this diagram captures\n",
      "the essence of what I think people mean when they say “data science”: it is fundamen‐\n",
      "tally an interdisciplinary subject. Data science comprises three distinct and overlap‐\n",
      "ping areas: the skills of a statistician who knows how to model and summarize\n",
      "datasets (which are growing ever larger); the skills of a computer scientist who can\n",
      "design and use algorithms to efficiently store, process, and visualize this data; and the\n",
      "domain expertise—what we might think of as “classical” training in a subject—neces‐\n",
      "sary both to formulate the right questions and to put their answers in context.\n",
      "With this in mind, I would encourage you to think of data science not as a new\n",
      "domain of knowledge to learn, but as a new set of skills that you can apply within\n",
      "your current area of expertise. Whether you are reporting election results, forecasting\n",
      "stock returns, optimizing online ad clicks, identifying microorganisms in microscope\n",
      "photos, seeking new classes of astronomical objects, or working with data in any\n",
      "other field, the goal of this book is to give you the ability to ask and answer new ques‐\n",
      "tions about your chosen subject area.\n",
      "Who Is This Book For?\n",
      "In my teaching both at the University of Washington and at various tech-focused\n",
      "conferences and meetups, one of the most common questions I have heard is this:\n",
      "“how should I learn Python?” The people asking are generally technically minded\n",
      "students, developers, or researchers, often with an already strong background in writ‐\n",
      "ing code and using computational and numerical tools. Most of these folks don’t want\n",
      "to learn Python per se, but want to learn the language with the aim of using it as a\n",
      "tool for data-intensive and computational science. While a large patchwork of videos,\n",
      "blog posts, and tutorials for this audience is available online, I’ve long been frustrated\n",
      "by the lack of a single good answer to this question; that is what inspired this book.\n",
      "The book is not meant to be an introduction to Python or to programming in gen‐\n",
      "eral; I assume the reader has familiarity with the Python language, including defining\n",
      "functions, assigning variables, calling methods of objects, controlling the flow of a\n",
      "program, and other basic tasks. Instead, it is meant to help Python users learn to use\n",
      "Python’s data science stack—libraries such as IPython, NumPy, Pandas, Matplotlib,\n",
      "Scikit-Learn, and related tools—to effectively store, manipulate, and gain insight\n",
      "from data.\n",
      "Why Python?\n",
      "Python has emerged over the last couple decades as a first-class tool for scientific\n",
      "computing tasks, including the analysis and visualization of large datasets. This may\n",
      "have come as a surprise to early proponents of the Python language: the language\n",
      "itself was not specifically designed with data analysis or scientific computing in mind.\n",
      "xii | Preface...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 13, 'page_label': 'xii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4948}\n",
      "\n",
      "--- Chunk 4949 ---\n",
      "Content:\n",
      "The usefulness of Python for data science stems primarily from the large and active\n",
      "ecosystem of third-party packages: NumPy for manipulation of homogeneous array-\n",
      "based data, Pandas for manipulation of heterogeneous and labeled data, SciPy for\n",
      "common scientific computing tasks, Matplotlib for publication-quality visualizations,\n",
      "IPython for interactive execution and sharing of code, Scikit-Learn for machine\n",
      "learning, and many more tools that will be mentioned in the following pages.\n",
      "If you are looking for a guide to the Python language itself, I would suggest the sister\n",
      "project to this book, A Whirlwind Tour of the Python Language. This short report pro‐\n",
      "vides a tour of the essential features of the Python language, aimed at data scientists\n",
      "who already are familiar with one or more other programming languages.\n",
      "Python 2 Versus Python 3\n",
      "This book uses the syntax of Python 3, which contains language enhancements that\n",
      "are not compatible with the 2.x series of Python. Though Python 3.0 was first released\n",
      "in 2008, adoption has been relatively slow, particularly in the scientific and web devel‐\n",
      "opment communities. This is primarily because it took some time for many of the\n",
      "essential third-party packages and toolkits to be made compatible with the new lan‐\n",
      "guage internals. Since early 2014, however, stable releases of the most important tools\n",
      "in the data science ecosystem have been fully compatible with both Python 2 and 3,\n",
      "and so this book will use the newer Python 3 syntax. However, the vast majority of\n",
      "code snippets in this book will also work without modification in Python 2: in cases\n",
      "where a Py2-incompatible syntax is used, I will make every effort to note it explicitly.\n",
      "Outline of This Book\n",
      "Each chapter of this book focuses on a particular package or tool that contributes a\n",
      "fundamental piece of the Python data science story.\n",
      "IPython and Jupyter (Chapter 1)\n",
      "These packages provide the computational environment in which many Python-\n",
      "using data scientists work.\n",
      "NumPy (Chapter 2)\n",
      "This library provides the ndarray object for efficient storage and manipulation of\n",
      "dense data arrays in Python.\n",
      "Pandas (Chapter 3)\n",
      "This library provides the DataFrame object for efficient storage and manipulation\n",
      "of labeled/columnar data in Python.\n",
      "Matplotlib (Chapter 4)\n",
      "This library provides capabilities for a flexible range of data visualizations in\n",
      "Python.\n",
      "Preface | xiii...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 14, 'page_label': 'xiii', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4949}\n",
      "\n",
      "--- Chunk 4950 ---\n",
      "Content:\n",
      "Scikit-Learn (Chapter 5)\n",
      "This library provides efficient and clean Python implementations of the most\n",
      "important and established machine learning algorithms.\n",
      "The PyData world is certainly much larger than these five packages, and is growing\n",
      "every day. With this in mind, I make every attempt through these pages to provide\n",
      "references to other interesting efforts, projects, and packages that are pushing the\n",
      "boundaries of what can be done in Python. Nevertheless, these five are currently fun‐\n",
      "damental to much of the work being done in the Python data science space, and I\n",
      "expect they will remain important even as the ecosystem continues growing around\n",
      "them.\n",
      "Using Code Examples\n",
      "Supplemental material (code examples, figures, etc.) is available for download at\n",
      "https://github.com/jakevdp/PythonDataScienceHandbook. This book is here to help\n",
      "you get your job done. In general, if example code is offered with this book, you may\n",
      "use it in your programs and documentation. Y ou do not need to contact us for per‐\n",
      "mission unless you’re reproducing a significant portion of the code. For example,\n",
      "writing a program that uses several chunks of code from this book does not require\n",
      "permission. Selling or distributing a CD-ROM of examples from O’Reilly books does\n",
      "require permission. Answering a question by citing this book and quoting example\n",
      "code does not require permission. Incorporating a significant amount of example\n",
      "code from this book into your product’s documentation does require permission.\n",
      "We appreciate, but do not require, attribution. An attribution usually includes the\n",
      "title, author, publisher, and ISBN. For example, “ Python Data Science Handbook  by\n",
      "Jake VanderPlas (O’Reilly). Copyright 2017 Jake VanderPlas, 978-1-491-91205-8. ”\n",
      "If you feel your use of code examples falls outside fair use or the permission given\n",
      "above, feel free to contact us at permissions@oreilly.com.\n",
      "Installation Considerations\n",
      "Installing Python and the suite of libraries that enable scientific computing is\n",
      "straightforward. This section will outline some of the considerations to keep in mind\n",
      "when setting up your computer.\n",
      "Though there are various ways to install Python, the one I would suggest for use in\n",
      "data science is the Anaconda distribution, which works similarly whether you use\n",
      "Windows, Linux, or Mac OS X. The Anaconda distribution comes in two flavors:\n",
      "• Miniconda gives you the Python interpreter itself, along with a command-line\n",
      "tool called conda that operates as a cross-platform package manager geared\n",
      "xiv | Preface...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 15, 'page_label': 'xiv', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4950}\n",
      "\n",
      "--- Chunk 4951 ---\n",
      "Content:\n",
      "toward Python packages, similar in spirit to the apt or yum tools that Linux users\n",
      "might be familiar with.\n",
      "• Anaconda includes both Python and conda, and additionally bundles a suite of\n",
      "other preinstalled packages geared toward scientific computing. Because of the\n",
      "size of this bundle, expect the installation to consume several gigabytes of disk\n",
      "space.\n",
      "Any of the packages included with Anaconda can also be installed manually on top of\n",
      "Miniconda; for this reason I suggest starting with Miniconda.\n",
      "To get started, download and install the Miniconda package (make sure to choose a\n",
      "version with Python 3), and then install the core packages used in this book:\n",
      "[~]$ conda install numpy pandas scikit-learn matplotlib seaborn ipython-notebook\n",
      "Throughout the text, we will also make use of other, more specialized tools in\n",
      "Python’s scientific ecosystem; installation is usually as easy as typing conda install\n",
      "packagename. For more information on conda, including information about creating\n",
      "and using conda environments (which I would highly recommend), refer to conda’s\n",
      "online documentation.\n",
      "Conventions Used in This Book\n",
      "The following typographical conventions are used in this book:\n",
      "Italic\n",
      "Indicates new terms, URLs, email addresses, filenames, and file extensions.\n",
      "Constant width\n",
      "Used for program listings, as well as within paragraphs to refer to program ele‐\n",
      "ments such as variable or function names, databases, data types, environment\n",
      "variables, statements, and keywords.\n",
      "Constant width bold\n",
      "Shows commands or other text that should be typed literally by the user.\n",
      "Constant width italic\n",
      "Shows text that should be replaced with user-supplied values or by values deter‐\n",
      "mined by context.\n",
      "O’Reilly Safari\n",
      "Safari (formerly Safari Books Online) is a membership-based\n",
      "training and reference platform for enterprise, government,\n",
      "educators, and individuals.\n",
      "Preface | xv...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 16, 'page_label': 'xv', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4951}\n",
      "\n",
      "--- Chunk 4952 ---\n",
      "Content:\n",
      "Members have access to thousands of books, training videos, Learning Paths, interac‐\n",
      "tive tutorials, and curated playlists from over 250 publishers, including O’Reilly\n",
      "Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\n",
      "sional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\n",
      "John Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\n",
      "Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\n",
      "Course Technology, among others.\n",
      "For more information, please visit http://oreilly.com/safari.\n",
      "How to Contact Us\n",
      "Please address comments and questions concerning this book to the publisher:\n",
      "O’Reilly Media, Inc.\n",
      "1005 Gravenstein Highway North\n",
      "Sebastopol, CA 95472\n",
      "800-998-9938 (in the United States or Canada)\n",
      "707-829-0515 (international or local)\n",
      "707-829-0104 (fax)\n",
      "We have a web page for this book, where we list errata, examples, and any additional\n",
      "information. Y ou can access this page at http://bit.ly/python-data-sci-handbook.\n",
      "To comment or ask technical questions about this book, send email to bookques‐\n",
      "tions@oreilly.com.\n",
      "For more information about our books, courses, conferences, and news, see our web‐\n",
      "site at http://www.oreilly.com.\n",
      "Find us on Facebook: http://facebook.com/oreilly\n",
      "Follow us on Twitter: http://twitter.com/oreillymedia\n",
      "Watch us on Y ouTube: http://www.youtube.com/oreillymedia\n",
      "xvi | Preface...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 17, 'page_label': 'xvi', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4952}\n",
      "\n",
      "--- Chunk 4953 ---\n",
      "Content:\n",
      "CHAPTER 1\n",
      "IPython: Beyond Normal Python\n",
      "There are many options for development environments for Python, and I’m often\n",
      "asked which one I use in my own work. My answer sometimes surprises people: my\n",
      "preferred environment is IPython plus a text editor (in my case, Emacs or Atom\n",
      "depending on my mood). IPython (short for Interactive Python) was started in 2001\n",
      "by Fernando Perez as an enhanced Python interpreter, and has since grown into a\n",
      "project aiming to provide, in Perez’s words, “Tools for the entire lifecycle of research\n",
      "computing. ” If Python is the engine of our data science task, you might think of IPy‐\n",
      "thon as the interactive control panel.\n",
      "As well as being a useful interactive interface to Python, IPython also provides a\n",
      "number of useful syntactic additions to the language; we’ll cover the most useful of\n",
      "these additions here. In addition, IPython is closely tied with the Jupyter project ,\n",
      "which provides a browser-based notebook that is useful for development, collabora‐\n",
      "tion, sharing, and even publication of data science results. The IPython notebook is\n",
      "actually a special case of the broader Jupyter notebook structure, which encompasses\n",
      "notebooks for Julia, R, and other programming languages. As an example of the use‐\n",
      "fulness of the notebook format, look no further than the page you are reading: the\n",
      "entire manuscript for this book was composed as a set of IPython notebooks.\n",
      "IPython is about using Python effectively for interactive scientific and data-intensive\n",
      "computing. This chapter will start by stepping through some of the IPython features\n",
      "that are useful to the practice of data science, focusing especially on the syntax it\n",
      "offers beyond the standard features of Python. Next, we will go into a bit more depth\n",
      "on some of the more useful “magic commands” that can speed up common tasks in\n",
      "creating and using data science code. Finally, we will touch on some of the features of\n",
      "the notebook that make it useful in understanding data and sharing results.\n",
      "1...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 18, 'page_label': '1', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4953}\n",
      "\n",
      "--- Chunk 4954 ---\n",
      "Content:\n",
      "Shell or Notebook?\n",
      "There are two primary means of using IPython that we’ll discuss in this chapter: the\n",
      "IPython shell and the IPython notebook. The bulk of the material in this chapter is\n",
      "relevant to both, and the examples will switch between them depending on what is\n",
      "most convenient. In the few sections that are relevant to just one or the other, I will\n",
      "explicitly state that fact. Before we start, some words on how to launch the IPython\n",
      "shell and IPython notebook.\n",
      "Launching the IPython Shell\n",
      "This chapter, like most of this book, is not designed to be absorbed passively. I recom‐\n",
      "mend that as you read through it, you follow along and experiment with the tools and\n",
      "syntax we cover: the muscle-memory you build through doing this will be far more\n",
      "useful than the simple act of reading about it. Start by launching the IPython inter‐\n",
      "preter by typing ipython on the command line; alternatively, if you’ve installed a dis‐\n",
      "tribution like Anaconda or EPD, there may be a launcher specific to your system\n",
      "(we’ll discuss this more fully in “Help and Documentation in IPython” on page 3).\n",
      "Once you do this, you should see a prompt like the following:\n",
      "IPython 4.0.1 -- An enhanced Interactive Python.\n",
      "?         -> Introduction and overview of IPython's features.\n",
      "%quickref -> Quick reference.\n",
      "help      -> Python's own help system.\n",
      "object?   -> Details about 'object', use 'object??' for extra details.\n",
      "In [1]:\n",
      "With that, you’re ready to follow along.\n",
      "Launching the Jupyter Notebook\n",
      "The Jupyter notebook is a browser-based graphical interface to the IPython shell, and\n",
      "builds on it a rich set of dynamic display capabilities. As well as executing Python/\n",
      "IPython statements, the notebook allows the user to include formatted text, static and\n",
      "dynamic visualizations, mathematical equations, JavaScript widgets, and much more.\n",
      "Furthermore, these documents can be saved in a way that lets other people open them\n",
      "and execute the code on their own systems.\n",
      "Though the IPython notebook is viewed and edited through your web browser win‐\n",
      "dow, it must connect to a running Python process in order to execute code. To start\n",
      "this process (known as a “kernel”), run the following command in your system shell:\n",
      "$ jupyter notebook\n",
      "This command will launch a local web server that will be visible to your browser. It\n",
      "immediately spits out a log showing what it is doing; that log will look something like\n",
      "this:\n",
      "2 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 19, 'page_label': '2', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4954}\n",
      "\n",
      "--- Chunk 4955 ---\n",
      "Content:\n",
      "$ jupyter notebook\n",
      "[NotebookApp] Serving notebooks from local directory: /Users/jakevdp/...\n",
      "[NotebookApp] 0 active kernels\n",
      "[NotebookApp] The IPython Notebook is running at: http://localhost:8888/\n",
      "[NotebookApp] Use Control-C to stop this server and shut down all kernels...\n",
      "Upon issuing the command, your default browser should automatically open and\n",
      "navigate to the listed local URL; the exact address will depend on your system. If the\n",
      "browser does not open automatically, you can open a window and manually open this\n",
      "address (http://localhost:8888/ in this example).\n",
      "Help and Documentation in IPython\n",
      "If you read no other section in this chapter, read this one: I find the tools discussed\n",
      "here to be the most transformative contributions of IPython to my daily workflow.\n",
      "When a technologically minded person is asked to help a friend, family member, or\n",
      "colleague with a computer problem, most of the time it’s less a matter of knowing the\n",
      "answer as much as knowing how to quickly find an unknown answer. In data science\n",
      "it’s the same: searchable web resources such as online documentation, mailing-list\n",
      "threads, and Stack Overflow answers contain a wealth of information, even (espe‐\n",
      "cially?) if it is a topic you’ve found yourself searching before. Being an effective prac‐\n",
      "titioner of data science is less about memorizing the tool or command you should use\n",
      "for every possible situation, and more about learning to effectively find the informa‐\n",
      "tion you don’t know, whether through a web search engine or another means.\n",
      "One of the most useful functions of IPython/Jupyter is to shorten the gap between the\n",
      "user and the type of documentation and search that will help them do their work\n",
      "effectively. While web searches still play a role in answering complicated questions,\n",
      "an amazing amount of information can be found through IPython alone. Some\n",
      "examples of the questions IPython can help answer in a few keystrokes:\n",
      "• How do I call this function? What arguments and options does it have?\n",
      "• What does the source code of this Python object look like?\n",
      "• What is in this package I imported? What attributes or methods does this object\n",
      "have?\n",
      "Here we’ll discuss IPython’s tools to quickly access this information, namely the ?\n",
      "character to explore documentation, the ?? characters to explore source code, and the\n",
      "Tab key for autocompletion.\n",
      "Accessing Documentation with ?\n",
      "The Python language and its data science ecosystem are built with the user in mind,\n",
      "and one big part of that is access to documentation. Every Python object contains the\n",
      "Help and Documentation in IPython | 3...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 20, 'page_label': '3', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4955}\n",
      "\n",
      "--- Chunk 4956 ---\n",
      "Content:\n",
      "reference to a string, known as a docstring, which in most cases will contain a concise\n",
      "summary of the object and how to use it. Python has a built-in help() function that\n",
      "can access this information and print the results. For example, to see the documenta‐\n",
      "tion of the built-in len function, you can do the following:\n",
      "In [1]: help(len)\n",
      "Help on built-in function len in module builtins:\n",
      "len(...)\n",
      "    len(object) -> integer\n",
      "    Return the number of items of a sequence or mapping.\n",
      "Depending on your interpreter, this information may be displayed as inline text, or in\n",
      "some separate pop-up window.\n",
      "Because finding help on an object is so common and useful, IPython introduces the ?\n",
      "character as a shorthand for accessing this documentation and other relevant\n",
      "information:\n",
      "In [2]: len?\n",
      "Type:        builtin_function_or_method\n",
      "String form: <built-in function len>\n",
      "Namespace:   Python builtin\n",
      "Docstring:\n",
      "len(object) -> integer\n",
      "Return the number of items of a sequence or mapping.\n",
      "This notation works for just about anything, including object methods:\n",
      "In [3]: L = [1, 2, 3]\n",
      "In [4]: L.insert?\n",
      "Type:        builtin_function_or_method\n",
      "String form: <built-in method insert of list object at 0x1024b8ea8>\n",
      "Docstring:   L.insert(index, object) -- insert object before index\n",
      "or even objects themselves, with the documentation from their type:\n",
      "In [5]: L?\n",
      "Type:        list\n",
      "String form: [1, 2, 3]\n",
      "Length:      3\n",
      "Docstring:\n",
      "list() -> new empty list\n",
      "list(iterable) -> new list initialized from iterable's items\n",
      "Importantly, this will even work for functions or other objects you create yourself!\n",
      "Here we’ll define a small function with a docstring:\n",
      "In [6]: def square(a):\n",
      "  ....:     \"\"\"Return the square of a.\"\"\"\n",
      "4 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 21, 'page_label': '4', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4956}\n",
      "\n",
      "--- Chunk 4957 ---\n",
      "Content:\n",
      "....:     return a ** 2\n",
      "  ....:\n",
      "Note that to create a docstring for our function, we simply placed a string literal in\n",
      "the first line. Because docstrings are usually multiple lines, by convention we used\n",
      "Python’s triple-quote notation for multiline strings.\n",
      "Now we’ll use the ? mark to find this docstring:\n",
      "In [7]: square?\n",
      "Type:        function\n",
      "String form: <function square at 0x103713cb0>\n",
      "Definition:  square(a)\n",
      "Docstring:   Return the square of a.\n",
      "This quick access to documentation via docstrings is one reason you should get in the\n",
      "habit of always adding such inline documentation to the code you write!\n",
      "Accessing Source Code with ??\n",
      "Because the Python language is so easily readable, you can usually gain another level\n",
      "of insight by reading the source code of the object you’re curious about. IPython pro‐\n",
      "vides a shortcut to the source code with the double question mark (??):\n",
      "In [8]: square??\n",
      "Type:        function\n",
      "String form: <function square at 0x103713cb0>\n",
      "Definition:  square(a)\n",
      "Source:\n",
      "def square(a):\n",
      "    \"Return the square of a\"\n",
      "    return a ** 2\n",
      "For simple functions like this, the double question mark can give quick insight into\n",
      "the under-the-hood details.\n",
      "If you play with this much, you’ll notice that sometimes the ?? suffix doesn’t display\n",
      "any source code: this is generally because the object in question is not implemented in\n",
      "Python, but in C or some other compiled extension language. If this is the case, the ??\n",
      "suffix gives the same output as the ? suffix. Y ou’ll find this particularly with many of\n",
      "Python’s built-in objects and types, for example len from above:\n",
      "In [9]: len??\n",
      "Type:        builtin_function_or_method\n",
      "String form: <built-in function len>\n",
      "Namespace:   Python builtin\n",
      "Docstring:\n",
      "len(object) -> integer\n",
      "Return the number of items of a sequence or mapping.\n",
      "Help and Documentation in IPython | 5...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 22, 'page_label': '5', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4957}\n",
      "\n",
      "--- Chunk 4958 ---\n",
      "Content:\n",
      "Using ? and/or ?? gives a powerful and quick interface for finding information about\n",
      "what any Python function or module does.\n",
      "Exploring Modules with Tab Completion\n",
      "IPython’s other useful interface is the use of the Tab key for autocompletion and\n",
      "exploration of the contents of objects, modules, and namespaces. In the examples that\n",
      "follow, we’ll use <TAB> to indicate when the Tab key should be pressed.\n",
      "Tab completion of object contents\n",
      "Every Python object has various attributes and methods associated with it. Like with\n",
      "the help function discussed before, Python has a built-in dir function that returns a\n",
      "list of these, but the tab-completion interface is much easier to use in practice. To see\n",
      "a list of all available attributes of an object, you can type the name of the object fol‐\n",
      "lowed by a period (.) character and the Tab key:\n",
      "In [10]: L.<TAB>\n",
      "L.append   L.copy     L.extend   L.insert   L.remove   L.sort\n",
      "L.clear    L.count    L.index    L.pop      L.reverse\n",
      "To narrow down the list, you can type the first character or several characters of the\n",
      "name, and the Tab key will find the matching attributes and methods:\n",
      "In [10]: L.c<TAB>\n",
      "L.clear  L.copy   L.count\n",
      "In [10]: L.co<TAB>\n",
      "L.copy   L.count\n",
      "If there is only a single option, pressing the Tab key will complete the line for you. For\n",
      "example, the following will instantly be replaced with L.count:\n",
      "In [10]: L.cou<TAB>\n",
      "Though Python has no strictly enforced distinction between public/external\n",
      "attributes and private/internal attributes, by convention a preceding underscore is\n",
      "used to denote such methods. For clarity, these private methods and special methods\n",
      "are omitted from the list by default, but it’s possible to list them by explicitly typing\n",
      "the underscore:\n",
      "In [10]: L._<TAB>\n",
      "L.__add__           L.__gt__            L.__reduce__\n",
      "L.__class__         L.__hash__          L.__reduce_ex__\n",
      "For brevity, we’ve only shown the first couple lines of the output. Most of these are\n",
      "Python’s special double-underscore methods (often nicknamed “dunder” methods).\n",
      "6 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 23, 'page_label': '6', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4958}\n",
      "\n",
      "--- Chunk 4959 ---\n",
      "Content:\n",
      "Tab completion when importing\n",
      "Tab completion is also useful when importing objects from packages. Here we’ll use it\n",
      "to find all possible imports in the itertools package that start with co:\n",
      "In [10]: from itertools import co<TAB>\n",
      "combinations                   compress\n",
      "combinations_with_replacement  count\n",
      "Similarly, you can use tab completion to see which imports are available on your sys‐\n",
      "tem (this will change depending on which third-party scripts and modules are visible\n",
      "to your Python session):\n",
      "In [10]: import <TAB>\n",
      "Display all 399 possibilities? (y or n)\n",
      "Crypto              dis                 py_compile\n",
      "Cython              distutils           pyclbr\n",
      "...                 ...                 ...\n",
      "difflib             pwd                 zmq\n",
      "In [10]: import h<TAB>\n",
      "hashlib             hmac                http\n",
      "heapq               html                husl\n",
      "(Note that for brevity, I did not print here all 399 importable packages and modules\n",
      "on my system.)\n",
      "Beyond tab completion: Wildcard matching\n",
      "Tab completion is useful if you know the first few characters of the object or attribute\n",
      "you’re looking for, but is little help if you’ d like to match characters at the middle or\n",
      "end of the word. For this use case, IPython provides a means of wildcard matching\n",
      "for names using the * character.\n",
      "For example, we can use this to list every object in the namespace that ends with\n",
      "Warning:\n",
      "In [10]: *Warning?\n",
      "BytesWarning                  RuntimeWarning\n",
      "DeprecationWarning            SyntaxWarning\n",
      "FutureWarning                 UnicodeWarning\n",
      "ImportWarning                 UserWarning\n",
      "PendingDeprecationWarning     Warning\n",
      "ResourceWarning\n",
      "Notice that the * character matches any string, including the empty string.\n",
      "Similarly, suppose we are looking for a string method that contains the word find\n",
      "somewhere in its name. We can search for it this way:\n",
      "Help and Documentation in IPython | 7...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 24, 'page_label': '7', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4959}\n",
      "\n",
      "--- Chunk 4960 ---\n",
      "Content:\n",
      "In [10]: str.*find*?\n",
      "str.find\n",
      "str.rfind\n",
      "I find this type of flexible wildcard search can be very useful for finding a particular\n",
      "command when I’m getting to know a new package or reacquainting myself with a\n",
      "familiar one.\n",
      "Keyboard Shortcuts in the IPython Shell\n",
      "If you spend any amount of time on the computer, you’ve probably found a use for\n",
      "keyboard shortcuts in your workflow. Most familiar perhaps are Cmd-C and Cmd-V\n",
      "(or Ctrl-C and Ctrl-V) for copying and pasting in a wide variety of programs and sys‐\n",
      "tems. Power users tend to go even further: popular text editors like Emacs, Vim, and\n",
      "others provide users an incredible range of operations through intricate combina‐\n",
      "tions of keystrokes.\n",
      "The IPython shell doesn’t go this far, but does provide a number of keyboard short‐\n",
      "cuts for fast navigation while you’re typing commands. These shortcuts are not in fact\n",
      "provided by IPython itself, but through its dependency on the GNU Readline library:\n",
      "thus, some of the following shortcuts may differ depending on your system configu‐\n",
      "ration. Also, while some of these shortcuts do work in the browser-based notebook,\n",
      "this section is primarily about shortcuts in the IPython shell.\n",
      "Once you get accustomed to these, they can be very useful for quickly performing\n",
      "certain commands without moving your hands from the “home” keyboard position.\n",
      "If you’re an Emacs user or if you have experience with Linux-style shells, the follow‐\n",
      "ing will be very familiar. We’ll group these shortcuts into a few categories: navigation\n",
      "shortcuts, text entry shortcuts, command history shortcuts, and miscellaneous shortcuts.\n",
      "Navigation Shortcuts\n",
      "While the use of the left and right arrow keys to move backward and forward in the\n",
      "line is quite obvious, there are other options that don’t require moving your hands\n",
      "from the “home” keyboard position:\n",
      "Keystroke Action\n",
      "Ctrl-a Move cursor to the beginning of the line\n",
      "Ctrl-e Move cursor to the end of the line\n",
      "Ctrl-b (or the left arrow key) Move cursor back one character\n",
      "Ctrl-f (or the right arrow key) Move cursor forward one character\n",
      "8 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 25, 'page_label': '8', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4960}\n",
      "\n",
      "--- Chunk 4961 ---\n",
      "Content:\n",
      "Text Entry Shortcuts\n",
      "While everyone is familiar with using the Backspace key to delete the previous char‐\n",
      "acter, reaching for the key often requires some minor finger gymnastics, and it only\n",
      "deletes a single character at a time. In IPython there are several shortcuts for remov‐\n",
      "ing some portion of the text you’re typing. The most immediately useful of these are\n",
      "the commands to delete entire lines of text. Y ou’ll know these have become second\n",
      "nature if you find yourself using a combination of Ctrl-b and Ctrl-d instead of reach‐\n",
      "ing for the Backspace key to delete the previous character!\n",
      "Keystroke Action\n",
      "Backspace key Delete previous character in line\n",
      "Ctrl-d Delete next character in line\n",
      "Ctrl-k Cut text from cursor to end of line\n",
      "Ctrl-u Cut text from beginning fo line to cursor\n",
      "Ctrl-y Yank (i.e., paste) text that was previously cut\n",
      "Ctrl-t Transpose (i.e., switch) previous two characters\n",
      "Command History Shortcuts\n",
      "Perhaps the most impactful shortcuts discussed here are the ones IPython provides\n",
      "for navigating the command history. This command history goes beyond your cur‐\n",
      "rent IPython session: your entire command history is stored in a SQLite database in\n",
      "your IPython profile directory. The most straightforward way to access these is with\n",
      "the up and down arrow keys to step through the history, but other options exist as\n",
      "well:\n",
      "Keystroke Action\n",
      "Ctrl-p (or the up arrow key) Access previous command in history\n",
      "Ctrl-n (or the down arrow key) Access next command in history\n",
      "Ctrl-r Reverse-search through command history\n",
      "The reverse-search can be particularly useful. Recall that in the previous section we\n",
      "defined a function called square. Let’s reverse-search our Python history from a new\n",
      "IPython shell and find this definition again. When you press Ctrl-r in the IPython\n",
      "terminal, you’ll see the following prompt:\n",
      "In [1]:\n",
      "(reverse-i-search)`':\n",
      "If you start typing characters at this prompt, IPython will auto-fill the most recent\n",
      "command, if any, that matches those characters:\n",
      "Keyboard Shortcuts in the IPython Shell | 9...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 26, 'page_label': '9', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4961}\n",
      "\n",
      "--- Chunk 4962 ---\n",
      "Content:\n",
      "In [1]:\n",
      "(reverse-i-search)`sqa': square??\n",
      "At any point, you can add more characters to refine the search, or press Ctrl-r again\n",
      "to search further for another command that matches the query. If you followed along\n",
      "in the previous section, pressing Ctrl-r twice more gives:\n",
      "In [1]:\n",
      "(reverse-i-search)`sqa': def square(a):\n",
      "    \"\"\"Return the square of a\"\"\"\n",
      "    return a ** 2\n",
      "Once you have found the command you’re looking for, press Return and the search\n",
      "will end. We can then use the retrieved command, and carry on with our session:\n",
      "In [1]: def square(a):\n",
      "    \"\"\"Return the square of a\"\"\"\n",
      "    return a ** 2\n",
      "In [2]: square(2)\n",
      "Out[2]: 4\n",
      "Note that you can also use Ctrl-p/Ctrl-n or the up/down arrow keys to search\n",
      "through history, but only by matching characters at the beginning of the line. That is,\n",
      "if you type def and then press Ctrl-p, it would find the most recent command (if any)\n",
      "in your history that begins with the characters def.\n",
      "Miscellaneous Shortcuts\n",
      "Finally, there are a few miscellaneous shortcuts that don’t fit into any of the preceding\n",
      "categories, but are nevertheless useful to know:\n",
      "Keystroke Action\n",
      "Ctrl-l Clear terminal screen\n",
      "Ctrl-c Interrupt current Python command\n",
      "Ctrl-d Exit IPython session\n",
      "The Ctrl-c shortcut in particular can be useful when you inadvertently start a very\n",
      "long-running job.\n",
      "While some of the shortcuts discussed here may seem a bit tedious at first, they\n",
      "quickly become automatic with practice. Once you develop that muscle memory, I\n",
      "suspect you will even find yourself wishing they were available in other contexts.\n",
      "IPython Magic Commands\n",
      "The previous two sections showed how IPython lets you use and explore Python effi‐\n",
      "ciently and interactively. Here we’ll begin discussing some of the enhancements that\n",
      "10 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 27, 'page_label': '10', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4962}\n",
      "\n",
      "--- Chunk 4963 ---\n",
      "Content:\n",
      "IPython adds on top of the normal Python syntax. These are known in IPython as\n",
      "magic commands, and are prefixed by the % character. These magic commands are\n",
      "designed to succinctly solve various common problems in standard data analysis.\n",
      "Magic commands come in two flavors: line magics, which are denoted by a single %\n",
      "prefix and operate on a single line of input, and cell magics, which are denoted by a\n",
      "double %% prefix and operate on multiple lines of input. We’ll demonstrate and dis‐\n",
      "cuss a few brief examples here, and come back to more focused discussion of several\n",
      "useful magic commands later in the chapter.\n",
      "Pasting Code Blocks: %paste and %cpaste\n",
      "When you’re working in the IPython interpreter, one common gotcha is that pasting\n",
      "multiline code blocks can lead to unexpected errors, especially when indentation and\n",
      "interpreter markers are involved. A common case is that you find some example code\n",
      "on a website and want to paste it into your interpreter. Consider the following simple\n",
      "function:\n",
      ">>> def donothing(x):\n",
      "...     return x\n",
      "The code is formatted as it would appear in the Python interpreter, and if you copy\n",
      "and paste this directly into IPython you get an error:\n",
      "In [2]: >>> def donothing(x):\n",
      "   ...:     ...     return x\n",
      "   ...:\n",
      "  File \"<ipython-input-20-5a66c8964687>\", line 2\n",
      "    ...     return x\n",
      "                 ^\n",
      "SyntaxError: invalid syntax\n",
      "In the direct paste, the interpreter is confused by the additional prompt characters.\n",
      "But never fear—IPython’s %paste magic function is designed to handle this exact type\n",
      "of multiline, marked-up input:\n",
      "In [3]: %paste\n",
      ">>> def donothing(x):\n",
      "...     return x\n",
      "## -- End pasted text --\n",
      "The %paste command both enters and executes the code, so now the function is\n",
      "ready to be used:\n",
      "In [4]: donothing(10)\n",
      "Out[4]: 10\n",
      "A command with a similar intent is %cpaste, which opens up an interactive multiline\n",
      "prompt in which you can paste one or more chunks of code to be executed in a batch:\n",
      "IPython Magic Commands | 11...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 28, 'page_label': '11', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4963}\n",
      "\n",
      "--- Chunk 4964 ---\n",
      "Content:\n",
      "In [5]: %cpaste\n",
      "Pasting code; enter '--' alone on the line to stop or use Ctrl-D.\n",
      ":>>> def donothing(x):\n",
      ":...     return x\n",
      ":--\n",
      "These magic commands, like others we’ll see, make available functionality that would\n",
      "be difficult or impossible in a standard Python interpreter.\n",
      "Running External Code: %run\n",
      "As you begin developing more extensive code, you will likely find yourself working in\n",
      "both IPython for interactive exploration, as well as a text editor to store code that you\n",
      "want to reuse. Rather than running this code in a new window, it can be convenient\n",
      "to run it within your IPython session. This can be done with the %run magic.\n",
      "For example, imagine you’ve created a myscript.py file with the following contents:\n",
      "#-------------------------------------\n",
      "# file: myscript.py\n",
      "def square(x):\n",
      "    \"\"\"square a number\"\"\"\n",
      "    return x ** 2\n",
      "for N in range(1, 4):\n",
      "    print(N, \"squared is\", square(N))\n",
      "Y ou can execute this from your IPython session as follows:\n",
      "In [6]: %run myscript.py\n",
      "1 squared is 1\n",
      "2 squared is 4\n",
      "3 squared is 9\n",
      "Note also that after you’ve run this script, any functions defined within it are available\n",
      "for use in your IPython session:\n",
      "In [7]: square(5)\n",
      "Out[7]: 25\n",
      "There are several options to fine-tune how your code is run; you can see the docu‐\n",
      "mentation in the normal way, by typing %run? in the IPython interpreter.\n",
      "Timing Code Execution: %timeit\n",
      "Another example of a useful magic function is %timeit, which will automatically\n",
      "determine the execution time of the single-line Python statement that follows it. For\n",
      "example, we may want to check the performance of a list comprehension:\n",
      "In [8]: %timeit L = [n ** 2 for n in range(1000)]\n",
      "1000 loops, best of 3: 325 µs per loop\n",
      "12 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 29, 'page_label': '12', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4964}\n",
      "\n",
      "--- Chunk 4965 ---\n",
      "Content:\n",
      "The benefit of %timeit is that for short commands it will automatically perform mul‐\n",
      "tiple runs in order to attain more robust results. For multiline statements, adding a\n",
      "second % sign will turn this into a cell magic that can handle multiple lines of input.\n",
      "For example, here’s the equivalent construction with a for loop:\n",
      "In [9]: %%timeit\n",
      "   ...: L = []\n",
      "   ...: for n in range(1000):\n",
      "   ...:     L.append(n ** 2)\n",
      "   ...:\n",
      "1000 loops, best of 3: 373 µs per loop\n",
      "We can immediately see that list comprehensions are about 10% faster than the\n",
      "equivalent for loop construction in this case. We’ll explore %timeit and other\n",
      "approaches to timing and profiling code in “Profiling and Timing Code” on page 25.\n",
      "Help on Magic Functions: ?, %magic, and %lsmagic\n",
      "Like normal Python functions, IPython magic functions have docstrings, and this\n",
      "useful documentation can be accessed in the standard manner. So, for example, to\n",
      "read the documentation of the %timeit magic, simply type this:\n",
      "In [10]: %timeit?\n",
      "Documentation for other functions can be accessed similarly. To access a general\n",
      "description of available magic functions, including some examples, you can type this:\n",
      "In [11]: %magic\n",
      "For a quick and simple list of all available magic functions, type this:\n",
      "In [12]: %lsmagic\n",
      "Finally, I’ll mention that it is quite straightforward to define your own magic func‐\n",
      "tions if you wish. We won’t discuss it here, but if you are interested, see the references\n",
      "listed in “More IPython Resources” on page 30.\n",
      "Input and Output History\n",
      "Previously we saw that the IPython shell allows you to access previous commands\n",
      "with the up and down arrow keys, or equivalently the Ctrl-p/Ctrl-n shortcuts. Addi‐\n",
      "tionally, in both the shell and the notebook, IPython exposes several ways to obtain\n",
      "the output of previous commands, as well as string versions of the commands them‐\n",
      "selves. We’ll explore those here.\n",
      "IPython’s In and Out Objects\n",
      "By now I imagine you’re quite familiar with the In[1]:/Out[1]: style prompts used\n",
      "by IPython. But it turns out that these are not just pretty decoration: they give a clue\n",
      "Input and Output History | 13...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 30, 'page_label': '13', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4965}\n",
      "\n",
      "--- Chunk 4966 ---\n",
      "Content:\n",
      "as to how you can access previous inputs and outputs in your current session. Imag‐\n",
      "ine you start a session that looks like this:\n",
      "In [1]: import math\n",
      "In [2]: math.sin(2)\n",
      "Out[2]: 0.9092974268256817\n",
      "In [3]: math.cos(2)\n",
      "Out[3]: -0.4161468365471424\n",
      "We’ve imported the built-in math package, then computed the sine and the cosine of\n",
      "the number 2. These inputs and outputs are displayed in the shell with In/Out labels,\n",
      "but there’s more—IPython actually creates some Python variables called In and Out\n",
      "that are automatically updated to reflect this history:\n",
      "In [4]: print(In)\n",
      "['', 'import math', 'math.sin(2)', 'math.cos(2)', 'print(In)']\n",
      "In [5]: Out\n",
      "Out[5]: {2: 0.9092974268256817, 3: -0.4161468365471424}\n",
      "The In object is a list, which keeps track of the commands in order (the first item in\n",
      "the list is a placeholder so that In[1] can refer to the first command):\n",
      "In [6]: print(In[1])\n",
      "import math\n",
      "The Out object is not a list but a dictionary mapping input numbers to their outputs\n",
      "(if any):\n",
      "In [7]: print(Out[2])\n",
      "0.9092974268256817\n",
      "Note that not all operations have outputs: for example, import statements and print\n",
      "statements don’t affect the output. The latter may be surprising, but makes sense if\n",
      "you consider that print is a function that returns None; for brevity, any command\n",
      "that returns None is not added to Out.\n",
      "Where this can be useful is if you want to interact with past results. For example, let’s\n",
      "check the sum of sin(2) ** 2  and cos(2) ** 2  using the previously computed\n",
      "results:\n",
      "In [8]: Out[2] ** 2 + Out[3] ** 2\n",
      "Out[8]: 1.0\n",
      "The result is 1.0 as we’ d expect from the well-known trigonometric identity. In this\n",
      "case, using these previous results probably is not necessary, but it can become very\n",
      "handy if you execute a very expensive computation and want to reuse the result!\n",
      "14 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 31, 'page_label': '14', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4966}\n",
      "\n",
      "--- Chunk 4967 ---\n",
      "Content:\n",
      "Underscore Shortcuts and Previous Outputs\n",
      "The standard Python shell contains just one simple shortcut for accessing previous\n",
      "output; the variable _ (i.e., a single underscore) is kept updated with the previous out‐\n",
      "put; this works in IPython as well:\n",
      "In [9]: print(_)\n",
      "1.0\n",
      "But IPython takes this a bit further—you can use a double underscore to access the\n",
      "second-to-last output, and a triple underscore to access the third-to-last output (skip‐\n",
      "ping any commands with no output):\n",
      "In [10]: print(__)\n",
      "-0.4161468365471424\n",
      "In [11]: print(___)\n",
      "0.9092974268256817\n",
      "IPython stops there: more than three underscores starts to get a bit hard to count,\n",
      "and at that point it’s easier to refer to the output by line number.\n",
      "There is one more shortcut we should mention, however—a shorthand for Out[X] is\n",
      "_X (i.e., a single underscore followed by the line number):\n",
      "In [12]: Out[2]\n",
      "Out[12]: 0.9092974268256817\n",
      "In [13]: _2\n",
      "Out[13]: 0.9092974268256817\n",
      "Suppressing Output\n",
      "Sometimes you might wish to suppress the output of a statement (this is perhaps\n",
      "most common with the plotting commands that we’ll explore in Chapter 4 ). Or\n",
      "maybe the command you’re executing produces a result that you’ d prefer not to store\n",
      "in your output history, perhaps so that it can be deallocated when other references are\n",
      "removed. The easiest way to suppress the output of a command is to add a semicolon\n",
      "to the end of the line:\n",
      "In [14]: math.sin(2) + math.cos(2);\n",
      "Note that the result is computed silently, and the output is neither displayed on the\n",
      "screen or stored in the Out dictionary:\n",
      "In [15]: 14 in Out\n",
      "Out[15]: False\n",
      "Input and Output History | 15...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 32, 'page_label': '15', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4967}\n",
      "\n",
      "--- Chunk 4968 ---\n",
      "Content:\n",
      "Related Magic Commands\n",
      "For accessing a batch of previous inputs at once, the %history magic command is\n",
      "very helpful. Here is how you can print the first four inputs:\n",
      "In [16]: %history -n 1-4\n",
      "   1: import math\n",
      "   2: math.sin(2)\n",
      "   3: math.cos(2)\n",
      "   4: print(In)\n",
      "As usual, you can type %history? for more information and a description of options\n",
      "available. Other similar magic commands are %rerun (which will re-execute some\n",
      "portion of the command history) and %save (which saves some set of the command\n",
      "history to a file). For more information, I suggest exploring these using the ? help\n",
      "functionality discussed in “Help and Documentation in IPython” on page 3.\n",
      "IPython and Shell Commands\n",
      "When working interactively with the standard Python interpreter, one of the frustra‐\n",
      "tions you’ll face is the need to switch between multiple windows to access Python\n",
      "tools and system command-line tools. IPython bridges this gap, and gives you a syn‐\n",
      "tax for executing shell commands directly from within the IPython terminal. The\n",
      "magic happens with the exclamation point: anything appearing after ! on a line will\n",
      "be executed not by the Python kernel, but by the system command line.\n",
      "The following assumes you’re on a Unix-like system, such as Linux or Mac OS X.\n",
      "Some of the examples that follow will fail on Windows, which uses a different type of\n",
      "shell by default (though with the 2016 announcement of native Bash shells on Win‐\n",
      "dows, soon this may no longer be an issue!). If you’re unfamiliar with shell com‐\n",
      "mands, I’ d suggest reviewing the Shell Tutorial put together by the always excellent\n",
      "Software Carpentry Foundation.\n",
      "Quick Introduction to the Shell\n",
      "A full intro to using the shell/terminal/command line is well beyond the scope of this\n",
      "chapter, but for the uninitiated we will offer a quick introduction here. The shell is a\n",
      "way to interact textually with your computer. Ever since the mid-1980s, when Micro‐\n",
      "soft and Apple introduced the first versions of their now ubiquitous graphical operat‐\n",
      "ing systems, most computer users have interacted with their operating system\n",
      "through familiar clicking of menus and drag-and-drop movements. But operating\n",
      "systems existed long before these graphical user interfaces, and were primarily con‐\n",
      "trolled through sequences of text input: at the prompt, the user would type a com‐\n",
      "mand, and the computer would do what the user told it to. Those early prompt\n",
      "16 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 33, 'page_label': '16', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4968}\n",
      "\n",
      "--- Chunk 4969 ---\n",
      "Content:\n",
      "systems are the precursors of the shells and terminals that most active data scientists\n",
      "still use today.\n",
      "Someone unfamiliar with the shell might ask why you would bother with this, when\n",
      "you can accomplish many results by simply clicking on icons and menus. A shell user\n",
      "might reply with another question: why hunt icons and click menus when you can\n",
      "accomplish things much more easily by typing? While it might sound like a typical\n",
      "tech preference impasse, when moving beyond basic tasks it quickly becomes clear\n",
      "that the shell offers much more control of advanced tasks, though admittedly the\n",
      "learning curve can intimidate the average computer user.\n",
      "As an example, here is a sample of a Linux/OS X shell session where a user explores,\n",
      "creates, and modifies directories and files on their system ( osx:~ $ is the prompt,\n",
      "and everything after the $ sign is the typed command; text that is preceded by a # is\n",
      "meant just as description, rather than something you would actually type in):\n",
      "osx:~ $ echo \"hello world\"              # echo is like Python's print function\n",
      "hello world\n",
      "osx:~ $ pwd                             # pwd = print working directory\n",
      "/home/jake                              # this is the \"path\" that we're in\n",
      "osx:~ $ ls                              # ls = list working directory contents\n",
      "notebooks  projects\n",
      "osx:~ $ cd projects/                    # cd = change directory\n",
      "osx:projects $ pwd\n",
      "/home/jake/projects\n",
      "osx:projects $ ls\n",
      "datasci_book   mpld3   myproject.txt\n",
      "osx:projects $ mkdir myproject          # mkdir = make new directory\n",
      "osx:projects $ cd myproject/\n",
      "osx:myproject $ mv ../myproject.txt ./  # mv = move file. Here we're moving the\n",
      "                                        # file myproject.txt from one directory\n",
      "                                        # up (../) to the current directory (./)\n",
      "osx:myproject $ ls\n",
      "myproject.txt\n",
      "Notice that all of this is just a compact way to do familiar operations (navigating a\n",
      "directory structure, creating a directory, moving a file, etc.) by typing commands\n",
      "rather than clicking icons and menus. Note that with just a few commands ( pwd, ls,\n",
      "cd, mkdir, and cp) you can do many of the most common file operations. It’s when\n",
      "you go beyond these basics that the shell approach becomes really powerful.\n",
      "IPython and Shell Commands | 17...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 34, 'page_label': '17', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4969}\n",
      "\n",
      "--- Chunk 4970 ---\n",
      "Content:\n",
      "Shell Commands in IPython\n",
      "Y ou can use any command that works at the command line in IPython by prefixing it\n",
      "with the ! character. For example, the ls, pwd, and echo commands can be run as\n",
      "follows:\n",
      "In [1]: !ls\n",
      "myproject.txt\n",
      "In [2]: !pwd\n",
      "/home/jake/projects/myproject\n",
      "In [3]: !echo \"printing from the shell\"\n",
      "printing from the shell\n",
      "Passing Values to and from the Shell\n",
      "Shell commands can not only be called from IPython, but can also be made to inter‐\n",
      "act with the IPython namespace. For example, you can save the output of any shell\n",
      "command to a Python list using the assignment operator:\n",
      "In [4]: contents = !ls\n",
      "In [5]: print(contents)\n",
      "['myproject.txt']\n",
      "In [6]: directory = !pwd\n",
      "In [7]: print(directory)\n",
      "['/Users/jakevdp/notebooks/tmp/myproject']\n",
      "Note that these results are not returned as lists, but as a special shell return type\n",
      "defined in IPython:\n",
      "In [8]: type(directory)\n",
      "IPython.utils.text.SList\n",
      "This looks and acts a lot like a Python list, but has additional functionality, such as\n",
      "the grep and fields methods and the s, n, and p properties that allow you to search,\n",
      "filter, and display the results in convenient ways. For more information on these, you\n",
      "can use IPython’s built-in help features.\n",
      "Communication in the other direction—passing Python variables into the shell—is\n",
      "possible through the {varname} syntax:\n",
      "In [9]: message = \"hello from Python\"\n",
      "In [10]: !echo {message}\n",
      "hello from Python\n",
      "18 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 35, 'page_label': '18', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4970}\n",
      "\n",
      "--- Chunk 4971 ---\n",
      "Content:\n",
      "The curly braces contain the variable name, which is replaced by the variable’s con‐\n",
      "tents in the shell command.\n",
      "Shell-Related Magic Commands\n",
      "If you play with IPython’s shell commands for a while, you might notice that you can‐\n",
      "not use !cd to navigate the filesystem:\n",
      "In [11]: !pwd\n",
      "/home/jake/projects/myproject\n",
      "In [12]: !cd ..\n",
      "In [13]: !pwd\n",
      "/home/jake/projects/myproject\n",
      "The reason is that shell commands in the notebook are executed in a temporary sub‐\n",
      "shell. If you’ d like to change the working directory in a more enduring way, you can\n",
      "use the %cd magic command:\n",
      "In [14]: %cd ..\n",
      "/home/jake/projects\n",
      "In fact, by default you can even use this without the % sign:\n",
      "In [15]: cd myproject\n",
      "/home/jake/projects/myproject\n",
      "This is known as an automagic function, and this behavior can be toggled with the\n",
      "%automagic magic function.\n",
      "Besides %cd, other available shell-like magic functions are %cat, %cp, %env, %ls, %man,\n",
      "%mkdir, %more, %mv, %pwd, %rm, and %rmdir, any of which can be used without the %\n",
      "sign if automagic is on. This makes it so that you can almost treat the IPython\n",
      "prompt as if it’s a normal shell:\n",
      "In [16]: mkdir tmp\n",
      "In [17]: ls\n",
      "myproject.txt  tmp/\n",
      "In [18]: cp myproject.txt tmp/\n",
      "In [19]: ls tmp\n",
      "myproject.txt\n",
      "In [20]: rm -r tmp\n",
      "This access to the shell from within the same terminal window as your Python ses‐\n",
      "sion means that there is a lot less switching back and forth between interpreter and\n",
      "shell as you write your Python code.\n",
      "Shell-Related Magic Commands | 19...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 36, 'page_label': '19', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4971}\n",
      "\n",
      "--- Chunk 4972 ---\n",
      "Content:\n",
      "Errors and Debugging\n",
      "Code development and data analysis always require a bit of trial and error, and\n",
      "IPython contains tools to streamline this process. This section will briefly cover some\n",
      "options for controlling Python’s exception reporting, followed by exploring tools for\n",
      "debugging errors in code.\n",
      "Controlling Exceptions: %xmode\n",
      "Most of the time when a Python script fails, it will raise an exception. When the inter‐\n",
      "preter hits one of these exceptions, information about the cause of the error can be\n",
      "found in the traceback, which can be accessed from within Python. With the %xmode\n",
      "magic function, IPython allows you to control the amount of information printed\n",
      "when the exception is raised. Consider the following code:\n",
      "In[1]: def func1(a, b):\n",
      "           return a / b\n",
      "       def func2(x):\n",
      "           a = x\n",
      "           b = x - 1\n",
      "           return func1(a, b)\n",
      "In[2]: func2(1)\n",
      "---------------------------------------------------------------------------\n",
      "ZeroDivisionError                         Traceback (most recent call last)\n",
      "<ipython-input-2-b2e110f6fc8f^gt; in <module>()\n",
      "----> 1 func2(1)\n",
      "<ipython-input-1-d849e34d61fb> in func2(x)\n",
      "      5     a = x\n",
      "      6     b = x - 1\n",
      "----> 7     return func1(a, b)\n",
      "<ipython-input-1-d849e34d61fb> in func1(a, b)\n",
      "      1 def func1(a, b):\n",
      "----> 2     return a / b\n",
      "      3\n",
      "      4 def func2(x):\n",
      "      5     a = x\n",
      "ZeroDivisionError: division by zero\n",
      "Calling func2 results in an error, and reading the printed trace lets us see exactly what\n",
      "happened. By default, this trace includes several lines showing the context of each\n",
      "20 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 37, 'page_label': '20', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4972}\n",
      "\n",
      "--- Chunk 4973 ---\n",
      "Content:\n",
      "step that led to the error. Using the %xmode magic function (short for exception mode),\n",
      "we can change what information is printed.\n",
      "%xmode takes a single argument, the mode, and there are three possibilities: Plain,\n",
      "Context, and Verbose. The default is Context, and gives output like that just shown.\n",
      "Plain is more compact and gives less information:\n",
      "In[3]: %xmode Plain\n",
      "Exception reporting mode: Plain\n",
      "In[4]: func2(1)\n",
      "------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-b2e110f6fc8f>\", line 1, in <module>\n",
      "    func2(1)\n",
      "  File \"<ipython-input-1-d849e34d61fb>\", line 7, in func2\n",
      "    return func1(a, b)\n",
      "  File \"<ipython-input-1-d849e34d61fb>\", line 2, in func1\n",
      "    return a / b\n",
      "ZeroDivisionError: division by zero\n",
      "The Verbose mode adds some extra information, including the arguments to any\n",
      "functions that are called:\n",
      "In[5]: %xmode Verbose\n",
      "Exception reporting mode: Verbose\n",
      "In[6]: func2(1)\n",
      "---------------------------------------------------------------------------\n",
      "ZeroDivisionError                         Traceback (most recent call last)\n",
      "<ipython-input-6-b2e110f6fc8f> in <module>()\n",
      "----> 1 func2(1)\n",
      "        global func2 = <function func2 at 0x103729320>\n",
      "<ipython-input-1-d849e34d61fb> in func2(x=1)\n",
      "      5     a = x\n",
      "      6     b = x - 1\n",
      "----> 7     return func1(a, b)\n",
      "        global func1 = <function func1 at 0x1037294d0>\n",
      "        a = 1\n",
      "        b = 0\n",
      "Errors and Debugging | 21...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 38, 'page_label': '21', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4973}\n",
      "\n",
      "--- Chunk 4974 ---\n",
      "Content:\n",
      "<ipython-input-1-d849e34d61fb> in func1(a=1, b=0)\n",
      "      1 def func1(a, b):\n",
      "----> 2     return a / b\n",
      "        a = 1\n",
      "        b = 0\n",
      "      3\n",
      "      4 def func2(x):\n",
      "      5     a = x\n",
      "ZeroDivisionError: division by zero\n",
      "This extra information can help you narrow in on why the exception is being raised.\n",
      "So why not use the Verbose mode all the time? As code gets complicated, this kind of\n",
      "traceback can get extremely long. Depending on the context, sometimes the brevity of\n",
      "Default mode is easier to work with.\n",
      "Debugging: When Reading Tracebacks Is Not Enough\n",
      "The standard Python tool for interactive debugging is pdb, the Python debugger. This\n",
      "debugger lets the user step through the code line by line in order to see what might be\n",
      "causing a more difficult error. The IPython-enhanced version of this is ipdb, the\n",
      "IPython debugger.\n",
      "There are many ways to launch and use both these debuggers; we won’t cover them\n",
      "fully here. Refer to the online documentation of these two utilities to learn more.\n",
      "In IPython, perhaps the most convenient interface to debugging is the %debug magic\n",
      "command. If you call it after hitting an exception, it will automatically open an inter‐\n",
      "active debugging prompt at the point of the exception. The ipdb prompt lets you\n",
      "explore the current state of the stack, explore the available variables, and even run\n",
      "Python commands!\n",
      "Let’s look at the most recent exception, then do some basic tasks—print the values of\n",
      "a and b, and type quit to quit the debugging session:\n",
      "In[7]: %debug\n",
      "> <ipython-input-1-d849e34d61fb>(2)func1()\n",
      "      1 def func1(a, b):\n",
      "----> 2     return a / b\n",
      "      3\n",
      "ipdb> print(a)\n",
      "1\n",
      "ipdb> print(b)\n",
      "0\n",
      "ipdb> quit\n",
      "22 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 39, 'page_label': '22', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4974}\n",
      "\n",
      "--- Chunk 4975 ---\n",
      "Content:\n",
      "The interactive debugger allows much more than this, though—we can even step up\n",
      "and down through the stack and explore the values of variables there:\n",
      "In[8]: %debug\n",
      "> <ipython-input-1-d849e34d61fb>(2)func1()\n",
      "      1 def func1(a, b):\n",
      "----> 2     return a / b\n",
      "      3\n",
      "ipdb> up\n",
      "> <ipython-input-1-d849e34d61fb>(7)func2()\n",
      "      5     a = x\n",
      "      6     b = x - 1\n",
      "----> 7     return func1(a, b)\n",
      "ipdb> print(x)\n",
      "1\n",
      "ipdb> up\n",
      "> <ipython-input-6-b2e110f6fc8f>(1)<module>()\n",
      "----> 1 func2(1)\n",
      "ipdb> down\n",
      "> <ipython-input-1-d849e34d61fb>(7)func2()\n",
      "      5     a = x\n",
      "      6     b = x - 1\n",
      "----> 7     return func1(a, b)\n",
      "ipdb> quit\n",
      "This allows you to quickly find out not only what caused the error, but also what\n",
      "function calls led up to the error.\n",
      "If you’ d like the debugger to launch automatically whenever an exception is raised,\n",
      "you can use the %pdb magic function to turn on this automatic behavior:\n",
      "In[9]: %xmode Plain\n",
      "       %pdb on\n",
      "       func2(1)\n",
      "Exception reporting mode: Plain\n",
      "Automatic pdb calling has been turned ON\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-569a67d2d312>\", line 3, in <module>\n",
      "    func2(1)\n",
      "  File \"<ipython-input-1-d849e34d61fb>\", line 7, in func2\n",
      "    return func1(a, b)\n",
      "Errors and Debugging | 23...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 40, 'page_label': '23', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4975}\n",
      "\n",
      "--- Chunk 4976 ---\n",
      "Content:\n",
      "File \"<ipython-input-1-d849e34d61fb>\", line 2, in func1\n",
      "    return a / b\n",
      "ZeroDivisionError: division by zero\n",
      "> <ipython-input-1-d849e34d61fb>(2)func1()\n",
      "      1 def func1(a, b):\n",
      "----> 2     return a / b\n",
      "      3\n",
      "ipdb> print(b)\n",
      "0\n",
      "ipdb> quit\n",
      "Finally, if you have a script that you’ d like to run from the beginning in interactive\n",
      "mode, you can run it with the command %run -d, and use the next command to step\n",
      "through the lines of code interactively.\n",
      "Partial list of debugging commands\n",
      "There are many more available commands for interactive debugging than we’ve listed\n",
      "here; the following table contains a description of some of the more common and\n",
      "useful ones:\n",
      "Command Description\n",
      "list Show the current location in the file\n",
      "h(elp) Show a list of commands, or find help on a specific command\n",
      "q(uit) Quit the debugger and the program\n",
      "c(ontinue) Quit the debugger; continue in the program\n",
      "n(ext) Go to the next step of the program\n",
      "<enter> Repeat the previous command\n",
      "p(rint) Print variables\n",
      "s(tep) Step into a subroutine\n",
      "r(eturn) Return out of a subroutine\n",
      "For more information, use the help command in the debugger, or take a look at\n",
      "ipdb’s online documentation.\n",
      "24 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 41, 'page_label': '24', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4976}\n",
      "\n",
      "--- Chunk 4977 ---\n",
      "Content:\n",
      "Profiling and Timing Code\n",
      "In the process of developing code and creating data processing pipelines, there are\n",
      "often trade-offs you can make between various implementations. Early in developing\n",
      "your algorithm, it can be counterproductive to worry about such things. As Donald\n",
      "Knuth famously quipped, “We should forget about small efficiencies, say about 97%\n",
      "of the time: premature optimization is the root of all evil. ”\n",
      "But once you have your code working, it can be useful to dig into its efficiency a bit.\n",
      "Sometimes it’s useful to check the execution time of a given command or set of com‐\n",
      "mands; other times it’s useful to dig into a multiline process and determine where the\n",
      "bottleneck lies in some complicated series of operations. IPython provides access to a\n",
      "wide array of functionality for this kind of timing and profiling of code. Here we’ll\n",
      "discuss the following IPython magic commands:\n",
      "%time\n",
      "Time the execution of a single statement\n",
      "%timeit\n",
      "Time repeated execution of a single statement for more accuracy\n",
      "%prun\n",
      "Run code with the profiler\n",
      "%lprun\n",
      "Run code with the line-by-line profiler\n",
      "%memit\n",
      "Measure the memory use of a single statement\n",
      "%mprun\n",
      "Run code with the line-by-line memory profiler\n",
      "The last four commands are not bundled with IPython—you’ll need to install the\n",
      "line_profiler and memory_profiler extensions, which we will discuss in the fol‐\n",
      "lowing sections.\n",
      "Timing Code Snippets: %timeit and %time\n",
      "We saw the %timeit line magic and %%timeit cell magic in the introduction to magic\n",
      "functions in “IPython Magic Commands” on page 10; %%timeit can be used to time\n",
      "the repeated execution of snippets of code:\n",
      "In[1]: %timeit sum(range(100))\n",
      "100000 loops, best of 3: 1.54 µs per loop\n",
      "Profiling and Timing Code | 25...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 42, 'page_label': '25', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4977}\n",
      "\n",
      "--- Chunk 4978 ---\n",
      "Content:\n",
      "Note that because this operation is so fast, %timeit automatically does a large number\n",
      "of repetitions. For slower commands, %timeit will automatically adjust and perform\n",
      "fewer repetitions:\n",
      "In[2]: %%timeit\n",
      "       total = 0\n",
      "       for i in range(1000):\n",
      "           for j in range(1000):\n",
      "               total += i * (-1) ** j\n",
      "1 loops, best of 3: 407 ms per loop\n",
      "Sometimes repeating an operation is not the best option. For example, if we have a\n",
      "list that we’ d like to sort, we might be misled by a repeated operation. Sorting a pre-\n",
      "sorted list is much faster than sorting an unsorted list, so the repetition will skew the\n",
      "result:\n",
      "In[3]: import random\n",
      "       L = [random.random() for i in range(100000)]\n",
      "       %timeit L.sort()\n",
      "100 loops, best of 3: 1.9 ms per loop\n",
      "For this, the %time magic function may be a better choice. It also is a good choice for\n",
      "longer-running commands, when short, system-related delays are unlikely to affect\n",
      "the result. Let’s time the sorting of an unsorted and a presorted list:\n",
      "In[4]: import random\n",
      "       L = [random.random() for i in range(100000)]\n",
      "       print(\"sorting an unsorted list:\")\n",
      "       %time L.sort()\n",
      "sorting an unsorted list:\n",
      "CPU times: user 40.6 ms, sys: 896 µs, total: 41.5 ms\n",
      "Wall time: 41.5 ms\n",
      "In[5]: print(\"sorting an already sorted list:\")\n",
      "       %time L.sort()\n",
      "sorting an already sorted list:\n",
      "CPU times: user 8.18 ms, sys: 10 µs, total: 8.19 ms\n",
      "Wall time: 8.24 ms\n",
      "Notice how much faster the presorted list is to sort, but notice also how much longer\n",
      "the timing takes with %time versus %timeit, even for the presorted list! This is a\n",
      "result of the fact that %timeit does some clever things under the hood to prevent sys‐\n",
      "tem calls from interfering with the timing. For example, it prevents cleanup of unused\n",
      "Python objects (known as garbage collection) that might otherwise affect the timing.\n",
      "For this reason, %timeit results are usually noticeably faster than %time results.\n",
      "For %time as with %timeit, using the double-percent-sign cell-magic syntax allows\n",
      "timing of multiline scripts:\n",
      "26 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 43, 'page_label': '26', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4978}\n",
      "\n",
      "--- Chunk 4979 ---\n",
      "Content:\n",
      "In[6]: %%time\n",
      "       total = 0\n",
      "       for i in range(1000):\n",
      "           for j in range(1000):\n",
      "               total += i * (-1) ** j\n",
      "CPU times: user 504 ms, sys: 979 µs, total: 505 ms\n",
      "Wall time: 505 ms\n",
      "For more information on %time and %timeit, as well as their available options, use\n",
      "the IPython help functionality (i.e., type %time? at the IPython prompt).\n",
      "Profiling Full Scripts: %prun\n",
      "A program is made of many single statements, and sometimes timing these state‐\n",
      "ments in context is more important than timing them on their own. Python contains\n",
      "a built-in code profiler (which you can read about in the Python documentation), but\n",
      "IPython offers a much more convenient way to use this profiler, in the form of the\n",
      "magic function %prun.\n",
      "By way of example, we’ll define a simple function that does some calculations:\n",
      "In[7]: def sum_of_lists(N):\n",
      "           total = 0\n",
      "           for i in range(5):\n",
      "               L = [j ^ (j >> i) for j in range(N)]\n",
      "               total += sum(L)\n",
      "           return total\n",
      "Now we can call %prun with a function call to see the profiled results:\n",
      "In[8]: %prun sum_of_lists(1000000)\n",
      "In the notebook, the output is printed to the pager, and looks something like this:\n",
      "14 function calls in 0.714 seconds\n",
      "   Ordered by: internal time\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        5    0.599    0.120    0.599    0.120 <ipython-input-19>:4(<listcomp>)\n",
      "        5    0.064    0.013    0.064    0.013 {built-in method sum}\n",
      "        1    0.036    0.036    0.699    0.699 <ipython-input-19>:1(sum_of_lists)\n",
      "        1    0.014    0.014    0.714    0.714 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.714    0.714 {built-in method exec}\n",
      "The result is a table that indicates, in order of total time on each function call, where\n",
      "the execution is spending the most time. In this case, the bulk of execution time is in\n",
      "the list comprehension inside sum_of_lists. From here, we could start thinking\n",
      "about what changes we might make to improve the performance in the algorithm.\n",
      "Profiling and Timing Code | 27...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 44, 'page_label': '27', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4979}\n",
      "\n",
      "--- Chunk 4980 ---\n",
      "Content:\n",
      "For more information on %prun, as well as its available options, use the IPython help\n",
      "functionality (i.e., type %prun? at the IPython prompt).\n",
      "Line-by-Line Profiling with %lprun\n",
      "The function-by-function profiling of %prun is useful, but sometimes it’s more conve‐\n",
      "nient to have a line-by-line profile report. This is not built into Python or IPython,\n",
      "but there is a line_profiler package available for installation that can do this. Start\n",
      "by using Python’s packaging tool, pip, to install the line_profiler package:\n",
      "$ pip install line_profiler\n",
      "Next, you can use IPython to load the line_profiler IPython extension, offered as\n",
      "part of this package:\n",
      "In[9]: %load_ext line_profiler\n",
      "Now the %lprun command will do a line-by-line profiling of any function—in this\n",
      "case, we need to tell it explicitly which functions we’re interested in profiling:...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 45, 'page_label': '28', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4980}\n",
      "\n",
      "--- Chunk 4981 ---\n",
      "Content:\n",
      "In[9]: %load_ext line_profiler\n",
      "Now the %lprun command will do a line-by-line profiling of any function—in this\n",
      "case, we need to tell it explicitly which functions we’re interested in profiling:\n",
      "In[10]: %lprun -f sum_of_lists sum_of_lists(5000)\n",
      "As before, the notebook sends the result to the pager, but it looks something like this:\n",
      "Timer unit: 1e-06 s\n",
      "Total time: 0.009382 s\n",
      "File: <ipython-input-19-fa2be176cc3e>\n",
      "Function: sum_of_lists at line 1\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def sum_of_lists(N):\n",
      "     2         1            2      2.0      0.0      total = 0\n",
      "     3         6            8      1.3      0.1      for i in range(5):\n",
      "     4         5         9001   1800.2     95.9          L = [j ^ (j >> i) ...\n",
      "     5         5          371     74.2      4.0          total += sum(L)\n",
      "     6         1            0      0.0      0.0      return total\n",
      "The information at the top gives us the key to reading the results: the time is reported\n",
      "in microseconds and we can see where the program is spending the most time. At this\n",
      "point, we may be able to use this information to modify aspects of the script and\n",
      "make it perform better for our desired use case.\n",
      "For more information on %lprun, as well as its available options, use the IPython help\n",
      "functionality (i.e., type %lprun? at the IPython prompt).\n",
      "28 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 45, 'page_label': '28', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4981}\n",
      "\n",
      "--- Chunk 4982 ---\n",
      "Content:\n",
      "Profiling Memory Use: %memit and %mprun\n",
      "Another aspect of profiling is the amount of memory an operation uses. This can be\n",
      "evaluated with another IPython extension, the memory_profiler. As with the\n",
      "line_profiler, we start by pip-installing the extension:\n",
      "$ pip install memory_profiler\n",
      "Then we can use IPython to load the extension:\n",
      "In[12]: %load_ext memory_profiler\n",
      "The memory profiler extension contains two useful magic functions: the %memit\n",
      "magic (which offers a memory-measuring equivalent of %timeit) and the %mprun\n",
      "function (which offers a memory-measuring equivalent of %lprun). The %memit func‐\n",
      "tion can be used rather simply:\n",
      "In[13]: %memit sum_of_lists(1000000)\n",
      "peak memory: 100.08 MiB, increment: 61.36 MiB\n",
      "We see that this function uses about 100 MB of memory.\n",
      "For a line-by-line description of memory use, we can use the %mprun magic. Unfortu‐\n",
      "nately, this magic works only for functions defined in separate modules rather than\n",
      "the notebook itself, so we’ll start by using the %%file magic to create a simple module\n",
      "called mprun_demo.py, which contains our sum_of_lists function, with one addition\n",
      "that will make our memory profiling results more clear:\n",
      "In[14]: %%file mprun_demo.py\n",
      "        def sum_of_lists(N):\n",
      "            total = 0\n",
      "            for i in range(5):\n",
      "                L = [j ^ (j >> i) for j in range(N)]\n",
      "                total += sum(L)\n",
      "                del L # remove reference to L\n",
      "            return total\n",
      "Overwriting mprun_demo.py\n",
      "We can now import the new version of this function and run the memory line\n",
      "profiler:\n",
      "In[15]: from mprun_demo import sum_of_lists\n",
      "        %mprun -f sum_of_lists sum_of_lists(1000000)\n",
      "The result, printed to the pager, gives us a summary of the memory use of the func‐\n",
      "tion, and looks something like this:\n",
      "Profiling and Timing Code | 29...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 46, 'page_label': '29', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4982}\n",
      "\n",
      "--- Chunk 4983 ---\n",
      "Content:\n",
      "Filename: ./mprun_demo.py\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "     4     71.9 MiB      0.0 MiB           L = [j ^ (j >> i) for j in range(N)]\n",
      "Filename: ./mprun_demo.py\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "     1     39.0 MiB      0.0 MiB   def sum_of_lists(N):\n",
      "     2     39.0 MiB      0.0 MiB       total = 0\n",
      "     3     46.5 MiB      7.5 MiB       for i in range(5):\n",
      "     4     71.9 MiB     25.4 MiB           L = [j ^ (j >> i) for j in range(N)]\n",
      "     5     71.9 MiB      0.0 MiB           total += sum(L)\n",
      "     6     46.5 MiB    -25.4 MiB           del L # remove reference to L\n",
      "     7     39.1 MiB     -7.4 MiB       return total\n",
      "Here the Increment column tells us how much each line affects the total memory\n",
      "budget: observe that when we create and delete the list L, we are adding about 25 MB\n",
      "of memory usage. This is on top of the background memory usage from the Python\n",
      "interpreter itself.\n",
      "For more information on %memit and %mprun, as well as their available options, use\n",
      "the IPython help functionality (i.e., type %memit? at the IPython prompt).\n",
      "More IPython Resources\n",
      "In this chapter, we’ve just scratched the surface of using IPython to enable data sci‐\n",
      "ence tasks. Much more information is available both in print and on the Web, and\n",
      "here we’ll list some other resources that you may find helpful.\n",
      "Web Resources\n",
      "The IPython website\n",
      "The IPython website links to documentation, examples, tutorials, and a variety of\n",
      "other resources.\n",
      "The nbviewer website\n",
      "This site shows static renderings of any IPython notebook available on the Inter‐\n",
      "net. The front page features some example notebooks that you can browse to see\n",
      "what other folks are using IPython for!\n",
      "30 | Chapter 1: IPython: Beyond Normal Python...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 47, 'page_label': '30', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4983}\n",
      "\n",
      "--- Chunk 4984 ---\n",
      "Content:\n",
      "A Gallery of Interesting IPython Notebooks\n",
      "This ever-growing list of notebooks, powered by nbviewer, shows the depth and\n",
      "breadth of numerical analysis you can do with IPython. It includes everything\n",
      "from short examples and tutorials to full-blown courses and books composed in\n",
      "the notebook format!\n",
      "Video tutorials\n",
      "Searching the Internet, you will find many video-recorded tutorials on IPython.\n",
      "I’ d especially recommend seeking tutorials from the PyCon, SciPy, and PyData\n",
      "conferences by Fernando Perez and Brian Granger, two of the primary creators\n",
      "and maintainers of IPython and Jupyter.\n",
      "Books\n",
      "Python for Data Analysis\n",
      "Wes McKinney’s book includes a chapter that covers using IPython as a data sci‐\n",
      "entist. Although much of the material overlaps what we’ve discussed here,\n",
      "another perspective is always helpful.\n",
      "Learning IPython for Interactive Computing and Data Visualization\n",
      "This short book by Cyrille Rossant offers a good introduction to using IPython\n",
      "for data analysis.\n",
      "IPython Interactive Computing and Visualization Cookbook\n",
      "Also by Cyrille Rossant, this book is a longer and more advanced treatment of\n",
      "using IPython for data science. Despite its name, it’s not just about IPython—it\n",
      "also goes into some depth on a broad range of data science topics.\n",
      "Finally, a reminder that you can find help on your own: IPython’s ?-based help func‐\n",
      "tionality (discussed in “Help and Documentation in IPython” on page 3) can be very\n",
      "useful if you use it well and use it often. As you go through the examples here and\n",
      "elsewhere, you can use it to familiarize yourself with all the tools that IPython has to\n",
      "offer.\n",
      "More IPython Resources | 31...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 48, 'page_label': '31', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4984}\n",
      "\n",
      "--- Chunk 4985 ---\n",
      "Content:\n",
      "CHAPTER 2\n",
      "Introduction to NumPy\n",
      "This chapter, along with Chapter 3, outlines techniques for effectively loading, stor‐\n",
      "ing, and manipulating in-memory data in Python. The topic is very broad: datasets\n",
      "can come from a wide range of sources and a wide range of formats, including collec‐\n",
      "tions of documents, collections of images, collections of sound clips, collections of\n",
      "numerical measurements, or nearly anything else. Despite this apparent heterogene‐\n",
      "ity, it will help us to think of all data fundamentally as arrays of numbers.\n",
      "For example, images—particularly digital images—can be thought of as simply two-\n",
      "dimensional arrays of numbers representing pixel brightness across the area. Sound\n",
      "clips can be thought of as one-dimensional arrays of intensity versus time. Text can be\n",
      "converted in various ways into numerical representations, perhaps binary digits rep‐\n",
      "resenting the frequency of certain words or pairs of words. No matter what the data\n",
      "are, the first step in making them analyzable will be to transform them into arrays of\n",
      "numbers. (We will discuss some specific examples of this process later in “Feature\n",
      "Engineering” on page 375.)\n",
      "For this reason, efficient storage and manipulation of numerical arrays is absolutely\n",
      "fundamental to the process of doing data science. We’ll now take a look at the special‐\n",
      "ized tools that Python has for handling such numerical arrays: the NumPy package\n",
      "and the Pandas package (discussed in Chapter 3.)\n",
      "This chapter will cover NumPy in detail. NumPy (short for Numerical Python) pro‐\n",
      "vides an efficient interface to store and operate on dense data buffers. In some ways,\n",
      "NumPy arrays are like Python’s built-in list type, but NumPy arrays provide much\n",
      "more efficient storage and data operations as the arrays grow larger in size. NumPy\n",
      "arrays form the core of nearly the entire ecosystem of data science tools in Python, so\n",
      "time spent learning to use NumPy effectively will be valuable no matter what aspect\n",
      "of data science interests you.\n",
      "33...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 50, 'page_label': '33', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4985}\n",
      "\n",
      "--- Chunk 4986 ---\n",
      "Content:\n",
      "If you followed the advice outlined in the preface and installed the Anaconda stack,\n",
      "you already have NumPy installed and ready to go. If you’re more the do-it-yourself\n",
      "type, you can go to the NumPy website and follow the installation instructions found\n",
      "there. Once you do, you can import NumPy and double-check the version:\n",
      "In[1]: import numpy\n",
      "       numpy.__version__\n",
      "Out[1]: '1.11.1'\n",
      "For the pieces of the package discussed here, I’ d recommend NumPy version 1.8 or\n",
      "later. By convention, you’ll find that most people in the SciPy/PyData world will\n",
      "import NumPy using np as an alias:\n",
      "In[2]: import numpy as np\n",
      "Throughout this chapter, and indeed the rest of the book, you’ll find that this is the\n",
      "way we will import and use NumPy.\n",
      "Reminder About Built-In Documentation\n",
      "As you read through this chapter, don’t forget that IPython gives you the ability to\n",
      "quickly explore the contents of a package (by using the tab-completion feature) as\n",
      "well as the documentation of various functions (using the ? character). Refer back to\n",
      "“Help and Documentation in IPython” on page 3 if you need a refresher on this.\n",
      "For example, to display all the contents of the numpy namespace, you can type this:\n",
      "In [3]: np.<TAB>\n",
      "And to display NumPy’s built-in documentation, you can use this:\n",
      "In [4]: np?\n",
      "More detailed documentation, along with tutorials and other resources, can be found\n",
      "at http://www.numpy.org.\n",
      "Understanding Data Types in Python\n",
      "Effective data-driven science and computation requires understanding how data is\n",
      "stored and manipulated. This section outlines and contrasts how arrays of data are\n",
      "handled in the Python language itself, and how NumPy improves on this. Under‐\n",
      "standing this difference is fundamental to understanding much of the material\n",
      "throughout the rest of the book.\n",
      "Users of Python are often drawn in by its ease of use, one piece of which is dynamic\n",
      "typing. While a statically typed language like C or Java requires each variable to be\n",
      "34 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 51, 'page_label': '34', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4986}\n",
      "\n",
      "--- Chunk 4987 ---\n",
      "Content:\n",
      "explicitly declared, a dynamically typed language like Python skips this specification.\n",
      "For example, in C you might specify a particular operation as follows:\n",
      "/* C code */\n",
      "int result = 0;\n",
      "for(int i=0; i<100; i++){\n",
      "    result += i;\n",
      "}\n",
      "While in Python the equivalent operation could be written this way:\n",
      "# Python code\n",
      "result = 0\n",
      "for i in range(100):\n",
      "    result += i\n",
      "Notice the main difference: in C, the data types of each variable are explicitly\n",
      "declared, while in Python the types are dynamically inferred. This means, for exam‐\n",
      "ple, that we can assign any kind of data to any variable:\n",
      "# Python code\n",
      "x = 4\n",
      "x = \"four\"\n",
      "Here we’ve switched the contents of x from an integer to a string. The same thing in C\n",
      "would lead (depending on compiler settings) to a compilation error or other uninten‐\n",
      "ded consequences:\n",
      "/* C code */\n",
      "int x = 4;\n",
      "x = \"four\";  // FAILS\n",
      "This sort of flexibility is one piece that makes Python and other dynamically typed\n",
      "languages convenient and easy to use. Understanding how this works is an important\n",
      "piece of learning to analyze data efficiently and effectively with Python. But what this\n",
      "type flexibility also points to is the fact that Python variables are more than just their\n",
      "value; they also contain extra information about the type of the value. We’ll explore\n",
      "this more in the sections that follow.\n",
      "A Python Integer Is More Than Just an Integer\n",
      "The standard Python implementation is written in C. This means that every Python\n",
      "object is simply a cleverly disguised C structure, which contains not only its value, but\n",
      "other information as well. For example, when we define an integer in Python, such as\n",
      "x = 10000, x is not just a “raw” integer. It’s actually a pointer to a compound C struc‐\n",
      "ture, which contains several values. Looking through the Python 3.4 source code, we\n",
      "find that the integer (long) type definition effectively looks like this (once the C mac‐\n",
      "ros are expanded):\n",
      "Understanding Data Types in Python | 35...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 52, 'page_label': '35', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4987}\n",
      "\n",
      "--- Chunk 4988 ---\n",
      "Content:\n",
      "struct _longobject {\n",
      "    long ob_refcnt;\n",
      "    PyTypeObject *ob_type;\n",
      "    size_t ob_size;\n",
      "    long ob_digit[1];\n",
      "};\n",
      "A single integer in Python 3.4 actually contains four pieces:\n",
      "• ob_refcnt, a reference count that helps Python silently handle memory alloca‐\n",
      "tion and deallocation\n",
      "• ob_type, which encodes the type of the variable\n",
      "• ob_size, which specifies the size of the following data members\n",
      "• ob_digit, which contains the actual integer value that we expect the Python vari‐\n",
      "able to represent\n",
      "This means that there is some overhead in storing an integer in Python as compared\n",
      "to an integer in a compiled language like C, as illustrated in Figure 2-1.\n",
      "Figure 2-1. The difference between C and Python integers\n",
      "Here PyObject_HEAD is the part of the structure containing the reference count, type\n",
      "code, and other pieces mentioned before.\n",
      "Notice the difference here: a C integer is essentially a label for a position in memory\n",
      "whose bytes encode an integer value. A Python integer is a pointer to a position in\n",
      "memory containing all the Python object information, including the bytes that con‐\n",
      "tain the integer value. This extra information in the Python integer structure is what\n",
      "allows Python to be coded so freely and dynamically. All this additional information\n",
      "in Python types comes at a cost, however, which becomes especially apparent in\n",
      "structures that combine many of these objects.\n",
      "36 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 53, 'page_label': '36', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4988}\n",
      "\n",
      "--- Chunk 4989 ---\n",
      "Content:\n",
      "A Python List Is More Than Just a List\n",
      "Let’s consider now what happens when we use a Python data structure that holds\n",
      "many Python objects. The standard mutable multielement container in Python is the\n",
      "list. We can create a list of integers as follows:\n",
      "In[1]: L = list(range(10))\n",
      "       L\n",
      "Out[1]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "In[2]: type(L[0])\n",
      "Out[2]: int\n",
      "Or, similarly, a list of strings:\n",
      "In[3]: L2 = [str(c) for c in L]\n",
      "       L2\n",
      "Out[3]: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "In[4]: type(L2[0])\n",
      "Out[4]: str\n",
      "Because of Python’s dynamic typing, we can even create heterogeneous lists:\n",
      "In[5]: L3 = [True, \"2\", 3.0, 4]\n",
      "       [type(item) for item in L3]\n",
      "Out[5]: [bool, str, float, int]\n",
      "But this flexibility comes at a cost: to allow these flexible types, each item in the list\n",
      "must contain its own type info, reference count, and other information—that is, each\n",
      "item is a complete Python object. In the special case that all variables are of the same\n",
      "type, much of this information is redundant: it can be much more efficient to store\n",
      "data in a fixed-type array. The difference between a dynamic-type list and a fixed-type\n",
      "(NumPy-style) array is illustrated in Figure 2-2.\n",
      "At the implementation level, the array essentially contains a single pointer to one con‐\n",
      "tiguous block of data. The Python list, on the other hand, contains a pointer to a\n",
      "block of pointers, each of which in turn points to a full Python object like the Python\n",
      "integer we saw earlier. Again, the advantage of the list is flexibility: because each list\n",
      "element is a full structure containing both data and type information, the list can be\n",
      "filled with data of any desired type. Fixed-type NumPy-style arrays lack this flexibil‐\n",
      "ity, but are much more efficient for storing and manipulating data.\n",
      "Understanding Data Types in Python | 37...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 54, 'page_label': '37', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4989}\n",
      "\n",
      "--- Chunk 4990 ---\n",
      "Content:\n",
      "Figure 2-2. The difference between C and Python lists\n",
      "Fixed-Type Arrays in Python\n",
      "Python offers several different options for storing data in efficient, fixed-type data\n",
      "buffers. The built-in array module (available since Python 3.3) can be used to create\n",
      "dense arrays of a uniform type:\n",
      "In[6]: import array\n",
      "       L = list(range(10))\n",
      "       A = array.array('i', L)\n",
      "       A\n",
      "Out[6]: array('i', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "Here 'i' is a type code indicating the contents are integers.\n",
      "Much more useful, however, is the ndarray object of the NumPy package. While\n",
      "Python’s array object provides efficient storage of array-based data, NumPy adds to\n",
      "this efficient operations on that data. We will explore these operations in later sec‐\n",
      "tions; here we’ll demonstrate several ways of creating a NumPy array.\n",
      "We’ll start with the standard NumPy import, under the alias np:\n",
      "In[7]: import numpy as np\n",
      "38 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 55, 'page_label': '38', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4990}\n",
      "\n",
      "--- Chunk 4991 ---\n",
      "Content:\n",
      "Creating Arrays from Python Lists\n",
      "First, we can use np.array to create arrays from Python lists:\n",
      "In[8]: # integer array:\n",
      "       np.array([1, 4, 2, 5, 3])\n",
      "Out[8]: array([1, 4, 2, 5, 3])\n",
      "Remember that unlike Python lists, NumPy is constrained to arrays that all contain\n",
      "the same type. If types do not match, NumPy will upcast if possible (here, integers are\n",
      "upcast to floating point):\n",
      "In[9]: np.array([3.14, 4, 2, 3])\n",
      "Out[9]: array([ 3.14,  4.  ,  2.  ,  3.  ])\n",
      "If we want to explicitly set the data type of the resulting array, we can use the dtype\n",
      "keyword:\n",
      "In[10]: np.array([1, 2, 3, 4], dtype='float32')\n",
      "Out[10]: array([ 1.,  2.,  3.,  4.], dtype=float32)\n",
      "Finally, unlike Python lists, NumPy arrays can explicitly be multidimensional; here’s\n",
      "one way of initializing a multidimensional array using a list of lists:\n",
      "In[11]: # nested lists result in multidimensional arrays\n",
      "        np.array([range(i, i + 3) for i in [2, 4, 6]])\n",
      "Out[11]: array([[2, 3, 4],\n",
      "                [4, 5, 6],\n",
      "                [6, 7, 8]])\n",
      "The inner lists are treated as rows of the resulting two-dimensional array.\n",
      "Creating Arrays from Scratch\n",
      "Especially for larger arrays, it is more efficient to create arrays from scratch using rou‐\n",
      "tines built into NumPy. Here are several examples:\n",
      "In[12]: # Create a length-10 integer array filled with zeros\n",
      "        np.zeros(10, dtype=int)\n",
      "Out[12]: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "In[13]: # Create a 3x5 floating-point array filled with 1s\n",
      "        np.ones((3, 5), dtype=float)\n",
      "Out[13]: array([[ 1.,  1.,  1.,  1.,  1.],\n",
      "                [ 1.,  1.,  1.,  1.,  1.],\n",
      "                [ 1.,  1.,  1.,  1.,  1.]])\n",
      "In[14]: # Create a 3x5 array filled with 3.14\n",
      "        np.full((3, 5), 3.14)\n",
      "Understanding Data Types in Python | 39...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 56, 'page_label': '39', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4991}\n",
      "\n",
      "--- Chunk 4992 ---\n",
      "Content:\n",
      "Out[14]: array([[ 3.14,  3.14,  3.14,  3.14,  3.14],\n",
      "                [ 3.14,  3.14,  3.14,  3.14,  3.14],\n",
      "                [ 3.14,  3.14,  3.14,  3.14,  3.14]])\n",
      "In[15]: # Create an array filled with a linear sequence\n",
      "        # Starting at 0, ending at 20, stepping by 2\n",
      "        # (this is similar to the built-in range() function)\n",
      "        np.arange(0, 20, 2)\n",
      "Out[15]: array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])\n",
      "In[16]: # Create an array of five values evenly spaced between 0 and 1\n",
      "        np.linspace(0, 1, 5)\n",
      "Out[16]: array([ 0.  ,  0.25,  0.5 ,  0.75,  1.  ])\n",
      "In[17]: # Create a 3x3 array of uniformly distributed\n",
      "        # random values between 0 and 1\n",
      "        np.random.random((3, 3))\n",
      "Out[17]: array([[ 0.99844933,  0.52183819,  0.22421193],\n",
      "                [ 0.08007488,  0.45429293,  0.20941444],\n",
      "                [ 0.14360941,  0.96910973,  0.946117  ]])\n",
      "In[18]: # Create a 3x3 array of normally distributed random values\n",
      "        # with mean 0 and standard deviation 1\n",
      "        np.random.normal(0, 1, (3, 3))\n",
      "Out[18]: array([[ 1.51772646,  0.39614948, -0.10634696],\n",
      "                [ 0.25671348,  0.00732722,  0.37783601],\n",
      "                [ 0.68446945,  0.15926039, -0.70744073]])\n",
      "In[19]: # Create a 3x3 array of random integers in the interval [0, 10)\n",
      "        np.random.randint(0, 10, (3, 3))\n",
      "Out[19]: array([[2, 3, 4],\n",
      "                [5, 7, 8],\n",
      "                [0, 5, 0]])...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 57, 'page_label': '40', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4992}\n",
      "\n",
      "--- Chunk 4993 ---\n",
      "Content:\n",
      "In[19]: # Create a 3x3 array of random integers in the interval [0, 10)\n",
      "        np.random.randint(0, 10, (3, 3))\n",
      "Out[19]: array([[2, 3, 4],\n",
      "                [5, 7, 8],\n",
      "                [0, 5, 0]])\n",
      "In[20]: # Create a 3x3 identity matrix\n",
      "        np.eye(3)\n",
      "Out[20]: array([[ 1.,  0.,  0.],\n",
      "                [ 0.,  1.,  0.],\n",
      "                [ 0.,  0.,  1.]])\n",
      "In[21]: # Create an uninitialized array of three integers\n",
      "        # The values will be whatever happens to already exist at that\n",
      "        # memory location\n",
      "        np.empty(3)\n",
      "Out[21]: array([ 1.,  1.,  1.])\n",
      "40 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 57, 'page_label': '40', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4993}\n",
      "\n",
      "--- Chunk 4994 ---\n",
      "Content:\n",
      "NumPy Standard Data Types\n",
      "NumPy arrays contain values of a single type, so it is important to have detailed\n",
      "knowledge of those types and their limitations. Because NumPy is built in C, the\n",
      "types will be familiar to users of C, Fortran, and other related languages.\n",
      "The standard NumPy data types are listed in Table 2-1. Note that when constructing\n",
      "an array, you can specify them using a string:\n",
      "np.zeros(10, dtype='int16')\n",
      "Or using the associated NumPy object:\n",
      "np.zeros(10, dtype=np.int16)\n",
      "Table 2-1. Standard NumPy data types\n",
      "Data type Description\n",
      "bool_ Boolean (True or False) stored as a byte\n",
      "int_ Default integer type (same as C long; normally either int64 or int32)\n",
      "intc Identical to C int (normally int32 or int64)\n",
      "intp Integer used for indexing (same as C ssize_t; normally either int32 or int64)\n",
      "int8 Byte (–128 to 127)\n",
      "int16 Integer (–32768 to 32767)\n",
      "int32 Integer (–2147483648 to 2147483647)\n",
      "int64 Integer (–9223372036854775808 to 9223372036854775807)\n",
      "uint8 Unsigned integer (0 to 255)\n",
      "uint16 Unsigned integer (0 to 65535)\n",
      "uint32 Unsigned integer (0 to 4294967295)\n",
      "uint64 Unsigned integer (0 to 18446744073709551615)\n",
      "float_ Shorthand for float64\n",
      "float16 Half-precision float: sign bit, 5 bits exponent, 10 bits mantissa\n",
      "float32 Single-precision float: sign bit, 8 bits exponent, 23 bits mantissa\n",
      "float64 Double-precision float: sign bit, 11 bits exponent, 52 bits mantissa\n",
      "complex_ Shorthand for complex128\n",
      "complex64 Complex number, represented by two 32-bit floats\n",
      "complex128 Complex number, represented by two 64-bit floats\n",
      "More advanced type specification is possible, such as specifying big or little endian\n",
      "numbers; for more information, refer to the NumPy documentation. NumPy also\n",
      "supports compound data types, which will be covered in “Structured Data: NumPy’s\n",
      "Structured Arrays” on page 92.\n",
      "Understanding Data Types in Python | 41...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 58, 'page_label': '41', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4994}\n",
      "\n",
      "--- Chunk 4995 ---\n",
      "Content:\n",
      "The Basics of NumPy Arrays\n",
      "Data manipulation in Python is nearly synonymous with NumPy array manipulation:\n",
      "even newer tools like Pandas (Chapter 3) are built around the NumPy array. This sec‐\n",
      "tion will present several examples using NumPy array manipulation to access data\n",
      "and subarrays, and to split, reshape, and join the arrays. While the types of operations\n",
      "shown here may seem a bit dry and pedantic, they comprise the building blocks of\n",
      "many other examples used throughout the book. Get to know them well!\n",
      "We’ll cover a few categories of basic array manipulations here:\n",
      "Attributes of arrays\n",
      "Determining the size, shape, memory consumption, and data types of arrays\n",
      "Indexing of arrays\n",
      "Getting and setting the value of individual array elements\n",
      "Slicing of arrays\n",
      "Getting and setting smaller subarrays within a larger array\n",
      "Reshaping of arrays\n",
      "Changing the shape of a given array\n",
      "Joining and splitting of arrays\n",
      "Combining multiple arrays into one, and splitting one array into many\n",
      "NumPy Array Attributes\n",
      "First let’s discuss some useful array attributes. We’ll start by defining three random\n",
      "arrays: a one-dimensional, two-dimensional, and three-dimensional array. We’ll use\n",
      "NumPy’s random number generator, which we will seed with a set value in order to\n",
      "ensure that the same random arrays are generated each time this code is run:\n",
      "In[1]: import numpy as np\n",
      "       np.random.seed(0)  # seed for reproducibility\n",
      "       x1 = np.random.randint(10, size=6)  # One-dimensional array\n",
      "       x2 = np.random.randint(10, size=(3, 4))  # Two-dimensional array\n",
      "       x3 = np.random.randint(10, size=(3, 4, 5))  # Three-dimensional array\n",
      "Each array has attributes ndim (the number of dimensions), shape (the size of each\n",
      "dimension), and size (the total size of the array):\n",
      "In[2]: print(\"x3 ndim: \", x3.ndim)\n",
      "       print(\"x3 shape:\", x3.shape)\n",
      "       print(\"x3 size: \", x3.size)\n",
      "x3 ndim:  3\n",
      "x3 shape: (3, 4, 5)\n",
      "x3 size:  60\n",
      "42 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 59, 'page_label': '42', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4995}\n",
      "\n",
      "--- Chunk 4996 ---\n",
      "Content:\n",
      "Another useful attribute is the dtype, the data type of the array (which we discussed\n",
      "previously in “Understanding Data Types in Python” on page 34):\n",
      "In[3]: print(\"dtype:\", x3.dtype)\n",
      "dtype: int64\n",
      "Other attributes include itemsize, which lists the size (in bytes) of each array ele‐\n",
      "ment, and nbytes, which lists the total size (in bytes) of the array:\n",
      "In[4]: print(\"itemsize:\", x3.itemsize, \"bytes\")\n",
      "       print(\"nbytes:\", x3.nbytes, \"bytes\")\n",
      "itemsize: 8 bytes\n",
      "nbytes: 480 bytes\n",
      "In general, we expect that nbytes is equal to itemsize times size.\n",
      "Array Indexing: Accessing Single Elements\n",
      "If you are familiar with Python’s standard list indexing, indexing in NumPy will feel\n",
      "quite familiar. In a one-dimensional array, you can access the ith value (counting from\n",
      "zero) by specifying the desired index in square brackets, just as with Python lists:\n",
      "In[5]: x1\n",
      "Out[5]: array([5, 0, 3, 3, 7, 9])\n",
      "In[6]: x1[0]\n",
      "Out[6]: 5\n",
      "In[7]: x1[4]\n",
      "Out[7]: 7\n",
      "To index from the end of the array, you can use negative indices:\n",
      "In[8]: x1[-1]\n",
      "Out[8]: 9\n",
      "In[9]: x1[-2]\n",
      "Out[9]: 7\n",
      "In a multidimensional array, you access items using a comma-separated tuple of\n",
      "indices:\n",
      "In[10]: x2\n",
      "Out[10]: array([[3, 5, 2, 4],\n",
      "                [7, 6, 8, 8],\n",
      "                [1, 6, 7, 7]])\n",
      "In[11]: x2[0, 0]\n",
      "Out[11]: 3\n",
      "The Basics of NumPy Arrays | 43...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 60, 'page_label': '43', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4996}\n",
      "\n",
      "--- Chunk 4997 ---\n",
      "Content:\n",
      "In[12]: x2[2, 0]\n",
      "Out[12]: 1\n",
      "In[13]: x2[2, -1]\n",
      "Out[13]: 7\n",
      "Y ou can also modify values using any of the above index notation:\n",
      "In[14]: x2[0, 0] = 12\n",
      "        x2\n",
      "Out[14]: array([[12,  5,  2,  4],\n",
      "                [ 7,  6,  8,  8],\n",
      "                [ 1,  6,  7,  7]])\n",
      "Keep in mind that, unlike Python lists, NumPy arrays have a fixed type. This means,\n",
      "for example, that if you attempt to insert a floating-point value to an integer array, the\n",
      "value will be silently truncated. Don’t be caught unaware by this behavior!\n",
      "In[15]: x1[0] = 3.14159  # this will be truncated!\n",
      "        x1\n",
      "Out[15]: array([3, 0, 3, 3, 7, 9])\n",
      "Array Slicing: Accessing Subarrays\n",
      "Just as we can use square brackets to access individual array elements, we can also use\n",
      "them to access subarrays with the slice notation, marked by the colon (:) character.\n",
      "The NumPy slicing syntax follows that of the standard Python list; to access a slice of\n",
      "an array x, use this:\n",
      "x[start:stop:step]\n",
      "If any of these are unspecified, they default to the values start=0, stop=size of\n",
      "dimension, step=1. We’ll take a look at accessing subarrays in one dimension and in\n",
      "multiple dimensions.\n",
      "One-dimensional subarrays\n",
      "In[16]: x = np.arange(10)\n",
      "        x\n",
      "Out[16]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "In[17]: x[:5]  # first five elements\n",
      "Out[17]: array([0, 1, 2, 3, 4])\n",
      "In[18]: x[5:]  # elements after index 5\n",
      "Out[18]: array([5, 6, 7, 8, 9])\n",
      "In[19]: x[4:7]  # middle subarray\n",
      "Out[19]: array([4, 5, 6])\n",
      "44 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 61, 'page_label': '44', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4997}\n",
      "\n",
      "--- Chunk 4998 ---\n",
      "Content:\n",
      "In[20]: x[::2]  # every other element\n",
      "Out[20]: array([0, 2, 4, 6, 8])\n",
      "In[21]: x[1::2]  # every other element, starting at index 1\n",
      "Out[21]: array([1, 3, 5, 7, 9])\n",
      "A potentially confusing case is when the step value is negative. In this case, the\n",
      "defaults for start and stop are swapped. This becomes a convenient way to reverse\n",
      "an array:\n",
      "In[22]: x[::-1]  # all elements, reversed\n",
      "Out[22]: array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n",
      "In[23]: x[5::-2]  # reversed every other from index 5\n",
      "Out[23]: array([5, 3, 1])\n",
      "Multidimensional subarrays\n",
      "Multidimensional slices work in the same way, with multiple slices separated by com‐\n",
      "mas. For example:\n",
      "In[24]: x2\n",
      "Out[24]: array([[12,  5,  2,  4],\n",
      "                [ 7,  6,  8,  8],\n",
      "                [ 1,  6,  7,  7]])\n",
      "In[25]: x2[:2, :3]  # two rows, three columns\n",
      "Out[25]: array([[12,  5,  2],\n",
      "                [ 7,  6,  8]])\n",
      "In[26]: x2[:3, ::2]  # all rows, every other column\n",
      "Out[26]: array([[12,  2],\n",
      "                [ 7,  8],\n",
      "                [ 1,  7]])\n",
      "Finally, subarray dimensions can even be reversed together:\n",
      "In[27]: x2[::-1, ::-1]\n",
      "Out[27]: array([[ 7,  7,  6,  1],\n",
      "                [ 8,  8,  6,  7],\n",
      "                [ 4,  2,  5, 12]])\n",
      "Accessing array rows and columns.    One commonly needed routine is accessing single\n",
      "rows or columns of an array. Y ou can do this by combining indexing and slicing,\n",
      "using an empty slice marked by a single colon (:):\n",
      "In[28]: print(x2[:, 0])  # first column of x2\n",
      "[12  7  1]\n",
      "The Basics of NumPy Arrays | 45...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 62, 'page_label': '45', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4998}\n",
      "\n",
      "--- Chunk 4999 ---\n",
      "Content:\n",
      "In[29]: print(x2[0, :])  # first row of x2\n",
      "[12  5  2  4]\n",
      "In the case of row access, the empty slice can be omitted for a more compact syntax:\n",
      "In[30]: print(x2[0])  # equivalent to x2[0, :]\n",
      "[12  5  2  4]\n",
      "Subarrays as no-copy views\n",
      "One important—and extremely useful—thing to know about array slices is that they\n",
      "return views rather than copies of the array data. This is one area in which NumPy\n",
      "array slicing differs from Python list slicing: in lists, slices will be copies. Consider our\n",
      "two-dimensional array from before:\n",
      "In[31]: print(x2)\n",
      "[[12  5  2  4]\n",
      " [ 7  6  8  8]\n",
      " [ 1  6  7  7]]\n",
      "Let’s extract a 2×2 subarray from this:\n",
      "In[32]: x2_sub = x2[:2, :2]\n",
      "        print(x2_sub)\n",
      "[[12  5]\n",
      " [ 7  6]]\n",
      "Now if we modify this subarray, we’ll see that the original array is changed! Observe:\n",
      "In[33]: x2_sub[0, 0] = 99\n",
      "        print(x2_sub)\n",
      "[[99  5]\n",
      " [ 7  6]]\n",
      "In[34]: print(x2)\n",
      "[[99  5  2  4]\n",
      " [ 7  6  8  8]\n",
      " [ 1  6  7  7]]\n",
      "This default behavior is actually quite useful: it means that when we work with large\n",
      "datasets, we can access and process pieces of these datasets without the need to copy\n",
      "the underlying data buffer.\n",
      "Creating copies of arrays\n",
      "Despite the nice features of array views, it is sometimes useful to instead explicitly\n",
      "copy the data within an array or a subarray. This can be most easily done with the\n",
      "copy() method:\n",
      "46 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 63, 'page_label': '46', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 4999}\n",
      "\n",
      "--- Chunk 5000 ---\n",
      "Content:\n",
      "In[35]: x2_sub_copy = x2[:2, :2].copy()\n",
      "        print(x2_sub_copy)\n",
      "[[99  5]\n",
      " [ 7  6]]\n",
      "If we now modify this subarray, the original array is not touched:\n",
      "In[36]: x2_sub_copy[0, 0] = 42\n",
      "        print(x2_sub_copy)\n",
      "[[42  5]\n",
      " [ 7  6]]\n",
      "In[37]: print(x2)\n",
      "[[99  5  2  4]\n",
      " [ 7  6  8  8]\n",
      " [ 1  6  7  7]]\n",
      "Reshaping of Arrays\n",
      "Another useful type of operation is reshaping of arrays. The most flexible way of\n",
      "doing this is with the reshape() method. For example, if you want to put the num‐\n",
      "bers 1 through 9 in a 3×3 grid, you can do the following:\n",
      "In[38]: grid = np.arange(1, 10).reshape((3, 3))\n",
      "        print(grid)\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "Note that for this to work, the size of the initial array must match the size of the\n",
      "reshaped array. Where possible, the reshape method will use a no-copy view of the\n",
      "initial array, but with noncontiguous memory buffers this is not always the case.\n",
      "Another common reshaping pattern is the conversion of a one-dimensional array\n",
      "into a two-dimensional row or column matrix. Y ou can do this with the reshape\n",
      "method, or more easily by making use of the newaxis keyword within a slice opera‐\n",
      "tion:\n",
      "In[39]: x = np.array([1, 2, 3])\n",
      "        # row vector via reshape\n",
      "        x.reshape((1, 3))\n",
      "Out[39]: array([[1, 2, 3]])\n",
      "In[40]: # row vector via newaxis\n",
      "        x[np.newaxis, :]\n",
      "Out[40]: array([[1, 2, 3]])\n",
      "The Basics of NumPy Arrays | 47...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 64, 'page_label': '47', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5000}\n",
      "\n",
      "--- Chunk 5001 ---\n",
      "Content:\n",
      "In[41]: # column vector via reshape\n",
      "        x.reshape((3, 1))\n",
      "Out[41]: array([[1],\n",
      "                [2],\n",
      "                [3]])\n",
      "In[42]: # column vector via newaxis\n",
      "        x[:, np.newaxis]\n",
      "Out[42]: array([[1],\n",
      "                [2],\n",
      "                [3]])\n",
      "We will see this type of transformation often throughout the remainder of the book.\n",
      "Array Concatenation and Splitting\n",
      "All of the preceding routines worked on single arrays. It’s also possible to combine\n",
      "multiple arrays into one, and to conversely split a single array into multiple arrays.\n",
      "We’ll take a look at those operations here.\n",
      "Concatenation of arrays\n",
      "Concatenation, or joining of two arrays in NumPy, is primarily accomplished\n",
      "through the routines np.concatenate, np.vstack, and np.hstack. np.concatenate\n",
      "takes a tuple or list of arrays as its first argument, as we can see here:\n",
      "In[43]: x = np.array([1, 2, 3])\n",
      "        y = np.array([3, 2, 1])\n",
      "        np.concatenate([x, y])\n",
      "Out[43]: array([1, 2, 3, 3, 2, 1])\n",
      "Y ou can also concatenate more than two arrays at once:\n",
      "In[44]: z = [99, 99, 99]\n",
      "        print(np.concatenate([x, y, z]))\n",
      "[ 1  2  3  3  2  1 99 99 99]\n",
      "np.concatenate can also be used for two-dimensional arrays:\n",
      "In[45]: grid = np.array([[1, 2, 3],\n",
      "                         [4, 5, 6]])\n",
      "In[46]: # concatenate along the first axis\n",
      "        np.concatenate([grid, grid])\n",
      "Out[46]: array([[1, 2, 3],\n",
      "                [4, 5, 6],\n",
      "                [1, 2, 3],\n",
      "                [4, 5, 6]])\n",
      "In[47]: # concatenate along the second axis (zero-indexed)\n",
      "        np.concatenate([grid, grid], axis=1)\n",
      "48 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 65, 'page_label': '48', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5001}\n",
      "\n",
      "--- Chunk 5002 ---\n",
      "Content:\n",
      "Out[47]: array([[1, 2, 3, 1, 2, 3],\n",
      "                [4, 5, 6, 4, 5, 6]])\n",
      "For working with arrays of mixed dimensions, it can be clearer to use the np.vstack\n",
      "(vertical stack) and np.hstack (horizontal stack) functions:\n",
      "In[48]: x = np.array([1, 2, 3])\n",
      "        grid = np.array([[9, 8, 7],\n",
      "                         [6, 5, 4]])\n",
      "        # vertically stack the arrays\n",
      "        np.vstack([x, grid])\n",
      "Out[48]: array([[1, 2, 3],\n",
      "                [9, 8, 7],\n",
      "                [6, 5, 4]])\n",
      "In[49]: # horizontally stack the arrays\n",
      "        y = np.array([[99],\n",
      "                      [99]])\n",
      "        np.hstack([grid, y])\n",
      "Out[49]: array([[ 9,  8,  7, 99],\n",
      "                [ 6,  5,  4, 99]])\n",
      "Similarly, np.dstack will stack arrays along the third axis.\n",
      "Splitting of arrays\n",
      "The opposite of concatenation is splitting, which is implemented by the functions\n",
      "np.split, np.hsplit, and np.vsplit. For each of these, we can pass a list of indices\n",
      "giving the split points:\n",
      "In[50]: x = [1, 2, 3, 99, 99, 3, 2, 1]\n",
      "        x1, x2, x3 = np.split(x, [3, 5])\n",
      "        print(x1, x2, x3)\n",
      "[1 2 3] [99 99] [3 2 1]\n",
      "Notice that N split points lead to N + 1  subarrays. The related functions np.hsplit\n",
      "and np.vsplit are similar:\n",
      "In[51]: grid = np.arange(16).reshape((4, 4))\n",
      "        grid\n",
      "Out[51]: array([[ 0,  1,  2,  3],\n",
      "                [ 4,  5,  6,  7],\n",
      "                [ 8,  9, 10, 11],\n",
      "                [12, 13, 14, 15]])\n",
      "In[52]: upper, lower = np.vsplit(grid, [2])\n",
      "        print(upper)\n",
      "        print(lower)\n",
      "[[0 1 2 3]\n",
      " [4 5 6 7]]\n",
      "The Basics of NumPy Arrays | 49...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 66, 'page_label': '49', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5002}\n",
      "\n",
      "--- Chunk 5003 ---\n",
      "Content:\n",
      "[[ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "In[53]: left, right = np.hsplit(grid, [2])\n",
      "        print(left)\n",
      "        print(right)\n",
      "[[ 0  1]\n",
      " [ 4  5]\n",
      " [ 8  9]\n",
      " [12 13]]\n",
      "[[ 2  3]\n",
      " [ 6  7]\n",
      " [10 11]\n",
      " [14 15]]\n",
      "Similarly, np.dsplit will split arrays along the third axis.\n",
      "Computation on NumPy Arrays: Universal Functions\n",
      "Up until now, we have been discussing some of the basic nuts and bolts of NumPy; in\n",
      "the next few sections, we will dive into the reasons that NumPy is so important in the\n",
      "Python data science world. Namely, it provides an easy and flexible interface to opti‐\n",
      "mized computation with arrays of data.\n",
      "Computation on NumPy arrays can be very fast, or it can be very slow. The key to\n",
      "making it fast is to use vectorized operations, generally implemented through Num‐\n",
      "Py’s universal functions (ufuncs). This section motivates the need for NumPy’s ufuncs,\n",
      "which can be used to make repeated calculations on array elements much more effi‐\n",
      "cient. It then introduces many of the most common and useful arithmetic ufuncs\n",
      "available in the NumPy package.\n",
      "The Slowness of Loops\n",
      "Python’s default implementation (known as CPython) does some operations very\n",
      "slowly. This is in part due to the dynamic, interpreted nature of the language: the fact\n",
      "that types are flexible, so that sequences of operations cannot be compiled down to\n",
      "efficient machine code as in languages like C and Fortran. Recently there have been\n",
      "various attempts to address this weakness: well-known examples are the PyPy project,\n",
      "a just-in-time compiled implementation of Python; the Cython project, which con‐\n",
      "verts Python code to compilable C code; and the Numba project , which converts\n",
      "snippets of Python code to fast LLVM bytecode. Each of these has its strengths and\n",
      "weaknesses, but it is safe to say that none of the three approaches has yet surpassed\n",
      "the reach and popularity of the standard CPython engine.\n",
      "The relative sluggishness of Python generally manifests itself in situations where\n",
      "many small operations are being repeated—for instance, looping over arrays to oper‐\n",
      "50 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 67, 'page_label': '50', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5003}\n",
      "\n",
      "--- Chunk 5004 ---\n",
      "Content:\n",
      "ate on each element. For example, imagine we have an array of values and we’ d like to\n",
      "compute the reciprocal of each. A straightforward approach might look like this:\n",
      "In[1]: import numpy as np\n",
      "       np.random.seed(0)\n",
      "       def compute_reciprocals(values):\n",
      "           output = np.empty(len(values))\n",
      "           for i in range(len(values)):\n",
      "               output[i] = 1.0 / values[i]\n",
      "           return output\n",
      "       values = np.random.randint(1, 10, size=5)\n",
      "       compute_reciprocals(values)\n",
      "Out[1]: array([ 0.16666667,  1.        ,  0.25      ,  0.25      ,  0.125     ])\n",
      "This implementation probably feels fairly natural to someone from, say, a C or Java\n",
      "background. But if we measure the execution time of this code for a large input, we\n",
      "see that this operation is very slow, perhaps surprisingly so! We’ll benchmark this\n",
      "with IPython’s %timeit magic (discussed in “Profiling and Timing Code” on page 25):\n",
      "In[2]: big_array = np.random.randint(1, 100, size=1000000)\n",
      "       %timeit compute_reciprocals(big_array)\n",
      "1 loop, best of 3: 2.91 s per loop\n",
      "It takes several seconds to compute these million operations and to store the result!\n",
      "When even cell phones have processing speeds measured in Giga-FLOPS (i.e., bil‐\n",
      "lions of numerical operations per second), this seems almost absurdly slow. It turns\n",
      "out that the bottleneck here is not the operations themselves, but the type-checking\n",
      "and function dispatches that CPython must do at each cycle of the loop. Each time\n",
      "the reciprocal is computed, Python first examines the object’s type and does a\n",
      "dynamic lookup of the correct function to use for that type. If we were working in\n",
      "compiled code instead, this type specification would be known before the code exe‐\n",
      "cutes and the result could be computed much more efficiently.\n",
      "Introducing UFuncs\n",
      "For many types of operations, NumPy provides a convenient interface into just this\n",
      "kind of statically typed, compiled routine. This is known as a vectorized operation.\n",
      "Y ou can accomplish this by simply performing an operation on the array, which will\n",
      "then be applied to each element. This vectorized approach is designed to push the\n",
      "loop into the compiled layer that underlies NumPy, leading to much faster execution.\n",
      "Compare the results of the following two:\n",
      "In[3]: print(compute_reciprocals(values))\n",
      "       print(1.0 / values)\n",
      "Computation on NumPy Arrays: Universal Functions | 51...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 68, 'page_label': '51', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5004}\n",
      "\n",
      "--- Chunk 5005 ---\n",
      "Content:\n",
      "[ 0.16666667  1.          0.25        0.25        0.125     ]\n",
      "[ 0.16666667  1.          0.25        0.25        0.125     ]\n",
      "Looking at the execution time for our big array, we see that it completes orders of\n",
      "magnitude faster than the Python loop:\n",
      "In[4]: %timeit (1.0 / big_array)\n",
      "100 loops, best of 3: 4.6 ms per loop\n",
      "Vectorized operations in NumPy are implemented via ufuncs, whose main purpose is\n",
      "to quickly execute repeated operations on values in NumPy arrays. Ufuncs are\n",
      "extremely flexible—before we saw an operation between a scalar and an array, but we\n",
      "can also operate between two arrays:\n",
      "In[5]: np.arange(5) / np.arange(1, 6)\n",
      "Out[5]: array([ 0.        ,  0.5       ,  0.66666667,  0.75      ,  0.8       ])\n",
      "And ufunc operations are not limited to one-dimensional arrays—they can act on\n",
      "multidimensional arrays as well:\n",
      "In[6]: x = np.arange(9).reshape((3, 3))\n",
      "       2 ** x\n",
      "Out[6]: array([[  1,   2,   4],\n",
      "               [  8,  16,  32],\n",
      "               [ 64, 128, 256]])\n",
      "Computations using vectorization through ufuncs are nearly always more efficient\n",
      "than their counterpart implemented through Python loops, especially as the arrays\n",
      "grow in size. Any time you see such a loop in a Python script, you should consider\n",
      "whether it can be replaced with a vectorized expression.\n",
      "Exploring NumPy’s UFuncs\n",
      "Ufuncs exist in two flavors: unary ufuncs, which operate on a single input, and binary\n",
      "ufuncs, which operate on two inputs. We’ll see examples of both these types of func‐\n",
      "tions here.\n",
      "Array arithmetic\n",
      "NumPy’s ufuncs feel very natural to use because they make use of Python’s native\n",
      "arithmetic operators. The standard addition, subtraction, multiplication, and division\n",
      "can all be used:\n",
      "In[7]: x = np.arange(4)\n",
      "       print(\"x     =\", x)\n",
      "       print(\"x + 5 =\", x + 5)\n",
      "       print(\"x - 5 =\", x - 5)\n",
      "       print(\"x * 2 =\", x * 2)\n",
      "52 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 69, 'page_label': '52', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5005}\n",
      "\n",
      "--- Chunk 5006 ---\n",
      "Content:\n",
      "print(\"x / 2 =\", x / 2)\n",
      "       print(\"x // 2 =\", x // 2)  # floor division\n",
      "x     = [0 1 2 3]\n",
      "x + 5 = [5 6 7 8]\n",
      "x - 5 = [-5 -4 -3 -2]\n",
      "x * 2 = [0 2 4 6]\n",
      "x / 2 = [ 0.   0.5  1.   1.5]\n",
      "x // 2 = [0 0 1 1]\n",
      "There is also a unary ufunc for negation, a ** operator for exponentiation, and a %\n",
      "operator for modulus:\n",
      "In[8]: print(\"-x     = \", -x)\n",
      "       print(\"x ** 2 = \", x ** 2)\n",
      "       print(\"x % 2  = \", x % 2)\n",
      "-x     =  [ 0 -1 -2 -3]\n",
      "x ** 2 =  [0 1 4 9]\n",
      "x % 2  =  [0 1 0 1]\n",
      "In addition, these can be strung together however you wish, and the standard order\n",
      "of operations is respected:\n",
      "In[9]: -(0.5*x + 1) ** 2\n",
      "Out[9]: array([-1.  , -2.25, -4.  , -6.25])\n",
      "All of these arithmetic operations are simply convenient wrappers around specific\n",
      "functions built into NumPy; for example, the + operator is a wrapper for the add\n",
      "function:\n",
      "In[10]: np.add(x, 2)\n",
      "Out[10]: array([2, 3, 4, 5])\n",
      "Table 2-2 lists the arithmetic operators implemented in NumPy.\n",
      "Table 2-2. Arithmetic operators implemented in NumPy\n",
      "Operator Equivalent ufunc Description\n",
      "+ np.add Addition (e.g., 1 + 1 = 2)\n",
      "- np.subtract Subtraction (e.g., 3 - 2 = 1)\n",
      "- np.negative Unary negation (e.g., -2)\n",
      "* np.multiply Multiplication (e.g., 2 * 3 = 6)\n",
      "/ np.divide Division (e.g., 3 / 2 = 1.5)\n",
      "// np.floor_divide Floor division (e.g., 3 // 2 = 1)\n",
      "** np.power Exponentiation (e.g., 2 ** 3 = 8)\n",
      "% np.mod Modulus/remainder (e.g., 9 % 4 = 1)\n",
      "Computation on NumPy Arrays: Universal Functions | 53...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 70, 'page_label': '53', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5006}\n",
      "\n",
      "--- Chunk 5007 ---\n",
      "Content:\n",
      "Additionally there are Boolean/bitwise operators; we will explore these in “Compari‐\n",
      "sons, Masks, and Boolean Logic” on page 70.\n",
      "Absolute value\n",
      "Just as NumPy understands Python’s built-in arithmetic operators, it also understands\n",
      "Python’s built-in absolute value function:\n",
      "In[11]: x = np.array([-2, -1, 0, 1, 2])\n",
      "        abs(x)\n",
      "Out[11]: array([2, 1, 0, 1, 2])\n",
      "The corresponding NumPy ufunc is np.absolute, which is also available under the\n",
      "alias np.abs:\n",
      "In[12]: np.absolute(x)\n",
      "Out[12]: array([2, 1, 0, 1, 2])\n",
      "In[13]: np.abs(x)\n",
      "Out[13]: array([2, 1, 0, 1, 2])\n",
      "This ufunc can also handle complex data, in which the absolute value returns the\n",
      "magnitude:\n",
      "In[14]: x = np.array([3 - 4j, 4 - 3j, 2 + 0j, 0 + 1j])\n",
      "        np.abs(x)\n",
      "Out[14]: array([ 5.,  5.,  2.,  1.])\n",
      "Trigonometric functions\n",
      "NumPy provides a large number of useful ufuncs, and some of the most useful for the\n",
      "data scientist are the trigonometric functions. We’ll start by defining an array of\n",
      "angles:\n",
      "In[15]: theta = np.linspace(0, np.pi, 3)\n",
      "Now we can compute some trigonometric functions on these values:\n",
      "In[16]: print(\"theta      = \", theta)\n",
      "        print(\"sin(theta) = \", np.sin(theta))\n",
      "        print(\"cos(theta) = \", np.cos(theta))\n",
      "        print(\"tan(theta) = \", np.tan(theta))\n",
      "theta      =  [ 0.          1.57079633  3.14159265]\n",
      "sin(theta) =  [  0.00000000e+00   1.00000000e+00   1.22464680e-16]\n",
      "cos(theta) =  [  1.00000000e+00   6.12323400e-17  -1.00000000e+00]\n",
      "tan(theta) =  [  0.00000000e+00   1.63312394e+16  -1.22464680e-16]\n",
      "The values are computed to within machine precision, which is why values that\n",
      "should be zero do not always hit exactly zero. Inverse trigonometric functions are also\n",
      "available:\n",
      "54 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 71, 'page_label': '54', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5007}\n",
      "\n",
      "--- Chunk 5008 ---\n",
      "Content:\n",
      "In[17]: x = [-1, 0, 1]\n",
      "        print(\"x         = \", x)\n",
      "        print(\"arcsin(x) = \", np.arcsin(x))\n",
      "        print(\"arccos(x) = \", np.arccos(x))\n",
      "        print(\"arctan(x) = \", np.arctan(x))\n",
      "x         =  [-1, 0, 1]\n",
      "arcsin(x) =  [-1.57079633  0.          1.57079633]\n",
      "arccos(x) =  [ 3.14159265  1.57079633  0.        ]\n",
      "arctan(x) =  [-0.78539816  0.          0.78539816]\n",
      "Exponents and logarithms\n",
      "Another common type of operation available in a NumPy ufunc are the exponentials:\n",
      "In[18]: x = [1, 2, 3]\n",
      "        print(\"x     =\", x)\n",
      "        print(\"e^x   =\", np.exp(x))\n",
      "        print(\"2^x   =\", np.exp2(x))\n",
      "        print(\"3^x   =\", np.power(3, x))\n",
      "x     = [1, 2, 3]\n",
      "e^x   = [  2.71828183   7.3890561   20.08553692]\n",
      "2^x   = [ 2.  4.  8.]\n",
      "3^x   = [ 3  9 27]\n",
      "The inverse of the exponentials, the logarithms, are also available. The basic np.log\n",
      "gives the natural logarithm; if you prefer to compute the base-2 logarithm or the\n",
      "base-10 logarithm, these are available as well:\n",
      "In[19]: x = [1, 2, 4, 10]\n",
      "        print(\"x        =\", x)\n",
      "        print(\"ln(x)    =\", np.log(x))\n",
      "        print(\"log2(x)  =\", np.log2(x))\n",
      "        print(\"log10(x) =\", np.log10(x))\n",
      "x        = [1, 2, 4, 10]\n",
      "ln(x)    = [ 0.          0.69314718  1.38629436  2.30258509]\n",
      "log2(x)  = [ 0.          1.          2.          3.32192809]\n",
      "log10(x) = [ 0.          0.30103     0.60205999  1.        ]\n",
      "There are also some specialized versions that are useful for maintaining precision\n",
      "with very small input:...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 72, 'page_label': '55', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5008}\n",
      "\n",
      "--- Chunk 5009 ---\n",
      "Content:\n",
      "In[20]: x = [0, 0.001, 0.01, 0.1]\n",
      "        print(\"exp(x) - 1 =\", np.expm1(x))\n",
      "        print(\"log(1 + x) =\", np.log1p(x))\n",
      "exp(x) - 1 = [ 0.          0.0010005   0.01005017  0.10517092]\n",
      "log(1 + x) = [ 0.          0.0009995   0.00995033  0.09531018]\n",
      "When x is very small, these functions give more precise values than if the raw np.log\n",
      "or np.exp were used.\n",
      "Computation on NumPy Arrays: Universal Functions | 55...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 72, 'page_label': '55', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5009}\n",
      "\n",
      "--- Chunk 5010 ---\n",
      "Content:\n",
      "Specialized ufuncs\n",
      "NumPy has many more ufuncs available, including hyperbolic trig functions, bitwise\n",
      "arithmetic, comparison operators, conversions from radians to degrees, rounding and\n",
      "remainders, and much more. A look through the NumPy documentation reveals a lot\n",
      "of interesting functionality.\n",
      "Another excellent source for more specialized and obscure ufuncs is the submodule \n",
      "scipy.special. If you want to compute some obscure mathematical function on\n",
      "your data, chances are it is implemented in scipy.special. There are far too many\n",
      "functions to list them all, but the following snippet shows a couple that might come\n",
      "up in a statistics context:\n",
      "In[21]: from scipy import special\n",
      "In[22]: # Gamma functions (generalized factorials) and related functions\n",
      "        x = [1, 5, 10]\n",
      "        print(\"gamma(x)     =\", special.gamma(x))\n",
      "        print(\"ln|gamma(x)| =\", special.gammaln(x))\n",
      "        print(\"beta(x, 2)   =\", special.beta(x, 2))\n",
      "gamma(x)     = [  1.00000000e+00   2.40000000e+01   3.62880000e+05]\n",
      "ln|gamma(x)| = [  0.           3.17805383  12.80182748]\n",
      "beta(x, 2)   = [ 0.5         0.03333333  0.00909091]...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 73, 'page_label': '56', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5010}\n",
      "\n",
      "--- Chunk 5011 ---\n",
      "Content:\n",
      "In[23]: # Error function (integral of Gaussian)\n",
      "        # its complement, and its inverse\n",
      "        x = np.array([0, 0.3, 0.7, 1.0])\n",
      "        print(\"erf(x)  =\", special.erf(x))\n",
      "        print(\"erfc(x) =\", special.erfc(x))\n",
      "        print(\"erfinv(x) =\", special.erfinv(x))\n",
      "erf(x)  = [ 0.          0.32862676  0.67780119  0.84270079]\n",
      "erfc(x) = [ 1.          0.67137324  0.32219881  0.15729921]\n",
      "erfinv(x) = [ 0.          0.27246271  0.73286908         inf]\n",
      "There are many, many more ufuncs available in both NumPy and scipy.special.\n",
      "Because the documentation of these packages is available online, a web search along\n",
      "the lines of “gamma function python” will generally find the relevant information.\n",
      "Advanced Ufunc Features\n",
      "Many NumPy users make use of ufuncs without ever learning their full set of features.\n",
      "We’ll outline a few specialized features of ufuncs here.\n",
      "Specifying output\n",
      "For large calculations, it is sometimes useful to be able to specify the array where the\n",
      "result of the calculation will be stored. Rather than creating a temporary array, you\n",
      "can use this to write computation results directly to the memory location where you’ d\n",
      "56 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 73, 'page_label': '56', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5011}\n",
      "\n",
      "--- Chunk 5012 ---\n",
      "Content:\n",
      "like them to be. For all ufuncs, you can do this using the out argument of the\n",
      "function:\n",
      "In[24]: x = np.arange(5)\n",
      "        y = np.empty(5)\n",
      "        np.multiply(x, 10, out=y)\n",
      "        print(y)\n",
      "[  0.  10.  20.  30.  40.]\n",
      "This can even be used with array views. For example, we can write the results of a\n",
      "computation to every other element of a specified array:\n",
      "In[25]: y = np.zeros(10)\n",
      "        np.power(2, x, out=y[::2])\n",
      "        print(y)\n",
      "[  1.   0.   2.   0.   4.   0.   8.   0.  16.   0.]\n",
      "If we had instead written y[::2] = 2 ** x, this would have resulted in the creation\n",
      "of a temporary array to hold the results of 2 ** x, followed by a second operation\n",
      "copying those values into the y array. This doesn’t make much of a difference for such\n",
      "a small computation, but for very large arrays the memory savings from careful use of\n",
      "the out argument can be significant.\n",
      "Aggregates\n",
      "For binary ufuncs, there are some interesting aggregates that can be computed\n",
      "directly from the object. For example, if we’ d like to reduce an array with a particular\n",
      "operation, we can use the reduce method of any ufunc. A reduce repeatedly applies a\n",
      "given operation to the elements of an array until only a single result remains.\n",
      "For example, calling reduce on the add ufunc returns the sum of all elements in the\n",
      "array:\n",
      "In[26]: x = np.arange(1, 6)\n",
      "        np.add.reduce(x)\n",
      "Out[26]: 15\n",
      "Similarly, calling reduce on the multiply ufunc results in the product of all array\n",
      "elements:\n",
      "In[27]: np.multiply.reduce(x)\n",
      "Out[27]: 120\n",
      "If we’ d like to store all the intermediate results of the computation, we can instead use\n",
      "accumulate:\n",
      "In[28]: np.add.accumulate(x)\n",
      "Out[28]: array([ 1,  3,  6, 10, 15])\n",
      "Computation on NumPy Arrays: Universal Functions | 57...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 74, 'page_label': '57', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5012}\n",
      "\n",
      "--- Chunk 5013 ---\n",
      "Content:\n",
      "In[29]: np.multiply.accumulate(x)\n",
      "Out[29]: array([  1,   2,   6,  24, 120])\n",
      "Note that for these particular cases, there are dedicated NumPy functions to compute\n",
      "the results (np.sum, np.prod, np.cumsum, np.cumprod), which we’ll explore in “ Aggre‐\n",
      "gations: Min, Max, and Everything in Between” on page 58.\n",
      "Outer products\n",
      "Finally, any ufunc can compute the output of all pairs of two different inputs using\n",
      "the outer method. This allows you, in one line, to do things like create a multiplica‐\n",
      "tion table:\n",
      "In[30]: x = np.arange(1, 6)\n",
      "        np.multiply.outer(x, x)\n",
      "Out[30]: array([[ 1,  2,  3,  4,  5],\n",
      "                [ 2,  4,  6,  8, 10],\n",
      "                [ 3,  6,  9, 12, 15],\n",
      "                [ 4,  8, 12, 16, 20],\n",
      "                [ 5, 10, 15, 20, 25]])\n",
      "The ufunc.at and ufunc.reduceat methods, which we’ll explore in “Fancy Index‐\n",
      "ing” on page 78, are very helpful as well.\n",
      "Another extremely useful feature of ufuncs is the ability to operate between arrays of\n",
      "different sizes and shapes, a set of operations known as broadcasting. This subject is\n",
      "important enough that we will devote a whole section to it (see “Computation on\n",
      "Arrays: Broadcasting” on page 63).\n",
      "Ufuncs: Learning More\n",
      "More information on universal functions (including the full list of available func‐\n",
      "tions) can be found on the NumPy and SciPy documentation websites.\n",
      "Recall that you can also access information directly from within IPython by import‐\n",
      "ing the packages and using IPython’s tab-completion and help ( ?) functionality, as\n",
      "described in “Help and Documentation in IPython” on page 3.\n",
      "Aggregations: Min, Max, and Everything in Between\n",
      "Often when you are faced with a large amount of data, a first step is to compute sum‐\n",
      "mary statistics for the data in question. Perhaps the most common summary statistics\n",
      "are the mean and standard deviation, which allow you to summarize the “typical” val‐\n",
      "ues in a dataset, but other aggregates are useful as well (the sum, product, median,\n",
      "minimum and maximum, quantiles, etc.).\n",
      "58 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 75, 'page_label': '58', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5013}\n",
      "\n",
      "--- Chunk 5014 ---\n",
      "Content:\n",
      "NumPy has fast built-in aggregation functions for working on arrays; we’ll discuss\n",
      "and demonstrate some of them here.\n",
      "Summing the Values in an Array\n",
      "As a quick example, consider computing the sum of all values in an array. Python\n",
      "itself can do this using the built-in sum function:\n",
      "In[1]: import numpy as np\n",
      "In[2]: L = np.random.random(100)\n",
      "       sum(L)\n",
      "Out[2]: 55.61209116604941\n",
      "The syntax is quite similar to that of NumPy’s sum function, and the result is the same\n",
      "in the simplest case:\n",
      "In[3]: np.sum(L)\n",
      "Out[3]: 55.612091166049424\n",
      "However, because it executes the operation in compiled code, NumPy’s version of the\n",
      "operation is computed much more quickly:\n",
      "In[4]: big_array = np.random.rand(1000000)\n",
      "       %timeit sum(big_array)\n",
      "       %timeit np.sum(big_array)\n",
      "10 loops, best of 3: 104 ms per loop\n",
      "1000 loops, best of 3: 442 µs per loop\n",
      "Be careful, though: the sum function and the np.sum function are not identical, which\n",
      "can sometimes lead to confusion! In particular, their optional arguments have differ‐\n",
      "ent meanings, and np.sum is aware of multiple array dimensions, as we will see in the\n",
      "following section.\n",
      "Minimum and Maximum\n",
      "Similarly, Python has built-in min and max functions, used to find the minimum value\n",
      "and maximum value of any given array:\n",
      "In[5]: min(big_array), max(big_array)\n",
      "Out[5]: (1.1717128136634614e-06, 0.9999976784968716)\n",
      "NumPy’s corresponding functions have similar syntax, and again operate much more\n",
      "quickly:\n",
      "In[6]: np.min(big_array), np.max(big_array)\n",
      "Out[6]: (1.1717128136634614e-06, 0.9999976784968716)\n",
      "Aggregations: Min, Max, and Everything in Between | 59...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 76, 'page_label': '59', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5014}\n",
      "\n",
      "--- Chunk 5015 ---\n",
      "Content:\n",
      "In[7]: %timeit min(big_array)\n",
      "       %timeit np.min(big_array)\n",
      "10 loops, best of 3: 82.3 ms per loop\n",
      "1000 loops, best of 3: 497 µs per loop\n",
      "For min, max, sum, and several other NumPy aggregates, a shorter syntax is to use\n",
      "methods of the array object itself:\n",
      "In[8]: print(big_array.min(), big_array.max(), big_array.sum())\n",
      "1.17171281366e-06 0.999997678497 499911.628197\n",
      "Whenever possible, make sure that you are using the NumPy version of these aggre‐\n",
      "gates when operating on NumPy arrays!\n",
      "Multidimensional aggregates\n",
      "One common type of aggregation operation is an aggregate along a row or column.\n",
      "Say you have some data stored in a two-dimensional array:\n",
      "In[9]: M = np.random.random((3, 4))\n",
      "       print(M)\n",
      "[[ 0.8967576   0.03783739  0.75952519  0.06682827]\n",
      " [ 0.8354065   0.99196818  0.19544769  0.43447084]\n",
      " [ 0.66859307  0.15038721  0.37911423  0.6687194 ]]\n",
      "By default, each NumPy aggregation function will return the aggregate over the entire\n",
      "array:\n",
      "In[10]: M.sum()\n",
      "Out[10]: 6.0850555667307118\n",
      "Aggregation functions take an additional argument specifying the axis along which\n",
      "the aggregate is computed. For example, we can find the minimum value within each\n",
      "column by specifying axis=0:\n",
      "In[11]: M.min(axis=0)\n",
      "Out[11]: array([ 0.66859307,  0.03783739,  0.19544769,  0.06682827])\n",
      "The function returns four values, corresponding to the four columns of numbers.\n",
      "Similarly, we can find the maximum value within each row:\n",
      "In[12]: M.max(axis=1)\n",
      "Out[12]: array([ 0.8967576 ,  0.99196818,  0.6687194 ])\n",
      "The way the axis is specified here can be confusing to users coming from other lan‐\n",
      "guages. The axis keyword specifies the dimension of the array that will be collapsed ,\n",
      "rather than the dimension that will be returned. So specifying axis=0 means that the\n",
      "60 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 77, 'page_label': '60', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5015}\n",
      "\n",
      "--- Chunk 5016 ---\n",
      "Content:\n",
      "first axis will be collapsed: for two-dimensional arrays, this means that values within\n",
      "each column will be aggregated.\n",
      "Other aggregation functions\n",
      "NumPy provides many other aggregation functions, but we won’t discuss them in\n",
      "detail here. Additionally, most aggregates have a NaN-safe counterpart that computes\n",
      "the result while ignoring missing values, which are marked by the special IEEE\n",
      "floating-point NaN value (for a fuller discussion of missing data, see “Handling Miss‐\n",
      "ing Data” on page 119). Some of these NaN-safe functions were not added until\n",
      "NumPy 1.8, so they will not be available in older NumPy versions.\n",
      "Table 2-3 provides a list of useful aggregation functions available in NumPy.\n",
      "Table 2-3. Aggregation functions available in NumPy\n",
      "Function Name NaN-safe Version Description\n",
      "np.sum np.nansum Compute sum of elements\n",
      "np.prod np.nanprod Compute product of elements\n",
      "np.mean np.nanmean Compute median of elements\n",
      "np.std np.nanstd Compute standard deviation\n",
      "np.var np.nanvar Compute variance\n",
      "np.min np.nanmin Find minimum value\n",
      "np.max np.nanmax Find maximum value\n",
      "np.argmin np.nanargmin Find index of minimum value\n",
      "np.argmax np.nanargmax Find index of maximum value\n",
      "np.median np.nanmedian Compute median of elements\n",
      "np.percentile np.nanpercentile Compute rank-based statistics of elements\n",
      "np.any N/A Evaluate whether any elements are true\n",
      "np.all N/A Evaluate whether all elements are true\n",
      "We will see these aggregates often throughout the rest of the book.\n",
      "Example: What Is the Average Height of US Presidents?\n",
      "Aggregates available in NumPy can be extremely useful for summarizing a set of val‐\n",
      "ues. As a simple example, let’s consider the heights of all US presidents. This data is\n",
      "available in the file president_heights.csv, which is a simple comma-separated list of\n",
      "labels and values:\n",
      "In[13]: !head -4 data/president_heights.csv\n",
      "order,name,height(cm)\n",
      "1,George Washington,189\n",
      "Aggregations: Min, Max, and Everything in Between | 61...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 78, 'page_label': '61', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5016}\n",
      "\n",
      "--- Chunk 5017 ---\n",
      "Content:\n",
      "2,John Adams,170\n",
      "3,Thomas Jefferson,189\n",
      "We’ll use the Pandas package, which we’ll explore more fully in Chapter 3, to read the\n",
      "file and extract this information (note that the heights are measured in centimeters):\n",
      "In[14]: import pandas as pd\n",
      "        data = pd.read_csv('data/president_heights.csv')\n",
      "        heights = np.array(data['height(cm)'])\n",
      "        print(heights)\n",
      "[189 170 189 163 183 171 185 168 173 183 173 173 175 178 183 193 178 173\n",
      " 174 183 183 168 170 178 182 180 183 178 182 188 175 179 183 193 182 183\n",
      " 177 185 188 188 182 185]\n",
      "Now that we have this data array, we can compute a variety of summary statistics:\n",
      "In[15]: print(\"Mean height:       \", heights.mean())\n",
      "        print(\"Standard deviation:\", heights.std())\n",
      "        print(\"Minimum height:    \", heights.min())\n",
      "        print(\"Maximum height:    \", heights.max())\n",
      "Mean height:        179.738095238\n",
      "Standard deviation: 6.93184344275\n",
      "Minimum height:     163\n",
      "Maximum height:     193\n",
      "Note that in each case, the aggregation operation reduced the entire array to a single\n",
      "summarizing value, which gives us information about the distribution of values. We\n",
      "may also wish to compute quantiles:\n",
      "In[16]: print(\"25th percentile:   \", np.percentile(heights, 25))\n",
      "        print(\"Median:            \", np.median(heights))\n",
      "        print(\"75th percentile:   \", np.percentile(heights, 75))\n",
      "25th percentile:    174.25\n",
      "Median:             182.0\n",
      "75th percentile:    183.0\n",
      "We see that the median height of US presidents is 182 cm, or just shy of six feet.\n",
      "Of course, sometimes it’s more useful to see a visual representation of this data, which\n",
      "we can accomplish using tools in Matplotlib (we’ll discuss Matplotlib more fully in\n",
      "Chapter 4). For example, this code generates the chart shown in Figure 2-3:\n",
      "In[17]: %matplotlib inline\n",
      "        import matplotlib.pyplot as plt\n",
      "        import seaborn; seaborn.set()  # set plot style\n",
      "In[18]: plt.hist(heights)\n",
      "        plt.title('Height Distribution of US Presidents')\n",
      "        plt.xlabel('height (cm)')\n",
      "        plt.ylabel('number');\n",
      "62 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 79, 'page_label': '62', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5017}\n",
      "\n",
      "--- Chunk 5018 ---\n",
      "Content:\n",
      "Figure 2-3. Histogram of presidential heights\n",
      "These aggregates are some of the fundamental pieces of exploratory data analysis that\n",
      "we’ll explore in more depth in later chapters of the book.\n",
      "Computation on Arrays: Broadcasting\n",
      "We saw in the previous section how NumPy’s universal functions can be used to vec‐\n",
      "torize operations and thereby remove slow Python loops. Another means of vectoriz‐\n",
      "ing operations is to use NumPy’s broadcasting functionality. Broadcasting is simply a\n",
      "set of rules for applying binary ufuncs (addition, subtraction, multiplication, etc.) on\n",
      "arrays of different sizes.\n",
      "Introducing Broadcasting\n",
      "Recall that for arrays of the same size, binary operations are performed on an\n",
      "element-by-element basis:\n",
      "In[1]: import numpy as np\n",
      "In[2]: a = np.array([0, 1, 2])\n",
      "       b = np.array([5, 5, 5])\n",
      "       a + b\n",
      "Out[2]: array([5, 6, 7])\n",
      "Broadcasting allows these types of binary operations to be performed on arrays of dif‐\n",
      "ferent sizes—for example, we can just as easily add a scalar (think of it as a zero-\n",
      "dimensional array) to an array:\n",
      "Computation on Arrays: Broadcasting | 63...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 80, 'page_label': '63', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5018}\n",
      "\n",
      "--- Chunk 5019 ---\n",
      "Content:\n",
      "In[3]: a + 5\n",
      "Out[3]: array([5, 6, 7])\n",
      "We can think of this as an operation that stretches or duplicates the value 5 into the\n",
      "array [5, 5, 5], and adds the results. The advantage of NumPy’s broadcasting is that\n",
      "this duplication of values does not actually take place, but it is a useful mental model\n",
      "as we think about broadcasting.\n",
      "We can similarly extend this to arrays of higher dimension. Observe the result when\n",
      "we add a one-dimensional array to a two-dimensional array:\n",
      "In[4]: M = np.ones((3, 3))\n",
      "       M\n",
      "Out[4]: array([[ 1.,  1.,  1.],\n",
      "               [ 1.,  1.,  1.],\n",
      "               [ 1.,  1.,  1.]])\n",
      "In[5]: M + a\n",
      "Out[5]: array([[ 1.,  2.,  3.],\n",
      "               [ 1.,  2.,  3.],\n",
      "               [ 1.,  2.,  3.]])\n",
      "Here the one-dimensional array a is stretched, or broadcast, across the second\n",
      "dimension in order to match the shape of M.\n",
      "While these examples are relatively easy to understand, more complicated cases can\n",
      "involve broadcasting of both arrays. Consider the following example:\n",
      "In[6]: a = np.arange(3)\n",
      "       b = np.arange(3)[:, np.newaxis]\n",
      "       print(a)\n",
      "       print(b)\n",
      "[0 1 2]\n",
      "[[0]\n",
      " [1]\n",
      " [2]]\n",
      "In[7]: a + b\n",
      "Out[7]: array([[0, 1, 2],\n",
      "               [1, 2, 3],\n",
      "               [2, 3, 4]])\n",
      "64 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 81, 'page_label': '64', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5019}\n",
      "\n",
      "--- Chunk 5020 ---\n",
      "Content:\n",
      "1 Code to produce this plot can be found in the online appendix, and is adapted from source published in the\n",
      "astroML documentation. Used with permission.\n",
      "Just as before we stretched or broadcasted one value to match the shape of the other,\n",
      "here we’ve stretched both a and b to match a common shape, and the result is a two-\n",
      "dimensional array! The geometry of these examples is visualized in Figure 2-4.1\n",
      "Figure 2-4. Visualization of NumPy broadcasting\n",
      "The light boxes represent the broadcasted values: again, this extra memory is not\n",
      "actually allocated in the course of the operation, but it can be useful conceptually to\n",
      "imagine that it is.\n",
      "Rules of Broadcasting\n",
      "Broadcasting in NumPy follows a strict set of rules to determine the interaction\n",
      "between the two arrays:\n",
      "• Rule 1: If the two arrays differ in their number of dimensions, the shape of the\n",
      "one with fewer dimensions is padded with ones on its leading (left) side.\n",
      "• Rule 2: If the shape of the two arrays does not match in any dimension, the array\n",
      "with shape equal to 1 in that dimension is stretched to match the other shape.\n",
      "• Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is\n",
      "raised.\n",
      "Computation on Arrays: Broadcasting | 65...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 82, 'page_label': '65', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5020}\n",
      "\n",
      "--- Chunk 5021 ---\n",
      "Content:\n",
      "To make these rules clear, let’s consider a few examples in detail.\n",
      "Broadcasting example 1\n",
      "Let’s look at adding a two-dimensional array to a one-dimensional array:\n",
      "In[8]: M = np.ones((2, 3))\n",
      "       a = np.arange(3)\n",
      "Let’s consider an operation on these two arrays. The shapes of the arrays are:\n",
      "M.shape = (2, 3)\n",
      "a.shape = (3,)\n",
      "We see by rule 1 that the array a has fewer dimensions, so we pad it on the left with\n",
      "ones:\n",
      "M.shape -> (2, 3)\n",
      "a.shape -> (1, 3)\n",
      "By rule 2, we now see that the first dimension disagrees, so we stretch this dimension\n",
      "to match:\n",
      "M.shape -> (2, 3)\n",
      "a.shape -> (2, 3)\n",
      "The shapes match, and we see that the final shape will be (2, 3):\n",
      "In[9]: M + a\n",
      "Out[9]: array([[ 1.,  2.,  3.],\n",
      "               [ 1.,  2.,  3.]])\n",
      "Broadcasting example 2\n",
      "Let’s take a look at an example where both arrays need to be broadcast:\n",
      "In[10]: a = np.arange(3).reshape((3, 1))\n",
      "        b = np.arange(3)\n",
      "Again, we’ll start by writing out the shape of the arrays:\n",
      "a.shape = (3, 1)\n",
      "b.shape = (3,)\n",
      "66 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 83, 'page_label': '66', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5021}\n",
      "\n",
      "--- Chunk 5022 ---\n",
      "Content:\n",
      "Rule 1 says we must pad the shape of b with ones:\n",
      "a.shape -> (3, 1)\n",
      "b.shape -> (1, 3)\n",
      "And rule 2 tells us that we upgrade each of these ones to match the corresponding\n",
      "size of the other array:\n",
      "a.shape -> (3, 3)\n",
      "b.shape -> (3, 3)\n",
      "Because the result matches, these shapes are compatible. We can see this here:\n",
      "In[11]: a + b\n",
      "Out[11]: array([[0, 1, 2],\n",
      "                [1, 2, 3],\n",
      "                [2, 3, 4]])\n",
      "Broadcasting example 3\n",
      "Now let’s take a look at an example in which the two arrays are not compatible:\n",
      "In[12]: M = np.ones((3, 2))\n",
      "        a = np.arange(3)\n",
      "This is just a slightly different situation than in the first example: the matrix M is\n",
      "transposed. How does this affect the calculation? The shapes of the arrays are:\n",
      "M.shape = (3, 2)\n",
      "a.shape = (3,)\n",
      "Again, rule 1 tells us that we must pad the shape of a with ones:\n",
      "M.shape -> (3, 2)\n",
      "a.shape -> (1, 3)\n",
      "By rule 2, the first dimension of a is stretched to match that of M:\n",
      "M.shape -> (3, 2)\n",
      "a.shape -> (3, 3)\n",
      "Now we hit rule 3—the final shapes do not match, so these two arrays are incompati‐\n",
      "ble, as we can observe by attempting this operation:\n",
      "Computation on Arrays: Broadcasting | 67...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 84, 'page_label': '67', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5022}\n",
      "\n",
      "--- Chunk 5023 ---\n",
      "Content:\n",
      "In[13]: M + a\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-13-9e16e9f98da6> in <module>()\n",
      "----> 1 M + a\n",
      "ValueError: operands could not be broadcast together with shapes (3,2) (3,)\n",
      "Note the potential confusion here: you could imagine making a and M compatible by,\n",
      "say, padding a’s shape with ones on the right rather than the left. But this is not how\n",
      "the broadcasting rules work! That sort of flexibility might be useful in some cases, but\n",
      "it would lead to potential areas of ambiguity. If right-side padding is what you’ d like,\n",
      "you can do this explicitly by reshaping the array (we’ll use the np.newaxis keyword\n",
      "introduced in “The Basics of NumPy Arrays” on page 42):\n",
      "In[14]: a[:, np.newaxis].shape\n",
      "Out[14]: (3, 1)\n",
      "In[15]: M + a[:, np.newaxis]\n",
      "Out[15]: array([[ 1.,  1.],\n",
      "                [ 2.,  2.],\n",
      "                [ 3.,  3.]])\n",
      "Also note that while we’ve been focusing on the + operator here, these broadcasting\n",
      "rules apply to any binary ufunc. For example, here is the logaddexp(a, b) function,\n",
      "which computes log(exp(a) + exp(b))  with more precision than the naive\n",
      "approach:\n",
      "In[16]: np.logaddexp(M, a[:, np.newaxis])\n",
      "Out[16]: array([[ 1.31326169,  1.31326169],\n",
      "                [ 1.69314718,  1.69314718],\n",
      "                [ 2.31326169,  2.31326169]])\n",
      "For more information on the many available universal functions, refer to “Computa‐\n",
      "tion on NumPy Arrays: Universal Functions” on page 50.\n",
      "Broadcasting in Practice\n",
      "Broadcasting operations form the core of many examples we’ll see throughout this\n",
      "book. We’ll now take a look at a couple simple examples of where they can be useful.\n",
      "Centering an array\n",
      "In the previous section, we saw that ufuncs allow a NumPy user to remove the need\n",
      "to explicitly write slow Python loops. Broadcasting extends this ability. One com‐\n",
      "68 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 85, 'page_label': '68', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5023}\n",
      "\n",
      "--- Chunk 5024 ---\n",
      "Content:\n",
      "monly seen example is centering an array of data. Imagine you have an array of 10\n",
      "observations, each of which consists of 3 values. Using the standard convention (see\n",
      "“Data Representation in Scikit-Learn” on page 343), we’ll store this in a 10×3 array:\n",
      "In[17]: X = np.random.random((10, 3))\n",
      "We can compute the mean of each feature using the mean aggregate across the first\n",
      "dimension:\n",
      "In[18]: Xmean = X.mean(0)\n",
      "        Xmean\n",
      "Out[18]: array([ 0.53514715,  0.66567217,  0.44385899])\n",
      "And now we can center the X array by subtracting the mean (this is a broadcasting\n",
      "operation):\n",
      "In[19]: X_centered = X - Xmean\n",
      "To double-check that we’ve done this correctly, we can check that the centered array\n",
      "has near zero mean:\n",
      "In[20]: X_centered.mean(0)\n",
      "Out[20]: array([  2.22044605e-17,  -7.77156117e-17,  -1.66533454e-17])\n",
      "To within-machine precision, the mean is now zero.\n",
      "Plotting a two-dimensional function\n",
      "One place that broadcasting is very useful is in displaying images based on two-\n",
      "dimensional functions. If we want to define a function z = f(x, y), broadcasting can be\n",
      "used to compute the function across the grid:\n",
      "In[21]: # x and y have 50 steps from 0 to 5\n",
      "        x = np.linspace(0, 5, 50)\n",
      "        y = np.linspace(0, 5, 50)[:, np.newaxis]\n",
      "        z = np.sin(x) ** 10 + np.cos(10 + y * x) * np.cos(x)\n",
      "We’ll use Matplotlib to plot this two-dimensional array (these tools will be discussed\n",
      "in full in “Density and Contour Plots” on page 241):\n",
      "In[22]: %matplotlib inline\n",
      "       import matplotlib.pyplot as plt\n",
      "In[23]: plt.imshow(z, origin='lower', extent=[0, 5, 0, 5],\n",
      "                   cmap='viridis')\n",
      "        plt.colorbar();\n",
      "The result, shown in Figure 2-5, is a compelling visualization of the two-dimensional\n",
      "function.\n",
      "Computation on Arrays: Broadcasting | 69...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 86, 'page_label': '69', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5024}\n",
      "\n",
      "--- Chunk 5025 ---\n",
      "Content:\n",
      "Figure 2-5. Visualization of a 2D array\n",
      "Comparisons, Masks, and Boolean Logic\n",
      "This section covers the use of Boolean masks to examine and manipulate values\n",
      "within NumPy arrays. Masking comes up when you want to extract, modify, count, or\n",
      "otherwise manipulate values in an array based on some criterion: for example, you\n",
      "might wish to count all values greater than a certain value, or perhaps remove all out‐\n",
      "liers that are above some threshold. In NumPy, Boolean masking is often the most\n",
      "efficient way to accomplish these types of tasks.\n",
      "Example: Counting Rainy Days\n",
      "Imagine you have a series of data that represents the amount of precipitation each day\n",
      "for a year in a given city. For example, here we’ll load the daily rainfall statistics for\n",
      "the city of Seattle in 2014, using Pandas (which is covered in more detail in Chap‐\n",
      "ter 3):\n",
      "In[1]: import numpy as np\n",
      "       import pandas as pd\n",
      "       # use Pandas to extract rainfall inches as a NumPy array\n",
      "       rainfall = pd.read_csv('data/Seattle2014.csv')['PRCP'].values\n",
      "       inches = rainfall / 254  # 1/10mm -> inches\n",
      "       inches.shape\n",
      "Out[1]: (365,)\n",
      "The array contains 365 values, giving daily rainfall in inches from January 1 to\n",
      "December 31, 2014.\n",
      "As a first quick visualization, let’s look at the histogram of rainy days shown in\n",
      "Figure 2-6, which was generated using Matplotlib (we will explore this tool more fully\n",
      "in Chapter 4):\n",
      "70 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 87, 'page_label': '70', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5025}\n",
      "\n",
      "--- Chunk 5026 ---\n",
      "Content:\n",
      "In[2]: %matplotlib inline\n",
      "       import matplotlib.pyplot as plt\n",
      "       import seaborn; seaborn.set()  # set plot styles\n",
      "In[3]: plt.hist(inches, 40);\n",
      "Figure 2-6. Histogram of 2014 rainfall in Seattle\n",
      "This histogram gives us a general idea of what the data looks like: despite its reputa‐\n",
      "tion, the vast majority of days in Seattle saw near zero measured rainfall in 2014. But\n",
      "this doesn’t do a good job of conveying some information we’ d like to see: for exam‐\n",
      "ple, how many rainy days were there in the year? What is the average precipitation on\n",
      "those rainy days? How many days were there with more than half an inch of rain?\n",
      "Digging into the data\n",
      "One approach to this would be to answer these questions by hand: loop through the\n",
      "data, incrementing a counter each time we see values in some desired range. For rea‐\n",
      "sons discussed throughout this chapter, such an approach is very inefficient, both\n",
      "from the standpoint of time writing code and time computing the result. We saw in\n",
      "“Computation on NumPy Arrays: Universal Functions” on page 50 that NumPy’s\n",
      "ufuncs can be used in place of loops to do fast element-wise arithmetic operations on\n",
      "arrays; in the same way, we can use other ufuncs to do element-wise comparisons over\n",
      "arrays, and we can then manipulate the results to answer the questions we have. We’ll\n",
      "leave the data aside for right now, and discuss some general tools in NumPy to use\n",
      "masking to quickly answer these types of questions.\n",
      "Comparison Operators as ufuncs\n",
      "In “Computation on NumPy Arrays: Universal Functions” on page 50 we introduced\n",
      "ufuncs, and focused in particular on arithmetic operators. We saw that using +, -, *, /,\n",
      "Comparisons, Masks, and Boolean Logic | 71...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 88, 'page_label': '71', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5026}\n",
      "\n",
      "--- Chunk 5027 ---\n",
      "Content:\n",
      "and others on arrays leads to element-wise operations. NumPy also implements com‐\n",
      "parison operators such as < (less than) and > (greater than) as element-wise ufuncs.\n",
      "The result of these comparison operators is always an array with a Boolean data type.\n",
      "All six of the standard comparison operations are available:\n",
      "In[4]: x = np.array([1, 2, 3, 4, 5])\n",
      "In[5]: x < 3  # less than\n",
      "Out[5]: array([ True,  True, False, False, False], dtype=bool)\n",
      "In[6]: x > 3  # greater than\n",
      "Out[6]: array([False, False, False,  True,  True], dtype=bool)\n",
      "In[7]: x <= 3  # less than or equal\n",
      "Out[7]: array([ True,  True,  True, False, False], dtype=bool)\n",
      "In[8]: x >= 3  # greater than or equal\n",
      "Out[8]: array([False, False,  True,  True,  True], dtype=bool)\n",
      "In[9]: x != 3  # not equal\n",
      "Out[9]: array([ True,  True, False,  True,  True], dtype=bool)\n",
      "In[10]: x == 3  # equal\n",
      "Out[10]: array([False, False,  True, False, False], dtype=bool)\n",
      "It is also possible to do an element-by-element comparison of two arrays, and to\n",
      "include compound expressions:\n",
      "In[11]: (2 * x) == (x ** 2)\n",
      "Out[11]: array([False,  True, False, False, False], dtype=bool)\n",
      "As in the case of arithmetic operators, the comparison operators are implemented as\n",
      "ufuncs in NumPy; for example, when you write x < 3 , internally NumPy uses\n",
      "np.less(x, 3). A summary of the comparison operators and their equivalent ufunc\n",
      "is shown here:\n",
      "Operator Equivalent ufunc\n",
      "== np.equal\n",
      "!= np.not_equal\n",
      "< np.less\n",
      "<= np.less_equal\n",
      "> np.greater\n",
      ">= np.greater_equal\n",
      "Just as in the case of arithmetic ufuncs, these will work on arrays of any size and\n",
      "shape. Here is a two-dimensional example:\n",
      "72 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 89, 'page_label': '72', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5027}\n",
      "\n",
      "--- Chunk 5028 ---\n",
      "Content:\n",
      "In[12]: rng = np.random.RandomState(0)\n",
      "        x = rng.randint(10, size=(3, 4))\n",
      "        x\n",
      "Out[12]: array([[5, 0, 3, 3],\n",
      "                [7, 9, 3, 5],\n",
      "                [2, 4, 7, 6]])\n",
      "In[13]: x < 6\n",
      "Out[13]: array([[ True,  True,  True,  True],\n",
      "                [False, False,  True,  True],\n",
      "                [ True,  True, False, False]], dtype=bool)\n",
      "In each case, the result is a Boolean array, and NumPy provides a number of straight‐\n",
      "forward patterns for working with these Boolean results.\n",
      "Working with Boolean Arrays\n",
      "Given a Boolean array, there are a host of useful operations you can do. We’ll work\n",
      "with x, the two-dimensional array we created earlier:\n",
      "In[14]: print(x)\n",
      "[[5 0 3 3]\n",
      " [7 9 3 5]\n",
      " [2 4 7 6]]\n",
      "Counting entries\n",
      "To count the number of True entries in a Boolean array, np.count_nonzero is useful:\n",
      "In[15]: # how many values less than 6?\n",
      "        np.count_nonzero(x < 6)\n",
      "Out[15]: 8\n",
      "We see that there are eight array entries that are less than 6. Another way to get at this\n",
      "information is to use np.sum; in this case, False is interpreted as 0, and True is inter‐\n",
      "preted as 1:\n",
      "In[16]: np.sum(x < 6)\n",
      "Out[16]: 8\n",
      "The benefit of sum() is that like with other NumPy aggregation functions, this sum‐\n",
      "mation can be done along rows or columns as well:\n",
      "In[17]: # how many values less than 6 in each row?\n",
      "        np.sum(x < 6, axis=1)\n",
      "Out[17]: array([4, 2, 2])\n",
      "This counts the number of values less than 6 in each row of the matrix.\n",
      "Comparisons, Masks, and Boolean Logic | 73...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 90, 'page_label': '73', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5028}\n",
      "\n",
      "--- Chunk 5029 ---\n",
      "Content:\n",
      "If we’re interested in quickly checking whether any or all the values are true, we can\n",
      "use (you guessed it) np.any() or np.all():\n",
      "In[18]: # are there any values greater than 8?\n",
      "        np.any(x > 8)\n",
      "Out[18]: True\n",
      "In[19]: # are there any values less than zero?\n",
      "        np.any(x < 0)\n",
      "Out[19]: False\n",
      "In[20]: # are all values less than 10?\n",
      "        np.all(x < 10)\n",
      "Out[20]: True\n",
      "In[21]: # are all values equal to 6?\n",
      "        np.all(x == 6)\n",
      "Out[21]: False\n",
      "np.all() and np.any() can be used along particular axes as well. For example:\n",
      "In[22]: # are all values in each row less than 8?\n",
      "        np.all(x < 8, axis=1)\n",
      "Out[22]: array([ True, False,  True], dtype=bool)\n",
      "Here all the elements in the first and third rows are less than 8, while this is not the\n",
      "case for the second row.\n",
      "Finally, a quick warning: as mentioned in “ Aggregations: Min, Max, and Everything\n",
      "in Between” on page 58, Python has built-in sum(), any(), and all() functions.\n",
      "These have a different syntax than the NumPy versions, and in particular will fail or\n",
      "produce unintended results when used on multidimensional arrays. Be sure that you\n",
      "are using np.sum(), np.any(), and np.all() for these examples!\n",
      "Boolean operators\n",
      "We’ve already seen how we might count, say, all days with rain less than four inches,\n",
      "or all days with rain greater than two inches. But what if we want to know about all\n",
      "days with rain less than four inches and greater than one inch? This is accomplished\n",
      "through Python’s bitwise logic operators, &, |, ^, and ~. Like with the standard arith‐\n",
      "metic operators, NumPy overloads these as ufuncs that work element-wise on (usu‐\n",
      "ally Boolean) arrays.\n",
      "For example, we can address this sort of compound question as follows:\n",
      "In[23]: np.sum((inches > 0.5) & (inches < 1))\n",
      "Out[23]: 29\n",
      "So we see that there are 29 days with rainfall between 0.5 and 1.0 inches.\n",
      "74 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 91, 'page_label': '74', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5029}\n",
      "\n",
      "--- Chunk 5030 ---\n",
      "Content:\n",
      "Note that the parentheses here are important—because of operator precedence rules,\n",
      "with parentheses removed this expression would be evaluated as follows, which\n",
      "results in an error:\n",
      "inches > (0.5 & inches) < 1\n",
      "Using the equivalence of A AND B and NOT (A OR B) (which you may remember if\n",
      "you’ve taken an introductory logic course), we can compute the same result in a dif‐\n",
      "ferent manner:\n",
      "In[24]: np.sum(~( (inches <= 0.5) | (inches >= 1) ))\n",
      "Out[24]: 29\n",
      "Combining comparison operators and Boolean operators on arrays can lead to a wide\n",
      "range of efficient logical operations.\n",
      "The following table summarizes the bitwise Boolean operators and their equivalent\n",
      "ufuncs:\n",
      "Operator Equivalent ufunc\n",
      "& np.bitwise_and\n",
      "| np.bitwise_or\n",
      "^ np.bitwise_xor\n",
      "~ np.bitwise_not\n",
      "Using these tools, we might start to answer the types of questions we have about our\n",
      "weather data. Here are some examples of results we can compute when combining\n",
      "masking with aggregations:\n",
      "In[25]: print(\"Number days without rain:      \", np.sum(inches == 0))\n",
      "        print(\"Number days with rain:         \", np.sum(inches != 0))\n",
      "        print(\"Days with more than 0.5 inches:\", np.sum(inches > 0.5))\n",
      "        print(\"Rainy days with < 0.1 inches  :\", np.sum((inches > 0) &\n",
      "                                                        (inches < 0.2)))\n",
      "Number days without rain:       215\n",
      "Number days with rain:          150\n",
      "Days with more than 0.5 inches: 37\n",
      "Rainy days with < 0.1 inches  : 75\n",
      "Boolean Arrays as Masks\n",
      "In the preceding section, we looked at aggregates computed directly on Boolean\n",
      "arrays. A more powerful pattern is to use Boolean arrays as masks, to select particular\n",
      "subsets of the data themselves. Returning to our x array from before, suppose we\n",
      "want an array of all values in the array that are less than, say, 5:\n",
      "Comparisons, Masks, and Boolean Logic | 75...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 92, 'page_label': '75', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5030}\n",
      "\n",
      "--- Chunk 5031 ---\n",
      "Content:\n",
      "In[26]: x\n",
      "Out[26]: array([[5, 0, 3, 3],\n",
      "                [7, 9, 3, 5],\n",
      "                [2, 4, 7, 6]])\n",
      "We can obtain a Boolean array for this condition easily, as we’ve already seen:\n",
      "In[27]: x < 5\n",
      "Out[27]: array([[False,  True,  True,  True],\n",
      "                [False, False,  True, False],\n",
      "                [ True,  True, False, False]], dtype=bool)\n",
      "Now to select these values from the array, we can simply index on this Boolean array;\n",
      "this is known as a masking operation:\n",
      "In[28]: x[x < 5]\n",
      "Out[28]: array([0, 3, 3, 3, 2, 4])\n",
      "What is returned is a one-dimensional array filled with all the values that meet this\n",
      "condition; in other words, all the values in positions at which the mask array is True.\n",
      "We are then free to operate on these values as we wish. For example, we can compute\n",
      "some relevant statistics on our Seattle rain data:\n",
      "In[29]:\n",
      "# construct a mask of all rainy days\n",
      "rainy = (inches > 0)\n",
      "# construct a mask of all summer days (June 21st is the 172nd day)\n",
      "summer = (np.arange(365) - 172 < 90) & (np.arange(365) - 172 > 0)\n",
      "print(\"Median precip on rainy days in 2014 (inches):   \",\n",
      "      np.median(inches[rainy]))\n",
      "print(\"Median precip on summer days in 2014 (inches):  \",\n",
      "      np.median(inches[summer]))\n",
      "print(\"Maximum precip on summer days in 2014 (inches): \",\n",
      "      np.max(inches[summer]))\n",
      "print(\"Median precip on non-summer rainy days (inches):\",\n",
      "      np.median(inches[rainy & ~summer]))\n",
      "Median precip on rainy days in 2014 (inches):    0.194881889764\n",
      "Median precip on summer days in 2014 (inches):   0.0\n",
      "Maximum precip on summer days in 2014 (inches):  0.850393700787\n",
      "Median precip on non-summer rainy days (inches): 0.200787401575\n",
      "By combining Boolean operations, masking operations, and aggregates, we can very\n",
      "quickly answer these sorts of questions for our dataset.\n",
      "76 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 93, 'page_label': '76', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5031}\n",
      "\n",
      "--- Chunk 5032 ---\n",
      "Content:\n",
      "Using the Keywords and/or Versus the Operators &/|\n",
      "One common point of confusion is the difference between the keywords and and or\n",
      "on one hand, and the operators & and | on the other hand. When would you use one\n",
      "versus the other?\n",
      "The difference is this: and and or gauge the truth or falsehood of entire object, while &\n",
      "and | refer to bits within each object.\n",
      "When you use and or or, it’s equivalent to asking Python to treat the object as a single\n",
      "Boolean entity. In Python, all nonzero integers will evaluate as True. Thus:\n",
      "In[30]: bool(42), bool(0)\n",
      "Out[30]: (True, False)\n",
      "In[31]: bool(42 and 0)\n",
      "Out[31]: False\n",
      "In[32]: bool(42 or 0)\n",
      "Out[32]: True\n",
      "When you use & and | on integers, the expression operates on the bits of the element,\n",
      "applying the and or the or to the individual bits making up the number:\n",
      "In[33]: bin(42)\n",
      "Out[33]: '0b101010'\n",
      "In[34]: bin(59)\n",
      "Out[34]: '0b111011'\n",
      "In[35]: bin(42 & 59)\n",
      "Out[35]: '0b101010'\n",
      "In[36]: bin(42 | 59)\n",
      "Out[36]: '0b111011'\n",
      "Notice that the corresponding bits of the binary representation are compared in order\n",
      "to yield the result.\n",
      "When you have an array of Boolean values in NumPy, this can be thought of as a\n",
      "string of bits where 1 = True and 0 = False, and the result of & and | operates in a\n",
      "similar manner as before:\n",
      "In[37]: A = np.array([1, 0, 1, 0, 1, 0], dtype=bool)\n",
      "        B = np.array([1, 1, 1, 0, 1, 1], dtype=bool)\n",
      "        A | B\n",
      "Out[37]: array([ True,  True,  True, False,  True,  True], dtype=bool)\n",
      "Comparisons, Masks, and Boolean Logic | 77...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 94, 'page_label': '77', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5032}\n",
      "\n",
      "--- Chunk 5033 ---\n",
      "Content:\n",
      "Using or on these arrays will try to evaluate the truth or falsehood of the entire array\n",
      "object, which is not a well-defined value:\n",
      "In[38]: A or B\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-38-5d8e4f2e21c0> in <module>()\n",
      "----> 1 A or B\n",
      "ValueError: The truth value of an array with more than one element is...\n",
      "Similarly, when doing a Boolean expression on a given array, you should use | or &\n",
      "rather than or or and:\n",
      "In[39]: x = np.arange(10)\n",
      "        (x > 4) & (x < 8)\n",
      "Out[39]: array([False, False, ...,  True,  True, False, False], dtype=bool)\n",
      "Trying to evaluate the truth or falsehood of the entire array will give the same\n",
      "ValueError we saw previously:\n",
      "In[40]: (x > 4) and (x < 8)\n",
      "---------------------------------------------------------------------------\n",
      "ValueError                                Traceback (most recent call last)\n",
      "<ipython-input-40-3d24f1ffd63d> in <module>()\n",
      "----> 1 (x > 4) and (x < 8)\n",
      "ValueError: The truth value of an array with more than one element is...\n",
      "So remember this: and and or perform a single Boolean evaluation on an entire\n",
      "object, while & and | perform multiple Boolean evaluations on the content (the indi‐\n",
      "vidual bits or bytes) of an object. For Boolean NumPy arrays, the latter is nearly\n",
      "always the desired operation.\n",
      "Fancy Indexing\n",
      "In the previous sections, we saw how to access and modify portions of arrays using\n",
      "simple indices (e.g., arr[0]), slices (e.g., arr[:5]), and Boolean masks (e.g., arr[arr\n",
      "> 0]). In this section, we’ll look at another style of array indexing, known as fancy\n",
      "indexing. Fancy indexing is like the simple indexing we’ve already seen, but we pass\n",
      "arrays of indices in place of single scalars. This allows us to very quickly access and\n",
      "modify complicated subsets of an array’s values.\n",
      "78 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 95, 'page_label': '78', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5033}\n",
      "\n",
      "--- Chunk 5034 ---\n",
      "Content:\n",
      "Exploring Fancy Indexing\n",
      "Fancy indexing is conceptually simple: it means passing an array of indices to access\n",
      "multiple array elements at once. For example, consider the following array:\n",
      "In[1]: import numpy as np\n",
      "       rand = np.random.RandomState(42)\n",
      "       x = rand.randint(100, size=10)\n",
      "       print(x)\n",
      "[51 92 14 71 60 20 82 86 74 74]\n",
      "Suppose we want to access three different elements. We could do it like this:\n",
      "In[2]: [x[3], x[7], x[2]]\n",
      "Out[2]: [71, 86, 14]\n",
      "Alternatively, we can pass a single list or array of indices to obtain the same result:\n",
      "In[3]: ind = [3, 7, 4]\n",
      "       x[ind]\n",
      "Out[3]: array([71, 86, 60])\n",
      "With fancy indexing, the shape of the result reflects the shape of the index arrays\n",
      "rather than the shape of the array being indexed:\n",
      "In[4]: ind = np.array([[3, 7],\n",
      "                       [4, 5]])\n",
      "       x[ind]\n",
      "Out[4]: array([[71, 86],\n",
      "               [60, 20]])\n",
      "Fancy indexing also works in multiple dimensions. Consider the following array:\n",
      "In[5]: X = np.arange(12).reshape((3, 4))\n",
      "       X\n",
      "Out[5]: array([[ 0,  1,  2,  3],\n",
      "               [ 4,  5,  6,  7],\n",
      "               [ 8,  9, 10, 11]])\n",
      "Like with standard indexing, the first index refers to the row, and the second to the\n",
      "column:\n",
      "In[6]: row = np.array([0, 1, 2])\n",
      "       col = np.array([2, 1, 3])\n",
      "       X[row, col]\n",
      "Out[6]: array([ 2,  5, 11])\n",
      "Notice that the first value in the result is X[0, 2], the second is X[1, 1], and the\n",
      "third is X[2, 3]. The pairing of indices in fancy indexing follows all the broadcasting\n",
      "rules that were mentioned in “Computation on Arrays: Broadcasting” on page 63. So,\n",
      "Fancy Indexing | 79...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 96, 'page_label': '79', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5034}\n",
      "\n",
      "--- Chunk 5035 ---\n",
      "Content:\n",
      "for example, if we combine a column vector and a row vector within the indices, we\n",
      "get a two-dimensional result:\n",
      "In[7]: X[row[:, np.newaxis], col]\n",
      "Out[7]: array([[ 2,  1,  3],\n",
      "               [ 6,  5,  7],\n",
      "               [10,  9, 11]])\n",
      "Here, each row value is matched with each column vector, exactly as we saw in broad‐\n",
      "casting of arithmetic operations. For example:\n",
      "In[8]: row[:, np.newaxis] * col\n",
      "Out[8]: array([[0, 0, 0],\n",
      "               [2, 1, 3],\n",
      "               [4, 2, 6]])\n",
      "It is always important to remember with fancy indexing that the return value reflects\n",
      "the broadcasted shape of the indices, rather than the shape of the array being indexed.\n",
      "Combined Indexing\n",
      "For even more powerful operations, fancy indexing can be combined with the other\n",
      "indexing schemes we’ve seen:\n",
      "In[9]: print(X)\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "We can combine fancy and simple indices:\n",
      "In[10]: X[2, [2, 0, 1]]\n",
      "Out[10]: array([10,  8,  9])\n",
      "We can also combine fancy indexing with slicing:\n",
      "In[11]: X[1:, [2, 0, 1]]\n",
      "Out[11]: array([[ 6,  4,  5],\n",
      "                [10,  8,  9]])\n",
      "And we can combine fancy indexing with masking:\n",
      "In[12]: mask = np.array([1, 0, 1, 0], dtype=bool)\n",
      "        X[row[:, np.newaxis], mask]\n",
      "Out[12]: array([[ 0,  2],\n",
      "                [ 4,  6],\n",
      "                [ 8, 10]])\n",
      "All of these indexing options combined lead to a very flexible set of operations for\n",
      "accessing and modifying array values.\n",
      "80 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 97, 'page_label': '80', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5035}\n",
      "\n",
      "--- Chunk 5036 ---\n",
      "Content:\n",
      "Example: Selecting Random Points\n",
      "One common use of fancy indexing is the selection of subsets of rows from a matrix.\n",
      "For example, we might have an N by D matrix representing N points in D dimen‐\n",
      "sions, such as the following points drawn from a two-dimensional normal distribu‐\n",
      "tion:\n",
      "In[13]: mean = [0, 0]\n",
      "        cov = [[1, 2],\n",
      "               [2, 5]]\n",
      "        X = rand.multivariate_normal(mean, cov, 100)\n",
      "        X.shape\n",
      "Out[13]: (100, 2)\n",
      "Using the plotting tools we will discuss in Chapter 4, we can visualize these points as\n",
      "a scatter plot (Figure 2-7):\n",
      "In[14]: %matplotlib inline\n",
      "        import matplotlib.pyplot as plt\n",
      "        import seaborn; seaborn.set()  # for plot styling\n",
      "        plt.scatter(X[:, 0], X[:, 1]);\n",
      "Figure 2-7. Normally distributed points\n",
      "Let’s use fancy indexing to select 20 random points. We’ll do this by first choosing 20\n",
      "random indices with no repeats, and use these indices to select a portion of the origi‐\n",
      "nal array:\n",
      "In[15]: indices = np.random.choice(X.shape[0], 20, replace=False)\n",
      "        indices\n",
      "Out[15]: array([93, 45, 73, 81, 50, 10, 98, 94,  4, 64, 65, 89, 47, 84, 82,\n",
      "                80, 25, 90, 63, 20])\n",
      "Fancy Indexing | 81...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 98, 'page_label': '81', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5036}\n",
      "\n",
      "--- Chunk 5037 ---\n",
      "Content:\n",
      "In[16]: selection = X[indices]  # fancy indexing here\n",
      "        selection.shape\n",
      "Out[16]: (20, 2)\n",
      "Now to see which points were selected, let’s over-plot large circles at the locations of\n",
      "the selected points (Figure 2-8):\n",
      "In[17]: plt.scatter(X[:, 0], X[:, 1], alpha=0.3)\n",
      "        plt.scatter(selection[:, 0], selection[:, 1],\n",
      "                    facecolor='none', s=200);\n",
      "Figure 2-8. Random selection among points\n",
      "This sort of strategy is often used to quickly partition datasets, as is often needed in\n",
      "train/test splitting for validation of statistical models (see “Hyperparameters and\n",
      "Model Validation” on page 359), and in sampling approaches to answering statistical\n",
      "questions.\n",
      "Modifying Values with Fancy Indexing\n",
      "Just as fancy indexing can be used to access parts of an array, it can also be used to\n",
      "modify parts of an array. For example, imagine we have an array of indices and we’ d\n",
      "like to set the corresponding items in an array to some value:\n",
      "In[18]: x = np.arange(10)\n",
      "        i = np.array([2, 1, 8, 4])\n",
      "        x[i] = 99\n",
      "        print(x)\n",
      "[ 0 99 99  3 99  5  6  7 99  9]\n",
      "We can use any assignment-type operator for this. For example:\n",
      "82 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 99, 'page_label': '82', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5037}\n",
      "\n",
      "--- Chunk 5038 ---\n",
      "Content:\n",
      "In[19]: x[i] -= 10\n",
      "        print(x)\n",
      "[ 0 89 89  3 89  5  6  7 89  9]\n",
      "Notice, though, that repeated indices with these operations can cause some poten‐\n",
      "tially unexpected results. Consider the following:\n",
      "In[20]: x = np.zeros(10)\n",
      "        x[[0, 0]] = [4, 6]\n",
      "        print(x)\n",
      "[ 6.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Where did the 4 go? The result of this operation is to first assign x[0] = 4, followed\n",
      "by x[0] = 6. The result, of course, is that x[0] contains the value 6.\n",
      "Fair enough, but consider this operation:\n",
      "In[21]: i = [2, 3, 3, 4, 4, 4]\n",
      "        x[i] += 1\n",
      "        x\n",
      "Out[21]: array([ 6.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.])\n",
      "Y ou might expect that x[3] would contain the value 2, and x[4] would contain the\n",
      "value 3, as this is how many times each index is repeated. Why is this not the case?\n",
      "Conceptually, this is because x[i] += 1 is meant as a shorthand of x[i] = x[i] + 1.\n",
      "x[i] + 1 is evaluated, and then the result is assigned to the indices in x. With this in\n",
      "mind, it is not the augmentation that happens multiple times, but the assignment,\n",
      "which leads to the rather nonintuitive results.\n",
      "So what if you want the other behavior where the operation is repeated? For this, you\n",
      "can use the at() method of ufuncs (available since NumPy 1.8), and do the following:\n",
      "In[22]: x = np.zeros(10)\n",
      "        np.add.at(x, i, 1)\n",
      "        print(x)\n",
      "[ 0.  0.  1.  2.  3.  0.  0.  0.  0.  0.]\n",
      "The at() method does an in-place application of the given operator at the specified\n",
      "indices (here, i) with the specified value (here, 1). Another method that is similar in\n",
      "spirit is the reduceat() method of ufuncs, which you can read about in the NumPy\n",
      "documentation.\n",
      "Example: Binning Data\n",
      "Y ou can use these ideas to efficiently bin data to create a histogram by hand. For\n",
      "example, imagine we have 1,000 values and would like to quickly find where they fall\n",
      "within an array of bins. We could compute it using ufunc.at like this:\n",
      "Fancy Indexing | 83...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 100, 'page_label': '83', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5038}\n",
      "\n",
      "--- Chunk 5039 ---\n",
      "Content:\n",
      "In[23]: np.random.seed(42)\n",
      "        x = np.random.randn(100)\n",
      "        # compute a histogram by hand\n",
      "        bins = np.linspace(-5, 5, 20)\n",
      "        counts = np.zeros_like(bins)\n",
      "        # find the appropriate bin for each x\n",
      "        i = np.searchsorted(bins, x)\n",
      "        # add 1 to each of these bins\n",
      "        np.add.at(counts, i, 1)\n",
      "The counts now reflect the number of points within each bin—in other words, a his‐\n",
      "togram (Figure 2-9):\n",
      "In[24]: # plot the results\n",
      "        plt.plot(bins, counts, linestyle='steps');\n",
      "Figure 2-9. A histogram computed by hand\n",
      "Of course, it would be silly to have to do this each time you want to plot a histogram.\n",
      "This is why Matplotlib provides the plt.hist() routine, which does the same in a\n",
      "single line:\n",
      "plt.hist(x, bins, histtype='step');\n",
      "This function will create a nearly identical plot to the one seen here. To compute the\n",
      "binning, Matplotlib uses the np.histogram function, which does a very similar com‐\n",
      "putation to what we did before. Let’s compare the two here:\n",
      "In[25]: print(\"NumPy routine:\")\n",
      "        %timeit counts, edges = np.histogram(x, bins)\n",
      "84 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 101, 'page_label': '84', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5039}\n",
      "\n",
      "--- Chunk 5040 ---\n",
      "Content:\n",
      "print(\"Custom routine:\")\n",
      "        %timeit np.add.at(counts, np.searchsorted(bins, x), 1)\n",
      "NumPy routine:\n",
      "10000 loops, best of 3: 97.6 µs per loop\n",
      "Custom routine:\n",
      "10000 loops, best of 3: 19.5 µs per loop\n",
      "Our own one-line algorithm is several times faster than the optimized algorithm in\n",
      "NumPy! How can this be? If you dig into the np.histogram source code (you can do\n",
      "this in IPython by typing np.histogram??), you’ll see that it’s quite a bit more\n",
      "involved than the simple search-and-count that we’ve done; this is because NumPy’s\n",
      "algorithm is more flexible, and particularly is designed for better performance when\n",
      "the number of data points becomes large:\n",
      "In[26]: x = np.random.randn(1000000)\n",
      "        print(\"NumPy routine:\")\n",
      "        %timeit counts, edges = np.histogram(x, bins)\n",
      "        print(\"Custom routine:\")\n",
      "        %timeit np.add.at(counts, np.searchsorted(bins, x), 1)\n",
      "NumPy routine:\n",
      "10 loops, best of 3: 68.7 ms per loop\n",
      "Custom routine:\n",
      "10 loops, best of 3: 135 ms per loop\n",
      "What this comparison shows is that algorithmic efficiency is almost never a simple\n",
      "question. An algorithm efficient for large datasets will not always be the best choice\n",
      "for small datasets, and vice versa (see “Big-O Notation” on page 92). But the advan‐\n",
      "tage of coding this algorithm yourself is that with an understanding of these basic\n",
      "methods, you could use these building blocks to extend this to do some very interest‐\n",
      "ing custom behaviors. The key to efficiently using Python in data-intensive applica‐\n",
      "tions is knowing about general convenience routines like np.histogram and when\n",
      "they’re appropriate, but also knowing how to make use of lower-level functionality\n",
      "when you need more pointed behavior.\n",
      "Sorting Arrays\n",
      "Up to this point we have been concerned mainly with tools to access and operate on\n",
      "array data with NumPy. This section covers algorithms related to sorting values in\n",
      "NumPy arrays. These algorithms are a favorite topic in introductory computer sci‐\n",
      "ence courses: if you’ve ever taken one, you probably have had dreams (or, depending\n",
      "on your temperament, nightmares) about insertion sorts, selection sorts, merge sorts,\n",
      "quick sorts , bubble sorts, and many, many more. All are means of accomplishing a\n",
      "similar task: sorting the values in a list or array.\n",
      "Sorting Arrays | 85...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 102, 'page_label': '85', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5040}\n",
      "\n",
      "--- Chunk 5041 ---\n",
      "Content:\n",
      "For example, a simple selection sort repeatedly finds the minimum value from a list,\n",
      "and makes swaps until the list is sorted. We can code this in just a few lines of Python:\n",
      "In[1]: import numpy as np\n",
      "       def selection_sort(x):\n",
      "           for i in range(len(x)):\n",
      "               swap = i + np.argmin(x[i:])\n",
      "               (x[i], x[swap]) = (x[swap], x[i])\n",
      "           return x\n",
      "In[2]: x = np.array([2, 1, 4, 3, 5])\n",
      "       selection_sort(x)\n",
      "Out[2]: array([1, 2, 3, 4, 5])\n",
      "As any first-year computer science major will tell you, the selection sort is useful for\n",
      "its simplicity, but is much too slow to be useful for larger arrays. For a list of N values,\n",
      "it requires N loops, each of which does on the order of ~ N comparisons to find the\n",
      "swap value. In terms of the “big-O” notation often used to characterize these algo‐\n",
      "rithms (see “Big-O Notation” on page 92), selection sort averages ?N2: if you dou‐\n",
      "ble the number of items in the list, the execution time will go up by about a factor of\n",
      "four.\n",
      "Even selection sort, though, is much better than my all-time favorite sorting algo‐\n",
      "rithms, the bogosort:\n",
      "In[3]: def bogosort(x):\n",
      "           while np.any(x[:-1] > x[1:]):\n",
      "               np.random.shuffle(x)\n",
      "           return x\n",
      "In[4]: x = np.array([2, 1, 4, 3, 5])\n",
      "       bogosort(x)\n",
      "Out[4]: array([1, 2, 3, 4, 5])\n",
      "This silly sorting method relies on pure chance: it repeatedly applies a random shuf‐\n",
      "fling of the array until the result happens to be sorted. With an average scaling of\n",
      "? N × N!  (that’s N times N factorial), this should—quite obviously—never be used\n",
      "for any real computation.\n",
      "Fortunately, Python contains built-in sorting algorithms that are much more efficient\n",
      "than either of the simplistic algorithms just shown. We’ll start by looking at the\n",
      "Python built-ins, and then take a look at the routines included in NumPy and opti‐\n",
      "mized for NumPy arrays.\n",
      "Fast Sorting in NumPy: np.sort and np.argsort\n",
      "Although Python has built-in sort and sorted functions to work with lists, we won’t\n",
      "discuss them here because NumPy’s np.sort function turns out to be much more\n",
      "86 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 103, 'page_label': '86', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5041}\n",
      "\n",
      "--- Chunk 5042 ---\n",
      "Content:\n",
      "efficient and useful for our purposes. By default np.sort uses an ? N log N , quick‐\n",
      "sort algorithm, though mergesort and heapsort are also available. For most applica‐\n",
      "tions, the default quicksort is more than sufficient.\n",
      "To return a sorted version of the array without modifying the input, you can use\n",
      "np.sort:\n",
      "In[5]: x = np.array([2, 1, 4, 3, 5])\n",
      "       np.sort(x)\n",
      "Out[5]: array([1, 2, 3, 4, 5])\n",
      "If you prefer to sort the array in-place, you can instead use the sort method of arrays:\n",
      "In[6]: x.sort()\n",
      "       print(x)\n",
      "[1 2 3 4 5]\n",
      "A related function is argsort, which instead returns the indices of the sorted\n",
      "elements:\n",
      "In[7]: x = np.array([2, 1, 4, 3, 5])\n",
      "       i = np.argsort(x)\n",
      "       print(i)\n",
      "[1 0 3 2 4]\n",
      "The first element of this result gives the index of the smallest element, the second\n",
      "value gives the index of the second smallest, and so on. These indices can then be\n",
      "used (via fancy indexing) to construct the sorted array if desired:\n",
      "In[8]: x[i]\n",
      "Out[8]: array([1, 2, 3, 4, 5])\n",
      "Sorting along rows or columns\n",
      "A useful feature of NumPy’s sorting algorithms is the ability to sort along specific\n",
      "rows or columns of a multidimensional array using the axis argument. For example:\n",
      "In[9]: rand = np.random.RandomState(42)\n",
      "       X = rand.randint(0, 10, (4, 6))\n",
      "       print(X)\n",
      "[[6 3 7 4 6 9]\n",
      " [2 6 7 4 3 7]\n",
      " [7 2 5 4 1 7]\n",
      " [5 1 4 0 9 5]]\n",
      "In[10]: # sort each column of X\n",
      "        np.sort(X, axis=0)\n",
      "Out[10]: array([[2, 1, 4, 0, 1, 5],\n",
      "                [5, 2, 5, 4, 3, 7],\n",
      "Sorting Arrays | 87...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 104, 'page_label': '87', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5042}\n",
      "\n",
      "--- Chunk 5043 ---\n",
      "Content:\n",
      "[6, 3, 7, 4, 6, 7],\n",
      "                [7, 6, 7, 4, 9, 9]])\n",
      "In[11]: # sort each row of X\n",
      "        np.sort(X, axis=1)\n",
      "Out[11]: array([[3, 4, 6, 6, 7, 9],\n",
      "                [2, 3, 4, 6, 7, 7],\n",
      "                [1, 2, 4, 5, 7, 7],\n",
      "                [0, 1, 4, 5, 5, 9]])\n",
      "Keep in mind that this treats each row or column as an independent array, and any\n",
      "relationships between the row or column values will be lost!\n",
      "Partial Sorts: Partitioning\n",
      "Sometimes we’re not interested in sorting the entire array, but simply want to find the\n",
      "K smallest values in the array. NumPy provides this in the np.partition function.\n",
      "np.partition takes an array and a number K; the result is a new array with the small‐\n",
      "est K values to the left of the partition, and the remaining values to the right, in arbi‐\n",
      "trary order:\n",
      "In[12]: x = np.array([7, 2, 3, 1, 6, 5, 4])\n",
      "        np.partition(x, 3)\n",
      "Out[12]: array([2, 1, 3, 4, 6, 5, 7])\n",
      "Note that the first three values in the resulting array are the three smallest in the\n",
      "array, and the remaining array positions contain the remaining values. Within the\n",
      "two partitions, the elements have arbitrary order.\n",
      "Similarly to sorting, we can partition along an arbitrary axis of a multidimensional\n",
      "array:\n",
      "In[13]: np.partition(X, 2, axis=1)\n",
      "Out[13]: array([[3, 4, 6, 7, 6, 9],\n",
      "                [2, 3, 4, 7, 6, 7],\n",
      "                [1, 2, 4, 5, 7, 7],\n",
      "                [0, 1, 4, 5, 9, 5]])\n",
      "The result is an array where the first two slots in each row contain the smallest values\n",
      "from that row, with the remaining values filling the remaining slots.\n",
      "Finally, just as there is a np.argsort that computes indices of the sort, there is a\n",
      "np.argpartition that computes indices of the partition. We’ll see this in action in the\n",
      "following section.\n",
      "Example: k-Nearest Neighbors\n",
      "Let’s quickly see how we might use this argsort function along multiple axes to find\n",
      "the nearest neighbors of each point in a set. We’ll start by creating a random set of 10\n",
      "88 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 105, 'page_label': '88', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5043}\n",
      "\n",
      "--- Chunk 5044 ---\n",
      "Content:\n",
      "points on a two-dimensional plane. Using the standard convention, we’ll arrange\n",
      "these in a 10×2 array:\n",
      "In[14]: X = rand.rand(10, 2)\n",
      "To get an idea of how these points look, let’s quickly scatter plot them (Figure 2-10):\n",
      "In[15]: %matplotlib inline\n",
      "        import matplotlib.pyplot as plt\n",
      "        import seaborn; seaborn.set() # Plot styling\n",
      "        plt.scatter(X[:, 0], X[:, 1], s=100);\n",
      "Figure 2-10. Visualization of points in the k-neighbors example\n",
      "Now we’ll compute the distance between each pair of points. Recall that the squared-\n",
      "distance between two points is the sum of the squared differences in each dimension;\n",
      "using the efficient broadcasting (“Computation on Arrays: Broadcasting” on page 63)\n",
      "and aggregation ( “ Aggregations: Min, Max, and Everything in Between” on page 58)\n",
      "routines provided by NumPy, we can compute the matrix of square distances in a sin‐\n",
      "gle line of code:\n",
      "In[16]: dist_sq = np.sum((X[:,np.newaxis,:] - X[np.newaxis,:,:]) ** 2, axis=-1)\n",
      "This operation has a lot packed into it, and it might be a bit confusing if you’re unfa‐\n",
      "miliar with NumPy’s broadcasting rules. When you come across code like this, it can\n",
      "be useful to break it down into its component steps:\n",
      "In[17]: # for each pair of points, compute differences in their coordinates\n",
      "        differences = X[:, np.newaxis, :] - X[np.newaxis, :, :]\n",
      "        differences.shape\n",
      "Out[17]: (10, 10, 2)\n",
      "Sorting Arrays | 89...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 106, 'page_label': '89', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5044}\n",
      "\n",
      "--- Chunk 5045 ---\n",
      "Content:\n",
      "In[18]: # square the coordinate differences\n",
      "        sq_differences = differences ** 2\n",
      "        sq_differences.shape\n",
      "Out[18]: (10, 10, 2)\n",
      "In[19]: # sum the coordinate differences to get the squared distance\n",
      "        dist_sq = sq_differences.sum(-1)\n",
      "        dist_sq.shape\n",
      "Out[19]: (10, 10)\n",
      "Just to double-check what we are doing, we should see that the diagonal of this matrix\n",
      "(i.e., the set of distances between each point and itself) is all zero:\n",
      "In[20]: dist_sq.diagonal()\n",
      "Out[20]: array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "It checks out! With the pairwise square-distances converted, we can now use np.arg\n",
      "sort to sort along each row. The leftmost columns will then give the indices of the\n",
      "nearest neighbors:\n",
      "In[21]: nearest = np.argsort(dist_sq, axis=1)\n",
      "        print(nearest)\n",
      "[[0 3 9 7 1 4 2 5 6 8]\n",
      " [1 4 7 9 3 6 8 5 0 2]\n",
      " [2 1 4 6 3 0 8 9 7 5]\n",
      " [3 9 7 0 1 4 5 8 6 2]\n",
      " [4 1 8 5 6 7 9 3 0 2]\n",
      " [5 8 6 4 1 7 9 3 2 0]\n",
      " [6 8 5 4 1 7 9 3 2 0]\n",
      " [7 9 3 1 4 0 5 8 6 2]\n",
      " [8 5 6 4 1 7 9 3 2 0]\n",
      " [9 7 3 0 1 4 5 8 6 2]]\n",
      "Notice that the first column gives the numbers 0 through 9 in order: this is due to the\n",
      "fact that each point’s closest neighbor is itself, as we would expect.\n",
      "By using a full sort here, we’ve actually done more work than we need to in this case.\n",
      "If we’re simply interested in the nearest k neighbors, all we need is to partition each\n",
      "row so that the smallest k + 1 squared distances come first, with larger distances fill‐\n",
      "ing the remaining positions of the array. We can do this with the np.argpartition\n",
      "function:\n",
      "In[22]: K = 2\n",
      "        nearest_partition = np.argpartition(dist_sq, K + 1, axis=1)\n",
      "In order to visualize this network of neighbors, let’s quickly plot the points along with\n",
      "lines representing the connections from each point to its two nearest neighbors\n",
      "(Figure 2-11):\n",
      "90 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 107, 'page_label': '90', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5045}\n",
      "\n",
      "--- Chunk 5046 ---\n",
      "Content:\n",
      "In[23]: plt.scatter(X[:, 0], X[:, 1], s=100)\n",
      "        # draw lines from each point to its two nearest neighbors\n",
      "        K = 2\n",
      "        for i in range(X.shape[0]):\n",
      "            for j in nearest_partition[i, :K+1]:\n",
      "                # plot a line from X[i] to X[j]\n",
      "                # use some zip magic to make it happen:\n",
      "                plt.plot(*zip(X[j], X[i]), color='black')\n",
      "Figure 2-11. Visualization of the neighbors of each point\n",
      "Each point in the plot has lines drawn to its two nearest neighbors. At first glance, it\n",
      "might seem strange that some of the points have more than two lines coming out of\n",
      "them: this is due to the fact that if point A is one of the two nearest neighbors of point\n",
      "B, this does not necessarily imply that point B is one of the two nearest neighbors of\n",
      "point A.\n",
      "Although the broadcasting and row-wise sorting of this approach might seem less\n",
      "straightforward than writing a loop, it turns out to be a very efficient way of operating\n",
      "on this data in Python. Y ou might be tempted to do the same type of operation by\n",
      "manually looping through the data and sorting each set of neighbors individually, but\n",
      "this would almost certainly lead to a slower algorithm than the vectorized version we\n",
      "used. The beauty of this approach is that it’s written in a way that’s agnostic to the size\n",
      "of the input data: we could just as easily compute the neighbors among 100 or\n",
      "1,000,000 points in any number of dimensions, and the code would look the same.\n",
      "Finally, I’ll note that when doing very large nearest-neighbor searches, there are tree-\n",
      "based and/or approximate algorithms that can scale as ? N log N  or better rather\n",
      "Sorting Arrays | 91...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 108, 'page_label': '91', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5046}\n",
      "\n",
      "--- Chunk 5047 ---\n",
      "Content:\n",
      "than the ?N2 of the brute-force algorithm. One example of this is the KD-Tree,\n",
      "implemented in Scikit-Learn.\n",
      "Big-O Notation\n",
      "Big-O notation is a means of describing how the number of operations required for\n",
      "an algorithm scales as the input grows in size. To use it correctly is to dive deeply into\n",
      "the realm of computer science theory, and to carefully distinguish it from the related\n",
      "small-o notation, big-θ notation, big-Ω notation, and probably many mutant hybrids\n",
      "thereof. While these distinctions add precision to statements about algorithmic scal‐\n",
      "ing, outside computer science theory exams and the remarks of pedantic blog com‐\n",
      "menters, you’ll rarely see such distinctions made in practice. Far more common in the\n",
      "data science world is a less rigid use of big-O notation: as a general (if imprecise)\n",
      "description of the scaling of an algorithm. With apologies to theorists and pedants,\n",
      "this is the interpretation we’ll use throughout this book.\n",
      "Big-O notation, in this loose sense, tells you how much time your algorithm will take\n",
      "as you increase the amount of data. If you have an ?N (read “order N”) algorithm\n",
      "that takes 1 second to operate on a list of length N=1,000, then you should expect it to\n",
      "take roughly 5 seconds for a list of length N=5,000. If you have an ?N2 (read “order\n",
      "N squared”) algorithm that takes 1 second for N=1,000, then you should expect it to\n",
      "take about 25 seconds for N=5,000.\n",
      "For our purposes, the N will usually indicate some aspect of the size of the dataset (the\n",
      "number of points, the number of dimensions, etc.). When trying to analyze billions or\n",
      "trillions of samples, the difference between ?N and ?N2 can be far from trivial!\n",
      "Notice that the big-O notation by itself tells you nothing about the actual wall-clock\n",
      "time of a computation, but only about its scaling as you change N. Generally, for\n",
      "example, an ?N algorithm is considered to have better scaling than an ?N2 algo‐\n",
      "rithm, and for good reason. But for small datasets in particular, the algorithm with\n",
      "better scaling might not be faster. For example, in a given problem an ?N2 algo‐\n",
      "rithm might take 0.01 seconds, while a “better” ?N algorithm might take 1 second.\n",
      "Scale up N by a factor of 1,000, though, and the ?N algorithm will win out.\n",
      "Even this loose version of Big-O notation can be very useful for comparing the per‐\n",
      "formance of algorithms, and we’ll use this notation throughout the book when talking\n",
      "about how algorithms scale.\n",
      "Structured Data: NumPy’s Structured Arrays\n",
      "While often our data can be well represented by a homogeneous array of values,\n",
      "sometimes this is not the case. This section demonstrates the use of NumPy’s struc‐\n",
      "tured arrays and record arrays, which provide efficient storage for compound, hetero‐\n",
      "92 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 109, 'page_label': '92', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5047}\n",
      "\n",
      "--- Chunk 5048 ---\n",
      "Content:\n",
      "geneous data. While the patterns shown here are useful for simple operations,\n",
      "scenarios like this often lend themselves to the use of Pandas DataFrames, which we’ll\n",
      "explore in Chapter 3.\n",
      "Imagine that we have several categories of data on a number of people (say, name,\n",
      "age, and weight), and we’ d like to store these values for use in a Python program. It\n",
      "would be possible to store these in three separate arrays:\n",
      "In[2]: name = ['Alice', 'Bob', 'Cathy', 'Doug']\n",
      "       age = [25, 45, 37, 19]\n",
      "       weight = [55.0, 85.5, 68.0, 61.5]\n",
      "But this is a bit clumsy. There’s nothing here that tells us that the three arrays are\n",
      "related; it would be more natural if we could use a single structure to store all of this\n",
      "data. NumPy can handle this through structured arrays, which are arrays with com‐\n",
      "pound data types.\n",
      "Recall that previously we created a simple array using an expression like this:\n",
      "In[3]: x = np.zeros(4, dtype=int)\n",
      "We can similarly create a structured array using a compound data type specification:\n",
      "In[4]: # Use a compound data type for structured arrays\n",
      "       data = np.zeros(4, dtype={'names':('name', 'age', 'weight'),\n",
      "                                 'formats':('U10', 'i4', 'f8')})\n",
      "       print(data.dtype)\n",
      "[('name', '<U10'), ('age', '<i4'), ('weight', '<f8')]\n",
      "Here 'U10' translates to “Unicode string of maximum length 10, ” 'i4' translates to\n",
      "“4-byte (i.e., 32 bit) integer, ” and 'f8' translates to “8-byte (i.e., 64 bit) float. ” We’ll\n",
      "discuss other options for these type codes in the following section.\n",
      "Now that we’ve created an empty container array, we can fill the array with our lists of\n",
      "values:\n",
      "In[5]: data['name'] = name\n",
      "       data['age'] = age\n",
      "       data['weight'] = weight\n",
      "       print(data)\n",
      "[('Alice', 25, 55.0) ('Bob', 45, 85.5) ('Cathy', 37, 68.0)\n",
      " ('Doug', 19, 61.5)]\n",
      "As we had hoped, the data is now arranged together in one convenient block of\n",
      "memory.\n",
      "The handy thing with structured arrays is that you can now refer to values either by\n",
      "index or by name:\n",
      "In[6]: # Get all names\n",
      "       data['name']\n",
      "Structured Data: NumPy’s Structured Arrays | 93...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 110, 'page_label': '93', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5048}\n",
      "\n",
      "--- Chunk 5049 ---\n",
      "Content:\n",
      "Out[6]: array(['Alice', 'Bob', 'Cathy', 'Doug'],\n",
      "              dtype='<U10')\n",
      "In[7]: # Get first row of data\n",
      "       data[0]\n",
      "Out[7]: ('Alice', 25, 55.0)\n",
      "In[8]: # Get the name from the last row\n",
      "       data[-1]['name']\n",
      "Out[8]: 'Doug'\n",
      "Using Boolean masking, this even allows you to do some more sophisticated opera‐\n",
      "tions such as filtering on age:\n",
      "In[9]: # Get names where age is under 30\n",
      "       data[data['age'] < 30]['name']\n",
      "Out[9]: array(['Alice', 'Doug'],\n",
      "              dtype='<U10')\n",
      "Note that if you’ d like to do any operations that are any more complicated than these,\n",
      "you should probably consider the Pandas package, covered in the next chapter. As\n",
      "we’ll see, Pandas provides a DataFrame object, which is a structure built on NumPy\n",
      "arrays that offers a variety of useful data manipulation functionality similar to what\n",
      "we’ve shown here, as well as much, much more.\n",
      "Creating Structured Arrays\n",
      "Structured array data types can be specified in a number of ways. Earlier, we saw the\n",
      "dictionary method:\n",
      "In[10]: np.dtype({'names':('name', 'age', 'weight'),\n",
      "                  'formats':('U10', 'i4', 'f8')})\n",
      "Out[10]: dtype([('name', '<U10'), ('age', '<i4'), ('weight', '<f8')])\n",
      "For clarity, numerical types can be specified with Python types or NumPy dtypes\n",
      "instead:\n",
      "In[11]: np.dtype({'names':('name', 'age', 'weight'),\n",
      "                  'formats':((np.str_, 10), int, np.float32)})\n",
      "Out[11]: dtype([('name', '<U10'), ('age', '<i8'), ('weight', '<f4')])\n",
      "A compound type can also be specified as a list of tuples:\n",
      "In[12]: np.dtype([('name', 'S10'), ('age', 'i4'), ('weight', 'f8')])\n",
      "Out[12]: dtype([('name', 'S10'), ('age', '<i4'), ('weight', '<f8')])\n",
      "If the names of the types do not matter to you, you can specify the types alone in a\n",
      "comma-separated string:\n",
      "In[13]: np.dtype('S10,i4,f8')\n",
      "94 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 111, 'page_label': '94', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5049}\n",
      "\n",
      "--- Chunk 5050 ---\n",
      "Content:\n",
      "Out[13]: dtype([('f0', 'S10'), ('f1', '<i4'), ('f2', '<f8')])\n",
      "The shortened string format codes may seem confusing, but they are built on simple\n",
      "principles. The first (optional) character is < or >, which means “little endian” or “big\n",
      "endian, ” respectively, and specifies the ordering convention for significant bits. The\n",
      "next character specifies the type of data: characters, bytes, ints, floating points, and so\n",
      "on (see Table 2-4). The last character or characters represents the size of the object in\n",
      "bytes.\n",
      "Table 2-4. NumPy data types\n",
      "Character Description Example\n",
      "'b' Byte np.dtype('b')\n",
      "'i' Signed integer np.dtype('i4') == np.int32\n",
      "'u' Unsigned integer np.dtype('u1') == np.uint8\n",
      "'f' Floating point np.dtype('f8') == np.int64\n",
      "'c' Complex floating point np.dtype('c16') == np.complex128\n",
      "'S', 'a' string np.dtype('S5')\n",
      "'U' Unicode string np.dtype('U') == np.str_\n",
      "'V' Raw data (void) np.dtype('V') == np.void\n",
      "More Advanced Compound Types\n",
      "It is possible to define even more advanced compound types. For example, you can\n",
      "create a type where each element contains an array or matrix of values. Here, we’ll\n",
      "create a data type with a mat component consisting of a 3×3 floating-point matrix:\n",
      "In[14]: tp = np.dtype([('id', 'i8'), ('mat', 'f8', (3, 3))])\n",
      "        X = np.zeros(1, dtype=tp)\n",
      "        print(X[0])\n",
      "        print(X['mat'][0])\n",
      "(0, [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\n",
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Now each element in the X array consists of an id and a 3×3 matrix. Why would you\n",
      "use this rather than a simple multidimensional array, or perhaps a Python dictionary?\n",
      "The reason is that this NumPy dtype directly maps onto a C structure definition, so\n",
      "the buffer containing the array content can be accessed directly within an appropri‐\n",
      "ately written C program. If you find yourself writing a Python interface to a legacy C\n",
      "or Fortran library that manipulates structured data, you’ll probably find structured\n",
      "arrays quite useful!\n",
      "Structured Data: NumPy’s Structured Arrays | 95...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 112, 'page_label': '95', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5050}\n",
      "\n",
      "--- Chunk 5051 ---\n",
      "Content:\n",
      "RecordArrays: Structured Arrays with a Twist\n",
      "NumPy also provides the np.recarray class, which is almost identical to the struc‐\n",
      "tured arrays just described, but with one additional feature: fields can be accessed as\n",
      "attributes rather than as dictionary keys. Recall that we previously accessed the ages\n",
      "by writing:\n",
      "In[15]: data['age']\n",
      "Out[15]: array([25, 45, 37, 19], dtype=int32)\n",
      "If we view our data as a record array instead, we can access this with slightly fewer\n",
      "keystrokes:\n",
      "In[16]: data_rec = data.view(np.recarray)\n",
      "        data_rec.age\n",
      "Out[16]: array([25, 45, 37, 19], dtype=int32)\n",
      "The downside is that for record arrays, there is some extra overhead involved in\n",
      "accessing the fields, even when using the same syntax. We can see this here:\n",
      "In[17]: %timeit data['age']\n",
      "        %timeit data_rec['age']\n",
      "        %timeit data_rec.age\n",
      "1000000 loops, best of 3: 241 ns per loop\n",
      "100000 loops, best of 3: 4.61 µs per loop\n",
      "100000 loops, best of 3: 7.27 µs per loop\n",
      "Whether the more convenient notation is worth the additional overhead will depend\n",
      "on your own application.\n",
      "On to Pandas\n",
      "This section on structured and record arrays is purposely at the end of this chapter,\n",
      "because it leads so well into the next package we will cover: Pandas. Structured arrays\n",
      "like the ones discussed here are good to know about for certain situations, especially\n",
      "in case you’re using NumPy arrays to map onto binary data formats in C, Fortran, or\n",
      "another language. For day-to-day use of structured data, the Pandas package is a\n",
      "much better choice, and we’ll dive into a full discussion of it in the next chapter.\n",
      "96 | Chapter 2: Introduction to NumPy...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 113, 'page_label': '96', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5051}\n",
      "\n",
      "--- Chunk 5052 ---\n",
      "Content:\n",
      "CHAPTER 3\n",
      "Data Manipulation with Pandas\n",
      "In the previous chapter, we dove into detail on NumPy and its ndarray object, which\n",
      "provides efficient storage and manipulation of dense typed arrays in Python. Here\n",
      "we’ll build on this knowledge by looking in detail at the data structures provided by\n",
      "the Pandas library. Pandas is a newer package built on top of NumPy, and provides an\n",
      "efficient implementation of a DataFrame. DataFrames are essentially multidimen‐\n",
      "sional arrays with attached row and column labels, and often with heterogeneous\n",
      "types and/or missing data. As well as offering a convenient storage interface for\n",
      "labeled data, Pandas implements a number of powerful data operations familiar to\n",
      "users of both database frameworks and spreadsheet programs.\n",
      "As we saw, NumPy’s ndarray data structure provides essential features for the type of\n",
      "clean, well-organized data typically seen in numerical computing tasks. While it\n",
      "serves this purpose very well, its limitations become clear when we need more flexi‐\n",
      "bility (attaching labels to data, working with missing data, etc.) and when attempting\n",
      "operations that do not map well to element-wise broadcasting (groupings, pivots,\n",
      "etc.), each of which is an important piece of analyzing the less structured data avail‐\n",
      "able in many forms in the world around us. Pandas, and in particular its Series and\n",
      "DataFrame objects, builds on the NumPy array structure and provides efficient access\n",
      "to these sorts of “data munging” tasks that occupy much of a data scientist’s time.\n",
      "In this chapter, we will focus on the mechanics of using Series, DataFrame, and\n",
      "related structures effectively. We will use examples drawn from real datasets where\n",
      "appropriate, but these examples are not necessarily the focus.\n",
      "Installing and Using Pandas\n",
      "Installing Pandas on your system requires NumPy to be installed, and if you’re build‐\n",
      "ing the library from source, requires the appropriate tools to compile the C and\n",
      "97...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 114, 'page_label': '97', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5052}\n",
      "\n",
      "--- Chunk 5053 ---\n",
      "Content:\n",
      "Cython sources on which Pandas is built. Details on this installation can be found in\n",
      "the Pandas documentation . If you followed the advice outlined in the preface and\n",
      "used the Anaconda stack, you already have Pandas installed.\n",
      "Once Pandas is installed, you can import it and check the version:\n",
      "In[1]: import pandas\n",
      "       pandas.__version__\n",
      "Out[1]: '0.18.1'\n",
      "Just as we generally import NumPy under the alias np, we will import Pandas under\n",
      "the alias pd:\n",
      "In[2]: import pandas as pd\n",
      "This import convention will be used throughout the remainder of this book.\n",
      "Reminder About Built-In Documentation\n",
      "As you read through this chapter, don’t forget that IPython gives you the ability to\n",
      "quickly explore the contents of a package (by using the tab-completion feature) as\n",
      "well as the documentation of various functions (using the ? character). (Refer back to\n",
      "“Help and Documentation in IPython” on page 3 if you need a refresher on this.)\n",
      "For example, to display all the contents of the pandas namespace, you can type this:\n",
      "In [3]: pd.<TAB>\n",
      "And to display the built-in Pandas documentation, you can use this:\n",
      "In [4]: pd?\n",
      "More detailed documentation, along with tutorials and other resources, can be found\n",
      "at http://pandas.pydata.org/.\n",
      "Introducing Pandas Objects\n",
      "At the very basic level, Pandas objects can be thought of as enhanced versions of\n",
      "NumPy structured arrays in which the rows and columns are identified with labels\n",
      "rather than simple integer indices. As we will see during the course of this chapter,\n",
      "Pandas provides a host of useful tools, methods, and functionality on top of the basic\n",
      "data structures, but nearly everything that follows will require an understanding of\n",
      "what these structures are. Thus, before we go any further, let’s introduce these three\n",
      "fundamental Pandas data structures: the Series, DataFrame, and Index.\n",
      "We will start our code sessions with the standard NumPy and Pandas imports:\n",
      "In[1]: import numpy as np\n",
      "       import pandas as pd\n",
      "98 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 115, 'page_label': '98', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5053}\n",
      "\n",
      "--- Chunk 5054 ---\n",
      "Content:\n",
      "The Pandas Series Object\n",
      "A Pandas Series is a one-dimensional array of indexed data. It can be created from a\n",
      "list or array as follows:\n",
      "In[2]: data = pd.Series([0.25, 0.5, 0.75, 1.0])\n",
      "       data\n",
      "Out[2]: 0    0.25\n",
      "        1    0.50\n",
      "        2    0.75\n",
      "        3    1.00\n",
      "        dtype: float64\n",
      "As we see in the preceding output, the Series wraps both a sequence of values and a\n",
      "sequence of indices, which we can access with the values and index attributes. The\n",
      "values are simply a familiar NumPy array:\n",
      "In[3]: data.values\n",
      "Out[3]: array([ 0.25,  0.5 ,  0.75,  1.  ])\n",
      "The index is an array-like object of type pd.Index, which we’ll discuss in more detail\n",
      "momentarily:\n",
      "In[4]: data.index\n",
      "Out[4]: RangeIndex(start=0, stop=4, step=1)\n",
      "Like with a NumPy array, data can be accessed by the associated index via the familiar\n",
      "Python square-bracket notation:\n",
      "In[5]: data[1]\n",
      "Out[5]: 0.5\n",
      "In[6]: data[1:3]\n",
      "Out[6]: 1    0.50\n",
      "        2    0.75\n",
      "        dtype: float64\n",
      "As we will see, though, the Pandas Series is much more general and flexible than the\n",
      "one-dimensional NumPy array that it emulates.\n",
      "Series as generalized NumPy array\n",
      "From what we’ve seen so far, it may look like the Series object is basically inter‐\n",
      "changeable with a one-dimensional NumPy array. The essential difference is the pres‐\n",
      "ence of the index: while the NumPy array has an implicitly defined integer index used\n",
      "to access the values, the Pandas Series has an explicitly defined index associated with\n",
      "the values.\n",
      "Introducing Pandas Objects | 99...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 116, 'page_label': '99', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5054}\n",
      "\n",
      "--- Chunk 5055 ---\n",
      "Content:\n",
      "This explicit index definition gives the Series object additional capabilities. For\n",
      "example, the index need not be an integer, but can consist of values of any desired\n",
      "type. For example, if we wish, we can use strings as an index:\n",
      "In[7]: data = pd.Series([0.25, 0.5, 0.75, 1.0],\n",
      "                        index=['a', 'b', 'c', 'd'])\n",
      "       data\n",
      "Out[7]: a    0.25\n",
      "        b    0.50\n",
      "        c    0.75\n",
      "        d    1.00\n",
      "        dtype: float64\n",
      "And the item access works as expected:\n",
      "In[8]: data['b']\n",
      "Out[8]: 0.5\n",
      "We can even use noncontiguous or nonsequential indices:\n",
      "In[9]: data = pd.Series([0.25, 0.5, 0.75, 1.0],\n",
      "                        index=[2, 5, 3, 7])\n",
      "       data\n",
      "Out[9]: 2    0.25\n",
      "        5    0.50\n",
      "        3    0.75\n",
      "        7    1.00\n",
      "        dtype: float64\n",
      "In[10]: data[5]\n",
      "Out[10]: 0.5\n",
      "Series as specialized dictionary\n",
      "In this way, you can think of a Pandas Series a bit like a specialization of a Python\n",
      "dictionary. A dictionary is a structure that maps arbitrary keys to a set of arbitrary\n",
      "values, and a Series is a structure that maps typed keys to a set of typed values. This\n",
      "typing is important: just as the type-specific compiled code behind a NumPy array\n",
      "makes it more efficient than a Python list for certain operations, the type information\n",
      "of a Pandas Series makes it much more efficient than Python dictionaries for certain\n",
      "operations.\n",
      "We can make the Series-as-dictionary analogy even more clear by constructing a\n",
      "Series object directly from a Python dictionary:\n",
      "100 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 117, 'page_label': '100', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5055}\n",
      "\n",
      "--- Chunk 5056 ---\n",
      "Content:\n",
      "In[11]: population_dict = {'California': 38332521,\n",
      "                           'Texas': 26448193,\n",
      "                           'New York': 19651127,\n",
      "                           'Florida': 19552860,\n",
      "                           'Illinois': 12882135}\n",
      "        population = pd.Series(population_dict)\n",
      "        population\n",
      "Out[11]: California    38332521\n",
      "         Florida       19552860\n",
      "         Illinois      12882135\n",
      "         New York      19651127\n",
      "         Texas         26448193\n",
      "         dtype: int64\n",
      "By default, a Series will be created where the index is drawn from the sorted keys.\n",
      "From here, typical dictionary-style item access can be performed:\n",
      "In[12]: population['California']\n",
      "Out[12]: 38332521\n",
      "Unlike a dictionary, though, the Series also supports array-style operations such as\n",
      "slicing:\n",
      "In[13]: population['California':'Illinois']\n",
      "Out[13]: California    38332521\n",
      "         Florida       19552860\n",
      "         Illinois      12882135\n",
      "         dtype: int64\n",
      "We’ll discuss some of the quirks of Pandas indexing and slicing in “Data Indexing and\n",
      "Selection” on page 107.\n",
      "Constructing Series objects\n",
      "We’ve already seen a few ways of constructing a Pandas Series from scratch; all of\n",
      "them are some version of the following:\n",
      ">>> pd.Series(data, index=index)\n",
      "where index is an optional argument, and data can be one of many entities.\n",
      "For example, data can be a list or NumPy array, in which case index defaults to an\n",
      "integer sequence:\n",
      "In[14]: pd.Series([2, 4, 6])\n",
      "Out[14]: 0    2\n",
      "         1    4\n",
      "         2    6\n",
      "         dtype: int64\n",
      "Introducing Pandas Objects | 101...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 118, 'page_label': '101', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5056}\n",
      "\n",
      "--- Chunk 5057 ---\n",
      "Content:\n",
      "data can be a scalar, which is repeated to fill the specified index:\n",
      "In[15]: pd.Series(5, index=[100, 200, 300])\n",
      "Out[15]: 100    5\n",
      "         200    5\n",
      "         300    5\n",
      "         dtype: int64\n",
      "data can be a dictionary, in which index defaults to the sorted dictionary keys:\n",
      "In[16]: pd.Series({2:'a', 1:'b', 3:'c'})\n",
      "Out[16]: 1    b\n",
      "         2    a\n",
      "         3    c\n",
      "         dtype: object\n",
      "In each case, the index can be explicitly set if a different result is preferred:\n",
      "In[17]: pd.Series({2:'a', 1:'b', 3:'c'}, index=[3, 2])\n",
      "Out[17]: 3    c\n",
      "         2    a\n",
      "         dtype: object\n",
      "Notice that in this case, the Series is populated only with the explicitly identified\n",
      "keys.\n",
      "The Pandas DataFrame Object\n",
      "The next fundamental structure in Pandas is the DataFrame. Like the Series object\n",
      "discussed in the previous section, the DataFrame can be thought of either as a gener‐\n",
      "alization of a NumPy array, or as a specialization of a Python dictionary. We’ll now\n",
      "take a look at each of these perspectives.\n",
      "DataFrame as a generalized NumPy array\n",
      "If a Series is an analog of a one-dimensional array with flexible indices, a DataFrame\n",
      "is an analog of a two-dimensional array with both flexible row indices and flexible\n",
      "column names. Just as you might think of a two-dimensional array as an ordered\n",
      "sequence of aligned one-dimensional columns, you can think of a DataFrame as a\n",
      "sequence of aligned Series objects. Here, by “aligned” we mean that they share the\n",
      "same index.\n",
      "To demonstrate this, let’s first construct a new Series listing the area of each of the\n",
      "five states discussed in the previous section:\n",
      "In[18]:\n",
      "area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\n",
      "             'Florida': 170312, 'Illinois': 149995}\n",
      "102 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 119, 'page_label': '102', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5057}\n",
      "\n",
      "--- Chunk 5058 ---\n",
      "Content:\n",
      "area = pd.Series(area_dict)\n",
      "area\n",
      "Out[18]: California    423967\n",
      "         Florida       170312\n",
      "         Illinois      149995\n",
      "         New York      141297\n",
      "         Texas         695662\n",
      "         dtype: int64\n",
      "Now that we have this along with the population Series from before, we can use a\n",
      "dictionary to construct a single two-dimensional object containing this information:\n",
      "In[19]: states = pd.DataFrame({'population': population,\n",
      "                               'area': area})\n",
      "        states\n",
      "Out[19]:             area      population\n",
      "         California  423967    38332521\n",
      "         Florida     170312    19552860\n",
      "         Illinois    149995    12882135\n",
      "         New York    141297    19651127\n",
      "         Texas       695662    26448193\n",
      "Like the Series object, the DataFrame has an index attribute that gives access to the\n",
      "index labels:\n",
      "In[20]: states.index\n",
      "Out[20]:\n",
      "Index(['California', 'Florida', 'Illinois', 'New York', 'Texas'], dtype='object')\n",
      "Additionally, the DataFrame has a columns attribute, which is an Index object holding\n",
      "the column labels:\n",
      "In[21]: states.columns\n",
      "Out[21]: Index(['area', 'population'], dtype='object')\n",
      "Thus the DataFrame can be thought of as a generalization of a two-dimensional\n",
      "NumPy array, where both the rows and columns have a generalized index for access‐\n",
      "ing the data.\n",
      "DataFrame as specialized dictionary\n",
      "Similarly, we can also think of a DataFrame as a specialization of a dictionary. Where\n",
      "a dictionary maps a key to a value, a DataFrame maps a column name to a Series of\n",
      "column data. For example, asking for the 'area' attribute returns the Series object\n",
      "containing the areas we saw earlier:\n",
      "In[22]: states['area']\n",
      "Out[22]: California    423967\n",
      "         Florida       170312\n",
      "Introducing Pandas Objects | 103...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 120, 'page_label': '103', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5058}\n",
      "\n",
      "--- Chunk 5059 ---\n",
      "Content:\n",
      "Illinois      149995\n",
      "         New York      141297\n",
      "         Texas         695662\n",
      "         Name: area, dtype: int64\n",
      "Notice the potential point of confusion here: in a two-dimensional NumPy array,\n",
      "data[0] will return the first row. For a DataFrame, data['col0'] will return the first\n",
      "column. Because of this, it is probably better to think about DataFrames as generalized\n",
      "dictionaries rather than generalized arrays, though both ways of looking at the situa‐\n",
      "tion can be useful. We’ll explore more flexible means of indexing DataFrames in “Data\n",
      "Indexing and Selection” on page 107.\n",
      "Constructing DataFrame objects\n",
      "A Pandas DataFrame can be constructed in a variety of ways. Here we’ll give several\n",
      "examples.\n",
      "From a single Series object.    A DataFrame is a collection of Series objects, and a single-\n",
      "column DataFrame can be constructed from a single Series:\n",
      "In[23]: pd.DataFrame(population, columns=['population'])\n",
      "Out[23]:               population\n",
      "         California    38332521\n",
      "         Florida       19552860\n",
      "         Illinois      12882135\n",
      "         New York      19651127\n",
      "         Texas         26448193\n",
      "From a list of dicts.    Any list of dictionaries can be made into a DataFrame. We’ll use a\n",
      "simple list comprehension to create some data:\n",
      "In[24]: data = [{'a': i, 'b': 2 * i}\n",
      "                for i in range(3)]\n",
      "        pd.DataFrame(data)\n",
      "Out[24]:    a  b\n",
      "         0  0  0\n",
      "         1  1  2\n",
      "         2  2  4\n",
      "Even if some keys in the dictionary are missing, Pandas will fill them in with NaN (i.e.,\n",
      "“not a number”) values:\n",
      "In[25]: pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])\n",
      "Out[25]:    a    b  c\n",
      "         0  1.0  2  NaN\n",
      "         1  NaN  3  4.0\n",
      "104 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 121, 'page_label': '104', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5059}\n",
      "\n",
      "--- Chunk 5060 ---\n",
      "Content:\n",
      "From a dictionary of Series objects.    As we saw before, a DataFrame can be constructed\n",
      "from a dictionary of Series objects as well:\n",
      "In[26]: pd.DataFrame({'population': population,\n",
      "                      'area': area})\n",
      "Out[26]:             area      population\n",
      "         California  423967    38332521\n",
      "         Florida     170312    19552860\n",
      "         Illinois    149995    12882135\n",
      "         New York    141297    19651127\n",
      "         Texas       695662    26448193\n",
      "From a two-dimensional NumPy array.    Given a two-dimensional array of data, we can\n",
      "create a DataFrame with any specified column and index names. If omitted, an integer\n",
      "index will be used for each:\n",
      "In[27]: pd.DataFrame(np.random.rand(3, 2),\n",
      "                     columns=['foo', 'bar'],\n",
      "                     index=['a', 'b', 'c'])\n",
      "Out[27]:    foo       bar\n",
      "         a  0.865257  0.213169\n",
      "         b  0.442759  0.108267\n",
      "         c  0.047110  0.905718\n",
      "From a NumPy structured array.    We covered structured arrays in “Structured Data:\n",
      "NumPy’s Structured Arrays” on page 92. A Pandas DataFrame operates much like a\n",
      "structured array, and can be created directly from one:\n",
      "In[28]: A = np.zeros(3, dtype=[('A', 'i8'), ('B', 'f8')])\n",
      "        A\n",
      "Out[28]: array([(0, 0.0), (0, 0.0), (0, 0.0)],\n",
      "               dtype=[('A', '<i8'), ('B', '<f8')])\n",
      "In[29]: pd.DataFrame(A)\n",
      "Out[29]:    A  B\n",
      "         0  0  0.0\n",
      "         1  0  0.0\n",
      "         2  0  0.0\n",
      "The Pandas Index Object\n",
      "We have seen here that both the Series and DataFrame objects contain an explicit\n",
      "index that lets you reference and modify data. This Index object is an interesting\n",
      "structure in itself, and it can be thought of either as an immutable array or as an\n",
      "ordered set  (technically a multiset, as Index objects may contain repeated values).\n",
      "Those views have some interesting consequences in the operations available on Index\n",
      "objects. As a simple example, let’s construct an Index from a list of integers:\n",
      "Introducing Pandas Objects | 105...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 122, 'page_label': '105', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5060}\n",
      "\n",
      "--- Chunk 5061 ---\n",
      "Content:\n",
      "In[30]: ind = pd.Index([2, 3, 5, 7, 11])\n",
      "        ind\n",
      "Out[30]: Int64Index([2, 3, 5, 7, 11], dtype='int64')\n",
      "Index as immutable array\n",
      "The Index object in many ways operates like an array. For example, we can use stan‐\n",
      "dard Python indexing notation to retrieve values or slices:\n",
      "In[31]: ind[1]\n",
      "Out[31]: 3\n",
      "In[32]: ind[::2]\n",
      "Out[32]: Int64Index([2, 5, 11], dtype='int64')\n",
      "Index objects also have many of the attributes familiar from NumPy arrays:\n",
      "In[33]: print(ind.size, ind.shape, ind.ndim, ind.dtype)\n",
      "5 (5,) 1 int64\n",
      "One difference between Index objects and NumPy arrays is that indices are immuta‐\n",
      "ble—that is, they cannot be modified via the normal means:\n",
      "In[34]: ind[1] = 0\n",
      "---------------------------------------------------------------------------\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "<ipython-input-34-40e631c82e8a> in <module>()\n",
      "----> 1 ind[1] = 0\n",
      "/Users/jakevdp/anaconda/lib/python3.5/site-packages/pandas/indexes/base.py ...\n",
      "   1243\n",
      "   1244     def __setitem__(self, key, value):\n",
      "-> 1245         raise TypeError(\"Index does not support mutable operations\")\n",
      "   1246\n",
      "   1247     def __getitem__(self, key):\n",
      "TypeError: Index does not support mutable operations\n",
      "This immutability makes it safer to share indices between multiple DataFrames and\n",
      "arrays, without the potential for side effects from inadvertent index modification.\n",
      "Index as ordered set\n",
      "Pandas objects are designed to facilitate operations such as joins across datasets,\n",
      "which depend on many aspects of set arithmetic. The Index object follows many of\n",
      "106 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 123, 'page_label': '106', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5061}\n",
      "\n",
      "--- Chunk 5062 ---\n",
      "Content:\n",
      "the conventions used by Python’s built-in set data structure, so that unions, intersec‐\n",
      "tions, differences, and other combinations can be computed in a familiar way:\n",
      "In[35]: indA = pd.Index([1, 3, 5, 7, 9])\n",
      "        indB = pd.Index([2, 3, 5, 7, 11])\n",
      "In[36]: indA & indB  # intersection\n",
      "Out[36]: Int64Index([3, 5, 7], dtype='int64')\n",
      "In[37]: indA | indB  # union\n",
      "Out[37]: Int64Index([1, 2, 3, 5, 7, 9, 11], dtype='int64')\n",
      "In[38]: indA ^ indB  # symmetric difference\n",
      "Out[38]: Int64Index([1, 2, 9, 11], dtype='int64')\n",
      "These operations may also be accessed via object methods—for example, indA.inter\n",
      "section(indB).\n",
      "Data Indexing and Selection\n",
      "In Chapter 2, we looked in detail at methods and tools to access, set, and modify val‐\n",
      "ues in NumPy arrays. These included indexing (e.g., arr[2, 1]), slicing (e.g., arr[:,\n",
      "1:5]), masking (e.g., arr[arr > 0] ), fancy indexing (e.g., arr[0, [1, 5]] ), and\n",
      "combinations thereof (e.g., arr[:, [1, 5]] ). Here we’ll look at similar means of\n",
      "accessing and modifying values in Pandas Series and DataFrame objects. If you have\n",
      "used the NumPy patterns, the corresponding patterns in Pandas will feel very famil‐\n",
      "iar, though there are a few quirks to be aware of.\n",
      "We’ll start with the simple case of the one-dimensional Series object, and then move\n",
      "on to the more complicated two-dimensional DataFrame object.\n",
      "Data Selection in Series\n",
      "As we saw in the previous section, a Series object acts in many ways like a one-\n",
      "dimensional NumPy array, and in many ways like a standard Python dictionary. If we\n",
      "keep these two overlapping analogies in mind, it will help us to understand the pat‐\n",
      "terns of data indexing and selection in these arrays.\n",
      "Series as dictionary\n",
      "Like a dictionary, the Series object provides a mapping from a collection of keys to a\n",
      "collection of values:\n",
      "In[1]: import pandas as pd\n",
      "       data = pd.Series([0.25, 0.5, 0.75, 1.0],\n",
      "                        index=['a', 'b', 'c', 'd'])\n",
      "       data\n",
      "Data Indexing and Selection | 107...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 124, 'page_label': '107', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5062}\n",
      "\n",
      "--- Chunk 5063 ---\n",
      "Content:\n",
      "Out[1]: a    0.25\n",
      "        b    0.50\n",
      "        c    0.75\n",
      "        d    1.00\n",
      "        dtype: float64\n",
      "In[2]: data['b']\n",
      "Out[2]: 0.5\n",
      "We can also use dictionary-like Python expressions and methods to examine the\n",
      "keys/indices and values:\n",
      "In[3]: 'a' in data\n",
      "Out[3]: True\n",
      "In[4]: data.keys()\n",
      "Out[4]: Index(['a', 'b', 'c', 'd'], dtype='object')\n",
      "In[5]: list(data.items())\n",
      "Out[5]: [('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)]\n",
      "Series objects can even be modified with a dictionary-like syntax. Just as you can\n",
      "extend a dictionary by assigning to a new key, you can extend a Series by assigning\n",
      "to a new index value:\n",
      "In[6]: data['e'] = 1.25\n",
      "       data\n",
      "Out[6]: a    0.25\n",
      "        b    0.50\n",
      "        c    0.75\n",
      "        d    1.00\n",
      "        e    1.25\n",
      "        dtype: float64\n",
      "This easy mutability of the objects is a convenient feature: under the hood, Pandas is\n",
      "making decisions about memory layout and data copying that might need to take\n",
      "place; the user generally does not need to worry about these issues.\n",
      "Series as one-dimensional array\n",
      "A Series builds on this dictionary-like interface and provides array-style item selec‐\n",
      "tion via the same basic mechanisms as NumPy arrays—that is, slices, masking, and\n",
      "fancy indexing. Examples of these are as follows:\n",
      "In[7]: # slicing by explicit index\n",
      "       data['a':'c']\n",
      "Out[7]: a    0.25\n",
      "        b    0.50\n",
      "        c    0.75\n",
      "        dtype: float64\n",
      "108 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 125, 'page_label': '108', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5063}\n",
      "\n",
      "--- Chunk 5064 ---\n",
      "Content:\n",
      "In[8]: # slicing by implicit integer index\n",
      "       data[0:2]\n",
      "Out[8]: a    0.25\n",
      "        b    0.50\n",
      "        dtype: float64\n",
      "In[9]: # masking\n",
      "       data[(data > 0.3) & (data < 0.8)]\n",
      "Out[9]: b    0.50\n",
      "        c    0.75\n",
      "        dtype: float64\n",
      "In[10]: # fancy indexing\n",
      "        data[['a', 'e']]\n",
      "Out[10]: a    0.25\n",
      "         e    1.25\n",
      "         dtype: float64\n",
      "Among these, slicing may be the source of the most confusion. Notice that when you\n",
      "are slicing with an explicit index (i.e., data['a':'c']), the final index is included in\n",
      "the slice, while when you’re slicing with an implicit index (i.e., data[0:2]), the final\n",
      "index is excluded from the slice.\n",
      "Indexers: loc, iloc, and ix\n",
      "These slicing and indexing conventions can be a source of confusion. For example, if\n",
      "your Series has an explicit integer index, an indexing operation such as data[1] will\n",
      "use the explicit indices, while a slicing operation like data[1:3] will use the implicit\n",
      "Python-style index.\n",
      "In[11]: data = pd.Series(['a', 'b', 'c'], index=[1, 3, 5])\n",
      "        data\n",
      "Out[11]: 1    a\n",
      "         3    b\n",
      "         5    c\n",
      "         dtype: object\n",
      "In[12]: # explicit index when indexing\n",
      "        data[1]\n",
      "Out[12]: 'a'\n",
      "In[13]: # implicit index when slicing\n",
      "        data[1:3]\n",
      "Out[13]: 3    b\n",
      "         5    c\n",
      "         dtype: object\n",
      "Because of this potential confusion in the case of integer indexes, Pandas provides\n",
      "some special indexer attributes that explicitly expose certain indexing schemes. These\n",
      "Data Indexing and Selection | 109...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 126, 'page_label': '109', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5064}\n",
      "\n",
      "--- Chunk 5065 ---\n",
      "Content:\n",
      "are not functional methods, but attributes that expose a particular slicing interface to\n",
      "the data in the Series.\n",
      "First, the loc attribute allows indexing and slicing that always references the explicit\n",
      "index:\n",
      "In[14]: data.loc[1]\n",
      "Out[14]: 'a'\n",
      "In[15]: data.loc[1:3]\n",
      "Out[15]: 1    a\n",
      "         3    b\n",
      "         dtype: object\n",
      "The iloc attribute allows indexing and slicing that always references the implicit\n",
      "Python-style index:\n",
      "In[16]: data.iloc[1]\n",
      "Out[16]: 'b'\n",
      "In[17]: data.iloc[1:3]\n",
      "Out[17]: 3    b\n",
      "         5    c\n",
      "         dtype: object\n",
      "A third indexing attribute, ix, is a hybrid of the two, and for Series objects is equiva‐\n",
      "lent to standard []-based indexing. The purpose of the ix indexer will become more\n",
      "apparent in the context of DataFrame objects, which we will discuss in a moment.\n",
      "One guiding principle of Python code is that “explicit is better than implicit. ” The\n",
      "explicit nature of loc and iloc make them very useful in maintaining clean and read‐\n",
      "able code; especially in the case of integer indexes, I recommend using these both to\n",
      "make code easier to read and understand, and to prevent subtle bugs due to the\n",
      "mixed indexing/slicing convention.\n",
      "Data Selection in DataFrame\n",
      "Recall that a DataFrame acts in many ways like a two-dimensional or structured array,\n",
      "and in other ways like a dictionary of Series structures sharing the same index.\n",
      "These analogies can be helpful to keep in mind as we explore data selection within\n",
      "this structure.\n",
      "DataFrame as a dictionary\n",
      "The first analogy we will consider is the DataFrame as a dictionary of related Series\n",
      "objects. Let’s return to our example of areas and populations of states:\n",
      "110 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 127, 'page_label': '110', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5065}\n",
      "\n",
      "--- Chunk 5066 ---\n",
      "Content:\n",
      "In[18]: area = pd.Series({'California': 423967, 'Texas': 695662,\n",
      "                          'New York': 141297, 'Florida': 170312,\n",
      "                          'Illinois': 149995})\n",
      "        pop = pd.Series({'California': 38332521, 'Texas': 26448193,\n",
      "                         'New York': 19651127, 'Florida': 19552860,\n",
      "                         'Illinois': 12882135})\n",
      "        data = pd.DataFrame({'area':area, 'pop':pop})\n",
      "        data\n",
      "Out[18]:             area    pop\n",
      "         California  423967  38332521\n",
      "         Florida     170312  19552860\n",
      "         Illinois    149995  12882135\n",
      "         New York    141297  19651127\n",
      "         Texas       695662  26448193\n",
      "The individual Series that make up the columns of the DataFrame can be accessed\n",
      "via dictionary-style indexing of the column name:\n",
      "In[19]: data['area']\n",
      "Out[19]: California    423967\n",
      "         Florida       170312\n",
      "         Illinois      149995\n",
      "         New York      141297\n",
      "         Texas         695662\n",
      "         Name: area, dtype: int64\n",
      "Equivalently, we can use attribute-style access with column names that are strings:\n",
      "In[20]: data.area\n",
      "Out[20]: California    423967\n",
      "         Florida       170312\n",
      "         Illinois      149995\n",
      "         New York      141297\n",
      "         Texas         695662\n",
      "         Name: area, dtype: int64\n",
      "This attribute-style column access actually accesses the exact same object as the\n",
      "dictionary-style access:\n",
      "In[21]: data.area is data['area']\n",
      "Out[21]: True\n",
      "Though this is a useful shorthand, keep in mind that it does not work for all cases!\n",
      "For example, if the column names are not strings, or if the column names conflict\n",
      "with methods of the DataFrame, this attribute-style access is not possible. For exam‐\n",
      "ple, the DataFrame has a pop() method, so data.pop will point to this rather than the\n",
      "\"pop\" column:\n",
      "In[22]: data.pop is data['pop']\n",
      "Out[22]: False\n",
      "Data Indexing and Selection | 111...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 128, 'page_label': '111', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5066}\n",
      "\n",
      "--- Chunk 5067 ---\n",
      "Content:\n",
      "In particular, you should avoid the temptation to try column assignment via attribute\n",
      "(i.e., use data['pop'] = z rather than data.pop = z).\n",
      "Like with the Series objects discussed earlier, this dictionary-style syntax can also be\n",
      "used to modify the object, in this case to add a new column:\n",
      "In[23]: data['density'] = data['pop'] / data['area']\n",
      "        data\n",
      "Out[23]:             area    pop       density\n",
      "         California  423967  38332521   90.413926\n",
      "         Florida     170312  19552860  114.806121\n",
      "         Illinois    149995  12882135   85.883763\n",
      "         New York    141297  19651127  139.076746\n",
      "         Texas       695662  26448193   38.018740\n",
      "This shows a preview of the straightforward syntax of element-by-element arithmetic\n",
      "between Series objects; we’ll dig into this further in “Operating on Data in Pandas”\n",
      "on page 115.\n",
      "DataFrame as two-dimensional array\n",
      "As mentioned previously, we can also view the DataFrame as an enhanced two-\n",
      "dimensional array. We can examine the raw underlying data array using the values\n",
      "attribute:\n",
      "In[24]: data.values\n",
      "Out[24]: array([[  4.23967000e+05,   3.83325210e+07,   9.04139261e+01],\n",
      "                [  1.70312000e+05,   1.95528600e+07,   1.14806121e+02],\n",
      "                [  1.49995000e+05,   1.28821350e+07,   8.58837628e+01],\n",
      "                [  1.41297000e+05,   1.96511270e+07,   1.39076746e+02],\n",
      "                [  6.95662000e+05,   2.64481930e+07,   3.80187404e+01]])\n",
      "With this picture in mind, we can do many familiar array-like observations on the\n",
      "DataFrame itself. For example, we can transpose the full DataFrame to swap rows and\n",
      "columns:...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 129, 'page_label': '112', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5067}\n",
      "\n",
      "--- Chunk 5068 ---\n",
      "Content:\n",
      "In[25]: data.T\n",
      "Out[25]:\n",
      "         California    Florida       Illinois      New York      Texas\n",
      "area     4.239670e+05  1.703120e+05  1.499950e+05  1.412970e+05  6.956620e+05\n",
      "pop      3.833252e+07  1.955286e+07  1.288214e+07  1.965113e+07  2.644819e+07\n",
      "density  9.041393e+01  1.148061e+02  8.588376e+01  1.390767e+02  3.801874e+01\n",
      "When it comes to indexing of DataFrame objects, however, it is clear that the\n",
      "dictionary-style indexing of columns precludes our ability to simply treat it as a\n",
      "NumPy array. In particular, passing a single index to an array accesses a row:\n",
      "In[26]: data.values[0]\n",
      "Out[26]: array([  4.23967000e+05,   3.83325210e+07,   9.04139261e+01])\n",
      "112 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 129, 'page_label': '112', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5068}\n",
      "\n",
      "--- Chunk 5069 ---\n",
      "Content:\n",
      "and passing a single “index” to a DataFrame accesses a column:\n",
      "In[27]: data['area']\n",
      "Out[27]: California    423967\n",
      "         Florida       170312\n",
      "         Illinois      149995\n",
      "         New York      141297\n",
      "         Texas         695662\n",
      "         Name: area, dtype: int64\n",
      "Thus for array-style indexing, we need another convention. Here Pandas again uses\n",
      "the loc, iloc, and ix indexers mentioned earlier. Using the iloc indexer, we can\n",
      "index the underlying array as if it is a simple NumPy array (using the implicit\n",
      "Python-style index), but the DataFrame index and column labels are maintained in\n",
      "the result:\n",
      "In[28]: data.iloc[:3, :2]\n",
      "Out[28]:             area    pop\n",
      "         California  423967  38332521\n",
      "         Florida     170312  19552860\n",
      "         Illinois    149995  12882135\n",
      "In[29]: data.loc[:'Illinois', :'pop']\n",
      "Out[29]:             area    pop\n",
      "         California  423967  38332521\n",
      "         Florida     170312  19552860\n",
      "         Illinois    149995  12882135\n",
      "The ix indexer allows a hybrid of these two approaches:\n",
      "In[30]: data.ix[:3, :'pop']\n",
      "Out[30]:             area    pop\n",
      "         California  423967  38332521\n",
      "         Florida     170312  19552860\n",
      "         Illinois    149995  12882135\n",
      "Keep in mind that for integer indices, the ix indexer is subject to the same potential\n",
      "sources of confusion as discussed for integer-indexed Series objects.\n",
      "Any of the familiar NumPy-style data access patterns can be used within these index‐\n",
      "ers. For example, in the loc indexer we can combine masking and fancy indexing as\n",
      "in the following:\n",
      "In[31]: data.loc[data.density > 100, ['pop', 'density']]\n",
      "Out[31]:           pop       density\n",
      "         Florida   19552860  114.806121\n",
      "         New York  19651127  139.076746\n",
      "Data Indexing and Selection | 113...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 130, 'page_label': '113', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5069}\n",
      "\n",
      "--- Chunk 5070 ---\n",
      "Content:\n",
      "Any of these indexing conventions may also be used to set or modify values; this is\n",
      "done in the standard way that you might be accustomed to from working with\n",
      "NumPy:\n",
      "In[32]: data.iloc[0, 2] = 90\n",
      "        data\n",
      "Out[32]:             area    pop       density\n",
      "         California  423967  38332521   90.000000\n",
      "         Florida     170312  19552860  114.806121\n",
      "         Illinois    149995  12882135   85.883763\n",
      "         New York    141297  19651127  139.076746\n",
      "         Texas       695662  26448193   38.018740\n",
      "To build up your fluency in Pandas data manipulation, I suggest spending some time\n",
      "with a simple DataFrame and exploring the types of indexing, slicing, masking, and\n",
      "fancy indexing that are allowed by these various indexing approaches.\n",
      "Additional indexing conventions\n",
      "There are a couple extra indexing conventions that might seem at odds with the pre‐\n",
      "ceding discussion, but nevertheless can be very useful in practice. First, while index‐\n",
      "ing refers to columns, slicing refers to rows:\n",
      "In[33]: data['Florida':'Illinois']\n",
      "Out[33]:           area    pop       density\n",
      "         Florida   170312  19552860  114.806121\n",
      "         Illinois  149995  12882135   85.883763\n",
      "Such slices can also refer to rows by number rather than by index:\n",
      "In[34]: data[1:3]\n",
      "Out[34]:           area    pop       density\n",
      "         Florida   170312  19552860  114.806121\n",
      "         Illinois  149995  12882135   85.883763\n",
      "Similarly, direct masking operations are also interpreted row-wise rather than\n",
      "column-wise:\n",
      "In[35]: data[data.density > 100]\n",
      "Out[35]:           area    pop       density\n",
      "         Florida   170312  19552860  114.806121\n",
      "         New York  141297  19651127  139.076746\n",
      "These two conventions are syntactically similar to those on a NumPy array, and while\n",
      "these may not precisely fit the mold of the Pandas conventions, they are nevertheless\n",
      "quite useful in practice.\n",
      "114 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 131, 'page_label': '114', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5070}\n",
      "\n",
      "--- Chunk 5071 ---\n",
      "Content:\n",
      "Operating on Data in Pandas\n",
      "One of the essential pieces of NumPy is the ability to perform quick element-wise\n",
      "operations, both with basic arithmetic (addition, subtraction, multiplication, etc.) and\n",
      "with more sophisticated operations (trigonometric functions, exponential and loga‐\n",
      "rithmic functions, etc.). Pandas inherits much of this functionality from NumPy, and\n",
      "the ufuncs that we introduced in “Computation on NumPy Arrays: Universal Func‐\n",
      "tions” on page 50 are key to this.\n",
      "Pandas includes a couple useful twists, however: for unary operations like negation\n",
      "and trigonometric functions, these ufuncs will preserve index and column labels in the\n",
      "output, and for binary operations such as addition and multiplication, Pandas will\n",
      "automatically align indices when passing the objects to the ufunc. This means that\n",
      "keeping the context of data and combining data from different sources—both poten‐\n",
      "tially error-prone tasks with raw NumPy arrays—become essentially foolproof ones\n",
      "with Pandas. We will additionally see that there are well-defined operations between\n",
      "one-dimensional Series structures and two-dimensional DataFrame structures.\n",
      "Ufuncs: Index Preservation\n",
      "Because Pandas is designed to work with NumPy, any NumPy ufunc will work on\n",
      "Pandas Series and DataFrame objects. Let’s start by defining a simple Series and\n",
      "DataFrame on which to demonstrate this:\n",
      "In[1]: import pandas as pd\n",
      "       import numpy as np\n",
      "In[2]: rng = np.random.RandomState(42)\n",
      "       ser = pd.Series(rng.randint(0, 10, 4))\n",
      "       ser\n",
      "Out[2]: 0    6\n",
      "        1    3\n",
      "        2    7\n",
      "        3    4\n",
      "        dtype: int64\n",
      "In[3]: df = pd.DataFrame(rng.randint(0, 10, (3, 4)),\n",
      "                         columns=['A', 'B', 'C', 'D'])\n",
      "       df\n",
      "Out[3]:    A  B  C  D\n",
      "        0  6  9  2  6\n",
      "        1  7  4  3  7\n",
      "        2  7  2  5  4\n",
      "If we apply a NumPy ufunc on either of these objects, the result will be another Pan‐\n",
      "das object with the indices preserved:\n",
      "In[4]: np.exp(ser)\n",
      "Operating on Data in Pandas | 115...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 132, 'page_label': '115', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5071}\n",
      "\n",
      "--- Chunk 5072 ---\n",
      "Content:\n",
      "Out[4]: 0     403.428793\n",
      "        1      20.085537\n",
      "        2    1096.633158\n",
      "        3      54.598150\n",
      "        dtype: float64\n",
      "Or, for a slightly more complex calculation:\n",
      "In[5]: np.sin(df * np.pi / 4)\n",
      "Out[5]:           A             B         C             D\n",
      "        0 -1.000000  7.071068e-01  1.000000 -1.000000e+00\n",
      "        1 -0.707107  1.224647e-16  0.707107 -7.071068e-01\n",
      "        2 -0.707107  1.000000e+00 -0.707107  1.224647e-16\n",
      "Any of the ufuncs discussed in “Computation on NumPy Arrays: Universal Func‐\n",
      "tions” on page 50 can be used in a similar manner.\n",
      "UFuncs: Index Alignment\n",
      "For binary operations on two Series or DataFrame objects, Pandas will align indices\n",
      "in the process of performing the operation. This is very convenient when you are\n",
      "working with incomplete data, as we’ll see in some of the examples that follow.\n",
      "Index alignment in Series\n",
      "As an example, suppose we are combining two different data sources, and find only\n",
      "the top three US states by area and the top three US states by population:\n",
      "In[6]: area = pd.Series({'Alaska': 1723337, 'Texas': 695662,\n",
      "                         'California': 423967}, name='area')\n",
      "       population = pd.Series({'California': 38332521, 'Texas': 26448193,\n",
      "                               'New York': 19651127}, name='population')\n",
      "Let’s see what happens when we divide these to compute the population density:\n",
      "In[7]: population / area\n",
      "Out[7]: Alaska              NaN\n",
      "        California    90.413926\n",
      "        New York            NaN\n",
      "        Texas         38.018740\n",
      "        dtype: float64\n",
      "The resulting array contains the union of indices of the two input arrays, which we\n",
      "could determine using standard Python set arithmetic on these indices:...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 133, 'page_label': '116', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5072}\n",
      "\n",
      "--- Chunk 5073 ---\n",
      "Content:\n",
      "In[7]: population / area\n",
      "Out[7]: Alaska              NaN\n",
      "        California    90.413926\n",
      "        New York            NaN\n",
      "        Texas         38.018740\n",
      "        dtype: float64\n",
      "The resulting array contains the union of indices of the two input arrays, which we\n",
      "could determine using standard Python set arithmetic on these indices:\n",
      "In[8]: area.index | population.index\n",
      "Out[8]: Index(['Alaska', 'California', 'New York', 'Texas'], dtype='object')\n",
      "Any item for which one or the other does not have an entry is marked with NaN, or\n",
      "“Not a Number, ” which is how Pandas marks missing data (see further discussion of\n",
      "missing data in “Handling Missing Data” on page 119). This index matching is imple‐\n",
      "116 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 133, 'page_label': '116', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5073}\n",
      "\n",
      "--- Chunk 5074 ---\n",
      "Content:\n",
      "mented this way for any of Python’s built-in arithmetic expressions; any missing val‐\n",
      "ues are filled in with NaN by default:\n",
      "In[9]: A = pd.Series([2, 4, 6], index=[0, 1, 2])\n",
      "       B = pd.Series([1, 3, 5], index=[1, 2, 3])\n",
      "       A + B\n",
      "Out[9]: 0    NaN\n",
      "        1    5.0\n",
      "        2    9.0\n",
      "        3    NaN\n",
      "        dtype: float64\n",
      "If using NaN values is not the desired behavior, we can modify the fill value using\n",
      "appropriate object methods in place of the operators. For example, calling A.add(B)\n",
      "is equivalent to calling A + B, but allows optional explicit specification of the fill value\n",
      "for any elements in A or B that might be missing:\n",
      "In[10]: A.add(B, fill_value=0)\n",
      "Out[10]: 0    2.0\n",
      "         1    5.0\n",
      "         2    9.0\n",
      "         3    5.0\n",
      "         dtype: float64\n",
      "Index alignment in DataFrame\n",
      "A similar type of alignment takes place for both columns and indices when you are\n",
      "performing operations on DataFrames:\n",
      "In[11]: A = pd.DataFrame(rng.randint(0, 20, (2, 2)),\n",
      "                         columns=list('AB'))\n",
      "        A\n",
      "Out[11]:    A   B\n",
      "         0  1  11\n",
      "         1  5   1\n",
      "In[12]: B = pd.DataFrame(rng.randint(0, 10, (3, 3)),\n",
      "                         columns=list('BAC'))\n",
      "        B\n",
      "Out[12]:    B  A  C\n",
      "         0  4  0  9\n",
      "         1  5  8  0\n",
      "         2  9  2  6\n",
      "In[13]: A + B\n",
      "Out[13]:       A     B   C\n",
      "         0   1.0  15.0 NaN\n",
      "         1  13.0   6.0 NaN\n",
      "         2   NaN   NaN NaN\n",
      "Operating on Data in Pandas | 117...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 134, 'page_label': '117', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5074}\n",
      "\n",
      "--- Chunk 5075 ---\n",
      "Content:\n",
      "Notice that indices are aligned correctly irrespective of their order in the two objects,\n",
      "and indices in the result are sorted. As was the case with Series, we can use the asso‐\n",
      "ciated object’s arithmetic method and pass any desired fill_value to be used in place\n",
      "of missing entries. Here we’ll fill with the mean of all values in A (which we compute\n",
      "by first stacking the rows of A):\n",
      "In[14]: fill = A.stack().mean()\n",
      "        A.add(B, fill_value=fill)\n",
      "Out[14]:       A     B     C\n",
      "         0   1.0  15.0  13.5\n",
      "         1  13.0   6.0   4.5\n",
      "         2   6.5  13.5  10.5\n",
      "Table 3-1 lists Python operators and their equivalent Pandas object methods.\n",
      "Table 3-1. Mapping between Python operators and Pandas methods\n",
      "Python operator Pandas method(s)\n",
      "+ add()\n",
      "- sub(), subtract()\n",
      "* mul(), multiply()\n",
      "/ truediv(), div(), divide()\n",
      "// floordiv()\n",
      "% mod()\n",
      "** pow()\n",
      "Ufuncs: Operations Between DataFrame and Series\n",
      "When you are performing operations between a DataFrame and a Series, the index\n",
      "and column alignment is similarly maintained. Operations between a DataFrame and\n",
      "a Series are similar to operations between a two-dimensional and one-dimensional\n",
      "NumPy array. Consider one common operation, where we find the difference of a\n",
      "two-dimensional array and one of its rows:\n",
      "In[15]: A = rng.randint(10, size=(3, 4))\n",
      "        A\n",
      "Out[15]: array([[3, 8, 2, 4],\n",
      "                [2, 6, 4, 8],\n",
      "                [6, 1, 3, 8]])\n",
      "In[16]: A - A[0]\n",
      "Out[16]: array([[ 0,  0,  0,  0],\n",
      "                [-1, -2,  2,  4],\n",
      "                [ 3, -7,  1,  4]])\n",
      "118 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 135, 'page_label': '118', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5075}\n",
      "\n",
      "--- Chunk 5076 ---\n",
      "Content:\n",
      "According to NumPy’s broadcasting rules (see “Computation on Arrays: Broadcast‐\n",
      "ing” on page 63), subtraction between a two-dimensional array and one of its rows is\n",
      "applied row-wise.\n",
      "In Pandas, the convention similarly operates row-wise by default:\n",
      "In[17]: df = pd.DataFrame(A, columns=list('QRST'))\n",
      "        df - df.iloc[0]\n",
      "Out[17]:    Q  R  S  T\n",
      "         0  0  0  0  0\n",
      "         1 -1 -2  2  4\n",
      "         2  3 -7  1  4\n",
      "If you would instead like to operate column-wise, you can use the object methods\n",
      "mentioned earlier, while specifying the axis keyword:\n",
      "In[18]: df.subtract(df['R'], axis=0)\n",
      "Out[18]:    Q  R  S  T\n",
      "         0 -5  0 -6 -4\n",
      "         1 -4  0 -2  2\n",
      "         2  5  0  2  7\n",
      "Note that these DataFrame/Series operations, like the operations discussed before,\n",
      "will automatically align indices between the two elements:\n",
      "In[19]: halfrow = df.iloc[0, ::2]\n",
      "        halfrow\n",
      "Out[19]: Q    3\n",
      "         S    2\n",
      "         Name: 0, dtype: int64\n",
      "In[20]: df - halfrow\n",
      "Out[20]:      Q   R    S   T\n",
      "         0  0.0 NaN  0.0 NaN\n",
      "         1 -1.0 NaN  2.0 NaN\n",
      "         2  3.0 NaN  1.0 NaN\n",
      "This preservation and alignment of indices and columns means that operations on\n",
      "data in Pandas will always maintain the data context, which prevents the types of silly\n",
      "errors that might come up when you are working with heterogeneous and/or mis‐\n",
      "aligned data in raw NumPy arrays.\n",
      "Handling Missing Data\n",
      "The difference between data found in many tutorials and data in the real world is that\n",
      "real-world data is rarely clean and homogeneous. In particular, many interesting\n",
      "datasets will have some amount of data missing. To make matters even more compli‐\n",
      "cated, different data sources may indicate missing data in different ways.\n",
      "Handling Missing Data | 119...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 136, 'page_label': '119', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5076}\n",
      "\n",
      "--- Chunk 5077 ---\n",
      "Content:\n",
      "In this section, we will discuss some general considerations for missing data, discuss\n",
      "how Pandas chooses to represent it, and demonstrate some built-in Pandas tools for\n",
      "handling missing data in Python. Here and throughout the book, we’ll refer to miss‐\n",
      "ing data in general as null, NaN, or NA values.\n",
      "Trade-Offs in Missing Data Conventions\n",
      "A number of schemes have been developed to indicate the presence of missing data in\n",
      "a table or DataFrame. Generally, they revolve around one of two strategies: using a\n",
      "mask that globally indicates missing values, or choosing a sentinel value that indicates\n",
      "a missing entry.\n",
      "In the masking approach, the mask might be an entirely separate Boolean array, or it\n",
      "may involve appropriation of one bit in the data representation to locally indicate the\n",
      "null status of a value.\n",
      "In the sentinel approach, the sentinel value could be some data-specific convention,\n",
      "such as indicating a missing integer value with –9999 or some rare bit pattern, or it\n",
      "could be a more global convention, such as indicating a missing floating-point value\n",
      "with NaN (Not a Number), a special value which is part of the IEEE floating-point\n",
      "specification.\n",
      "None of these approaches is without trade-offs: use of a separate mask array requires\n",
      "allocation of an additional Boolean array, which adds overhead in both storage and\n",
      "computation. A sentinel value reduces the range of valid values that can be repre‐\n",
      "sented, and may require extra (often non-optimized) logic in CPU and GPU arith‐\n",
      "metic. Common special values like NaN are not available for all data types.\n",
      "As in most cases where no universally optimal choice exists, different languages and\n",
      "systems use different conventions. For example, the R language uses reserved bit pat‐\n",
      "terns within each data type as sentinel values indicating missing data, while the SciDB\n",
      "system uses an extra byte attached to every cell to indicate a NA state.\n",
      "Missing Data in Pandas\n",
      "The way in which Pandas handles missing values is constrained by its reliance on the\n",
      "NumPy package, which does not have a built-in notion of NA values for non-\n",
      "floating-point data types.\n",
      "Pandas could have followed R’s lead in specifying bit patterns for each individual data\n",
      "type to indicate nullness, but this approach turns out to be rather unwieldy. While R\n",
      "contains four basic data types, NumPy supports far more than this: for example,\n",
      "while R has a single integer type, NumPy supports fourteen basic integer types once\n",
      "you account for available precisions, signedness, and endianness of the encoding.\n",
      "Reserving a specific bit pattern in all available NumPy types would lead to an\n",
      "unwieldy amount of overhead in special-casing various operations for various types,\n",
      "120 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 137, 'page_label': '120', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5077}\n",
      "\n",
      "--- Chunk 5078 ---\n",
      "Content:\n",
      "likely even requiring a new fork of the NumPy package. Further, for the smaller data\n",
      "types (such as 8-bit integers), sacrificing a bit to use as a mask will significantly\n",
      "reduce the range of values it can represent.\n",
      "NumPy does have support for masked arrays—that is, arrays that have a separate\n",
      "Boolean mask array attached for marking data as “good” or “bad. ” Pandas could have\n",
      "derived from this, but the overhead in both storage, computation, and code mainte‐\n",
      "nance makes that an unattractive choice.\n",
      "With these constraints in mind, Pandas chose to use sentinels for missing data, and\n",
      "further chose to use two already-existing Python null values: the special floating-\n",
      "point NaN value, and the Python None object. This choice has some side effects, as we\n",
      "will see, but in practice ends up being a good compromise in most cases of interest.\n",
      "None: Pythonic missing data\n",
      "The first sentinel value used by Pandas is None, a Python singleton object that is often\n",
      "used for missing data in Python code. Because None is a Python object, it cannot be\n",
      "used in any arbitrary NumPy/Pandas array, but only in arrays with data type\n",
      "'object' (i.e., arrays of Python objects):\n",
      "In[1]: import numpy as np\n",
      "       import pandas as pd\n",
      "In[2]: vals1 = np.array([1, None, 3, 4])\n",
      "       vals1\n",
      "Out[2]: array([1, None, 3, 4], dtype=object)\n",
      "This dtype=object means that the best common type representation NumPy could\n",
      "infer for the contents of the array is that they are Python objects. While this kind of\n",
      "object array is useful for some purposes, any operations on the data will be done at\n",
      "the Python level, with much more overhead than the typically fast operations seen for\n",
      "arrays with native types:\n",
      "In[3]: for dtype in ['object', 'int']:\n",
      "           print(\"dtype =\", dtype)\n",
      "           %timeit np.arange(1E6, dtype=dtype).sum()\n",
      "           print()\n",
      "dtype = object\n",
      "10 loops, best of 3: 78.2 ms per loop\n",
      "dtype = int\n",
      "100 loops, best of 3: 3.06 ms per loop\n",
      "The use of Python objects in an array also means that if you perform aggregations\n",
      "like sum() or min() across an array with a None value, you will generally get an error:\n",
      "Handling Missing Data | 121...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 138, 'page_label': '121', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5078}\n",
      "\n",
      "--- Chunk 5079 ---\n",
      "Content:\n",
      "In[4]: vals1.sum()\n",
      "TypeError                                 Traceback (most recent call last)\n",
      "<ipython-input-4-749fd8ae6030> in <module>()\n",
      "----> 1 vals1.sum()\n",
      "/Users/jakevdp/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py ...\n",
      "     30\n",
      "     31 def _sum(a, axis=None, dtype=None, out=None, keepdims=False):\n",
      "---> 32     return umr_sum(a, axis, dtype, out, keepdims)\n",
      "     33\n",
      "     34 def _prod(a, axis=None, dtype=None, out=None, keepdims=False):\n",
      "TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'\n",
      "This reflects the fact that addition between an integer and None is undefined.\n",
      "NaN: Missing numerical data\n",
      "The other missing data representation, NaN (acronym for Not a Number), is different;\n",
      "it is a special floating-point value recognized by all systems that use the standard\n",
      "IEEE floating-point representation:\n",
      "In[5]: vals2 = np.array([1, np.nan, 3, 4])\n",
      "       vals2.dtype\n",
      "Out[5]: dtype('float64')\n",
      "Notice that NumPy chose a native floating-point type for this array: this means that\n",
      "unlike the object array from before, this array supports fast operations pushed into\n",
      "compiled code. Y ou should be aware that NaN is a bit like a data virus—it infects any\n",
      "other object it touches. Regardless of the operation, the result of arithmetic with NaN\n",
      "will be another NaN:\n",
      "In[6]: 1 + np.nan\n",
      "Out[6]: nan\n",
      "In[7]: 0 *  np.nan\n",
      "Out[7]: nan\n",
      "Note that this means that aggregates over the values are well defined (i.e., they don’t\n",
      "result in an error) but not always useful:\n",
      "In[8]: vals2.sum(), vals2.min(), vals2.max()\n",
      "Out[8]: (nan, nan, nan)\n",
      "NumPy does provide some special aggregations that will ignore these missing values:\n",
      "122 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 139, 'page_label': '122', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5079}\n",
      "\n",
      "--- Chunk 5080 ---\n",
      "Content:\n",
      "In[9]: np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)\n",
      "Out[9]: (8.0, 1.0, 4.0)\n",
      "Keep in mind that NaN is specifically a floating-point value; there is no equivalent\n",
      "NaN value for integers, strings, or other types.\n",
      "NaN and None in Pandas\n",
      "NaN and None both have their place, and Pandas is built to handle the two of them\n",
      "nearly interchangeably, converting between them where appropriate:\n",
      "In[10]: pd.Series([1, np.nan, 2, None])\n",
      "Out[10]: 0    1.0\n",
      "         1    NaN\n",
      "         2    2.0\n",
      "         3    NaN\n",
      "         dtype: float64\n",
      "For types that don’t have an available sentinel value, Pandas automatically type-casts\n",
      "when NA values are present. For example, if we set a value in an integer array to\n",
      "np.nan, it will automatically be upcast to a floating-point type to accommodate the\n",
      "NA:\n",
      "In[11]: x = pd.Series(range(2), dtype=int)\n",
      "        x\n",
      "Out[11]: 0    0\n",
      "         1    1\n",
      "         dtype: int64\n",
      "In[12]: x[0] = None\n",
      "        x\n",
      "Out[12]: 0    NaN\n",
      "         1    1.0\n",
      "         dtype: float64\n",
      "Notice that in addition to casting the integer array to floating point, Pandas automati‐\n",
      "cally converts the None to a NaN value. (Be aware that there is a proposal to add a\n",
      "native integer NA to Pandas in the future; as of this writing, it has not been included.)\n",
      "While this type of magic may feel a bit hackish compared to the more unified\n",
      "approach to NA values in domain-specific languages like R, the Pandas sentinel/cast‐\n",
      "ing approach works quite well in practice and in my experience only rarely causes\n",
      "issues.\n",
      "Table 3-2 lists the upcasting conventions in Pandas when NA values are introduced.\n",
      "Handling Missing Data | 123...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 140, 'page_label': '123', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5080}\n",
      "\n",
      "--- Chunk 5081 ---\n",
      "Content:\n",
      "Table 3-2. Pandas handling of NAs by type\n",
      "Typeclass Conversion when storing NAs NA sentinel value\n",
      "floating No change np.nan\n",
      "object No change None or np.nan\n",
      "integer Cast to float64 np.nan\n",
      "boolean Cast to object None or np.nan\n",
      "Keep in mind that in Pandas, string data is always stored with an object dtype.\n",
      "Operating on Null Values\n",
      "As we have seen, Pandas treats None and NaN as essentially interchangeable for indi‐\n",
      "cating missing or null values. To facilitate this convention, there are several useful\n",
      "methods for detecting, removing, and replacing null values in Pandas data structures.\n",
      "They are:\n",
      "isnull()\n",
      "Generate a Boolean mask indicating missing values\n",
      "notnull()\n",
      "Opposite of isnull()\n",
      "dropna()\n",
      "Return a filtered version of the data\n",
      "fillna()\n",
      "Return a copy of the data with missing values filled or imputed\n",
      "We will conclude this section with a brief exploration and demonstration of these\n",
      "routines.\n",
      "Detecting null values\n",
      "Pandas data structures have two useful methods for detecting null data: isnull() and\n",
      "notnull(). Either one will return a Boolean mask over the data. For example:\n",
      "In[13]: data = pd.Series([1, np.nan, 'hello', None])\n",
      "In[14]: data.isnull()\n",
      "Out[14]: 0    False\n",
      "         1     True\n",
      "         2    False\n",
      "         3     True\n",
      "         dtype: bool\n",
      "As mentioned in “Data Indexing and Selection” on page 107, Boolean masks can be\n",
      "used directly as a Series or DataFrame index:\n",
      "124 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 141, 'page_label': '124', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5081}\n",
      "\n",
      "--- Chunk 5082 ---\n",
      "Content:\n",
      "In[15]: data[data.notnull()]\n",
      "Out[15]: 0        1\n",
      "         2    hello\n",
      "         dtype: object\n",
      "The isnull() and notnull() methods produce similar Boolean results for Data\n",
      "Frames.\n",
      "Dropping null values\n",
      "In addition to the masking used before, there are the convenience methods, dropna()\n",
      "(which removes NA values) and fillna() (which fills in NA values). For a Series,\n",
      "the result is straightforward:\n",
      "In[16]: data.dropna()\n",
      "Out[16]: 0        1\n",
      "         2    hello\n",
      "         dtype: object\n",
      "For a DataFrame, there are more options. Consider the following DataFrame:\n",
      "In[17]: df = pd.DataFrame([[1,      np.nan, 2],\n",
      "                           [2,      3,      5],\n",
      "                           [np.nan, 4,      6]])\n",
      "        df\n",
      "Out[17]:      0    1  2\n",
      "         0  1.0  NaN  2\n",
      "         1  2.0  3.0  5\n",
      "         2  NaN  4.0  6\n",
      "We cannot drop single values from a DataFrame; we can only drop full rows or full\n",
      "columns. Depending on the application, you might want one or the other, so\n",
      "dropna() gives a number of options for a DataFrame.\n",
      "By default, dropna() will drop all rows in which any null value is present:\n",
      "In[18]: df.dropna()\n",
      "Out[18]:      0    1  2\n",
      "         1  2.0  3.0  5\n",
      "Alternatively, you can drop NA values along a different axis; axis=1 drops all col‐\n",
      "umns containing a null value:\n",
      "In[19]: df.dropna(axis='columns')\n",
      "Out[19]:    2\n",
      "         0  2\n",
      "         1  5\n",
      "         2  6\n",
      "Handling Missing Data | 125...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 142, 'page_label': '125', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5082}\n",
      "\n",
      "--- Chunk 5083 ---\n",
      "Content:\n",
      "But this drops some good data as well; you might rather be interested in dropping\n",
      "rows or columns with all NA values, or a majority of NA values. This can be specified\n",
      "through the how or thresh parameters, which allow fine control of the number of\n",
      "nulls to allow through.\n",
      "The default is how='any', such that any row or column (depending on the axis key‐\n",
      "word) containing a null value will be dropped. Y ou can also specify how='all', which\n",
      "will only drop rows/columns that are all null values:\n",
      "In[20]: df[3] = np.nan\n",
      "        df\n",
      "Out[20]:      0    1  2   3\n",
      "         0  1.0  NaN  2 NaN\n",
      "         1  2.0  3.0  5 NaN\n",
      "         2  NaN  4.0  6 NaN\n",
      "In[21]: df.dropna(axis='columns', how='all')\n",
      "Out[21]:      0    1  2\n",
      "         0  1.0  NaN  2\n",
      "         1  2.0  3.0  5\n",
      "         2  NaN  4.0  6\n",
      "For finer-grained control, the thresh parameter lets you specify a minimum number\n",
      "of non-null values for the row/column to be kept:\n",
      "In[22]: df.dropna(axis='rows', thresh=3)\n",
      "Out[22]:      0    1  2   3\n",
      "         1  2.0  3.0  5 NaN\n",
      "Here the first and last row have been dropped, because they contain only two non-\n",
      "null values.\n",
      "Filling null values\n",
      "Sometimes rather than dropping NA values, you’ d rather replace them with a valid\n",
      "value. This value might be a single number like zero, or it might be some sort of\n",
      "imputation or interpolation from the good values. Y ou could do this in-place using\n",
      "the isnull() method as a mask, but because it is such a common operation Pandas\n",
      "provides the fillna() method, which returns a copy of the array with the null values\n",
      "replaced.\n",
      "Consider the following Series:\n",
      "In[23]: data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\n",
      "        data\n",
      "Out[23]: a    1.0\n",
      "         b    NaN\n",
      "         c    2.0\n",
      "         d    NaN\n",
      "126 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 143, 'page_label': '126', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5083}\n",
      "\n",
      "--- Chunk 5084 ---\n",
      "Content:\n",
      "e    3.0\n",
      "         dtype: float64\n",
      "We can fill NA entries with a single value, such as zero:\n",
      "In[24]: data.fillna(0)\n",
      "Out[24]: a    1.0\n",
      "         b    0.0\n",
      "         c    2.0\n",
      "         d    0.0\n",
      "         e    3.0\n",
      "         dtype: float64\n",
      "We can specify a forward-fill to propagate the previous value forward:\n",
      "In[25]: # forward-fill\n",
      "        data.fillna(method='ffill')\n",
      "Out[25]: a    1.0\n",
      "         b    1.0\n",
      "         c    2.0\n",
      "         d    2.0\n",
      "         e    3.0\n",
      "         dtype: float64\n",
      "Or we can specify a back-fill to propagate the next values backward:\n",
      "In[26]: # back-fill\n",
      "        data.fillna(method='bfill')\n",
      "Out[26]: a    1.0\n",
      "         b    2.0\n",
      "         c    2.0\n",
      "         d    3.0\n",
      "         e    3.0\n",
      "         dtype: float64\n",
      "For DataFrames, the options are similar, but we can also specify an axis along which\n",
      "the fills take place:\n",
      "In[27]: df\n",
      "Out[27]:      0    1  2   3\n",
      "         0  1.0  NaN  2 NaN\n",
      "         1  2.0  3.0  5 NaN\n",
      "         2  NaN  4.0  6 NaN\n",
      "In[28]: df.fillna(method='ffill', axis=1)\n",
      "Out[28]:      0    1    2    3\n",
      "         0  1.0  1.0  2.0  2.0\n",
      "         1  2.0  3.0  5.0  5.0\n",
      "         2  NaN  4.0  6.0  6.0\n",
      "Notice that if a previous value is not available during a forward fill, the NA value\n",
      "remains.\n",
      "Handling Missing Data | 127...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 144, 'page_label': '127', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5084}\n",
      "\n",
      "--- Chunk 5085 ---\n",
      "Content:\n",
      "Hierarchical Indexing\n",
      "Up to this point we’ve been focused primarily on one-dimensional and two-\n",
      "dimensional data, stored in Pandas Series and DataFrame objects, respectively. Often\n",
      "it is useful to go beyond this and store higher-dimensional data—that is, data indexed\n",
      "by more than one or two keys. While Pandas does provide Panel and Panel4D objects\n",
      "that natively handle three-dimensional and four-dimensional data (see “Panel Data”\n",
      "on page 141), a far more common pattern in practice is to make use of hierarchical\n",
      "indexing (also known as multi-indexing) to incorporate multiple index levels within a\n",
      "single index. In this way, higher-dimensional data can be compactly represented\n",
      "within the familiar one-dimensional Series and two-dimensional DataFrame objects.\n",
      "In this section, we’ll explore the direct creation of MultiIndex objects; considerations\n",
      "around indexing, slicing, and computing statistics across multiply indexed data; and\n",
      "useful routines for converting between simple and hierarchically indexed representa‐\n",
      "tions of your data.\n",
      "We begin with the standard imports:\n",
      "In[1]: import pandas as pd\n",
      "       import numpy as np\n",
      "A Multiply Indexed Series\n",
      "Let’s start by considering how we might represent two-dimensional data within a\n",
      "one-dimensional Series. For concreteness, we will consider a series of data where\n",
      "each point has a character and numerical key.\n",
      "The bad way\n",
      "Suppose you would like to track data about states from two different years. Using the\n",
      "Pandas tools we’ve already covered, you might be tempted to simply use Python\n",
      "tuples as keys:\n",
      "In[2]: index = [('California', 2000), ('California', 2010),\n",
      "                ('New York', 2000), ('New York', 2010),\n",
      "                ('Texas', 2000), ('Texas', 2010)]\n",
      "       populations = [33871648, 37253956,\n",
      "                      18976457, 19378102,\n",
      "                      20851820, 25145561]\n",
      "       pop = pd.Series(populations, index=index)\n",
      "       pop\n",
      "Out[2]: (California, 2000)    33871648\n",
      "        (California, 2010)    37253956\n",
      "        (New York, 2000)      18976457\n",
      "        (New York, 2010)      19378102\n",
      "        (Texas, 2000)         20851820\n",
      "128 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 145, 'page_label': '128', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5085}\n",
      "\n",
      "--- Chunk 5086 ---\n",
      "Content:\n",
      "(Texas, 2010)         25145561\n",
      "        dtype: int64\n",
      "With this indexing scheme, you can straightforwardly index or slice the series based\n",
      "on this multiple index:\n",
      "In[3]: pop[('California', 2010):('Texas', 2000)]\n",
      "Out[3]: (California, 2010)    37253956\n",
      "        (New York, 2000)      18976457\n",
      "        (New York, 2010)      19378102\n",
      "        (Texas, 2000)         20851820\n",
      "        dtype: int64\n",
      "But the convenience ends there. For example, if you need to select all values from\n",
      "2010, you’ll need to do some messy (and potentially slow) munging to make it\n",
      "happen:\n",
      "In[4]: pop[[i for i in pop.index if i[1] == 2010]]\n",
      "Out[4]: (California, 2010)    37253956\n",
      "        (New York, 2010)      19378102\n",
      "        (Texas, 2010)         25145561\n",
      "        dtype: int64\n",
      "This produces the desired result, but is not as clean (or as efficient for large datasets)\n",
      "as the slicing syntax we’ve grown to love in Pandas.\n",
      "The better way: Pandas MultiIndex\n",
      "Fortunately, Pandas provides a better way. Our tuple-based indexing is essentially a\n",
      "rudimentary multi-index, and the Pandas MultiIndex type gives us the type of opera‐\n",
      "tions we wish to have. We can create a multi-index from the tuples as follows:\n",
      "In[5]: index = pd.MultiIndex.from_tuples(index)\n",
      "       index\n",
      "Out[5]: MultiIndex(levels=[['California', 'New York', 'Texas'], [2000, 2010]],\n",
      "                   labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]])\n",
      "Notice that the MultiIndex contains multiple levels of indexing—in this case, the state\n",
      "names and the years, as well as multiple labels for each data point which encode these\n",
      "levels.\n",
      "If we reindex our series with this MultiIndex, we see the hierarchical representation\n",
      "of the data:\n",
      "In[6]: pop = pop.reindex(index)\n",
      "       pop\n",
      "Out[6]: California  2000    33871648\n",
      "                    2010    37253956\n",
      "        New York    2000    18976457\n",
      "                    2010    19378102\n",
      "Hierarchical Indexing | 129...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 146, 'page_label': '129', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5086}\n",
      "\n",
      "--- Chunk 5087 ---\n",
      "Content:\n",
      "Texas       2000    20851820\n",
      "                    2010    25145561\n",
      "        dtype: int64\n",
      "Here the first two columns of the Series representation show the multiple index val‐\n",
      "ues, while the third column shows the data. Notice that some entries are missing in\n",
      "the first column: in this multi-index representation, any blank entry indicates the\n",
      "same value as the line above it.\n",
      "Now to access all data for which the second index is 2010, we can simply use the Pan‐\n",
      "das slicing notation:\n",
      "In[7]: pop[:, 2010]\n",
      "Out[7]: California    37253956\n",
      "        New York      19378102\n",
      "        Texas         25145561\n",
      "        dtype: int64\n",
      "The result is a singly indexed array with just the keys we’re interested in. This syntax\n",
      "is much more convenient (and the operation is much more efficient!) than the home-\n",
      "spun tuple-based multi-indexing solution that we started with. We’ll now further dis‐\n",
      "cuss this sort of indexing operation on hierarchically indexed data.\n",
      "MultiIndex as extra dimension\n",
      "Y ou might notice something else here: we could easily have stored the same data\n",
      "using a simple DataFrame with index and column labels. In fact, Pandas is built with\n",
      "this equivalence in mind. The unstack() method will quickly convert a multiply-\n",
      "indexed Series into a conventionally indexed DataFrame:\n",
      "In[8]: pop_df = pop.unstack()\n",
      "       pop_df\n",
      "Out[8]:                 2000      2010\n",
      "        California  33871648  37253956\n",
      "        New York    18976457  19378102\n",
      "        Texas       20851820  25145561\n",
      "Naturally, the stack() method provides the opposite operation:\n",
      "In[9]: pop_df.stack()\n",
      "Out[9]:  California  2000    33871648\n",
      "                     2010    37253956\n",
      "         New York    2000    18976457\n",
      "                     2010    19378102\n",
      "         Texas       2000    20851820\n",
      "                     2010    25145561\n",
      "         dtype: int64\n",
      "Seeing this, you might wonder why would we would bother with hierarchical index‐\n",
      "ing at all. The reason is simple: just as we were able to use multi-indexing to represent\n",
      "130 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 147, 'page_label': '130', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5087}\n",
      "\n",
      "--- Chunk 5088 ---\n",
      "Content:\n",
      "two-dimensional data within a one-dimensional Series, we can also use it to repre‐\n",
      "sent data of three or more dimensions in a Series or DataFrame. Each extra level in a\n",
      "multi-index represents an extra dimension of data; taking advantage of this property\n",
      "gives us much more flexibility in the types of data we can represent. Concretely, we\n",
      "might want to add another column of demographic data for each state at each year\n",
      "(say, population under 18); with a MultiIndex this is as easy as adding another col‐\n",
      "umn to the DataFrame:\n",
      "In[10]: pop_df = pd.DataFrame({'total': pop,\n",
      "                               'under18': [9267089, 9284094,\n",
      "                                           4687374, 4318033,\n",
      "                                           5906301, 6879014]})\n",
      "        pop_df\n",
      "Out[10]:                     total  under18\n",
      "         California 2000  33871648  9267089\n",
      "                    2010  37253956  9284094\n",
      "         New York   2000  18976457  4687374\n",
      "                    2010  19378102  4318033\n",
      "         Texas      2000  20851820  5906301\n",
      "                    2010  25145561  6879014\n",
      "In addition, all the ufuncs and other functionality discussed in “Operating on Data in\n",
      "Pandas” on page 115 work with hierarchical indices as well. Here we compute the\n",
      "fraction of people under 18 by year, given the above data:\n",
      "In[11]: f_u18 = pop_df['under18'] / pop_df['total']\n",
      "        f_u18.unstack()\n",
      "Out[11]:                 2000      2010\n",
      "         California  0.273594  0.249211\n",
      "         New York    0.247010  0.222831\n",
      "         Texas       0.283251  0.273568\n",
      "This allows us to easily and quickly manipulate and explore even high-dimensional\n",
      "data.\n",
      "Methods of MultiIndex Creation\n",
      "The most straightforward way to construct a multiply indexed Series or DataFrame\n",
      "is to simply pass a list of two or more index arrays to the constructor. For example:...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 148, 'page_label': '131', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5088}\n",
      "\n",
      "--- Chunk 5089 ---\n",
      "Content:\n",
      "In[12]: df = pd.DataFrame(np.random.rand(4, 2),\n",
      "                          index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\n",
      "                          columns=['data1', 'data2'])\n",
      "        df\n",
      "Out[12]:         data1     data2\n",
      "         a 1  0.554233  0.356072\n",
      "           2  0.925244  0.219474\n",
      "         b 1  0.441759  0.610054\n",
      "           2  0.171495  0.886688\n",
      "Hierarchical Indexing | 131...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 148, 'page_label': '131', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5089}\n",
      "\n",
      "--- Chunk 5090 ---\n",
      "Content:\n",
      "The work of creating the MultiIndex is done in the background.\n",
      "Similarly, if you pass a dictionary with appropriate tuples as keys, Pandas will auto‐\n",
      "matically recognize this and use a MultiIndex by default:\n",
      "In[13]: data = {('California', 2000): 33871648,\n",
      "                ('California', 2010): 37253956,\n",
      "                ('Texas', 2000): 20851820,\n",
      "                ('Texas', 2010): 25145561,\n",
      "                ('New York', 2000): 18976457,\n",
      "                ('New York', 2010): 19378102}\n",
      "        pd.Series(data)\n",
      "Out[13]: California  2000    33871648\n",
      "                     2010    37253956\n",
      "         New York    2000    18976457\n",
      "                     2010    19378102\n",
      "         Texas       2000    20851820\n",
      "                     2010    25145561\n",
      "         dtype: int64\n",
      "Nevertheless, it is sometimes useful to explicitly create a MultiIndex; we’ll see a cou‐\n",
      "ple of these methods here.\n",
      "Explicit MultiIndex constructors\n",
      "For more flexibility in how the index is constructed, you can instead use the class\n",
      "method constructors available in the pd.MultiIndex. For example, as we did before,\n",
      "you can construct the MultiIndex from a simple list of arrays, giving the index values\n",
      "within each level:\n",
      "In[14]: pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]])\n",
      "Out[14]: MultiIndex(levels=[['a', 'b'], [1, 2]],\n",
      "                    labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n",
      "Y ou can construct it from a list of tuples, giving the multiple index values of each\n",
      "point:\n",
      "In[15]: pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])\n",
      "Out[15]: MultiIndex(levels=[['a', 'b'], [1, 2]],\n",
      "                    labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n",
      "Y ou can even construct it from a Cartesian product of single indices:...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 149, 'page_label': '132', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5090}\n",
      "\n",
      "--- Chunk 5091 ---\n",
      "Content:\n",
      "In[15]: pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])\n",
      "Out[15]: MultiIndex(levels=[['a', 'b'], [1, 2]],\n",
      "                    labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n",
      "Y ou can even construct it from a Cartesian product of single indices:\n",
      "In[16]: pd.MultiIndex.from_product([['a', 'b'], [1, 2]])\n",
      "Out[16]: MultiIndex(levels=[['a', 'b'], [1, 2]],\n",
      "                    labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n",
      "Similarly, you can construct the MultiIndex directly using its internal encoding by\n",
      "passing levels (a list of lists containing available index values for each level) and\n",
      "labels (a list of lists that reference these labels):\n",
      "132 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 149, 'page_label': '132', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5091}\n",
      "\n",
      "--- Chunk 5092 ---\n",
      "Content:\n",
      "In[17]: pd.MultiIndex(levels=[['a', 'b'], [1, 2]],\n",
      "                       labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n",
      "Out[17]: MultiIndex(levels=[['a', 'b'], [1, 2]],\n",
      "                    labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n",
      "Y ou can pass any of these objects as the index argument when creating a Series or\n",
      "DataFrame, or to the reindex method of an existing Series or DataFrame.\n",
      "MultiIndex level names\n",
      "Sometimes it is convenient to name the levels of the MultiIndex. Y ou can accomplish\n",
      "this by passing the names argument to any of the above MultiIndex constructors, or\n",
      "by setting the names attribute of the index after the fact:\n",
      "In[18]: pop.index.names = ['state', 'year']\n",
      "        pop\n",
      "Out[18]: state       year\n",
      "         California  2000    33871648\n",
      "                     2010    37253956\n",
      "         New York    2000    18976457\n",
      "                     2010    19378102\n",
      "         Texas       2000    20851820\n",
      "                     2010    25145561\n",
      "         dtype: int64\n",
      "With more involved datasets, this can be a useful way to keep track of the meaning of\n",
      "various index values.\n",
      "MultiIndex for columns\n",
      "In a DataFrame, the rows and columns are completely symmetric, and just as the rows\n",
      "can have multiple levels of indices, the columns can have multiple levels as well. Con‐\n",
      "sider the following, which is a mock-up of some (somewhat realistic) medical data:\n",
      "In[19]:\n",
      "# hierarchical indices and columns\n",
      "index = pd.MultiIndex.from_product([[2013, 2014], [1, 2]],\n",
      "                                   names=['year', 'visit'])\n",
      "columns = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'], ['HR', 'Temp']],\n",
      "                                     names=['subject', 'type'])\n",
      "# mock some data\n",
      "data = np.round(np.random.randn(4, 6), 1)\n",
      "data[:, ::2] *= 10\n",
      "data += 37\n",
      "# create the DataFrame\n",
      "health_data = pd.DataFrame(data, index=index, columns=columns)\n",
      "health_data\n",
      "Hierarchical Indexing | 133...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 150, 'page_label': '133', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5092}\n",
      "\n",
      "--- Chunk 5093 ---\n",
      "Content:\n",
      "Out[19]: subject      Bob       Guido         Sue\n",
      "         type          HR  Temp    HR  Temp    HR  Temp\n",
      "         year visit\n",
      "         2013 1      31.0  38.7  32.0  36.7  35.0  37.2\n",
      "              2      44.0  37.7  50.0  35.0  29.0  36.7\n",
      "         2014 1      30.0  37.4  39.0  37.8  61.0  36.9\n",
      "              2      47.0  37.8  48.0  37.3  51.0  36.5\n",
      "Here we see where the multi-indexing for both rows and columns can come in very\n",
      "handy. This is fundamentally four-dimensional data, where the dimensions are the\n",
      "subject, the measurement type, the year, and the visit number. With this in place we\n",
      "can, for example, index the top-level column by the person’s name and get a full Data\n",
      "Frame containing just that person’s information:\n",
      "In[20]: health_data['Guido']\n",
      "Out[20]: type          HR  Temp\n",
      "         year visit\n",
      "         2013 1      32.0  36.7\n",
      "              2      50.0  35.0\n",
      "         2014 1      39.0  37.8\n",
      "              2      48.0  37.3\n",
      "For complicated records containing multiple labeled measurements across multiple\n",
      "times for many subjects (people, countries, cities, etc.), use of hierarchical rows and\n",
      "columns can be extremely convenient!\n",
      "Indexing and Slicing a MultiIndex\n",
      "Indexing and slicing on a MultiIndex is designed to be intuitive, and it helps if you\n",
      "think about the indices as added dimensions. We’ll first look at indexing multiply\n",
      "indexed Series, and then multiply indexed DataFrames.\n",
      "Multiply indexed Series\n",
      "Consider the multiply indexed Series of state populations we saw earlier:\n",
      "In[21]: pop\n",
      "Out[21]: state       year\n",
      "         California  2000    33871648\n",
      "                     2010    37253956\n",
      "         New York    2000    18976457\n",
      "                     2010    19378102\n",
      "         Texas       2000    20851820\n",
      "                     2010    25145561\n",
      "         dtype: int64\n",
      "We can access single elements by indexing with multiple terms:...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 151, 'page_label': '134', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5093}\n",
      "\n",
      "--- Chunk 5094 ---\n",
      "Content:\n",
      "In[22]: pop['California', 2000]\n",
      "Out[22]: 33871648\n",
      "134 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 151, 'page_label': '134', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5094}\n",
      "\n",
      "--- Chunk 5095 ---\n",
      "Content:\n",
      "The MultiIndex also supports partial indexing, or indexing just one of the levels in\n",
      "the index. The result is another Series, with the lower-level indices maintained:\n",
      "In[23]: pop['California']\n",
      "Out[23]: year\n",
      "         2000    33871648\n",
      "         2010    37253956\n",
      "         dtype: int64\n",
      "Partial slicing is available as well, as long as the MultiIndex is sorted (see discussion\n",
      "in “Sorted and unsorted indices” on page 137):\n",
      "In[24]: pop.loc['California':'New York']\n",
      "Out[24]: state       year\n",
      "         California  2000    33871648\n",
      "                     2010    37253956\n",
      "         New York    2000    18976457\n",
      "                     2010    19378102\n",
      "         dtype: int64\n",
      "With sorted indices, we can perform partial indexing on lower levels by passing an\n",
      "empty slice in the first index:\n",
      "In[25]: pop[:, 2000]\n",
      "Out[25]: state\n",
      "         California    33871648\n",
      "         New York      18976457\n",
      "         Texas         20851820\n",
      "         dtype: int64\n",
      "Other types of indexing and selection (discussed in “Data Indexing and Selection” on\n",
      "page 107) work as well; for example, selection based on Boolean masks:\n",
      "In[26]: pop[pop > 22000000]\n",
      "Out[26]: state       year\n",
      "         California  2000    33871648\n",
      "                     2010    37253956\n",
      "         Texas       2010    25145561\n",
      "         dtype: int64\n",
      "Selection based on fancy indexing also works:\n",
      "In[27]: pop[['California', 'Texas']]\n",
      "Out[27]: state       year\n",
      "         California  2000    33871648\n",
      "                     2010    37253956\n",
      "         Texas       2000    20851820\n",
      "                     2010    25145561\n",
      "         dtype: int64\n",
      "Hierarchical Indexing | 135...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 152, 'page_label': '135', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5095}\n",
      "\n",
      "--- Chunk 5096 ---\n",
      "Content:\n",
      "Multiply indexed DataFrames\n",
      "A multiply indexed DataFrame behaves in a similar manner. Consider our toy medi‐\n",
      "cal DataFrame from before:\n",
      "In[28]: health_data\n",
      "Out[28]: subject      Bob       Guido         Sue\n",
      "         type          HR  Temp    HR  Temp    HR  Temp\n",
      "         year visit\n",
      "         2013 1      31.0  38.7  32.0  36.7  35.0  37.2\n",
      "              2      44.0  37.7  50.0  35.0  29.0  36.7\n",
      "         2014 1      30.0  37.4  39.0  37.8  61.0  36.9\n",
      "              2      47.0  37.8  48.0  37.3  51.0  36.5\n",
      "Remember that columns are primary in a DataFrame, and the syntax used for multi‐\n",
      "ply indexed Series applies to the columns. For example, we can recover Guido’s heart\n",
      "rate data with a simple operation:\n",
      "In[29]: health_data['Guido', 'HR']\n",
      "Out[29]: year  visit\n",
      "         2013  1        32.0\n",
      "               2        50.0\n",
      "         2014  1        39.0\n",
      "               2        48.0\n",
      "         Name: (Guido, HR), dtype: float64\n",
      "Also, as with the single-index case, we can use the loc, iloc, and ix indexers intro‐\n",
      "duced in “Data Indexing and Selection” on page 107. For example:\n",
      "In[30]: health_data.iloc[:2, :2]\n",
      "Out[30]: subject      Bob\n",
      "         type          HR  Temp\n",
      "         year visit\n",
      "         2013 1      31.0  38.7\n",
      "              2      44.0  37.7\n",
      "These indexers provide an array-like view of the underlying two-dimensional data,\n",
      "but each individual index in loc or iloc can be passed a tuple of multiple indices. For\n",
      "example:...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 153, 'page_label': '136', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5096}\n",
      "\n",
      "--- Chunk 5097 ---\n",
      "Content:\n",
      "In[31]: health_data.loc[:, ('Bob', 'HR')]\n",
      "Out[31]: year  visit\n",
      "         2013  1        31.0\n",
      "               2        44.0\n",
      "         2014  1        30.0\n",
      "               2        47.0\n",
      "         Name: (Bob, HR), dtype: float64\n",
      "Working with slices within these index tuples is not especially convenient; trying to\n",
      "create a slice within a tuple will lead to a syntax error:\n",
      "136 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 153, 'page_label': '136', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5097}\n",
      "\n",
      "--- Chunk 5098 ---\n",
      "Content:\n",
      "In[32]: health_data.loc[(:, 1), (:, 'HR')]\n",
      "  File \"<ipython-input-32-8e3cc151e316>\", line 1\n",
      "    health_data.loc[(:, 1), (:, 'HR')]\n",
      "                     ^\n",
      "SyntaxError: invalid syntax\n",
      "Y ou could get around this by building the desired slice explicitly using Python’s built-\n",
      "in slice() function, but a better way in this context is to use an IndexSlice object,\n",
      "which Pandas provides for precisely this situation. For example:\n",
      "In[33]: idx = pd.IndexSlice\n",
      "        health_data.loc[idx[:, 1], idx[:, 'HR']]\n",
      "Out[33]: subject      Bob Guido   Sue\n",
      "         type          HR    HR    HR\n",
      "         year visit\n",
      "         2013 1      31.0  32.0  35.0\n",
      "         2014 1      30.0  39.0  61.0\n",
      "There are so many ways to interact with data in multiply indexed Series and Data\n",
      "Frames, and as with many tools in this book the best way to become familiar with\n",
      "them is to try them out!\n",
      "Rearranging Multi-Indices\n",
      "One of the keys to working with multiply indexed data is knowing how to effectively\n",
      "transform the data. There are a number of operations that will preserve all the infor‐\n",
      "mation in the dataset, but rearrange it for the purposes of various computations. We\n",
      "saw a brief example of this in the stack() and unstack() methods, but there are\n",
      "many more ways to finely control the rearrangement of data between hierarchical\n",
      "indices and columns, and we’ll explore them here.\n",
      "Sorted and unsorted indices\n",
      "Earlier, we briefly mentioned a caveat, but we should emphasize it more here. Many of\n",
      "the MultiIndex slicing operations will fail if the index is not sorted.  Let’s take a look at\n",
      "this here.\n",
      "We’ll start by creating some simple multiply indexed data where the indices are not\n",
      "lexographically sorted:\n",
      "In[34]: index = pd.MultiIndex.from_product([['a', 'c', 'b'], [1, 2]])\n",
      "        data = pd.Series(np.random.rand(6), index=index)\n",
      "        data.index.names = ['char', 'int']\n",
      "        data\n",
      "Out[34]: char  int\n",
      "         a     1      0.003001\n",
      "               2      0.164974\n",
      "         c     1      0.741650\n",
      "Hierarchical Indexing | 137...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 154, 'page_label': '137', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5098}\n",
      "\n",
      "--- Chunk 5099 ---\n",
      "Content:\n",
      "2      0.569264\n",
      "         b     1      0.001693\n",
      "               2      0.526226\n",
      "         dtype: float64\n",
      "If we try to take a partial slice of this index, it will result in an error:\n",
      "In[35]: try:\n",
      "            data['a':'b']\n",
      "        except KeyError as e:\n",
      "            print(type(e))\n",
      "            print(e)\n",
      "<class 'KeyError'>\n",
      "'Key length (1) was greater than MultiIndex lexsort depth (0)'\n",
      "Although it is not entirely clear from the error message, this is the result of the Multi\n",
      "Index not being sorted. For various reasons, partial slices and other similar opera‐\n",
      "tions require the levels in the MultiIndex to be in sorted (i.e., lexographical) order.\n",
      "Pandas provides a number of convenience routines to perform this type of sorting;\n",
      "examples are the sort_index() and sortlevel() methods of the DataFrame. We’ll\n",
      "use the simplest, sort_index(), here:\n",
      "In[36]: data = data.sort_index()\n",
      "        data\n",
      "Out[36]: char  int\n",
      "         a     1      0.003001\n",
      "               2      0.164974\n",
      "         b     1      0.001693\n",
      "               2      0.526226\n",
      "         c     1      0.741650\n",
      "               2      0.569264\n",
      "         dtype: float64\n",
      "With the index sorted in this way, partial slicing will work as expected:\n",
      "In[37]: data['a':'b']\n",
      "Out[37]: char  int\n",
      "         a     1      0.003001\n",
      "               2      0.164974\n",
      "         b     1      0.001693\n",
      "               2      0.526226\n",
      "         dtype: float64\n",
      "Stacking and unstacking indices\n",
      "As we saw briefly before, it is possible to convert a dataset from a stacked multi-index\n",
      "to a simple two-dimensional representation, optionally specifying the level to use:\n",
      "138 | Chapter 3: Data Manipulation with Pandas...\n",
      "Metadata:\n",
      "{'producer': 'Antenna House PDF Output Library 6.2.609 (Linux64)', 'creator': 'AH CSS Formatter V6.2 MR4 for Linux64 : 6.2.6.18551 (2014/09/24 15:00JST)', 'creationdate': '2016-11-16T18:58:13+00:00', 'author': 'Jake VanderPlas', 'moddate': '2016-11-17T09:24:46-05:00', 'title': 'Python Data Science Handbook', 'trapped': '/False', 'ebx_publisher': \"/O'Reilly Media\", 'source': '/home/bengtegard/github/data-science-rag/data/python/Python Data Science Handbook - Jake VanderPlas.pdf', 'total_pages': 548, 'page': 155, 'page_label': '138', 'rel_path': 'python/Python Data Science Handbook - Jake VanderPlas.pdf', 'category': 'python', 'contains_code': True, 'language': 'python', 'content_category': 'python', 'chunk_id': 5099}\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks[4000:5100], start=4000):  \n",
    "    if chunk.metadata.get(\"category\") == \"python\":\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        print(f\"Content:\\n{chunk.page_content}...\")  \n",
    "        print(f\"Metadata:\\n{chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "632d5890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1000 ---\n",
      "Content:\n",
      "14.3. Logistic Regression by Stochastic Gradient Descent 59\n",
      "14.3.3 Repeat the Process\n",
      "We can repeat this process and update the model for each training instance in the dataset. A\n",
      "single iteration through the training dataset is called an epoch. It is common to repeat the\n",
      "stochastic gradient descent procedure for a ﬁxed number of epochs. At the end of epoch you\n",
      "can calculate error values for the model. Because this is a classiﬁcation problem, it would be\n",
      "nice to get an idea of how accurate the model is at each iteration. The graph below show a plot\n",
      "of accuracy of the model over 10 epochs.\n",
      "Figure 14.2: Logistic Regression with Gradient Descent Accuracy versus Iteration.\n",
      "You can see that the model very quickly achieves 100% accuracy on the training dataset.\n",
      "The coeﬃcients calculated after 10 epochs of stochastic gradient descent are:\n",
      "B0 = −0.406605464\n",
      "B1 = 0.852573316\n",
      "B2 = −1.104746259\n",
      "(14.9)\n",
      "14.3.4 Make Predictions\n",
      "Now that we have trained the model, we can use it to make predictions. We can make predictions\n",
      "on the training dataset, but this could just as easily be new data. Using the coeﬃcients above\n",
      "learned after 10 epochs, we can calculate output values for each training instance:\n",
      "X1 X2 Prediction\n",
      "2.7810836 2.550537003 0.298756986\n",
      "1.465489372 2.362125076 0.145951056\n",
      "3.396561688 4.400293529 0.085333265\n",
      "1.38807019 1.850220317 0.219737314\n",
      "3.06407232 3.005305973 0.247059\n",
      "7.627531214 2.759262235 0.954702135\n",
      "5.332441248 2.088626775 0.862034191\n",
      "6.922596716 1.77106367 0.971772905...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 68, 'page_label': '69', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1000}\n",
      "\n",
      "--- Chunk 1001 ---\n",
      "Content:\n",
      "14.4. Summary 60\n",
      "8.675418651 -0.242068655 0.999295452\n",
      "7.673756466 3.508563011 0.905489323\n",
      "Listing 14.2: Raw Logistic Regression Predictions.\n",
      "These are the probabilities of each instance belonging to Y = 0. We can convert these into\n",
      "crisp class values using:\n",
      "prediction= IF (output< 0.5) Then 0 Else 1 (14.10)\n",
      "With this simple procedure we can convert all of the outputs to class values:\n",
      "Prediction Crisp\n",
      "0.298756986 0\n",
      "0.145951056 0\n",
      "0.085333265 0\n",
      "0.219737314 0\n",
      "0.247059 0\n",
      "0.954702135 1\n",
      "0.862034191 1\n",
      "0.971772905 1\n",
      "0.999295452 1\n",
      "0.905489323 1\n",
      "Listing 14.3: Crisp Logistic Regression Predictions.\n",
      "Finally, we can calculate the accuracy for the model on the training dataset:\n",
      "accuracy = CorrectPredictions\n",
      "TotalPredictions ×100\n",
      "accuracy = 10\n",
      "10 ×100\n",
      "accuracy = 100%\n",
      "(14.11)\n",
      "14.4 Summary\n",
      "In this chapter you discovered how you can implement logistic regression from scratch, step-by-\n",
      "step. You learned:\n",
      " How to calculate the logistic function.\n",
      " How to learn the coeﬃcients for a logistic regression model using stochastic gradient\n",
      "descent.\n",
      " How to make predictions using a logistic regression model.\n",
      "You now know how to implement logistic regression from scratch using stochastic gradient\n",
      "descent. In the next chapter you will discover the linear discriminant analysis algorithm for\n",
      "classiﬁcation....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 69, 'page_label': '70', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1001}\n",
      "\n",
      "--- Chunk 1002 ---\n",
      "Content:\n",
      "Chapter 15\n",
      "Linear Discriminant Analysis\n",
      "Logistic regression is a classiﬁcation algorithm traditionally limited to only two-class classiﬁcation\n",
      "problems. If you have more than two classes then the Linear Discriminant Analysis is the\n",
      "preferred linear classiﬁcation technique. In this chapter you will discover the Linear Discriminant\n",
      "Analysis (LDA) algorithm for classiﬁcation predictive modeling problems. After reading this\n",
      "chapter you will know.\n",
      " The limitations of logistic regression and the need for linear discriminant analysis.\n",
      " The representation of the model that is learned from data and can be saved to ﬁle.\n",
      " How the model is estimated from your data.\n",
      " How to make predictions from a learned LDA model.\n",
      " How to prepare your data to get the most from the LDA model.\n",
      "Let’s get started.\n",
      "15.1 Limitations of Logistic Regression\n",
      "Logistic regression is a simple and powerful linear classiﬁcation algorithm. It also has limitations\n",
      "that suggest at the need for alternate linear classiﬁcation algorithms.\n",
      " Two-Class Problems. Logistic regression is intended for two-class or binary classiﬁcation\n",
      "problems. It can be extended for multiclass classiﬁcation, but is rarely used for this purpose.\n",
      " Unstable With Well Separated Classes . Logistic regression can become unstable\n",
      "when the classes are well separated.\n",
      " Unstable With Few Examples. Logistic regression can become unstable when there\n",
      "are few examples from which to estimate the parameters.\n",
      "Linear discriminant analysis does address each of these points and is the go-to linear method\n",
      "for multiclass classiﬁcation problems. Even with binary-classiﬁcation problems, it is a good idea\n",
      "to try both logistic regression and linear discriminant analysis.\n",
      "61...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 70, 'page_label': '71', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1002}\n",
      "\n",
      "--- Chunk 1003 ---\n",
      "Content:\n",
      "15.2. Representation of LDA Models 62\n",
      "15.2 Representation of LDA Models\n",
      "The representation of LDA is pretty straight forward. It consists of statistical properties of your\n",
      "data, calculated for each class. For a single input variable ( x) this is the mean and the variance\n",
      "of the variable for each class.\n",
      "For multiple variables, this is same properties calculated over the multivariate Gaussian,\n",
      "namely the means and the covariance matrix (this is a multi-dimensional generalization of\n",
      "variance). These statistical properties are estimated from your data and plug into the LDA\n",
      "equation to make predictions. These are the model values that you would save to ﬁle for your\n",
      "model. Let’s look at how these parameters are estimated.\n",
      "15.3 Learning LDA Models\n",
      "LDA makes some simplifying assumptions about your data\n",
      " That your data is Gaussian, that each variable is is shaped like a bell curve when plotted.\n",
      " That each attribute has the same variance, that values of each variable vary around the\n",
      "mean by the same amount on average.\n",
      "With these assumptions, the LDA model estimates the mean and variance from your data\n",
      "for each class. It is easy to think about this in the univariate (single input variable) case with\n",
      "two classes. The mean ( mean) value of each input ( x) for each class ( k) can be estimated in\n",
      "the normal way by dividing the sum of values by the total number of values.\n",
      "meank = 1\n",
      "nk\n",
      "×\n",
      "n∑\n",
      "i=1\n",
      "xi (15.1)\n",
      "Where meank is the mean value of xfor the class k, nk is the number of instances with class\n",
      "k. The variance is calculated across all classes as the average squared diﬀerence of each value\n",
      "from the mean.\n",
      "sigma2 = 1\n",
      "n−K ×\n",
      "n∑\n",
      "i=1\n",
      "(xi −meank)2 (15.2)\n",
      "Where sigma2 is the variance across all inputs ( x), n is the number of instances, K is the\n",
      "number of classes and meank is the mean of x for the class to which xi belongs. Put another\n",
      "way, we calculate the squared diﬀerence of each value from the mean within the class groups\n",
      "but average these diﬀerences across all class groups. Remember that that when we talk about\n",
      "variance (variance2, sigma2 or σ2) that the units are squared units, not that we need to square\n",
      "the variance value.\n",
      "15.4 Making Predictions with LDA\n",
      "LDA makes predictions by estimating the probability that a new set of inputs belongs to each\n",
      "class. The class that gets the highest probability is the output class and a prediction is made.\n",
      "The model uses Bayes Theorem to estimate the probabilities. Brieﬂy Bayes Theorem can be...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 71, 'page_label': '72', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1003}\n",
      "\n",
      "--- Chunk 1004 ---\n",
      "Content:\n",
      "15.5. Preparing Data For LDA 63\n",
      "used to estimate the probability of the output class ( k) given the input (x) using the probability\n",
      "of each class and the probability of the data belonging to each class:\n",
      "P(Y = k|X = x) = P(k) ×P(x|k)∑K\n",
      "l=1(P(l) ×P(x|l)\n",
      "(15.3)\n",
      "Where:\n",
      " P(Y = k|X = x) is the probability of the class Y = k given the input data x.\n",
      " P(k) is the base probability of a given class k we are considering (Y = k), e.g. the ratio\n",
      "of instances with this class in the training dataset.\n",
      " P(x|k) is the estimated probability of x belonging to the class k.\n",
      " The denominator normalizes across for each class l, e.g. the probability of the class P(l)\n",
      "and the probability of the input given the class P(x|l).\n",
      "A Gaussian distribution function is can be used to estimate P(x|k). Plugging the Gaussian\n",
      "into the above equation and simplifying we end up with the equation below. It is no longer a\n",
      "probability as we discard some terms. Instead it is called a discriminate function for class k. It\n",
      "is calculated for each class k and the class that has the largest discriminant value will make the\n",
      "output classiﬁcation (Y = k):\n",
      "Dk(x) = x×meank\n",
      "sigma2 − meank2\n",
      "2 ×sigma2 + ln(P(k)) (15.4)\n",
      "Dk(x) is the discriminate function for class k given input x, the meank, sigma2 and P(k)\n",
      "are all estimated from your data. The ln() function is the natural logarithm.\n",
      "15.5 Preparing Data For LDA\n",
      "This section lists some suggestions you may consider when preparing your data for use with\n",
      "LDA.\n",
      " Classiﬁcation Problems . This might go without saying, but LDA is intended for\n",
      "classiﬁcation problems where the output variable is categorical. LDA supports both binary\n",
      "and multiclass classiﬁcation.\n",
      " Gaussian Distribution. The standard implementation of the model assumes a Gaussian\n",
      "distribution of the input variables. Consider reviewing the univariate distributions of each\n",
      "attribute and using transforms to make them more Gaussian-looking (e.g. log and root\n",
      "for exponential distributions and Box-Cox for skewed distributions).\n",
      " Remove Outliers. Consider removing outliers from your data. These can skew the basic\n",
      "statistics used to separate classes in LDA such the mean and the standard deviation.\n",
      " Same Variance. LDA assumes that each input variable has the same variance. It almost\n",
      "always a good idea to standardize your data before using LDA so that it has a mean of 0\n",
      "and a standard deviation of 1....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 72, 'page_label': '73', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1004}\n",
      "\n",
      "--- Chunk 1005 ---\n",
      "Content:\n",
      "15.6. Extensions to LDA 64\n",
      "15.6 Extensions to LDA\n",
      "Linear Discriminant Analysis is a simple and eﬀective method for classiﬁcation. Because it is\n",
      "simple and so well understood, there are many extensions and variations to the method. Some\n",
      "popular extensions include:\n",
      " Quadratic Discriminant Analysis: Each class uses its own estimate of variance (or covari-\n",
      "ance when there are multiple input variables).\n",
      " Flexible Discriminant Analysis: Where nonlinear combination of inputs is used such as\n",
      "splines.\n",
      " Regularized Discriminant Analysis: Introduces regularization into the estimate of the\n",
      "variance (or covariance), moderating the inﬂuence of diﬀerent variables on LDA.\n",
      "The original development was called the Linear Discriminant or Fisher’s Discriminant\n",
      "Analysis. The multiclass version was referred to Multiple Discriminant Analysis. These are all\n",
      "simply referred to as Linear Discriminant Analysis now.\n",
      "15.7 Summary\n",
      "In this chapter you discovered Linear Discriminant Analysis for classiﬁcation predictive modeling\n",
      "problems. You learned:\n",
      " The model representation for LDA and what is actually distinct about a learned model.\n",
      " How the parameters of the LDA model can be estimated from training data.\n",
      " How the model can be used to make predictions on new data.\n",
      " How to prepare your data to get the most from the method.\n",
      "You now know about the linear discriminant analysis algorithm for classiﬁcation. In the next\n",
      "chapter you will discover how to implement the LDA algorithm from scratch for classiﬁcation....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 73, 'page_label': '74', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1005}\n",
      "\n",
      "--- Chunk 1006 ---\n",
      "Content:\n",
      "Chapter 16\n",
      "Linear Discriminant Analysis Tutorial\n",
      "Linear Discriminant Analysis is a linear method for classiﬁcation predictive modeling problems.\n",
      "In this chapter you will discover how Linear Discriminant Analysis (LDA) works by implementing\n",
      "the algorithm step-by-step from scratch for a simple dataset. After reading this chapter you\n",
      "will know:\n",
      " The assumptions made by LDA about your training data.\n",
      " How to calculate statistics required by the LDA model from your data.\n",
      " How to make predictions using the learned LDA model.\n",
      "Let’s get started.\n",
      "16.1 Tutorial Overview\n",
      "We are going to step through how to calculate an LDA model for simple dataset with one input\n",
      "and one output variable. This is the simplest case for LDA. This tutorial will to cover:\n",
      "1. Dataset: Introduce the dataset that we are going to model. We will use the same dataset\n",
      "as the training and the test dataset.\n",
      "2. Learning the Model : How to learn the LDA model from the dataset including all of\n",
      "the statistics needed to make predictions.\n",
      "3. Making Predictions: How to use the learned model to make predictions for each instance\n",
      "in the training dataset.\n",
      "16.2 Tutorial Dataset\n",
      "LDA makes some assumptions about your data:\n",
      " The input variables have a Gaussian (bell curve) distribution.\n",
      " The variance (average squared diﬀerence from the mean) calculated for each input variables\n",
      "by class grouping is the same.\n",
      "65...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 74, 'page_label': '75', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1006}\n",
      "\n",
      "--- Chunk 1007 ---\n",
      "Content:\n",
      "16.2. Tutorial Dataset 66\n",
      " That the mix of classes in your training set is representative of the problem.\n",
      "Below is a contrived simple two-dimensional dataset containing the input variable X and the\n",
      "output class variable Y. All values for X were drawn from a Gaussian distribution and the class\n",
      "variable Y has the value 0 or 1. The instances in the two classes were separated to make the\n",
      "prediction problem simpler. All instances in class 0 were drawn from a Gaussian distribution\n",
      "with a mean of 5 and a standard deviation of 1. All instances in class 1 were drawn from a\n",
      "Gaussian distribution with a mean of 20 and a standard deviation of 1.\n",
      "The classes do not interact and should be separable with a linear model like LDA. It is also\n",
      "handy to know the actual statistical properties of the data because we can generate more test\n",
      "instances later to see how well LDA has learned the model. Below is the complete dataset.\n",
      "X Y\n",
      "4.667797637 0\n",
      "5.509198779 0\n",
      "4.702791608 0\n",
      "5.956706641 0\n",
      "5.738622413 0\n",
      "5.027283325 0\n",
      "4.805434058 0\n",
      "4.425689143 0\n",
      "5.009368635 0\n",
      "5.116718815 0\n",
      "6.370917709 0\n",
      "2.895041947 0\n",
      "4.666842365 0\n",
      "5.602154638 0\n",
      "4.902797978 0\n",
      "5.032652964 0\n",
      "4.083972925 0\n",
      "4.875524106 0\n",
      "4.732801047 0\n",
      "5.385993407 0\n",
      "20.74393514 1\n",
      "21.41752855 1\n",
      "20.57924186 1\n",
      "20.7386947 1\n",
      "19.44605384 1\n",
      "18.36360265 1\n",
      "19.90363232 1\n",
      "19.10870851 1\n",
      "18.18787593 1\n",
      "19.71767611 1\n",
      "19.09629027 1\n",
      "20.52741312 1\n",
      "20.63205608 1\n",
      "19.86218119 1\n",
      "21.34670569 1\n",
      "20.333906 1\n",
      "21.02714855 1\n",
      "18.27536089 1\n",
      "21.77371156 1\n",
      "20.65953546 1\n",
      "Listing 16.1: LDA Tutorial Data Set....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 75, 'page_label': '76', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1007}\n",
      "\n",
      "--- Chunk 1008 ---\n",
      "Content:\n",
      "16.3. Learning The Model 67\n",
      "Below is a plot of the dataset, showing the separation of the two classes.\n",
      "Figure 16.1: LDA Tutorial dataset.\n",
      "16.3 Learning The Model\n",
      "The LDA model requires the estimation of statistics from the training data:\n",
      "1. Mean of each input value for each class\n",
      "2. Probability of an instance belong to each class.\n",
      "3. Covariance for the input data for each class.\n",
      "16.3.1 Calculate the Class Means\n",
      "The mean can be calculated for each class using:\n",
      "mean(x) = 1\n",
      "n ×\n",
      "n∑\n",
      "i=1\n",
      "xi (16.1)\n",
      "Where x are the input values for a class and n is the total number of input values. You can\n",
      "use the AVERAGE() function in you spreadsheet. The mean value of x for each class is:\n",
      " Y = 0: 4.975415507\n",
      " Y = 1 1: 20.06447458...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 76, 'page_label': '77', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1008}\n",
      "\n",
      "--- Chunk 1009 ---\n",
      "Content:\n",
      "16.3. Learning The Model 68\n",
      "16.3.2 Calculate the Class Probabilities\n",
      "Next we need to estimate the probability that a given instance will belong to Y = 0 or Y = 1.\n",
      "This can be calculated as:\n",
      "P(y= 0) = count(y= 0)\n",
      "count(y= 0) + count(y= 1)\n",
      "P(y= 1) = count(y= 1)\n",
      "count(y= 0) + count(y= 1)\n",
      "(16.2)\n",
      "or\n",
      "P(y= 0) = 20\n",
      "20 + 20\n",
      "P(y= 1) = 20\n",
      "20 + 20\n",
      "(16.3)\n",
      "This is 0.5 for each class. We knew this already because we created the dataset, but it is a\n",
      "good idea to work through each step of the model learning process.\n",
      "16.3.3 Calculate the Variance\n",
      "Now we need to calculate the variance for the input variable for each class. You can understand\n",
      "the variance as the diﬀerence of each instance from the mean. The diﬀerence is squared so\n",
      "the variance is often written to include these units. It does not mean you need to square the\n",
      "variance value when using it. We can calculate the variance for our dataset in two steps:\n",
      "1. Calculate the squared diﬀerence for each input variable from the group mean.\n",
      "2. Calculate the mean of the squared diﬀerence.\n",
      "We can divide the dataset into two groups by the Y values 0 and 1. We can then calculate\n",
      "the diﬀerence for each input value X from the mean from that group. We can calculate the\n",
      "diﬀerence of each input value from the mean using:\n",
      "SquaredDifference = (x−meank)2 (16.4)\n",
      "Where meank is the mean value of x for the class k to which x belongs. We can sum those\n",
      "values which gives us (rounded to 4 places):\n",
      " Y = 0: 10.15823013\n",
      " Y = 1: 21.49316708\n",
      "Next we can calculate the variance as the average squared diﬀerence from the mean as:\n",
      "variance = 1\n",
      "count(x) −count(classes) ×\n",
      "n∑\n",
      "i=1\n",
      "SquaredDifference (xi)\n",
      "variance = 1\n",
      "20 −2 ×(10.15823013 + 21.49316708)\n",
      "variance = 0.832931506\n",
      "(16.5)...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 77, 'page_label': '78', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1009}\n",
      "\n",
      "--- Chunk 1010 ---\n",
      "Content:\n",
      "16.4. Making Predictions 69\n",
      "Notice that this is slightly diﬀerence from using the mean above as we need to subtract two\n",
      "degrees of freedom for the two class values or two groups we are using. The explanation for\n",
      "exactly why this is the case is a beyond this tutorial.\n",
      "16.4 Making Predictions\n",
      "We are now ready to make predictions. Predictions are made by calculating the discriminant\n",
      "function for each class and predicting the class with the largest value. The discriminant function\n",
      "for a class given an input ( x) is calculated using:\n",
      "discriminant(x) = x× mean\n",
      "variance − mean2\n",
      "2 ×variance + ln(probability) (16.6)\n",
      "Where xis the input value, mean, variance and probability are calculated above for the class\n",
      "we are discriminating. After calculating the discriminant value for each class, the class with\n",
      "the largest discriminant value is taken as the prediction. Let’s step through the calculation of\n",
      "the discriminate value of each class for the ﬁrst instance. The ﬁrst instance in the dataset is:\n",
      "X = 4.667797637 and Y = 0. The discriminant value for Y = 0 is calculated as follows:\n",
      "discriminant(Y = 0|x) = 4.667797637 ×4.975415507\n",
      "0.832931506 − 4.9754155072\n",
      "2 ×0.832931506 + ln(0.5)\n",
      "discriminant(Y = 0|x) = 12.3293558\n",
      "(16.7)\n",
      "We can also calculate the discriminant value for Y = 1:\n",
      "discriminant(Y = 1|x) = 4.667797637 ×20.08706292\n",
      "0.832931506 − 20.087062922\n",
      "2 ×0.832931506 + ln(0.5)\n",
      "discriminant(Y = 1|x) = −130.3349038\n",
      "(16.8)\n",
      "We can see that the discriminant value for Y = 0 (12.3293558) is larger than the discriminate\n",
      "value for Y = 1 (-130.3349038), therefore the model predicts Y = 0. Which we know is correct.\n",
      "You can proceed to calculate the Y values for each instance in the dataset, as follows:\n",
      "X Disc. Y=0 Disc. Y=1 Prediction\n",
      "4.667797637 12.3293558 -130.3349038 0\n",
      "5.509198779 17.35536365 -110.0435863 0\n",
      "4.702791608 12.53838805 -129.4909856 0\n",
      "5.956706641 20.02849769 -99.25144007 0\n",
      "5.738622413 18.72579795 -104.510782 0\n",
      "5.027283325 14.47670003 -121.6655095 0\n",
      "4.805434058 13.15151029 -127.0156496 0\n",
      "4.425689143 10.88315002 -136.1736175 0\n",
      "5.009368635 14.3696888 -122.0975421 0\n",
      "5.116718815 15.0109321 -119.5086739 0\n",
      "6.370917709 22.50273735 -89.26228283 0\n",
      "2.895041947 1.740014309 -173.0868646 0\n",
      "4.666842365 12.3236496 -130.3579413 0\n",
      "5.602154638 17.91062422 -107.8018531 0\n",
      "4.902797978 13.73310188 -124.6676111 0...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 78, 'page_label': '79', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1010}\n",
      "\n",
      "--- Chunk 1011 ---\n",
      "Content:\n",
      "16.5. Summary 70\n",
      "5.032652964 14.50877492 -121.5360148 0\n",
      "4.083972925 8.841949563 -144.4144814 0\n",
      "4.875524106 13.57018471 -125.3253507 0\n",
      "4.732801047 12.7176458 -128.7672748 0\n",
      "5.385993407 16.61941128 -113.0148198 0\n",
      "20.74393514 108.3582168 257.3589021 1\n",
      "21.41752855 112.3818455 273.603351 1\n",
      "20.57924186 107.3744415 253.3871419 1\n",
      "20.7386947 108.3269137 257.2325231 1\n",
      "19.44605384 100.60548 226.0590616 1\n",
      "18.36360265 94.1395889 199.954556 1\n",
      "19.90363232 103.3387697 237.0940718 1\n",
      "19.10870851 98.59038855 217.9236065 1\n",
      "18.18787593 93.08990662 195.7167121 1\n",
      "19.71767611 102.2279828 232.6095325 1\n",
      "19.09629027 98.5162097 217.6241269 1\n",
      "20.52741312 107.0648488 252.1372346 1\n",
      "20.63205608 107.6899208 254.6608151 1\n",
      "19.86218119 103.0911664 236.0944321 1\n",
      "21.34670569 111.9587938 271.8953795 1\n",
      "20.333906 105.9089574 247.4705967 1\n",
      "21.02714855 110.0499579 264.1889062 1\n",
      "18.27536089 93.61248743 197.8265085 1\n",
      "21.77371156 114.5094616 282.1930975 1\n",
      "20.65953546 107.8540656 255.3235107 1\n",
      "Listing 16.2: LDA Predictions for the dataset.\n",
      "If you compare the predictions to the dataset, you can see that LDA has achieved an accuracy\n",
      "of 100% (no errors). This is not surprising given that the dataset was contrived so that the\n",
      "groups for Y = 0 and Y = 1 were clearly separable.\n",
      "16.5 Summary\n",
      "In this chapter you discovered how the LDA algorithm works step-by-step with a simple worked\n",
      "example. You learned:\n",
      " How to calculate the statistics from your dataset required by the LDA model.\n",
      " How to use the LDA model to calculate a discriminant value for each class and make a\n",
      "prediction.\n",
      "You now know how to implement the linear discriminant analysis algorithm from scratch for\n",
      "classiﬁcation. This concludes your introduction to linear machine learning algorithms. In the\n",
      "next part you will discover nonlinear machine learning algorithms, starting with decision trees....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 79, 'page_label': '80', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1011}\n",
      "\n",
      "--- Chunk 1012 ---\n",
      "Content:\n",
      "Part IV\n",
      "Nonlinear Algorithms\n",
      "71...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 80, 'page_label': '81', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1012}\n",
      "\n",
      "--- Chunk 1013 ---\n",
      "Content:\n",
      "Chapter 17\n",
      "Classiﬁcation and Regression Trees\n",
      "Decision Trees are an important type of algorithm for predictive modeling machine learning.\n",
      "The classical decision tree algorithms have been around for decades and modern variations\n",
      "like random forest are among the most powerful techniques available. In this chapter you will\n",
      "discover the humble decision tree algorithm known by it’s more modern name CART which\n",
      "stands for Classiﬁcation And Regression Trees. After reading this chapter, you will know:\n",
      " The many names used to describe the CART algorithm for machine learning.\n",
      " The representation used by learned CART models that is actually stored on disk.\n",
      " How a CART model can be learned from training data.\n",
      " How a learned CART model can be used to make predictions on unseen data.\n",
      " Additional resources that you can use to learn more about CART and related algorithms.\n",
      "If you have taken an algorithms and data structures course, it might be hard to hold you\n",
      "back from implementing this simple and powerful algorithm. And from there, you’re a small\n",
      "step away from your own implementation of Random Forests. Let’s get started.\n",
      "17.1 Decision Trees\n",
      "Classiﬁcation and Regression Trees or CART for short is a term introduced by Leo Breiman\n",
      "to refer to Decision Tree algorithms that can be used for classiﬁcation or regression predictive\n",
      "modeling problems. Classically, this algorithm is referred to as decision trees, but on some\n",
      "platforms like R they are referred to by the more modern term CART. The CART algorithm\n",
      "provides a foundation for important algorithms like bagged decision trees, random forest and\n",
      "boosted decision trees.\n",
      "17.2 CART Model Representation\n",
      "The representation for the CART model is a binary tree. This is your binary tree from algorithms\n",
      "and data structures, nothing too fancy. Each node represents a single input variable ( x) and\n",
      "a split point on that variable (assuming the variable is numeric). The leaf nodes of the tree\n",
      "72...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 81, 'page_label': '82', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1013}\n",
      "\n",
      "--- Chunk 1014 ---\n",
      "Content:\n",
      "17.3. Making Predictions 73\n",
      "contain an output variable (y) which is used to make a prediction. Given a dataset with two\n",
      "inputs of height in centimeters and weight in kilograms the output of sex as male or female,\n",
      "below is a crude example of a binary decision tree (completely ﬁctitious for demonstration\n",
      "purposes only).\n",
      "Figure 17.1: Example Decision Tree.\n",
      "The tree can be stored to ﬁle as a graph or a set of rules. For example, below is the above\n",
      "decision tree as a set of rules.\n",
      "If Height > 180 cm Then Male\n",
      "If Height <= 180 cm AND Weight > 80 kg Then Male\n",
      "If Height <= 180 cm AND Weight <= 80 kg Then Female\n",
      "Make Predictions With CART Models\n",
      "Listing 17.1: Example of a Rule Representation of a Decision Tree.\n",
      "17.3 Making Predictions\n",
      "With the binary tree representation of the CART model described above, making predictions is\n",
      "relatively straightforward. Given a new input, the tree is traversed by evaluating the speciﬁc\n",
      "input started at the root node of the tree. A learned binary tree is actually a partitioning of the\n",
      "input space. You can think of each input variable as a dimension on an p-dimensional space.\n",
      "The decision tree split this up into rectangles (when p= 2 input variables) or hyper-rectangles\n",
      "with more inputs. New data is ﬁltered through the tree and lands in one of the rectangles and...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 82, 'page_label': '83', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1014}\n",
      "\n",
      "--- Chunk 1015 ---\n",
      "Content:\n",
      "17.4. Learn a CART Model From Data 74\n",
      "the output value for that rectangle is the prediction made by the model. This gives you some\n",
      "feeling for the type of decisions that a CART model is capable of making, e.g. boxy decision\n",
      "boundaries. For example, given the input of height= 160 cm and weight = 65 kg, we would\n",
      "traverse the above tree as follows:\n",
      "1. Height > 180 cm: No\n",
      "2. Weight > 80 kg: No\n",
      "3. Therefore: Female\n",
      "17.4 Learn a CART Model From Data\n",
      "Creating a binary decision tree is actually a process of dividing up the input space. A greedy\n",
      "approach is used to divide the space called recursive binary splitting. This is a numerical\n",
      "procedure where all the values are lined up and diﬀerent split points are tried and tested using\n",
      "a cost function. The split with the best cost (lowest cost because we minimize cost) is selected.\n",
      "All input variables and all possible split points are evaluated and chosen in a greedy manner\n",
      "(e.g. the very best split point is chosen each time). For regression predictive modeling problems\n",
      "the cost function that is minimized to choose spit points is the sum squared error across all\n",
      "training samples that fall within the rectangle:\n",
      "n∑\n",
      "i=1\n",
      "(yi −predictioni)2 (17.1)\n",
      "Where y is the output for the training sample and prediction is the predicted output for the\n",
      "rectangle. For classiﬁcation the Gini cost function is used which provides an indication of how\n",
      "pure the leaf nodes are (how mixed the training data assigned to each node is).\n",
      "G=\n",
      "n∑\n",
      "k=1\n",
      "pk ×(1 −pk) (17.2)\n",
      "Where G is the Gini cost over all classes, pk are the number of training instances with class\n",
      "k in the rectangle of interest. A node that has all classes of the same type (perfect class purity)\n",
      "will have G= 0, where as a Gthat has a 50-50 split of classes for a binary classiﬁcation problem\n",
      "(worst purity) will have a G= 0.5.\n",
      "17.4.1 Stopping Criterion\n",
      "The recursive binary splitting procedure described above needs to know when to stop splitting\n",
      "as it works its way down the tree with the training data. The most common stopping procedure\n",
      "is to use a minimum count on the number of training instances assigned to each leaf node. If\n",
      "the count is less than some minimum then the split is not accepted and the node is taken as a\n",
      "ﬁnal leaf node. The count of training members is tuned to the dataset, e.g. 5 or 10. It deﬁnes\n",
      "how speciﬁc to the training data the tree will be. Too speciﬁc (e.g. a count of 1) and the tree\n",
      "will overﬁt the training data and likely have poor performance on the test set....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 83, 'page_label': '84', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1015}\n",
      "\n",
      "--- Chunk 1016 ---\n",
      "Content:\n",
      "17.5. Preparing Data For CART 75\n",
      "17.4.2 Pruning The Tree\n",
      "The stopping criterion is important as it strongly inﬂuences the performance of your tree. You\n",
      "can use pruning after learning your tree to further lift performance. The complexity of a decision\n",
      "tree is deﬁned as the number of splits in the tree. Simpler trees are preferred. They are easy to\n",
      "understand (you can print them out and show them to subject matter experts), and they are\n",
      "less likely to overﬁt your data.\n",
      "The fastest and simplest pruning method is to work through each leaf node in the tree and\n",
      "evaluate the eﬀect of removing it using a hold-out test set. Leaf nodes are removed only if it\n",
      "results in a drop in the overall cost function on the entire test set. You stop removing nodes\n",
      "when no further improvements can be made. More sophisticated pruning methods can be used\n",
      "such as cost complexity pruning (also called weakest link pruning) where a learning parameter\n",
      "(alpha) is used to weigh whether nodes can be removed based on the size of the sub-tree.\n",
      "17.5 Preparing Data For CART\n",
      "CART does not require any special data preparation other than a good representation of the\n",
      "problem.\n",
      "17.6 Summary\n",
      "In this chapter you have discovered the Classiﬁcation And Regression Trees (CART) for machine\n",
      "learning. You learned:\n",
      " The classical name Decision Tree and the more Modern name CART for the algorithm.\n",
      " The representation used for CART is a binary tree.\n",
      " Predictions are made with CART by traversing the binary tree given a new input record.\n",
      " The tree is learned using a greedy algorithm on the training data to pick splits in the tree\n",
      " Stopping criteria deﬁne how much a tree learns and pruning can be used to improve\n",
      "generalization on a learned tree.\n",
      "You now know about the Classiﬁcation and Regression Trees machine learning algorithm\n",
      "for classiﬁcation and regression. In the next chapter you will discover how you can implement\n",
      "CART from scratch....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 84, 'page_label': '85', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1016}\n",
      "\n",
      "--- Chunk 1017 ---\n",
      "Content:\n",
      "Chapter 18\n",
      "Classiﬁcation and Regression Trees\n",
      "Tutorial\n",
      "Decision trees are a ﬂexible and very powerful machine learning method for regression and\n",
      "classiﬁcation predictive modeling problems. In this chapter you will discover how to implement\n",
      "the CART machine learning algorithm from scratch step-by-step. After completing this chapter\n",
      "you will know:\n",
      " How to calculate the Gini index for a given split in a decision tree.\n",
      " How to evaluate diﬀerent split points when constructing a decision tree.\n",
      " How to make predictions on new data with a learned decision tree.\n",
      "Let’s get started.\n",
      "18.1 Tutorial Dataset\n",
      "In this tutorial we will work through a simple binary (two-class) classiﬁcation problem for\n",
      "CART. To keep things simple we will work with a two input variables (X1 and X2) and a single\n",
      "output variable (Y). This is not a real problem but a contrived problem to demonstrate how\n",
      "to implement the CART model and make predictions. The example was designed so that the\n",
      "algorithm will ﬁnd at least two split points in order to best classify the training dataset. The\n",
      "raw data for this problem is as follows:\n",
      "X1 X2 Y\n",
      "2.771244718 1.784783929 0\n",
      "1.728571309 1.169761413 0\n",
      "3.678319846 2.81281357 0\n",
      "3.961043357 2.61995032 0\n",
      "2.999208922 2.209014212 0\n",
      "7.497545867 3.162953546 1\n",
      "9.00220326 3.339047188 1\n",
      "7.444542326 0.476683375 1\n",
      "10.12493903 3.234550982 1\n",
      "6.642287351 3.319983761 1\n",
      "Listing 18.1: CART Tutorial Data Set.\n",
      "When visualized as a two-dimensional scatter plot the dataset looks as follows:\n",
      "76...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 85, 'page_label': '86', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1017}\n",
      "\n",
      "--- Chunk 1018 ---\n",
      "Content:\n",
      "18.2. Learning a CART Model 77\n",
      "Figure 18.1: Classiﬁcation And Regression Trees Tutorial Dataset.\n",
      "18.2 Learning a CART Model\n",
      "The CART model is learned by looking for split points in the data. A split point is a single\n",
      "value of a single attribute, e.g. the ﬁrst value of the X1 attribute 2.771244718. Partitioning\n",
      "data at a split point involves separating all data at that node into two groups, left of the split\n",
      "point and right of the split point. If we are working on the ﬁrst split point in the tree, then all\n",
      "of the dataset is aﬀected. If we are working on say a split point one level deep, then only the\n",
      "data that has ﬁltered down the tree from nodes above and is sitting at that node is aﬀected by\n",
      "the split point.\n",
      "We are not concerned with what the class value is of the chosen split point. We only care\n",
      "about the composition of the data assigned to the LEFT and to the RIGHT child nodes of the\n",
      "split point. A cost function is used to evaluate the mix of classes of training data assigned to\n",
      "each side of the split. In classiﬁcation problems the Gini index cost function is used.\n",
      "18.2.1 Gini Index Cost Function\n",
      "We calculate the Gini index for a split point as follows:\n",
      "Gini(split) =\n",
      "n∑\n",
      "k=1\n",
      "(pk ×(1 −pk)) (18.1)\n",
      "For each class (k), for each group (left and right). Where p is the proportion of training\n",
      "instances with a given class in a given group. We will always have two groups, a left and right\n",
      "group because we are using a binary tree. And we know from our dataset that we only have\n",
      "two classes. Therefore we can calculate the Gini index of any split point in our dataset as the\n",
      "sum of:\n",
      "Gini(split) = (left(0) ×(1 −left(0))) + (right(0) ×(1 −right(0))+\n",
      "(left(1) ×(1 −left(1))) + (right(1) ×(1 −right(1)) (18.2)...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 86, 'page_label': '87', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1018}\n",
      "\n",
      "--- Chunk 1019 ---\n",
      "Content:\n",
      "18.2. Learning a CART Model 78\n",
      "Where left(0) is the proportion of data instances in the left group with class 0, right(0) is\n",
      "the proportion of data instances in the right group with class 0, and so on. The proportion\n",
      "of data instances of a class is easy to calculate. If a LEFT group has 3 instances with class 0\n",
      "and 4 instances with class 1, then the proportion of data instances with class 0 would be 3\n",
      "7 or\n",
      "0.428571429. To get a feeling for Gini index scores, take a look at the table below. It provides 7\n",
      "diﬀerent scenarios for mixes of 0 and 1 classes in a single group.\n",
      "Class 0 Class 1 Count Class 0/Count Class 1/Count Gini\n",
      "10 10 20 0.5 0.5 0.5\n",
      "19 1 20 0.95 0.05 0.095\n",
      "1 19 20 0.05 0.95 0.095\n",
      "15 5 20 0.75 0.25 0.375\n",
      "5 15 20 0.25 0.75 0.375\n",
      "11 9 20 0.55 0.45 0.495\n",
      "20 0 20 1 0 0\n",
      "Listing 18.2: Sample Gini calculations.\n",
      "It is easy to visualize these scenarios for one group, but the concepts translate to summing\n",
      "the proportions across the LEFT and RIGHT groups. You can see when the group has a 50-50\n",
      "mix in the ﬁrst row, that Gini is 0.5. This is the worst possible split. You can also see a case\n",
      "where the group only has data instances with class 0 on the last row and a Gini index of 0. This\n",
      "is an example of a perfect split. Our goal in selecting a split point is to evaluate the Gini index\n",
      "of all possible split points and greedily select the split point with the lowest cost. Let’s make\n",
      "this more concrete by calculating the cost of selecting diﬀerent data points as our split point.\n",
      "18.2.2 First Candidate Split Point\n",
      "The ﬁrst step is to choose a split that will become the stump or root node of our decision tree.\n",
      "We will start with the ﬁrst candidate split point which is the X1 attribute and the value of X1\n",
      "in the ﬁrst instance: X1 = 2.771244718.\n",
      " IF X1 <2.771244718 THEN LEFT\n",
      " IF X1 ≥2.771244718 THEN RIGHT\n",
      "Let’s apply this rule to each X1 value in our training dataset. Below is the answer we get\n",
      "for each numbered instance in the dataset:\n",
      "X1 Y Group\n",
      "2.771244718 0 RIGHT\n",
      "1.728571309 0 LEFT\n",
      "3.678319846 0 RIGHT\n",
      "3.961043357 0 RIGHT\n",
      "2.999208922 0 RIGHT\n",
      "7.497545867 1 RIGHT\n",
      "9.00220326 1 RIGHT\n",
      "7.444542326 1 RIGHT\n",
      "10.12493903 1 RIGHT\n",
      "6.642287351 1 RIGHT\n",
      "Listing 18.3: Separation of Training Dataset by Candidate Split....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 87, 'page_label': '88', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1019}\n",
      "\n",
      "--- Chunk 1020 ---\n",
      "Content:\n",
      "18.2. Learning a CART Model 79\n",
      "How good was this split? We can evaluate the mixture of the classes in each of the LEFT\n",
      "and RIGHT nodes as a single cost of choosing this split point for our root node. The LEFT\n",
      "group only has one member, where as the RIGHT group has 9 members. Starting with the\n",
      "LEFT group, we can calculate the proportion of training instances that have each class:\n",
      " Y = 0: 1\n",
      "1 or 1.0\n",
      " Y = 1: 0\n",
      "1 or 0.0\n",
      "The RIGHT group is more interesting as it is comprised of a mix of classes (we are probably\n",
      "going to get a high Gini index).\n",
      " Y = 0: 7\n",
      "9 or 0.444444444\n",
      " Y = 1: 5\n",
      "9 or 0.555555556\n",
      "We now have enough information to calculate the Gini index for this split:\n",
      "Gini(X1 = 2.7712) = (1.0 ×(1 −1.0)) + (0 .0 ×(1 −0.0)) +\n",
      "(0.444444444 ×(1 −0.444444444) +\n",
      "(0.555555556 ×(1 −0.555555556))\n",
      "(18.3)\n",
      "or\n",
      "Gini(X1 = 2.771244718) = 0.49382716 (18.4)\n",
      "18.2.3 Best Candidate Split Point\n",
      "We can evaluate each candidate split point using the process above with the values from X1\n",
      "and X2. If we look at the graph of the data, we can see that we can probably draw a vertical\n",
      "line to separate the classes. This would translate to a split point for X1 with a value around\n",
      "0.5. A close ﬁt would be the value for X1 in the last instance: X1 = 6.642287351.\n",
      " IF X1 <6.642287351 THEN LEFT\n",
      " IF X1 ≥6.642287351 THEN RIGHT\n",
      "Let’s apply this rule to all instances, below we get the assigned group for each numbered\n",
      "data instance:\n",
      "X1 Y Group\n",
      "2.771244718 0 LEFT\n",
      "1.728571309 0 LEFT\n",
      "3.678319846 0 LEFT\n",
      "3.961043357 0 LEFT\n",
      "2.999208922 0 LEFT\n",
      "7.497545867 1 RIGHT\n",
      "9.00220326 1 RIGHT\n",
      "7.444542326 1 RIGHT\n",
      "10.12493903 1 RIGHT\n",
      "6.642287351 1 RIGHT\n",
      "Listing 18.4: Separation of Training Dataset by Bets Split....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 88, 'page_label': '89', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1020}\n",
      "\n",
      "--- Chunk 1021 ---\n",
      "Content:\n",
      "18.3. Making Predictions on Data 80\n",
      "There are 5 instances in each group, this looks like a good split. Starting with the LEFT\n",
      "group, we can calculate the proportion of training instances that have each class:\n",
      " Y = 0: 5\n",
      "5 or 1.0\n",
      " Y = 1: 0\n",
      "5 or 0.0\n",
      "The RIGHT group has the opposite proportions.\n",
      " Y = 0: 0\n",
      "5 or 0.0\n",
      " Y = 1: 5\n",
      "5 or 1.0\n",
      "We now have enough information to calculate the Gini index for this split:\n",
      "Gini(X1 = 6.642287351) = (1.0 ×(1 −1.0)) +\n",
      "(0.0 ×(1 −0.0)) +\n",
      "(1.0 ×(1 −1.0)) +\n",
      "(0.0 ×(1 −0.0))\n",
      "(18.5)\n",
      "or\n",
      "Gini(X1 = 6.642287351) = 0.0 (18.6)\n",
      "This is a split that results in a pure Gini index because the classes are perfectly separated.\n",
      "The LEFT child node will classify instances as class 0 and the right as class 1. We can stop\n",
      "there. You can see how this process could be repeated for each child node to build up a more\n",
      "complicated tree for a more challenging dataset.\n",
      "18.3 Making Predictions on Data\n",
      "We can now use this decision tree to make some predictions for all of the training instances.\n",
      "But we have already done that when we calculated the Gini index above. Instead, let’s classify\n",
      "some new data generated for each class using the same distribution. Here is the test dataset:\n",
      "X1 X2 Y\n",
      "2.343875381 2.051757824 0\n",
      "3.536904049 3.032932531 0\n",
      "2.801395588 2.786327755 0\n",
      "3.656342926 2.581460765 0\n",
      "2.853194386 1.052331062 0\n",
      "8.907647835 3.730540859 1\n",
      "9.752464513 3.740754624 1\n",
      "8.016361622 3.013408249 1\n",
      "6.58490395 2.436333477 1\n",
      "7.142525173 3.650120799 1\n",
      "Listing 18.5: Test Dataset.\n",
      "Using the decision tree with a single split at, X1 = 6.642287351 we can classify the test\n",
      "instances as follows:...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 89, 'page_label': '90', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1021}\n",
      "\n",
      "--- Chunk 1022 ---\n",
      "Content:\n",
      "18.4. Summary 81\n",
      "Y Prediction\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "1 0\n",
      "1 1\n",
      "Listing 18.6: Predictions for Test Dataset.\n",
      "Again, this is a perfect classiﬁcation or 100% accurate.\n",
      "18.4 Summary\n",
      "In this chapter you discovered exactly how to construct a CART model and use it to make\n",
      "predictions. You learned how to:\n",
      " Calculate the Gini index for a candidate split in a decision tree.\n",
      " Evaluate any candidate split points using Gini index.\n",
      " How to create a decision tree from scratch to make predictions.\n",
      " How to make predictions on new data using a learned decision tree.\n",
      "You now know how to implement CART from scratch. In the next chapter in you will\n",
      "discover the Naive Bayes machine learning algorithm for classiﬁcation....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 90, 'page_label': '91', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1022}\n",
      "\n",
      "--- Chunk 1023 ---\n",
      "Content:\n",
      "Chapter 19\n",
      "Naive Bayes\n",
      "Naive Bayes is a simple but surprisingly powerful algorithm for predictive modeling. In this\n",
      "chapter you will discover the Naive Bayes algorithm for classiﬁcation. After reading this chapter,\n",
      "you will know:\n",
      " The representation used by naive Bayes that is actually stored when a model is written to\n",
      "a ﬁle.\n",
      " How a learned model can be used to make predictions.\n",
      " How you can learn a naive Bayes model from training data.\n",
      " How to best prepare your data for the naive Bayes algorithm.\n",
      " Where to go for more information on naive Bayes.\n",
      "Let’s get started.\n",
      "19.1 Quick Introduction to Bayes’ Theorem\n",
      "In machine learning we are often interested in selecting the best hypothesis ( h) given data ( d).\n",
      "In a classiﬁcation problem, our hypothesis ( h) may be the class to assign for a new data instance\n",
      "(d). One of the easiest ways of selecting the most probable hypothesis given the data that we\n",
      "have that we can use as our prior knowledge about the problem. Bayes’ Theorem provides a\n",
      "way that we can calculate the probability of a hypothesis given our prior knowledge. Bayes’\n",
      "Theorem is stated as:\n",
      "P(h|d) = P(d|h) ×P(h)\n",
      "P(d) (19.1)\n",
      "Where:\n",
      " P(h|d) is the probability of hypothesis h given the data d. This is called the posterior\n",
      "probability.\n",
      " P(d|h) is the probability of data d given that the hypothesis h was true.\n",
      "82...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 91, 'page_label': '92', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1023}\n",
      "\n",
      "--- Chunk 1024 ---\n",
      "Content:\n",
      "19.2. Naive Bayes Classiﬁer 83\n",
      " P(h) is the probability of hypothesis h being true (regardless of the data). This is called\n",
      "the prior probability of h.\n",
      " P(d) is the probability of the data (regardless of the hypothesis).\n",
      "You can see that we are interested in calculating the posterior probability of P(h|d) from\n",
      "the prior probability p(h) with P(D) and P(d|h). After calculating the posterior probability for\n",
      "a number of diﬀerent hypotheses, you can select the hypothesis with the highest probability.\n",
      "This is the maximum probable hypothesis and may formally be called the maximum a posteriori\n",
      "(MAP) hypothesis. This can be written as:\n",
      "MAP(h) = max(P(h|d))\n",
      "MAP(h) = max(P(d|h) ×P(h)\n",
      "P(d) )\n",
      "MAP(h) = max(P(d|h) ×P(h))\n",
      "(19.2)\n",
      "The P(d) is a normalizing term which allows us to calculate the probability. We can drop\n",
      "it when we are interested in the most probable hypothesis as it is constant and only used to\n",
      "normalize. Back to classiﬁcation, if we have an even number of instances in each class in our\n",
      "training data, then the probability of each class (e.g. P(h)) will be the same value for each class\n",
      "(e.g. 0.5 for a 50-50 split). Again, this would be a constant term in our equation and we could\n",
      "drop it so that we end up with:\n",
      "MAP(h) = max(P(d|h)) (19.3)\n",
      "This is a useful exercise, because when reading up further on Naive Bayes you may see all of\n",
      "these forms of the theorem.\n",
      "19.2 Naive Bayes Classiﬁer\n",
      "Naive Bayes is a classiﬁcation algorithm for binary (two-class) and multiclass classiﬁcation\n",
      "problems. The technique is easiest to understand when described using binary or categorical\n",
      "input values. It is called naive Bayes or idiot Bayes because the calculation of the probabilities\n",
      "for each hypothesis are simpliﬁed to make their calculation tractable. Rather than attempting to\n",
      "calculate the values of each attribute value P(d1,d2,d3|h), they are assumed to be conditionally\n",
      "independent given the target value and calculated as P(d1|h) ×P(d2|h) and so on. This is a\n",
      "very strong assumption that is most unlikely in real data, i.e. that the attributes do not interact.\n",
      "Nevertheless, the approach performs surprisingly well on data where this assumption does not\n",
      "hold.\n",
      "19.2.1 Representation Used By Naive Bayes Models\n",
      "The representation for naive Bayes is probabilities. A list of probabilities is stored to ﬁle for a\n",
      "learned naive Bayes model. This includes:\n",
      " Class Probabilities: The probabilities of each class in the training dataset.\n",
      " Conditional Probabilities: The conditional probabilities of each input value given each\n",
      "class value....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 92, 'page_label': '93', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1024}\n",
      "\n",
      "--- Chunk 1025 ---\n",
      "Content:\n",
      "19.2. Naive Bayes Classiﬁer 84\n",
      "19.2.2 Learn a Naive Bayes Model From Data\n",
      "Learning a naive Bayes model from your training data is fast. Training is fast because only the\n",
      "probability of each class and the probability of each class given diﬀerent input ( x) values need\n",
      "to be calculated. No coeﬃcients need to be ﬁtted by optimization procedures.\n",
      "Calculating Class Probabilities\n",
      "The class probabilities are simply the frequency of instances that belong to each class divided\n",
      "by the total number of instances. For example in a binary classiﬁcation the probability of an\n",
      "instance belonging to class 1 would be calculated as:\n",
      "P(class= 1) = count(class= 1)\n",
      "count(class= 0) + count(class= 1) (19.4)\n",
      "In the simplest case each class would have the probability of 0.5 or 50% for a binary\n",
      "classiﬁcation problem with the same number of instances in each class.\n",
      "Calculating Conditional Probabilities\n",
      "The conditional probabilities are the frequency of each attribute value for a given class value\n",
      "divided by the frequency of instances with that class value. For example, if a weather attribute\n",
      "had the values sunny and rainy and the class attribute had the class values go-out and\n",
      "stay-home, then the conditional probabilities of each weather value for each class value could\n",
      "be calculated as:\n",
      "P(weather = sunny|class= go-out) = count(weather = sunny∧class=go-out)\n",
      "count(class=go-out)\n",
      "P(weather = sunny|class= stay-home) = count(weather = sunny∧class= stay-home)\n",
      "count(class= stay-home)\n",
      "P(weather = rainy|class= go-out) = count(weather = rainy∧class= go-out)\n",
      "count(class= go-out)\n",
      "P(weather = rainy|class= stay-home) = count(weather = rainy∧class= stay-home)\n",
      "count(class= stay-home)\n",
      "(19.5)\n",
      "Where ∧means conjunction (and).\n",
      "19.2.3 Make Predictions With a Naive Bayes Model\n",
      "Given a naive Bayes model, you can make predictions for new data using Bayes theorem.\n",
      "MAP(h) = max(P(d|h) ×P(h)) (19.6)\n",
      "Using our example above, if we had a new instance with the weather of sunny, we can\n",
      "calculate:\n",
      "go-out = P(weather = sunny|class= go-out) ×P(class= go-out)\n",
      "stay-home = P(weather = sunny|class= stay-home) ×P(class= stay-home) (19.7)...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 93, 'page_label': '94', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1025}\n",
      "\n",
      "--- Chunk 1026 ---\n",
      "Content:\n",
      "19.3. Gaussian Naive Bayes 85\n",
      "We can choose the class that has the largest calculated value. We can turn these values into\n",
      "probabilities by normalizing them as follows:\n",
      "P(go-out|weather = sunny) = go-out\n",
      "go-out + stay-home\n",
      "P(stay-home|weather = sunny) = stay-home\n",
      "go-out + stay-home\n",
      "(19.8)\n",
      "If we had more input variables we could extend the above example. For example, pretend\n",
      "we have a car attribute with the values working and broken. We can multiply this probability\n",
      "into the equation. For example below is the calculation for the go-out class label with the\n",
      "addition of the car input variable set to working:\n",
      "go-out =P(weather = sunny|class= go-out)×\n",
      "P(car= working|class= go-out)×\n",
      "P(class= go-out)\n",
      "(19.9)\n",
      "19.3 Gaussian Naive Bayes\n",
      "Naive Bayes can be extended to real-valued attributes, most commonly by assuming a Gaussian\n",
      "distribution. This extension of naive Bayes is called Gaussian Naive Bayes. Other functions can\n",
      "be used to estimate the distribution of the data, but the Gaussian (or Normal distribution) is\n",
      "the easiest to work with because you only need to estimate the mean and the standard deviation\n",
      "from your training data.\n",
      "19.3.1 Representation for Gaussian Naive Bayes\n",
      "Above, we calculated the probabilities for input values for each class using a frequency. With\n",
      "real-valued inputs, we can calculate the mean and standard deviation of input values ( x) for\n",
      "each class to summarize the distribution. This means that in addition to the probabilities for\n",
      "each class, we must also store the mean and standard deviations for each input variable for each\n",
      "class.\n",
      "19.3.2 Learn a Gaussian Naive Bayes Model From Data\n",
      "This is as simple as calculating the mean and standard deviation values of each input variable\n",
      "(x) for each class value.\n",
      "mean(x) = 1\n",
      "n ×\n",
      "n∑\n",
      "i=1\n",
      "xi (19.10)\n",
      "Where nis the number of instances and xare the values for an input variable in your training\n",
      "data. We can calculate the standard deviation using the following equation:\n",
      "StandardDeviation(x) =\n",
      "√1\n",
      "n ×\n",
      "n∑\n",
      "i=1\n",
      "(xi −mean(x))2 (19.11)...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 94, 'page_label': '95', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1026}\n",
      "\n",
      "--- Chunk 1027 ---\n",
      "Content:\n",
      "19.4. Preparing Data For Naive Bayes 86\n",
      "This is the square root of the average squared diﬀerence of each value of x from the mean\n",
      "value of x, where nis the number of instances, xi is a speciﬁc value of the xvariable for the i’th\n",
      "instance and mean(x) is described above.\n",
      "19.3.3 Make Predictions With a Gaussian Naive Bayes Model\n",
      "Probabilities of new x values are calculated using the Gaussian Probability Density Function\n",
      "(PDF). When making predictions these parameters can be plugged into the Gaussian PDF with\n",
      "a new input for the variable, and in return the Gaussian PDF will provide an estimate of the\n",
      "probability of that new input value for that class.\n",
      "pdf(x,mean,sd ) = 1√2 ×π×sd ×e−( (x−mean)2\n",
      "2×sd2 ) (19.12)\n",
      "Where pdf(x) is the Gaussian PDF, mean and sd are the mean and standard deviation\n",
      "calculated above, π is the numerical constant PI, e is the numerical constant Euler’s number\n",
      "raised to power and x is the input value for the input variable. We can then plug in the\n",
      "probabilities into the equation above to make predictions with real-valued inputs. For example,\n",
      "adapting one of the above calculations with numerical values for weather and car:\n",
      "go-out =P(pdf(weather)|class= go-out)×\n",
      "P(pdf(car)|class= go-out)×\n",
      "P(class= go-out)\n",
      "(19.13)\n",
      "19.4 Preparing Data For Naive Bayes\n",
      "This section provides some tips for preparing your data for Naive Bayes.\n",
      " Categorical Inputs: Naive Bayes assumes label attributes such as binary, categorical or\n",
      "nominal.\n",
      " Gaussian Inputs : If the input variables are real-valued, a Gaussian distribution is\n",
      "assumed. In which case the algorithm will perform better if the univariate distributions of\n",
      "your data are Gaussian or near-Gaussian. This may require removing outliers (e.g. values\n",
      "that are more than 3 or 4 standard deviations from the mean).\n",
      " Classiﬁcation Problems: Naive Bayes is a classiﬁcation algorithm suitable for binary\n",
      "and multiclass classiﬁcation.\n",
      " Log Probabilities: The calculation of the likelihood of diﬀerent class values involves\n",
      "multiplying a lot of small numbers together. This can lead to an underﬂow of numerical\n",
      "precision. As such it is good practice to use a log transform of the probabilities to avoid\n",
      "this underﬂow.\n",
      " Kernel Functions: Rather than assuming a Gaussian distribution for numerical input\n",
      "values, more complex distributions can be used such as a variety of kernel density functions.\n",
      " Update Probabilities: When new data becomes available, you can simply update the\n",
      "probabilities of your model. This can be helpful if the data changes frequently....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 95, 'page_label': '96', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1027}\n",
      "\n",
      "--- Chunk 1028 ---\n",
      "Content:\n",
      "19.5. Summary 87\n",
      "19.5 Summary\n",
      "In this chapter you discovered the Naive Bayes algorithm for classiﬁcation. You learned about:\n",
      " The Bayes Theorem and how to calculate it in practice.\n",
      " Naive Bayes algorithm including representation, making predictions and learning the\n",
      "model.\n",
      " The adaptation of Naive Bayes for real-valued input data called Gaussian Naive Bayes.\n",
      " How to prepare data for Naive Bayes.\n",
      "You now know about the Naive Bayes algorithm for classiﬁcation. In the next chapter you\n",
      "will discover how to implement Naive Bayes from scratch for categorical variables....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 96, 'page_label': '97', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1028}\n",
      "\n",
      "--- Chunk 1029 ---\n",
      "Content:\n",
      "Chapter 20\n",
      "Naive Bayes Tutorial\n",
      "Naive Bayes is a very simple classiﬁcation algorithm that makes some strong assumptions about\n",
      "the independence of each input variable. Nevertheless, it has been shown to be eﬀective in a\n",
      "large number of problem domains. In this chapter you will discover the Naive Bayes algorithm\n",
      "for categorical data. After reading this chapter you will know.\n",
      " How to work with categorical data for Naive Bayes.\n",
      " How to prepare the class and conditional probabilities for a Naive Bayes model.\n",
      " How to use a learned Naive Bayes model to make predictions.\n",
      "Let’s get started.\n",
      "20.1 Tutorial Dataset\n",
      "The dataset describes two categorical input variables and a class variable that has two outputs.\n",
      "Weather Car Class\n",
      "sunny working go-out\n",
      "rainy broken go-out\n",
      "sunny working go-out\n",
      "sunny working go-out\n",
      "sunny working go-out\n",
      "rainy broken stay-home\n",
      "rainy broken stay-home\n",
      "sunny working stay-home\n",
      "sunny broken stay-home\n",
      "rainy broken stay-home\n",
      "Listing 20.1: Naive Bayes Tutorial Data Set.\n",
      "We can convert this into numbers. Each input has only two values and the output class\n",
      "variable has two values. We can convert each variable to binary as follows:\n",
      " Weather: sunny = 1, rainy = 0\n",
      " Car: working = 1, broken = 0\n",
      " Class: go-out = 1, stay-home = 0\n",
      "88...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 97, 'page_label': '98', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1029}\n",
      "\n",
      "--- Chunk 1030 ---\n",
      "Content:\n",
      "20.2. Learn a Naive Bayes Model 89\n",
      "Therefore, we can restate the dataset as:\n",
      "Weather Car Class\n",
      "1 1 1\n",
      "0 0 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "1 1 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 1 0\n",
      "1 0 0\n",
      "0 0 0\n",
      "Listing 20.2: Simpliﬁed Naive Bayes Tutorial Data Set.\n",
      "This can make the data easier to work with in a spreadsheet or code if you are following\n",
      "along.\n",
      "20.2 Learn a Naive Bayes Model\n",
      "There are two types of quantities that need to be calculated from the dataset for the naive\n",
      "Bayes model:\n",
      " Class Probabilities.\n",
      " Conditional Probabilities.\n",
      "Let’s start with the class probabilities.\n",
      "20.2.1 Calculate the Class Probabilities\n",
      "The dataset is a two class problem and we already know the probability of each class because\n",
      "we contrived the dataset. Nevertheless, we can calculate the class probabilities for classes 0 and\n",
      "1 as follows:\n",
      "P(class= 1) = count(class= 1)\n",
      "count(class= 0) + count(class= 1)\n",
      "P(class= 0) = count(class= 0)\n",
      "count(class= 0) + count(class= 1)\n",
      "(20.1)\n",
      "or\n",
      "P(class= 1) = 5\n",
      "5 + 5\n",
      "P(class= 0) = 5\n",
      "5 + 5\n",
      "(20.2)\n",
      "This works out to be a probability of 0.5 for any given data instance belonging to class 0 or\n",
      "class 1....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 98, 'page_label': '99', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1030}\n",
      "\n",
      "--- Chunk 1031 ---\n",
      "Content:\n",
      "20.2. Learn a Naive Bayes Model 90\n",
      "20.2.2 Calculate the Conditional Probabilities\n",
      "The conditional probabilities are the probability of each input value given each class value. The\n",
      "conditional probabilities for the dataset can be calculated as follows:\n",
      "Weather Input Variable\n",
      "P(weather = sunny|class= go-out) = count(weather = sunny∧class= go-out)\n",
      "count(class= go-out)\n",
      "P(weather = rainy|class= go-out) = count(weather = rainy∧class= go-out)\n",
      "count(class= go-out)\n",
      "P(weather = sunny|class= stay-home) = count(weather = sunny∧class= stay-home)\n",
      "count(class= stay-home)\n",
      "P(weather = rainy|class= stay-home) = count(weather = rainy∧class= stay-home)\n",
      "count(class= stay-home)\n",
      "(20.3)\n",
      "Remember that the ∧symbol is just a shorthand for conjunction (AND). Plugging in the\n",
      "numbers we get:\n",
      "P(weather = sunny|class= go-out) = 0.8\n",
      "P(weather = rainy|class= go-out) = 0.2\n",
      "P(weather = sunny|class= stay-home) = 0.4\n",
      "P(weather = rainy|class= stay-home) = 0.6\n",
      "(20.4)\n",
      "Car Input Variable\n",
      "P(car= working|class= go-out) = count(car= working ∧class= go-out)\n",
      "count(class= go-out)\n",
      "P(car= broken|class= go-out) = count(car= broken∧class= go-out)\n",
      "count(class= go-out)\n",
      "P(car= working|class= stay-home) = count(car= working ∧class= stay-home)\n",
      "count(class= stay-home)\n",
      "P(car= broken|class= stay-home) = count(car= broken∧class= stay-home)\n",
      "count(class= stay-home)\n",
      "(20.5)\n",
      "Plugging in the numbers we get:\n",
      "P(car= working|class= go-out) = 0.8\n",
      "P(car= broken|class= go-out) = 0.2\n",
      "P(car= working|class= stay-home) = 0.2\n",
      "P(car= broken|class= stay-home) = 0.8\n",
      "(20.6)\n",
      "We now have every thing we need to make predictions using the Naive Bayes model....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 99, 'page_label': '100', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1031}\n",
      "\n",
      "--- Chunk 1032 ---\n",
      "Content:\n",
      "20.3. Make Predictions with Naive Bayes 91\n",
      "20.3 Make Predictions with Naive Bayes\n",
      "We can make predictions using Bayes Theorem, deﬁned and explained in the previous chapter.\n",
      "P(h|d) = P(d|h) ×P(h)\n",
      "P(d) (20.7)\n",
      "In fact, we don’t need a probability to predict the most likely class for a new data instance.\n",
      "We only need the numerator and the class that gives the largest response, which will be the\n",
      "predicted output.\n",
      "MAP(h) = max(P(d|h) ×P(h)) (20.8)\n",
      "Let’s take the ﬁrst record from our dataset and use our learned model to predict which class\n",
      "we think it belongs. First instance: weather = sunny,car = working.\n",
      "We plug the probabilities for our model in for both classes and calculate the response.\n",
      "Starting with the response for the output go-out. We multiply the conditional probabilities\n",
      "together and multiply it by the probability of any instance belonging to the class.\n",
      "go-out =P(weather = sunny|class= go-out)×\n",
      "P(car= working|class= go-out)×\n",
      "P(class= go-out)\n",
      "(20.9)\n",
      "or\n",
      "go-out = 0.8 ×0.8 ×0.5\n",
      "go-out = 0.32 (20.10)\n",
      "We can perform the same calculation for the stay-home case:\n",
      "stay-home =P(weather = sunny|class= stay-home)×\n",
      "P(car= working|class= stay-home)×\n",
      "P(class= stay-home)\n",
      "(20.11)\n",
      "or\n",
      "stay-home = 0.4 ×0.2 ×0.5\n",
      "stay-home = 0.04 (20.12)\n",
      "We can see that 0.32 is greater than 0.04, therefore we predict go-out for this instance,\n",
      "which is correct. We can repeat this operation for the entire dataset, as follows:\n",
      "Weather Car Class go-out? stay-home? Prediction\n",
      "sunny working go-out 0.32 0.04 go-out\n",
      "rainy broken go-out 0.02 0.24 stay-home\n",
      "sunny working go-out 0.32 0.04 go-out\n",
      "sunny working go-out 0.32 0.04 go-out\n",
      "sunny working go-out 0.32 0.04 go-out\n",
      "rainy broken stay-home 0.02 0.24 stay-home\n",
      "rainy broken stay-home 0.02 0.24 stay-home\n",
      "sunny working stay-home 0.32 0.04 go-out\n",
      "sunny broken stay-home 0.08 0.16 stay-home\n",
      "rainy broken stay-home 0.02 0.24 stay-home...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 100, 'page_label': '101', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1032}\n",
      "\n",
      "--- Chunk 1033 ---\n",
      "Content:\n",
      "20.4. Summary 92\n",
      "Listing 20.3: Naive Bayes Predictions for the Dataset.\n",
      "If we tally up the predictions compared to the actual class values, we get an accuracy of\n",
      "80%, which is excellent given that there are conﬂicting examples in the dataset.\n",
      "20.4 Summary\n",
      "In this chapter you discovered exactly how to implement Naive Bayes from scratch. You learned:\n",
      " How to work with categorical data with Naive Bayes.\n",
      " How to calculate class probabilities from training data.\n",
      " How to calculate conditional probabilities from training data.\n",
      " How to use a learned Naive Bayes model to make predictions on new data.\n",
      "You now know how to implement Naive Bayes from scratch for categorical data. In the next\n",
      "chapter you will discover how to can implement Naive Bayes from scratch for real-valued data....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 101, 'page_label': '102', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1033}\n",
      "\n",
      "--- Chunk 1034 ---\n",
      "Content:\n",
      "Chapter 21\n",
      "Gaussian Naive Bayes Tutorial\n",
      "Naive Bayes is a simple model that uses probabilities calculated from your training data to\n",
      "make predictions on new data. The basic Naive Bayes algorithm assumes categorical data. A\n",
      "simple extension for real-valued data is called Gaussian Naive Bayes. In this chapter you will\n",
      "discover how to implement Gaussian Naive Bayes from scratch. After reading this chapter you\n",
      "will know:\n",
      " The Gaussian Probability Density Function and how to calculate the probability of real\n",
      "values.\n",
      " How to learn the properties for a Gaussian Naive Bayes model from your training data.\n",
      " How to use a learned Gaussian Naive Bayes model to make predictions on new data.\n",
      "Let’s get started.\n",
      "21.1 Tutorial Dataset\n",
      "A simple dataset was contrived for our purposes. It is comprised of two input variables X1 and\n",
      "X2 and one output variable Y. The input variables are drawn from a Gaussian distribution,\n",
      "which is one assumption made by Gaussian Naive Bayes. The class variable has two values, 0\n",
      "and 1, therefore the problem is a binary classiﬁcation problem.\n",
      "Data from class 0 was drawn randomly from a Gaussian distribution with a standard\n",
      "deviation of 1.0 for X1 and X2. Data from class 1 was drawn randomly from a Gaussian\n",
      "distribution with a mean of 7.5 for X1 and 2.5 for X2. This means that the classes are nicely\n",
      "separated if we plot the input data on a scatter plot. The raw dataset is listed below:\n",
      "X1 X2 Y\n",
      "3.393533211 2.331273381 0\n",
      "3.110073483 1.781539638 0\n",
      "1.343808831 3.368360954 0\n",
      "3.582294042 4.67917911 0\n",
      "2.280362439 2.866990263 0\n",
      "7.423436942 4.696522875 1\n",
      "5.745051997 3.533989803 1\n",
      "9.172168622 2.511101045 1\n",
      "7.792783481 3.424088941 1\n",
      "7.939820817 0.791637231 1\n",
      "93...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 102, 'page_label': '103', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1034}\n",
      "\n",
      "--- Chunk 1035 ---\n",
      "Content:\n",
      "21.2. Gaussian Probability Density Function 94\n",
      "Listing 21.1: Gaussian Naive Bayes Tutorial Data Set.\n",
      "You can clearly see the separation of the classes in the plot below. This will make the data\n",
      "relatively easy to work with as we implement and test a Gaussian Naive Bayes model.\n",
      "Figure 21.1: Gaussian Naive Bayes Tutorial Dataset.\n",
      "21.2 Gaussian Probability Density Function\n",
      "The Gaussian Probability Density Function (PDF) will calculate the probability of a value given\n",
      "the mean and standard deviation of the distribution from which it came. The Gaussian PDF is\n",
      "calculated as follows:\n",
      "pdf(x,mean,sd ) = 1√2 ×π×sd ×e−( (x−mean)2\n",
      "2×sd2 ) (21.1)\n",
      "Where pdf(x) is the Gaussian PDF, mean and sd are the mean and standard deviation\n",
      "calculated above, π is the numerical constant PI, e is Euler’s number raised to a power and x\n",
      "is the input value for the input variable. Let’s look at an example. Let’s assume we have real\n",
      "values drawn from a population that has a mean of 0 and a standard deviation of 1. Using the\n",
      "Gaussian PDF we can estimate the likelihood of range of values.\n",
      "X PDF(x)\n",
      "-5 1.48672E-06\n",
      "-4 0.00013383\n",
      "-3 0.004431848\n",
      "-2 0.053990967\n",
      "-1 0.241970725\n",
      "0 0.39894228\n",
      "1 0.241970725\n",
      "2 0.053990967...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 103, 'page_label': '104', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1035}\n",
      "\n",
      "--- Chunk 1036 ---\n",
      "Content:\n",
      "21.3. Learn a Gaussian Naive Bayes Model 95\n",
      "3 0.004431848\n",
      "4 0.00013383\n",
      "5 1.48672E-06\n",
      "Listing 21.2: Test of the Gaussian Probability Density Function.\n",
      "You can see that the mean has the highest probability of nearly 0.4 (40%). You can also see\n",
      "values that are far away from the mean like -5 and +5 (5 standard deviations from the mean)\n",
      "have a very low probability. Below is a plot of the probabilities values.\n",
      "Figure 21.2: Gaussian Probability Density Function.\n",
      "This function is really useful for Naive Bayes. We can assume that the input variables are\n",
      "each drawn from a Gaussian distribution. By calculating the mean and standard deviation\n",
      "of each input variable from the training data, we can use the Gaussian PDF to estimate the\n",
      "likelihood of each value for each class. We will see how this is used to calculate conditional\n",
      "probabilities in the next section.\n",
      "21.3 Learn a Gaussian Naive Bayes Model\n",
      "There are two types of probabilities that we need to summarize from our training data for the\n",
      "naive Bayes model:\n",
      " Class Probabilities.\n",
      " Conditional Probabilities.\n",
      "21.3.1 Class Probabilities\n",
      "The dataset is a two class problem and we already know the probability of each class because\n",
      "we contrived the dataset. Nevertheless, we can calculate the class probabilities for classes 0 and...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 104, 'page_label': '105', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1036}\n",
      "\n",
      "--- Chunk 1037 ---\n",
      "Content:\n",
      "21.4. Make Prediction with Gaussian Naive Bayes 96\n",
      "1 as follows:\n",
      "P(Y = 1) = count(Y = 1)\n",
      "count(Y = 0) + count(Y = 1)\n",
      "P(Y = 0) = count(Y = 0)\n",
      "count(Y = 0) + count(Y = 1)\n",
      "(21.2)\n",
      "or\n",
      "P(Y = 1) = 5\n",
      "5 + 5\n",
      "P(Y = 0) = 5\n",
      "5 + 5\n",
      "(21.3)\n",
      "This works out to be a probability of 0.5 (50%) for any given data instance belonging to\n",
      "class 0 or class 1.\n",
      "21.3.2 Conditional Probabilities\n",
      "The conditional probabilities are the probabilities of each input value given each class value.\n",
      "The conditional probabilities that need to be collected from the training data are as follows:\n",
      "P(X1|Y = 0)\n",
      "P(X1|Y = 1)\n",
      "P(X2|Y = 0)\n",
      "P(X2|Y = 1)\n",
      "(21.4)\n",
      "The X1 and X2 input variables are real values. As such we will model them as having being\n",
      "drawn from a Gaussian distribution. This will allow us to estimate the probability of a given\n",
      "value using the Gaussian PDF described above. The Gaussian PDF requires two parameters in\n",
      "addition to the value for which the probability is being estimated: the mean and the standard\n",
      "deviation. Therefore we must estimate the mean and the standard deviation for each group of\n",
      "conditional probabilities that we require. We can estimate these directly from the dataset. The\n",
      "results are summarized below.\n",
      "P(X1|Y=0) P(X1|Y=1) P(X2|Y=0) P(X2|Y=1)\n",
      "Mean 2.742014401 7.614652372 3.005468669 2.991467979\n",
      "Stdev 0.926568329 1.234432155 1.107329589 1.454193138\n",
      "Listing 21.3: Summary of population statistics by class.\n",
      "We now have enough information to make predictions for the training data or even a new\n",
      "dataset.\n",
      "21.4 Make Prediction with Gaussian Naive Bayes\n",
      "We can make predictions using Bayes Theorem, introduced and explained in a previous chapter.\n",
      "We don’t need a probability to predict the most likely class for a new data instance. We only\n",
      "need the numerator and the class that gives the largest response is the predicted response.\n",
      "MAP(h) = max(P(d|h) ×P(h)) (21.5)...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 105, 'page_label': '106', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1037}\n",
      "\n",
      "--- Chunk 1038 ---\n",
      "Content:\n",
      "21.5. Summary 97\n",
      "Let’s take the ﬁrst record from our dataset and use our learned model to predict which class\n",
      "we think it belongs. Instance: X1 = 3.393533211, X2 = 2.331273381, Y = 0. We can plug the\n",
      "probabilities for our model in for both classes and calculate the response. Starting with the\n",
      "response for the output class 0. We multiply the conditional probabilities together and multiply\n",
      "it by the probability of any instance belonging to the class.\n",
      "class 0 = P(pdf(X1)|class= 0) ×P(pdf(X2)|class= 0) ×P(class= 0)\n",
      "class 0 = 0.358838152 ×0.272650889 ×0.5\n",
      "class 0 = 0.048918771\n",
      "(21.6)\n",
      "We can perform the same calculation for class 1:\n",
      "class 1 = P(pdf(X1)|class= 1) ×P(pdf(X2)|class= 1) ×P(class= 1)\n",
      "class 1 = 4.10796E−07 ×0.173039018 ×0.5\n",
      "class 1 = 3.55418E−08\n",
      "(21.7)\n",
      "We can see that 0.048918771 is greater than 3.55418E-08, therefore we predict the class as 0\n",
      "for this instance, which is correct. Repeating this process for all instances in the dataset we get\n",
      "the following outcomes for class 0 and class 1. By selecting the class with the highest output we\n",
      "make accurate predictions for all instances in the training dataset.\n",
      "X1 X2 Output Y=0 Output Y=1 Prediction\n",
      "3.393533211 2.331273381 0.048918771 3.55418E-08 0\n",
      "3.110073483 1.781539638 0.02920928 1.82065E-09 0\n",
      "1.343808831 3.368360954 0.030910813 3.7117E-15 0\n",
      "3.582294042 4.67917911 0.010283134 9.08728E-09 0\n",
      "2.280362439 2.866990263 0.069951664 1.67539E-11 0\n",
      "7.423436942 4.696522875 1.10289E-06 0.001993521 1\n",
      "5.745051997 3.533989803 0.001361494 0.002264317 1\n",
      "9.172168622 2.511101045 1.30731E-09 0.00547065 1\n",
      "7.792783481 3.424088941 1.22227E-06 0.0355024 1\n",
      "7.939820817 0.791637231 3.53132E-08 0.000245214 1\n",
      "Listing 21.4: Predictions using Gaussian Naive Bayes.\n",
      "The prediction accuracy is 100%, as was expected given the clear separation of the classes.\n",
      "21.5 Summary\n",
      "In this chapter you discovered how to implement the Gaussian Naive Bayes classiﬁer from\n",
      "scratch. You learned about:\n",
      " The Gaussian Probability Density Function for estimating the probability of any given\n",
      "real value.\n",
      " How to estimate the probabilities required by the Naive Bayes model from a training\n",
      "dataset.\n",
      " How to use the learned Naive Bayes model to make predictions.\n",
      "You now know how to implement Gaussian Naive Bayes from scratch for real-valued data.\n",
      "In the next chapter you will discover the K-Nearest Neighbors algorithm for classiﬁcation and\n",
      "regression....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 106, 'page_label': '107', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1038}\n",
      "\n",
      "--- Chunk 1039 ---\n",
      "Content:\n",
      "Chapter 22\n",
      "K-Nearest Neighbors\n",
      "In this chapter you will discover the k-Nearest Neighbors (KNN) algorithm for classiﬁcation\n",
      "and regression. After reading this chapter you will know.\n",
      " The model representation used by KNN.\n",
      " How a model is learned using KNN (hint, it’s not).\n",
      " How to make predictions using KNN\n",
      " The many names for KNN including how diﬀerent ﬁelds refer to it.\n",
      " How to prepare your data to get the most from KNN.\n",
      " Where to look to learn more about the KNN algorithm.\n",
      "Let’s get started.\n",
      "22.1 KNN Model Representation\n",
      "The model representation for KNN is the entire training dataset. It is as simple as that. KNN\n",
      "has no model other than storing the entire dataset, so there is no learning required. Eﬃcient\n",
      "implementations can store the data using complex data structures like k-d trees to make look-up\n",
      "and matching of new patterns during prediction eﬃcient. Because the entire training dataset is\n",
      "stored, you may want to think carefully about the consistency of your training data. It might be\n",
      "a good idea to curate it, update it often as new data becomes available and remove erroneous\n",
      "and outlier data.\n",
      "22.2 Making Predictions with KNN\n",
      "KNN makes predictions using the training dataset directly. Predictions are made for a new\n",
      "data point by searching through the entire training set for the K most similar instances (the\n",
      "neighbors) and summarizing the output variable for those K instances. For regression this might\n",
      "be the mean output variable, in classiﬁcation this might be the mode (or most common) class\n",
      "value.\n",
      "98...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 107, 'page_label': '108', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1039}\n",
      "\n",
      "--- Chunk 1040 ---\n",
      "Content:\n",
      "22.2. Making Predictions with KNN 99\n",
      "To determine which of the K instances in the training dataset are most similar to a new\n",
      "input a distance measure is used. For real-valued input variables, the most popular distance\n",
      "measure is Euclidean distance. Euclidean distance is calculated as the square root of the sum of\n",
      "the squared diﬀerences between a point a and point b across all input attributes i.\n",
      "EuclideanDistance(a,b) =\n",
      "√\n",
      "n∑\n",
      "i=1\n",
      "(ai −bi)2 (22.1)\n",
      "Other popular distance measures include:\n",
      " Hamming Distance: Calculate the distance between binary vectors.\n",
      " Manhattan Distance: Calculate the distance between real vectors using the sum of their\n",
      "absolute diﬀerence. Also called City Block Distance.\n",
      " Minkowski Distance: Generalization of Euclidean and Manhattan distance.\n",
      "There are many other distance measures that can be used, such as Tanimoto, Jaccard,\n",
      "Mahalanobis and cosine distance. You can choose the best distance metric based on the\n",
      "properties of your data. If you are unsure, you can experiment with diﬀerent distance metrics\n",
      "and diﬀerent values of K together and see which mix results in the most accurate models.\n",
      "Euclidean is a good distance measure to use if the input variables are similar in type (e.g.\n",
      "all measured widths and heights). Manhattan distance is a good measure to use if the input\n",
      "variables are not similar in type (such as age, gender, height, etc.).\n",
      "The value for K can be found by algorithm tuning. It is a good idea to try many diﬀerent\n",
      "values for K (e.g. values from 1 to 21) and see what works best for your problem. The\n",
      "computational complexity of KNN increases with the size of the training dataset. For very large\n",
      "training sets, KNN can be made stochastic by taking a sample from the training dataset from\n",
      "which to calculate the K-most similar instances. KNN has been around for a long time and has\n",
      "been very well studied. As such, diﬀerent disciplines have diﬀerent names for it, for example:\n",
      " Instance-Based Learning: The raw training instances are used to make predictions. As\n",
      "such KNN is often referred to as instance-based learning or a case-based learning (where\n",
      "each training instance is a case from the problem domain).\n",
      " Lazy Learning: No learning of the model is required and all of the work happens at\n",
      "the time a prediction is requested. As such, KNN is often referred to as a lazy learning\n",
      "algorithm.\n",
      " Nonparametric: KNN makes no assumptions about the functional form of the problem\n",
      "being solved. As such KNN is referred to as a nonparametric machine learning algorithm.\n",
      "KNN can be used for regression and classiﬁcation problems.\n",
      "22.2.1 KNN for Regression\n",
      "When KNN is used for regression problems the prediction is based on the mean or the median\n",
      "of the K-most similar instances....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 108, 'page_label': '109', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1040}\n",
      "\n",
      "--- Chunk 1041 ---\n",
      "Content:\n",
      "22.3. Curse of Dimensionality 100\n",
      "22.2.2 KNN for Classiﬁcation\n",
      "When KNN is used for classiﬁcation, the output can be calculated as the class with the highest\n",
      "frequency from the K-most similar instances. Each instance in essence votes for their class and\n",
      "the class with the most votes is taken as the prediction. Class probabilities can be calculated\n",
      "as the normalized frequency of samples that belong to each class in the set of K most similar\n",
      "instances for a new data instance. For example, in a binary classiﬁcation problem (class is 0 or\n",
      "1):\n",
      "p(class= 0) = count(class= 0)\n",
      "count(class= 0) + count(class= 1) (22.2)\n",
      "If you are using K and you have an even number of classes (e.g. 2) it is a good idea to choose\n",
      "a K value with an odd number to avoid a tie. And the inverse, use an even number for K when\n",
      "you have an odd number of classes. Ties can be broken consistently by expanding K by 1 and\n",
      "looking at the class of the next most similar instance in the training dataset.\n",
      "22.3 Curse of Dimensionality\n",
      "KNN works well with a small number of input variables ( p), but struggles when the number\n",
      "of inputs is very large. Each input variable can be considered a dimension of a p-dimensional\n",
      "input space. For example, if you had two input variables X1 and X2, the input space would be\n",
      "2-dimensional. As the number of dimensions increases the volume of the input space increases\n",
      "at an exponential rate. In high dimensions, points that may be similar may have very large\n",
      "distances. All points will be far away from each other and our intuition for distances in simple\n",
      "2 and 3-dimensional spaces breaks down. This might feel unintuitive at ﬁrst, but this general\n",
      "problem is called the Curse of Dimensionality.\n",
      "22.4 Preparing Data For KNN\n",
      " Rescale Data: KNN performs much better if all of the data has the same scale. Normal-\n",
      "izing your data to the range between 0 and 1 is a good idea. It may also be a good idea\n",
      "to standardize your data if it has a Gaussian distribution.\n",
      " Address Missing Data : Missing data will mean that the distance between samples\n",
      "cannot be calculated. These samples could be excluded or the missing values could be\n",
      "imputed.\n",
      " Lower Dimensionality: KNN is suited for lower dimensional data. You can try it on\n",
      "high dimensional data (hundreds or thousands of input variables) but be aware that it\n",
      "may not perform as well as other techniques. KNN can beneﬁt from feature selection that\n",
      "reduces the dimensionality of the input feature space.\n",
      "22.5 Summary\n",
      "In this chapter you discovered the KNN machine learning algorithm. You learned that:...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 109, 'page_label': '110', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1041}\n",
      "\n",
      "--- Chunk 1042 ---\n",
      "Content:\n",
      "22.5. Summary 101\n",
      " KNN stores the entire training dataset which it uses as its representation.\n",
      " KNN does not learn any model.\n",
      " KNN makes predictions just-in-time by calculating the similarity between an input sample\n",
      "and each training instance.\n",
      " There are many distance measures to choose from to match the structure of your input\n",
      "data.\n",
      " That it is a good idea to rescale your data, such as using normalization, when using KNN.\n",
      "You now know about the K-Nearest Neighbors algorithm for classiﬁcation and regression. In\n",
      "the next chapter you will discover how to implement KNN from scratch for classiﬁcation....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 110, 'page_label': '111', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1042}\n",
      "\n",
      "--- Chunk 1043 ---\n",
      "Content:\n",
      "Chapter 23\n",
      "K-Nearest Neighbors Tutorial\n",
      "The K-Nearest Neighbors (KNN) algorithm is very simple and very eﬀective. In this chapter\n",
      "you will discover exactly how to implement it from scratch, step-by-step. After reading this\n",
      "chapter you will know:\n",
      " How to calculate the Euclidean distance between real valued vectors.\n",
      " How to use Euclidean distance and the training dataset to make predictions for new data.\n",
      "Let’s get started.\n",
      "23.1 Tutorial Dataset\n",
      "The problem is a binary (two-class) classiﬁcation problem. This problem was contrived for this\n",
      "tutorial. The dataset contains two input variables ( X1 and X1) and the class output variable\n",
      "with the values 0 and 1. The dataset contains 10 records, 5 that belong to each class.\n",
      "X1 X2 Y\n",
      "3.393533211 2.331273381 0\n",
      "3.110073483 1.781539638 0\n",
      "1.343808831 3.368360954 0\n",
      "3.582294042 4.67917911 0\n",
      "2.280362439 2.866990263 0\n",
      "7.423436942 4.696522875 1\n",
      "5.745051997 3.533989803 1\n",
      "9.172168622 2.511101045 1\n",
      "7.792783481 3.424088941 1\n",
      "7.939820817 0.791637231 1\n",
      "Listing 23.1: KNN Tutorial Data Set.\n",
      "You can see that the data for each class is quite separated. This is intentionally to make the\n",
      "problem easy to work with so that we can focus on the learning algorithm.\n",
      "23.2 KNN and Euclidean Distance\n",
      "KNN uses a distance measure to locate the K most similar instances from the training dataset\n",
      "when making a prediction. The distance measure selected must respect the structure of the\n",
      "102...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 111, 'page_label': '112', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1043}\n",
      "\n",
      "--- Chunk 1044 ---\n",
      "Content:\n",
      "23.2. KNN and Euclidean Distance 103\n",
      "Figure 23.1: KNN Tutorial Dataset Scatter plot.\n",
      "problem so that data instances that are close to each other according to the distance measure\n",
      "also belong to the same class. The most common distance measure for real values that have the\n",
      "same units or scale is the Euclidean distance. Euclidean distance is calculated as the square root\n",
      "of the sum of the squared diﬀerences between a point a and point b across all input attributes i.\n",
      "EuclideanDistance(a,b) =\n",
      "√\n",
      "n∑\n",
      "i=1\n",
      "(ai −bi)2 (23.1)\n",
      "To make this concrete, we will work through the calculation of the Euclidean distance for\n",
      "two instances from our dataset.\n",
      "Instance X1 X2\n",
      "1 3.393533211 2.331273381\n",
      "2 3.110073483 1.781539638\n",
      "Listing 23.2: Two Instances For Calculating Euclidean Distance.\n",
      "The ﬁrst step is to calculate the squared diﬀerence for each attribute:\n",
      "SquaredDifference 1 = (X11 −X12)2\n",
      "SquaredDifference 2 = (X21 −X22)2 (23.2)\n",
      "or\n",
      "SquaredDifference 1 = (3.393533211 −3.110073483)2\n",
      "SquaredDifference 2 = (2.331273381 −1.781539638)2 (23.3)\n",
      "or\n",
      "SquaredDifference 1 = 0.08034941698\n",
      "SquaredDifference 2 = 0.3022071889 (23.4)...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 112, 'page_label': '113', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1044}\n",
      "\n",
      "--- Chunk 1045 ---\n",
      "Content:\n",
      "23.3. Making Predictions with KNN 104\n",
      "We calculate the sum of these squared diﬀerences as:\n",
      "SumSquaredDifference = SquaredDifference 1 + SquaredDifference 2\n",
      "SumSquaredDifference = 0.080349417 + 0.302207188\n",
      "SumSquaredDifference = 0.382556606\n",
      "(23.5)\n",
      "Finally, we need to take the square root of the sum. This will convert the units of the\n",
      "diﬀerence between the data instances (real vectors) from squared units to their original units.\n",
      "Distance=\n",
      "√\n",
      "SumSquaredDifference\n",
      "Distance= 0.618511605 (23.6)\n",
      "This ﬁnal step can be skipped for performance reasons. You probably don’t need the distance\n",
      "in the actual units, and the square root function is relatively expensive compared to other\n",
      "operations and will be performed many times per new data instance that is to be classiﬁed.\n",
      "Now that you know how the Euclidean distance measure is calculated, we can use it with the\n",
      "dataset to make predictions for new data.\n",
      "23.3 Making Predictions with KNN\n",
      "Given a new data instance for which we would like to make a prediction, the K instances with\n",
      "the smallest distance to the new data instance are chosen to contribute to that prediction. For\n",
      "classiﬁcation, this involves allowing each of the K members to vote of which class the new data\n",
      "instance belongs. To make this concrete, we will work through making a prediction for a new\n",
      "data instance using the training dataset as the model. The new data instance is listed below.\n",
      "We are cheating because we contrived the dataset, we know what class the instance should be\n",
      "allocated. Instance: X1 = 8.093607318, X2 = 3.365731514, Y = 1.\n",
      "The ﬁrst step is to calculate the Euclidean distance between the new input instance and\n",
      "all instances in the training dataset. The table below lists the distance between each training\n",
      "instance and the new data.\n",
      "No. X1 X2 Y (X1-X1)^2 (X2-X2)^2 Sum Distance\n",
      "1 3.393533211 2.331273381 0 22.09069661 1.070103629 23.16080024 4.812566908\n",
      "2 3.110073483 1.781539638 0 24.83560948 2.5096639 27.34527338 5.229270827\n",
      "3 1.343808831 3.368360954 0 45.55977962 6.91395E-06 45.55978653 6.749798999\n",
      "4 3.582294042 4.679179110 0 20.35194747 1.725144587 22.07709206 4.698626614\n",
      "5 2.280362439 2.866990263 0 33.79381602 0.248742835 34.04255886 5.834600146\n",
      "6 7.423436942 4.696522875 1 0.449128333 1.771005647 2.220133979 1.490011402\n",
      "7 5.745051997 3.533989803 1 5.515712096 0.028310852 5.544022948 2.354574897\n",
      "8 9.172168622 2.511101045 1 1.163294486 0.730393239 1.893687725 1.376113268\n",
      "9 7.792783481 3.424088941 1 0.090494981 0.003405589 0.09390057 0.306431999\n",
      "10 7.939820817 0.791637231 1 0.023650288 6.625961377 6.649611665 2.578684096\n",
      "Listing 23.3: Euclidean Distances for Training Data to New Instance.\n",
      "We will set K to 3 and choose the 3 most similar neighbors to the data instance. A value of\n",
      "K = 3 is small and easy to use in this example, it is also an odd number, meaning that when\n",
      "the neighbors vote on the output class, that we cannot have a tie. The K = 3 most similar\n",
      "neighbors to the new data instance are:...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 113, 'page_label': '114', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1045}\n",
      "\n",
      "--- Chunk 1046 ---\n",
      "Content:\n",
      "23.4. Summary 105\n",
      "No. Distance Y\n",
      "9 0.306431999 1\n",
      "8 1.376113268 1\n",
      "6 1.490011402 1\n",
      "Listing 23.4: K=3 Most Similar Neighbors To New Instance.\n",
      "Making a prediction is as easy as selecting the majority class in the neighbors. Because we\n",
      "are using 0 and 1 for the class values, we can use the MODE() statistical function in a spreadsheet\n",
      "to return the most frequent value.\n",
      "prediction= mode(class(i)) (23.7)\n",
      "In this case all 3 neighbors have a class of 1, therefore the prediction for this instance is 1,\n",
      "which is correct.\n",
      "23.4 Summary\n",
      "In this chapter you discovered how you can use K-Nearest Neighbors to make predictions on a\n",
      "binary classiﬁcation problem. You learned about:\n",
      " The Euclidean distance measure and how to calculate it step-by-step.\n",
      " How to use the Euclidean distance to locate the nearest neighbors for a new data instance.\n",
      " How to make a prediction from the K-nearest neighbors.\n",
      "You now know how to implement K-Nearest Neighbors from scratch for classiﬁcation. In the\n",
      "next chapter you will discover an extension to KNN called Learning Vector Quantization for\n",
      "classiﬁcation....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 114, 'page_label': '115', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1046}\n",
      "\n",
      "--- Chunk 1047 ---\n",
      "Content:\n",
      "Chapter 24\n",
      "Learning Vector Quantization\n",
      "A downside of K-Nearest Neighbors is that you need to hang on to your entire training dataset.\n",
      "The Learning Vector Quantization algorithm (or LVQ for short) is an artiﬁcial neural network\n",
      "algorithm that allows you choose how many training instances to hang onto and learns exactly\n",
      "what those instances should look like. In this chapter you will discover the Learning Vector\n",
      "Quantization algorithm. After reading this chapter you will know:\n",
      " The representation used by the LVQ algorithm that you actually save to a ﬁle.\n",
      " The procedure that you can use to make predictions with a learned LVQ model.\n",
      " How to learn an LVQ model from training data.\n",
      " The data preparation to use to get the best performance from the LVQ algorithm.\n",
      " Where to look for more information on LVQ.\n",
      "Let’s get started.\n",
      "24.1 LVQ Model Representation\n",
      "The representation for LVQ is a collection of codebook vectors. LVQ was developed and is best\n",
      "understood as a classiﬁcation algorithm. It supports both binary (two-class) and multiclass\n",
      "classiﬁcation problems. A codebook vector is a list of numbers that have the same input and\n",
      "output attributes as your training data. For example, if your problem is a binary classiﬁcation\n",
      "with classes 0 and 1, and the inputs width, length height, then a codebook vector would be\n",
      "comprised of all four attributes: width, length, height and class.\n",
      "The model representation is a ﬁxed pool of codebook vectors, learned from the training data.\n",
      "They look like training instances, but the values of each attribute have been adapted based on\n",
      "the learning procedure. In the language of neural networks, each codebook vector may be called\n",
      "a neuron, each attribute on a codebook vector is called a weight and the collection of codebook\n",
      "vectors is called a network.\n",
      "106...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 115, 'page_label': '116', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1047}\n",
      "\n",
      "--- Chunk 1048 ---\n",
      "Content:\n",
      "24.2. Making Predictions with an LVQ Model 107\n",
      "24.2 Making Predictions with an LVQ Model\n",
      "Predictions are made using the LVQ codebook vectors in the same way as K-Nearest Neighbors.\n",
      "Predictions are made for a new instance by searching through all codebook vectors for the\n",
      "K most similar instances and summarizing the output variable for those K instances. For\n",
      "classiﬁcation this is the mode (or most common) class value. Typically predictions are made\n",
      "with K = 1, and the codebook vector that matches is called the Best Matching Unit (BMU).\n",
      "To determine which of the K instances in the training dataset are most similar to a new\n",
      "input a distance measure is used. For real-valued input variables, the most popular distance\n",
      "measure is Euclidean distance. Euclidean distance is calculated as the square root of the sum of\n",
      "the squared diﬀerences between a point a and point b across all input attributes i.\n",
      "EuclideanDistance(a,b) =\n",
      "√\n",
      "n∑\n",
      "i=1\n",
      "(ai −bi)2 (24.1)\n",
      "24.3 Learning an LVQ Model From Data\n",
      "The LVQ algorithm learns the codebook vectors from the training data. You must choose\n",
      "the number of codebook vectors to use, such as 20 or 40. You can ﬁnd the best number of\n",
      "codebook vectors to use by testing diﬀerent conﬁgurations on your training dataset. The learning\n",
      "algorithm starts with a pool of random codebook vectors. These could be randomly selected\n",
      "instances from the training data, or randomly generated vectors with the same scale as the\n",
      "training data. Codebook vectors have the same number of input attributes as the training data.\n",
      "They also have an output class variable.\n",
      "The instances in the training dataset are processed one at a time. For a given training\n",
      "instance, the most similar codebook vector is selected from the pool. If the codebook vector has\n",
      "the same output as the training instance, the codebook vector is moved closer to the training\n",
      "instance. If it does not match, it is moved further away. The amount that the vector is moved is\n",
      "controlled by an algorithm parameter called the learning rate ( alpha). For example, the input\n",
      "variable (x) of a codebook vector is moved closer to the training input value ( t) by the amount\n",
      "in the learning rate ( alpha) if the classes match as follows:\n",
      "x= x+ alpha×(t−x) (24.2)\n",
      "The opposite case of moving the input variables of a codebook variable away from a training\n",
      "instance is calculated as:\n",
      "x= x−alpha×(t−x) (24.3)\n",
      "This would be repeated for each input variable. Because one codebook vector is selected for\n",
      "modiﬁcation for each training instance the algorithm is referred to as a winner-take-all algorithm\n",
      "or a type of competitive learning. This process is repeated for each instance in the training\n",
      "dataset. One iteration of the training dataset is called an epoch. The process is completed for a\n",
      "number of epochs that you must choose (MaxEpoch), such as 200, 2000 or 20,000.\n",
      "You must also choose an initial learning rate (such as alpha = 0.3). The learning rate is\n",
      "decreased with the epoch, starting at the large value you specify at epoch 1 which makes the\n",
      "most change to the codebook vectors and ﬁnishing with a small value near zero on the last...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 116, 'page_label': '117', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1048}\n",
      "\n",
      "--- Chunk 1049 ---\n",
      "Content:\n",
      "24.4. Preparing Data For LVQ 108\n",
      "epoch making very minor changes to the codebook vectors. The learning rate for each epoch is\n",
      "calculated as:\n",
      "LearningRate = alpha×(1 − Epoch\n",
      "MaxEpoch) (24.4)\n",
      "Where LearningRate is the learning rate for the current epoch (0 to MaxEpoch-1), alpha\n",
      "is the learning rate speciﬁed to the algorithm at the start of the training run and MaxEpoch\n",
      "is the total number of epochs to run the algorithm also speciﬁed at the start of the run. The\n",
      "intuition for the learning process is that the pool of codebook vectors is a compression of the\n",
      "training dataset to the points that best characterize the separation of the classes.\n",
      "24.4 Preparing Data For LVQ\n",
      "Generally, it is a good idea to prepare data for LVQ in the same way as you would prepare it\n",
      "for K-Nearest Neighbors.\n",
      " Classiﬁcation: LVQ is a classiﬁcation algorithm that works for both binary (two-class)\n",
      "and multiclass classiﬁcation algorithms. The technique has been adapted for regression.\n",
      " Multiple-Passes: Good technique with LVQ involves performing multiple passes of the\n",
      "training dataset over the codebook vectors (e.g. multiple learning runs). The ﬁrst with\n",
      "a higher learning rate to settle the pool of codebook vectors and the second run with a\n",
      "small learning rate to ﬁne tune the vectors.\n",
      " Multiple Best Matches : Extensions of LVQ select multiple best matching units to\n",
      "modify during learning, such as one of the same class and one of a diﬀerent class which\n",
      "are drawn toward and away from a training sample respectively. Other extensions use a\n",
      "custom learning rate for each codebook vector. These extensions can improve the learning\n",
      "process.\n",
      " Normalize Inputs: Traditionally, inputs are normalized (rescaled) to values between 0\n",
      "and 1. This is to avoid one attribute from dominating the distance measure. If the input\n",
      "data is normalized, then the initial values for the codebook vectors can be selected as\n",
      "random values between 0 and 1.\n",
      " Feature Selection: Feature selection that can reduce the dimensionality of the input\n",
      "variables can improve the accuracy of the method. LVQ suﬀers from the same curse of\n",
      "dimensionality in making predictions as K-Nearest Neighbors.\n",
      "24.5 Summary\n",
      "In this chapter you discovered the LVQ algorithm. You learned:\n",
      " The representation for LVQ is a small pool of codebook vectors, smaller than the training\n",
      "dataset.\n",
      " The codebook vectors are used to make predictions using the same technique as K-Nearest\n",
      "Neighbors....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 117, 'page_label': '118', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1049}\n",
      "\n",
      "--- Chunk 1050 ---\n",
      "Content:\n",
      "24.5. Summary 109\n",
      " The codebook vectors are learned from the training dataset by moving them closer when\n",
      "they are good match and further away when they are a bad match.\n",
      " The codebook vectors are a compression of the training data to best separate the classes.\n",
      " Data preparation traditionally involves normalizing the input values to the range between\n",
      "0 and 1.\n",
      "You now know about the Learning Vector Quantization algorithm for classiﬁcation. In the\n",
      "next chapter you will discover how you can implement LVQ from scratch....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 118, 'page_label': '119', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1050}\n",
      "\n",
      "--- Chunk 1051 ---\n",
      "Content:\n",
      "Chapter 25\n",
      "Learning Vector Quantization Tutorial\n",
      "The Learning Vector Quantization (LVQ) algorithm is a lot like the K-Nearest Neighbors\n",
      "algorithm, but it involves learning. In this chapter you will discover how to implement the LVQ\n",
      "algorithm from scratch. After reading this chapter you will know:\n",
      " How to initialize an LVQ model.\n",
      " How to Update the Best Matching Unit for a training instance.\n",
      " How to update the LVQ model for one and multiple epochs.\n",
      " How to use a learned LVQ model to make predictions.\n",
      "Let’s get started.\n",
      "25.1 Tutorial Dataset\n",
      "The problem is a binary (two-class) classiﬁcation problem. The problem was contrived for this\n",
      "tutorial. The dataset contains two input variables ( X1 and X2) and the class output variable\n",
      "with the values 0 and 1. The dataset contains 10 records, 5 that belong to each class.\n",
      "X1 X2 Y\n",
      "3.393533211 2.331273381 0\n",
      "3.110073483 1.781539638 0\n",
      "1.343808831 3.368360954 0\n",
      "3.582294042 4.67917911 0\n",
      "2.280362439 2.866990263 0\n",
      "7.423436942 4.696522875 1\n",
      "5.745051997 3.533989803 1\n",
      "9.172168622 2.511101045 1\n",
      "7.792783481 3.424088941 1\n",
      "7.939820817 0.791637231 1\n",
      "Listing 25.1: LVQ Tutorial Data Set.\n",
      "You can see that the data for each class is quite separated. This is intentional to make the\n",
      "problem easy to work with so that we can focus on the learning algorithm.\n",
      "110...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 119, 'page_label': '120', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1051}\n",
      "\n",
      "--- Chunk 1052 ---\n",
      "Content:\n",
      "25.2. Learn the LVQ Model 111\n",
      "Figure 25.1: LVQ Tutorial Dataset Scatter plot.\n",
      "25.2 Learn the LVQ Model\n",
      "LVQ learns a population of codebook vectors from the training data. This section is broken up\n",
      "into 3 parts:\n",
      "1. Initial Codebook Vectors.\n",
      "2. Update Codebook Vectors for One Pattern.\n",
      "3. Update For One Epoch.\n",
      "25.2.1 Initial Codebook Vectors\n",
      "The number of codebook vectors often depends on the size and complexity of the problem.\n",
      "Because we are working with a simple problem and for demonstration purposes we will select a\n",
      "small number of codebook vectors of 4, two of each class. The values for the vectors can be\n",
      "chosen randomly or selected from the input data. We will use the latter. The table below lists\n",
      "the selected codebook vectors:\n",
      "X1 X2 Y\n",
      "3.582294042 0.791637231 0\n",
      "7.792783481 2.331273381 0\n",
      "7.939820817 2.866990263 1\n",
      "3.393533211 4.67917911 1\n",
      "Listing 25.2: Initial LVQ codebook Vectors.\n",
      "25.2.2 Update Codebook Vectors for One Pattern\n",
      "In addition to choosing the number of codebook vectors, an initial learning rate must be\n",
      "speciﬁed. A good default value is 0.3, but values are typically between 0.1 and 0.5. The learning...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 120, 'page_label': '121', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1052}\n",
      "\n",
      "--- Chunk 1053 ---\n",
      "Content:\n",
      "25.2. Learn the LVQ Model 112\n",
      "rate is used to update the codebook vectors. In this section we will look at the rule used to\n",
      "update the codebook vectors for one training pattern. Let’s take the ﬁrst training pattern:\n",
      "X1 = 3.393533211, X2 = 2.331273381, Y = 0. The update rule for one pattern is as follows:\n",
      "1. Calculate the distance from the training pattern to each codebook vector.\n",
      "2. Select the most similar codebook vector, called the Best Matching Unit (BMU).\n",
      "3. Update the best matching unit to be closer to the training pattern if it has the same class,\n",
      "otherwise further away.\n",
      "Calculate Distance\n",
      "We can calculate the distance between a training instance and a codebook vector using Euclidean\n",
      "distance. This is the most common distance measure when all input attributes have the same\n",
      "scale, which they do in this case. Euclidean distance is calculated as the square root of the sum\n",
      "of the squared diﬀerences between a point a and point b across all input attributes i.\n",
      "EuclideanDistance(a,b) =\n",
      "√\n",
      "n∑\n",
      "i=1\n",
      "(ai −bi)2 (25.1)\n",
      "Let’s calculate the Euclidean distance between each codebook vector and the training\n",
      "instance. The results are listed below.\n",
      "(X1-X1)^2 (X2-X2)^2 Sum Distance BMU?\n",
      "0.035630651 2.370479474 2.406110125 1.551164119 YES\n",
      "19.35340294 0 19.35340294 4.39925027\n",
      "20.668731 0.286992578 20.95572357 4.577742192\n",
      "0 5.512661312 5.512661312 2.347905729\n",
      "Listing 25.3: Euclidean Distances of Codebook Vectors from Instance.\n",
      "From the distances, we can see that the codebook vector #1 has the smallest distance and\n",
      "is therefore the BMU.\n",
      "Update the Best Matching Unit\n",
      "The best matching unit need to be moved closer to the training pattern if it is the same class\n",
      "or further away if the classes diﬀer. The class for the BMU is 0 and this matches the class of\n",
      "the training instance 0. Therefore we need update the attributes to be closer to the training\n",
      "instance, limited by the learning rate. The learning rate controls how much change can be made\n",
      "to the codebook vectors for a single update. Let’s use an initial learning rate of 0.7 which is\n",
      "much larger than normal in order to show fast learning on this simple problem. The update\n",
      "procedure for a single attribute is therefore:\n",
      "bmuj = bmuj + alpha×(trainingj −bmuj) (25.2)\n",
      "Where j is the attribute on the codebook vector being updated (e.g. X1), alpha is the\n",
      "learning rate and trainingj is the same attribute on the training instance (e.g. X1). If the...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 121, 'page_label': '122', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1053}\n",
      "\n",
      "--- Chunk 1054 ---\n",
      "Content:\n",
      "25.3. Make Predictions with LVQ 113\n",
      "BMU had a diﬀerent class, the update would be almost identical except we would use a negative\n",
      "sign to push the BMU further away from the training instance. For example:\n",
      "bmuj = bmuj −alpha×(trainingj −bmuj) (25.3)\n",
      "Let’s apply the learning rule and update the attributes of the BMU.\n",
      "X1 = 3.582294042 + 0.7 ×(3.393533211 −3.582294042)\n",
      "X2 = 0.791637231 + 0.7 ×(2.331273381 −0.791637231) (25.4)\n",
      "or\n",
      "X1 = 3.45016146\n",
      "X2 = 1.869382536 (25.5)\n",
      "We have just completed an update for one training instance.\n",
      "25.2.3 Update For One Epoch\n",
      "An epoch is one pass through the entire training dataset (all 10 instances). This involves\n",
      "applying the above procedure for each training instance, locating the BMU and updating it.\n",
      "Before the start of the run we must choose the number of epochs to perform in order to train\n",
      "the model. The number could be hundreds, thousands or even tens of thousands of epochs,\n",
      "depending on the diﬃculty of the problem. Each epoch, the learning rate is decreased from the\n",
      "starting value. This means that at the start of the run the model is doing a lot of learning and\n",
      "towards the end of the run it is only doing very small adjustments to codebook vectors already\n",
      "learned. The learning rate can be calculated for a given epoch as follows:\n",
      "LearningRate = alpha×(1 − Epoch\n",
      "MaxEpoch) (25.6)\n",
      "Where LearningRate is the learning rate for the current epoch (0 to MaxEpoch-1), alpha\n",
      "is the learning rate speciﬁed to the algorithm at the start of the training run and MaxEpoch is\n",
      "the total number of epochs to run the algorithm also speciﬁed at the start of the run.\n",
      "We end up with the following codebook vectors:\n",
      "X1 X2 Y\n",
      "2.55988367 2.549260936 0\n",
      "6.048389028 3.195023766 0\n",
      "7.343461045 3.512289796 1\n",
      "5.700572642 6.239052716 1\n",
      "Listing 25.4: Updated Codebook Vector Values.\n",
      "The process probably needs to be repeated for another 10-to-20 epochs with a lower learning\n",
      "rate (e.g. 0.3) to settle down the codebook vectors.\n",
      "25.3 Make Predictions with LVQ\n",
      "Once the codebook vectors are learned, they can be used to make predictions. We can use the\n",
      "same procedure as KNN to make predictions, although K is set to 1. Just like in the learning...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 122, 'page_label': '123', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1054}\n",
      "\n",
      "--- Chunk 1055 ---\n",
      "Content:\n",
      "25.4. Summary 114\n",
      "process, we use a distance measure to locate the BMU for a new data instance. Instead of\n",
      "updating the BMU, we return the class value which becomes the prediction for our model. Using\n",
      "the codebook vectors prepared above, and the Euclidean distance measure, we can make the\n",
      "following predictions for each instance in the dataset:\n",
      "X1 X2 Prediction Y\n",
      "3.393533211 2.331273381 0 0\n",
      "3.110073483 1.781539638 0 0\n",
      "1.343808831 3.368360954 0 0\n",
      "3.582294042 4.67917911 0 0\n",
      "2.280362439 2.866990263 0 0\n",
      "7.423436942 4.696522875 1 1\n",
      "5.745051997 3.533989803 0 1\n",
      "9.172168622 2.511101045 1 1\n",
      "7.792783481 3.424088941 1 1\n",
      "7.939820817 0.791637231 1 1\n",
      "Listing 25.5: Predictions with Codebook Vectors.\n",
      "If we compare this to the actual values in the dataset for Y, we can see some errors. The\n",
      "classiﬁcation accuracy can be calculated as:\n",
      "accuracy = count(correct)\n",
      "count(instances) ×100\n",
      "accuracy = 9\n",
      "10 ×100\n",
      "accuracy = 90%\n",
      "(25.7)\n",
      "With some more training the codebook vectors will become more accurate.\n",
      "25.4 Summary\n",
      "In this chapter you discovered how to implement the LVQ machine learning algorithm from\n",
      "scratch for a binary classiﬁcation problem. You learned:\n",
      " How to initialize an LVQ model.\n",
      " How to Update the Best Matching Unit for a training instance.\n",
      " How to update the LVQ model for one and multiple epochs.\n",
      " How to use a learned LVQ model to make predictions.\n",
      "You now know how to implement Learning Vector Quantization from scratch for classiﬁcation.\n",
      "In the next chapter you will discover the Support Vector Machine machine learning algorithm\n",
      "for classiﬁcation....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 123, 'page_label': '124', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1055}\n",
      "\n",
      "--- Chunk 1056 ---\n",
      "Content:\n",
      "Chapter 26\n",
      "Support Vector Machines\n",
      "Support Vector Machines are perhaps one of the most popular and talked about machine learning\n",
      "algorithms. They were extremely popular around the time they were developed in the 1990s\n",
      "and continue to be the go-to method for a high-performing algorithm with little tuning. In this\n",
      "chapter you will discover the Support Vector Machine (SVM) machine learning algorithm. After\n",
      "reading this chapter you will know:\n",
      " How to disentangle the many names used to refer to support vector machines.\n",
      " The representation used by SVM when the model is actually stored on disk.\n",
      " How a learned SVM model representation can be used to make predictions for new data.\n",
      " How to learn an SVM model from training data.\n",
      " How to best prepare your data for the SVM algorithm.\n",
      " Where you might look to get more information on SVM.\n",
      "Let’s get started.\n",
      "26.1 Maximal-Margin Classiﬁer\n",
      "The Maximal-Margin Classiﬁer is a hypothetical classiﬁer that best explains how SVM works in\n",
      "practice. The numeric input variables ( x) in your data (the columns) form an n-dimensional\n",
      "space. For example, if you had two input variables, this would form a two-dimensional space. A\n",
      "hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to\n",
      "best separate the points in the input variable space by their class, either class 0 or class 1. In\n",
      "two-dimensions you can visualize this as a line and let’s assume that all of our input points can\n",
      "be completely separated by this line. For example:\n",
      "B0 + (B1 ×X1) + (B2 ×X2) = 0 (26.1)\n",
      "Where the coeﬃcients ( B1 and B2) that determine the slope of the line and the intercept\n",
      "(B0) are found by the learning algorithm, and X1 and X2 are the two input variables. You can\n",
      "make classiﬁcations using this line. By plugging in input values into the line equation, you can\n",
      "calculate whether a new point is above or below the line.\n",
      "115...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 124, 'page_label': '125', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1056}\n",
      "\n",
      "--- Chunk 1057 ---\n",
      "Content:\n",
      "26.2. Soft Margin Classiﬁer 116\n",
      " Above the line, the equation returns a value greater than 0 and the point belongs to the\n",
      "ﬁrst class (class 0).\n",
      " Below the line, the equation returns a value less than 0 and the point belongs to the\n",
      "second class (class 1).\n",
      " A value close to the line returns a value close to zero and the point may be diﬃcult to\n",
      "classify.\n",
      " If the magnitude of the value is large, the model may have more conﬁdence in the\n",
      "prediction.\n",
      "The distance between the line and the closest data points is referred to as the margin. The\n",
      "best or optimal line that can separate the two classes is the line that as the largest margin.\n",
      "This is called the Maximal-Margin hyperplane. The margin is calculated as the perpendicular\n",
      "distance from the line to only the closest points. Only these points are relevant in deﬁning\n",
      "the line and in the construction of the classiﬁer. These points are called the support vectors.\n",
      "They support or deﬁne the hyperplane. The hyperplane is learned from training data using an\n",
      "optimization procedure that maximizes the margin.\n",
      "26.2 Soft Margin Classiﬁer\n",
      "In practice, real data is messy and cannot be separated perfectly with a hyperplane. The\n",
      "constraint of maximizing the margin of the line that separates the classes must be relaxed. This\n",
      "is often called the soft margin classiﬁer. This change allows some points in the training data to\n",
      "violate the separating line. An additional set of coeﬃcients are introduced that give the margin\n",
      "wiggle room in each dimension. These coeﬃcients are sometimes called slack variables. This\n",
      "increases the complexity of the model as there are more parameters for the model to ﬁt to the\n",
      "data to provide this complexity.\n",
      "A tuning parameter is introduced called simply C that deﬁnes the magnitude of the wiggle\n",
      "allowed across all dimensions. The C parameters deﬁnes the amount of violation of the margin\n",
      "allowed. A C = 0 is no violation and we are back to the inﬂexible Maximal-Margin Classiﬁer\n",
      "described above. The larger the value of C the more violations of the hyperplane are permitted.\n",
      "During the learning of the hyperplane from data, all training instances that lie within the\n",
      "distance of the margin will aﬀect the placement of the hyperplane and are referred to as support\n",
      "vectors. And as C aﬀects the number of instances that are allowed to fall within the margin, C\n",
      "inﬂuences the number of support vectors used by the model.\n",
      " The smaller the value of C, the more sensitive the algorithm is to the training data (higher\n",
      "variance and lower bias).\n",
      " The larger the value of C, the less sensitive the algorithm is to the training data (lower\n",
      "variance and higher bias).\n",
      "26.3 Support Vector Machines (Kernels)\n",
      "The SVM algorithm is implemented in practice using a kernel. The learning of the hyperplane in\n",
      "linear SVM is done by transforming the problem using some linear algebra, which is out of the...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 125, 'page_label': '126', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1057}\n",
      "\n",
      "--- Chunk 1058 ---\n",
      "Content:\n",
      "26.3. Support Vector Machines (Kernels) 117\n",
      "scope of this introduction to SVM. A powerful insight is that the linear SVM can be rephrased\n",
      "using the inner product of any two given observations, rather than the observations themselves.\n",
      "The inner product between two vectors is the sum of the multiplication of each pair of input\n",
      "values. For example, the inner product of the vectors [2 ,3] and [5,6] is 2 ×5 + 3×6 or 28. The\n",
      "equation for making a prediction for a new input using the dot product between the input ( x)\n",
      "and each support vector ( xi) is calculated as follows:\n",
      "f(x) = B0 +\n",
      "n∑\n",
      "i=1\n",
      "(ai ×(x×xi)) (26.2)\n",
      "This is an equation that involves calculating the inner products of a new input vector ( x)\n",
      "with all support vectors in training data. The coeﬃcients B0 and ai (for each input) must be\n",
      "estimated from the training data by the learning algorithm.\n",
      "26.3.1 Linear Kernel SVM\n",
      "The dot-product is called the kernel and can be re-written as:\n",
      "K(x,xi) =\n",
      "∑\n",
      "(x×xi) (26.3)\n",
      "The kernel deﬁnes the similarity or a distance measure between new data and the support\n",
      "vectors. The dot product is the similarity measure used for linear SVM or a linear kernel because\n",
      "the distance is a linear combination of the inputs. Other kernels can be used that transform the\n",
      "input space into higher dimensions such as a Polynomial Kernel and a Radial Kernel. This is\n",
      "called the Kernel Trick. It is desirable to use more complex kernels as it allows lines to separate\n",
      "the classes that are curved or even more complex. This in turn can lead to more accurate\n",
      "classiﬁers.\n",
      "26.3.2 Polynomial Kernel SVM\n",
      "Instead of the dot-product, we can use a polynomial kernel, for example:\n",
      "K(x,xi) = 1 +\n",
      "∑\n",
      "(x×xi)d (26.4)\n",
      "Where the degree of the polynomial must be speciﬁed by hand to the learning algorithm.\n",
      "When d= 1 this is the same as the linear kernel. The polynomial kernel allows for curved lines\n",
      "in the input space.\n",
      "26.3.3 Radial Kernel SVM\n",
      "Finally, we can also have a more complex radial kernel. For example:\n",
      "K(x,xi) = e−gamma×∑((x−x2\n",
      "i ) (26.5)\n",
      "Where gamma is a parameter that must be speciﬁed to the learning algorithm. A good\n",
      "default value for gamma is 0.1, where gamma is often 0 < gamma <1. The radial kernel is\n",
      "very local and can create complex regions within the feature space, like closed polygons in a\n",
      "two-dimensional space....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 126, 'page_label': '127', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1058}\n",
      "\n",
      "--- Chunk 1059 ---\n",
      "Content:\n",
      "26.4. How to Learn a SVM Model 118\n",
      "26.4 How to Learn a SVM Model\n",
      "The SVM model needs to be solved using an optimization procedure. You can use a numerical\n",
      "optimization procedure to search for the coeﬃcients of the hyperplane. This is ineﬃcient and\n",
      "is not the approach used in widely used SVM implementations like LIBSVM. If implementing\n",
      "the algorithm as an exercise, you could use a variation of gradient descent called sub-gradient\n",
      "descent.\n",
      "There are specialized optimization procedures that re-formulate the optimization problem\n",
      "to be a Quadratic Programming problem. The most popular method for ﬁtting SVM is the\n",
      "Sequential Minimal Optimization (SMO) method that is very eﬃcient. It breaks the problem\n",
      "down into sub-problems that can be solved analytically (by calculating) rather than numerically\n",
      "(by searching or optimizing).\n",
      "26.5 Preparing Data For SVM\n",
      "This section lists some suggestions for how to best prepare your training data when learning an\n",
      "SVM model.\n",
      " Numerical Inputs: SVM assumes that your inputs are numeric. If you have categorical\n",
      "inputs you may need to covert them to binary dummy variables (one variable for each\n",
      "category).\n",
      " Binary Classiﬁcation: Basic SVM as described in this chapter is intended for binary\n",
      "(two-class) classiﬁcation problems. Although, extensions have been developed for regression\n",
      "and multiclass classiﬁcation.\n",
      "26.6 Summary\n",
      "In this chapter you discovered the Support Vector Machine Algorithm for machine learning.\n",
      "You learned about:\n",
      " The Maximal-Margin Classiﬁer that provides a simple theoretical model for understanding\n",
      "SVM.\n",
      " The Soft Margin Classiﬁer which is a modiﬁcation of the Maximal-Margin Classiﬁer to\n",
      "relax the margin to handle noisy class boundaries in real data.\n",
      " Support Vector Machines and how the learning algorithm can be reformulated as a\n",
      "dot-product kernel and how other kernels like Polynomial and Radial can be used.\n",
      " How you can use numerical optimization to learn the hyperplane and that eﬃcient imple-\n",
      "mentations use an alternate optimization scheme called Sequential Minimal Optimization.\n",
      "You now know about the Support Vector Machine algorithm. In the next chapter you will\n",
      "discover how you can implement SVM from scratch using sub-gradient descent....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 127, 'page_label': '128', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1059}\n",
      "\n",
      "--- Chunk 1060 ---\n",
      "Content:\n",
      "Chapter 27\n",
      "Support Vector Machine Tutorial\n",
      "Support Vector Machines are a ﬂexible nonparametric machine learning algorithm. In this\n",
      "chapter you will discover how to implement the Support Vector Machine algorithm step-by-step\n",
      "using sub-gradient descent. After completing this chapter you will know:\n",
      " How to use sub-gradient descent to update the coeﬃcients for an SVM model.\n",
      " How to iterate the sub-gradient descent algorithm to learn an SVM model for training\n",
      "data.\n",
      " How to make predictions given a learned SVM model.\n",
      "Let’s get started.\n",
      "27.1 Tutorial Dataset\n",
      "A test problem was devised so that the classes are linearly separable. This means that a straight\n",
      "line can be drawn to separate the classes. This is intentional so that we can explore how to\n",
      "implement an SVM with a linear kernel (straight line). An assumption made by the SVM\n",
      "algorithm is that ﬁrst class value is −1 and the second class value is +1.\n",
      "X1 X2 Y\n",
      "2.327868056 2.458016525 -1\n",
      "3.032830419 3.170770366 -1\n",
      "4.485465382 3.696728111 -1\n",
      "3.684815246 3.846846973 -1\n",
      "2.283558563 1.853215997 -1\n",
      "7.807521179 3.290132136 1\n",
      "6.132998136 2.140563087 1\n",
      "7.514829366 2.107056961 1\n",
      "5.502385039 1.404002608 1\n",
      "7.432932365 4.236232628 1\n",
      "Listing 27.1: SVM Tutorial Data Set.\n",
      "The visualization below provides a scatter plot of the dataset.\n",
      "119...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 128, 'page_label': '129', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1060}\n",
      "\n",
      "--- Chunk 1061 ---\n",
      "Content:\n",
      "27.2. Training SVM With Gradient Descent 120\n",
      "Figure 27.1: SVM Tutorial Dataset Scatter plot.\n",
      "27.2 Training SVM With Gradient Descent\n",
      "This section describes the form of the SVM model and how it can be learned using a variant of\n",
      "the gradient descent optimization procedure.\n",
      "27.2.1 Form of Linear SVM Model\n",
      "The Linear SVM model is a line and the goal of the learning algorithm is to ﬁnd values for the\n",
      "coeﬃcients that best separates the classes. The line is typically in the form (grouping the terms\n",
      "for readability):\n",
      "B0 + (B1 ×X1) + (B2 ×X2) = 0 (27.1)\n",
      "Where B0, B1 and B2 are the coeﬃcients and X1 and X2 are the input variables. This\n",
      "will be the form of the equation that we will be using with one small modiﬁcation, we will drop\n",
      "the bias term ( B0) also called the oﬀset or the intercept. For example B0 = 0.\n",
      "(B1 ×X1) + (B2 ×X2) = 0 (27.2)\n",
      "This means that the line will pass through the origin ( X1 = 0 and X2 = 0). This is just to\n",
      "make the tutorial easier to follow and because our simple problem does not really need it, you\n",
      "can add the bias term back in if you like.\n",
      "27.2.2 SVM Optimization Method\n",
      "The optimization algorithm to ﬁnd the coeﬃcients can be stated as a quadratic programming\n",
      "problem. This is a type of constraint optimization where fast solvers can be used. We will not\n",
      "be using this approach in this tutorial. Another approach that can be used to discover the\n",
      "coeﬃcient values for Linear SVM is sub-gradient descent. In this method a random training\n",
      "pattern is selected each iteration and used to update the coeﬃcients. After a large number...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 129, 'page_label': '130', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1061}\n",
      "\n",
      "--- Chunk 1062 ---\n",
      "Content:\n",
      "27.3. Learn an SVM Model from Training Data 121\n",
      "of iterations (thousands or hundreds of thousands) the algorithm will settle on a stable set of\n",
      "coeﬃcients. The coeﬃcient update equation works as follows. First an output value is calculated\n",
      "as:\n",
      "output= Y ×(B1 ×X1) + (B2 ×X2) (27.3)\n",
      "Two diﬀerent update procedures are used depending on the output value. If the output\n",
      "value is greater than 1 it suggests that the training pattern was not a support vector. This\n",
      "means that the instance was not directly involved in calculating the output, in which case the\n",
      "weights are slightly decreased:\n",
      "b= (1 −1\n",
      "t) ×b (27.4)\n",
      "Where b is the weight that is being updated (such as B1 or B2), t is the current iteration\n",
      "(e.g. 1 for the ﬁrst update, 2 for the second and so on). If the output is less than 1 then it is\n",
      "assumed that the training instance is a support vector and must be updated to better explain\n",
      "the data.\n",
      "b= (1 −1\n",
      "t) ×b+ 1\n",
      "lambda×t ×(y×x) (27.5)\n",
      "Where b is the weight that is being updated, t is the current iteration and lambda is a\n",
      "parameter to the learning algorithm. The lambda is a learning parameter and is often set to\n",
      "very small values such as 0.0001 or smaller. The procedure is repeated until the error rate drops\n",
      "to a desirable level or for a very large ﬁxed number of iterations. Smaller learning rates ofter\n",
      "require much longer training times. The number of iterations is a downside to this learning\n",
      "algorithm.\n",
      "27.3 Learn an SVM Model from Training Data\n",
      "In this section we will work through a few updates to the coeﬃcients to demonstrate the SVM\n",
      "learning algorithm. We will use a very large lambda value: lambda= 0.45. This is unusually\n",
      "large and will force a lot of change on each update. Normally lambda values are very small.\n",
      "27.3.1 Learning Iteration #1\n",
      "We will start by setting the coeﬃcients to 0.0.\n",
      "B1 = 0.0\n",
      "B2 = 0.0 (27.6)\n",
      "We also need to keep track of which iteration we are on: t = 1. We will train the model\n",
      "using the order of the training patterns. Ideally, the order of the patterns would be randomized\n",
      "to avoid the learning algorithm getting stuck. The ﬁrst training pattern we will use to update...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 130, 'page_label': '131', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1062}\n",
      "\n",
      "--- Chunk 1063 ---\n",
      "Content:\n",
      "27.3. Learn an SVM Model from Training Data 122\n",
      "the coeﬃcients is: Instance: X1 = 2 .327868056, X2 = 2 .458016525, Y = −1. We can now\n",
      "calculate the output value for this iteration.\n",
      "output= Y ×(B1 ×X1) + (B2 ×X2)\n",
      "output= −1 ×(0.0 ×2.327868056) + (0.0 ×2.458016525)\n",
      "output= 0.0\n",
      "(27.7)\n",
      "Easy enough. The output is less than 1.0, therefore we will use the more complex update\n",
      "procedure that assumes the training pattern is a support vector:\n",
      "b= (1 −1\n",
      "t) ×b+ 1\n",
      "lambda×t ×(y×x)\n",
      "B1 = (1 −1\n",
      "1) ×0.0 + 1\n",
      "0.5 ×1 ×(−1 ×2.327868056)\n",
      "B1 = −5.173040124\n",
      "(27.8)\n",
      "And for B2 this is:\n",
      "B2 = (1 −1\n",
      "1) ×0.0 + 1\n",
      "0.5 ×1 ×(−1 ×2.458016525)\n",
      "B2 = −5.462258944\n",
      "(27.9)\n",
      "27.3.2 Learning Iteration #2\n",
      "We now have updated coeﬃcients that we can use on the next iteration of the learning\n",
      "algorithm with the second instance from the training dataset: Instance: X1 = 3.032830419,\n",
      "X2 = 3.170770366, Y = −1. Again, we must keep track of the iteration: t= 2. Let’s repeat the\n",
      "process.\n",
      "B1 = −5.173040124\n",
      "B2 = −5.462258944 (27.10)\n",
      "The output value is calculated as:\n",
      "output= −1 ×(−5.173040124 ×3.032830419) + (−5.462258944 ×3.170770366)\n",
      "output= 33.00852224 (27.11)\n",
      "The output value is larger than 1.0, suggesting that this training instance is not a support\n",
      "vector. We can update the B1 coeﬃcient accordingly:\n",
      "b= (1 −1\n",
      "t) ×b\n",
      "B1 = (1 −1\n",
      "2) ×−5.173040124\n",
      "B1 = −2.586520062\n",
      "(27.12)\n",
      "And for the B2 coeﬃcient:\n",
      "B2 = (1 −1\n",
      "2) ×−5.462258944\n",
      "B2 = −2.731129472\n",
      "(27.13)...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 131, 'page_label': '132', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1063}\n",
      "\n",
      "--- Chunk 1064 ---\n",
      "Content:\n",
      "27.4. Make Predictions with SVM Model 123\n",
      "27.3.3 More Iterations\n",
      "Repeat this process for the rest of the dataset. One pass through the dataset is called an epoch.\n",
      "Now repeat the process for a further 15 epochs for a total of 160 iterations (16 epochs ×10\n",
      "updates per epoch). It is possible to keep track of the loss or the accuracy of the model for each\n",
      "epoch. This is a great way to get insight into whether the algorithm is converging or whether\n",
      "there is a bug in the implementation. If you plot the accuracy for the model at the end of each\n",
      "epoch, you should see something that looks like the following graph:\n",
      "Figure 27.2: Support Vector Machine Model Accuracy.\n",
      "You can see that after 16 epochs that we achieve an accuracy of 100% on the training data.\n",
      "You should arrive at ﬁnal values for the coeﬃcients that look like the following:\n",
      "B1 = 0.552391765\n",
      "B2 = −0.724533592 (27.14)\n",
      "The form of the learned hyperplane is therefore:\n",
      "0 + (0.552391765 ×X1) + (−0.724533592 ×X2) = 0 (27.15)\n",
      "27.4 Make Predictions with SVM Model\n",
      "Now that we have coeﬃcients for the line, we can make predictions. In this section we will make\n",
      "predictions for the training data, but this could just as easily be adapted to make predictions\n",
      "for new data. Predictions can be made using the following equation:\n",
      "output= (B1 ×X1) + (B2 ×X2)\n",
      "Y = −1 IF output< 0\n",
      "Y = +1 IF output> 0\n",
      "(27.16)...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 132, 'page_label': '133', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1064}\n",
      "\n",
      "--- Chunk 1065 ---\n",
      "Content:\n",
      "27.5. Summary 124\n",
      "Using the above coeﬃcients and these prediction rules, we can make a prediction for each\n",
      "instance in the training dataset:\n",
      "Output Crisp Y\n",
      "-0.495020399 -1 -1\n",
      "-0.622019096 -1 -1\n",
      "-0.20066956 -1 -1\n",
      "-0.75170826 -1 -1\n",
      "-0.081298299 -1 -1\n",
      "1.928999147 1 1\n",
      "1.836907801 1 1\n",
      "2.624496306 1 1\n",
      "2.022225129 1 1\n",
      "1.036597783 1 1\n",
      "Listing 27.2: Accuracy of the Learned SVM Model.\n",
      "Comparing the crisp prediction ( Crisp) to the expected output column ( Y), we can see that\n",
      "our model has achieved 100% accuracy.\n",
      "27.5 Summary\n",
      "In this chapter you discovered how to implement the Support Vector Machine algorithm from\n",
      "scratch using the sub-gradient descent optimization technique. You learned:\n",
      " How to update coeﬃcient values for SVM using sub-gradient descent.\n",
      " How to iterate the sub-gradient descent procedure to ﬁnd good coeﬃcient values.\n",
      " How to make predictions using a learned SVM model.\n",
      "You now know how to implement the Support Vector Machine algorithm from scratch using\n",
      "sub-gradient descent. This concludes your introduction to nonlinear machine learning algorithms.\n",
      "In the next part you will discover ensemble machine learning algorithms starting the bagging....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 133, 'page_label': '134', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1065}\n",
      "\n",
      "--- Chunk 1066 ---\n",
      "Content:\n",
      "Part V\n",
      "Ensemble Algorithms\n",
      "125...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 134, 'page_label': '135', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1066}\n",
      "\n",
      "--- Chunk 1067 ---\n",
      "Content:\n",
      "Chapter 28\n",
      "Bagging and Random Forest\n",
      "Random Forest is one of the most popular and most powerful machine learning algorithms. It is\n",
      "a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging. In this\n",
      "chapter you will discover the Bagging ensemble algorithm and the Random Forest algorithm for\n",
      "predictive modeling. After reading this chapter you will know about:\n",
      " The bootstrap method for estimating statistical quantities from samples.\n",
      " The Bootstrap Aggregation algorithm for creating multiple diﬀerent models from a single\n",
      "training dataset.\n",
      " The Random Forest algorithm that makes a small tweak to Bagging and results in a very\n",
      "powerful classiﬁer.\n",
      "Let’s get started.\n",
      "28.1 Bootstrap Method\n",
      "Before we get to Bagging, let’s take a quick look at an important foundation technique called\n",
      "the bootstrap. The bootstrap is a powerful statistical method for estimating a quantity from a\n",
      "data sample. This is easiest to understand if the quantity is a descriptive statistic such as a\n",
      "mean or a standard deviation. Let’s assume we have a sample of 100 values ( x) and we’d like to\n",
      "get an estimate of the mean of the sample. We can calculate the mean directly from the sample\n",
      "as:\n",
      "mean(x) = 1\n",
      "100 ×\n",
      "100∑\n",
      "i=1\n",
      "xi (28.1)\n",
      "We know that our sample is small and that our mean has error in it. We can improve the\n",
      "estimate of our mean using the bootstrap procedure:\n",
      "1. Create many (e.g. 1000) random sub-samples of our dataset with replacement (meaning\n",
      "we can select the same value multiple times).\n",
      "2. Calculate the mean of each sub-sample.\n",
      "126...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 135, 'page_label': '136', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1067}\n",
      "\n",
      "--- Chunk 1068 ---\n",
      "Content:\n",
      "28.2. Bootstrap Aggregation (Bagging) 127\n",
      "3. Calculate the average of all of our collected means and use that as our estimated mean for\n",
      "the data.\n",
      "For example, let’s say we used 3 resamples and got the mean values 2.3, 4.5 and 3.3. Taking\n",
      "the average of these we could take the estimated mean of the data to be 3.367. This process\n",
      "can be used to estimate other quantities like the standard deviation and even quantities used in\n",
      "machine learning algorithms, like learned coeﬃcients.\n",
      "28.2 Bootstrap Aggregation (Bagging)\n",
      "Bootstrap Aggregation (or Bagging for short), is a simple and very powerful ensemble method.\n",
      "An ensemble method is a technique that combines the predictions from multiple machine learning\n",
      "algorithms together to make more accurate predictions than any individual model. Bootstrap\n",
      "Aggregation is a general procedure that can be used to reduce the variance for those algorithms\n",
      "that have high variance. An algorithm that has high variance are decision trees, like classiﬁcation\n",
      "and regression trees (CART).\n",
      "Decision trees are sensitive to the speciﬁc data on which they are trained. If the training\n",
      "data is changed (e.g. a tree is trained on a subset of the training data) the resulting decision\n",
      "tree can be quite diﬀerent and in turn the predictions can be quite diﬀerent. Bagging is the\n",
      "application of the Bootstrap procedure to a high-variance machine learning algorithm, typically\n",
      "decision trees. Let’s assume we have a dataset of 1000 instances and we are using the CART\n",
      "algorithm. Bagging of the CART algorithm would work as follows.\n",
      "1. Create many (e.g. 100) random sub-samples of our dataset with replacement.\n",
      "2. Train a CART model on each sample.\n",
      "3. Given a new dataset, calculate the average prediction from each model.\n",
      "For example, if we had 5 bagged decision trees that made the following class predictions\n",
      "for an input instance: blue, blue, red, blue and red, we would take the most frequent class\n",
      "and predict blue. When bagging with decision trees, we are less concerned about individual\n",
      "trees overﬁtting the training data. For this reason and for eﬃciency, the individual decision\n",
      "trees are grown deep (e.g. few training samples at each leaf-node of the tree) and the trees\n",
      "are not pruned. These trees will have both high variance and low bias. These are important\n",
      "characteristics of sub-models when combining predictions using bagging.\n",
      "The only parameters when bagging decision trees is the number of trees to create. This\n",
      "can be chosen by increasing the number of trees on run after run until the accuracy begins to\n",
      "stop showing improvement (e.g. on a cross validation test harness). Creating large numbers of\n",
      "decision trees may take a long time, but will not overﬁt the training data. Just like the decision\n",
      "trees themselves, Bagging can be used for classiﬁcation and regression problems.\n",
      "28.3 Random Forest\n",
      "Random Forests are an improvement over bagged decision trees. A problem with decision\n",
      "trees like CART is that they are greedy. They choose which variable to split on using a...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 136, 'page_label': '137', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1068}\n",
      "\n",
      "--- Chunk 1069 ---\n",
      "Content:\n",
      "28.4. Estimated Performance 128\n",
      "greedy algorithm that minimizes error. As such, even with Bagging, the decision trees can\n",
      "have a lot of structural similarities and in turn result in high correlation in their predictions.\n",
      "Combining predictions from multiple models in ensembles works better if the predictions from\n",
      "the sub-models are uncorrelated or at best weakly correlated.\n",
      "Random forest changes the algorithm for the way that the sub-trees are learned so that\n",
      "the resulting predictions from all of the subtrees have less correlation. It is a simple tweak.\n",
      "In CART, when selecting a split point, the learning algorithm is allowed to look through all\n",
      "variables and all variable values in order to select the most optimal split-point. The random\n",
      "forest algorithm changes this procedure so that the learning algorithm is limited to a random\n",
      "sample of features of which to search. The number of features that can be searched at each split\n",
      "point (m) must be speciﬁed as a parameter to the algorithm. You can try diﬀerent values and\n",
      "tune it using cross validation.\n",
      " For classiﬁcation a good default is: m= √p.\n",
      " For regression a good default is: m= p\n",
      "3 .\n",
      "Where m is the number of randomly selected features that can be searched at a split point\n",
      "and p is the number of input variables. For example, if a dataset had 25 input variables for a\n",
      "classiﬁcation problem, then:\n",
      "m=\n",
      "√\n",
      "25\n",
      "m= 5 (28.2)\n",
      "28.4 Estimated Performance\n",
      "For each bootstrap sample taken from the training data, there will be samples left behind that\n",
      "were not included. These samples are called Out-Of-Bag samples or OOB. The performance\n",
      "of each model on its left out samples when averaged can provide an estimated accuracy of\n",
      "the bagged models. This estimated performance is often called the OOB estimate. These\n",
      "performance measures are a reliable estimate of test error and correlate well with cross validation\n",
      "estimates of error.\n",
      "28.5 Variable Importance\n",
      "As the Bagged decision trees are constructed, we can calculate how much the error function\n",
      "drops for a variable at each split point. In regression problems this may be the drop in sum\n",
      "squared error and in classiﬁcation this might be the Gini score. These drops in error can be\n",
      "averaged across all decision trees and output to provide an estimate of the importance of each\n",
      "input variable. The greater the drop when the variable was chosen, the greater the importance.\n",
      "These outputs can help identify subsets of input variables that may be most or least relevant\n",
      "to the problem and suggest at possible feature selection experiments you could perform where\n",
      "some features are removed from the dataset....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 137, 'page_label': '138', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1069}\n",
      "\n",
      "--- Chunk 1070 ---\n",
      "Content:\n",
      "28.6. Preparing Data For Bagged CART 129\n",
      "28.6 Preparing Data For Bagged CART\n",
      "Bagged CART does not require any special data preparation other than a good representation\n",
      "of the problem.\n",
      "28.7 Summary\n",
      "In this chapter you discovered the Bagging ensemble machine learning algorithm and the popular\n",
      "variation called Random Forest. You learned:\n",
      " How to estimate statistical quantities from a data sample.\n",
      " How to combine the predictions from multiple high-variance models using bagging.\n",
      " How to tweak the construction of decision trees when bagging to de-correlate their\n",
      "predictions, a technique called Random Forests.\n",
      "You now know about the bagging ensemble algorithm, bagged decision trees and random\n",
      "forests. In the next chapter you will discover how to implement bagged decision trees from\n",
      "scratch....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 138, 'page_label': '139', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1070}\n",
      "\n",
      "--- Chunk 1071 ---\n",
      "Content:\n",
      "Chapter 29\n",
      "Bagged Decision Trees Tutorial\n",
      "Bagging is a simple ensemble method that almost always results in a lift in performance over\n",
      "the baseline models. In this chapter you will discover how to implement bagged decision trees\n",
      "step-by-step. After reading this chapter you will discover:\n",
      " How to create decision trees from bootstrap samples of your training dataset.\n",
      " How to aggregate the predictions from multiple bootstrap samples.\n",
      " How bagging can create more accurate predictions than individual high-variance models.\n",
      "Let’s get started.\n",
      "29.1 Tutorial Dataset\n",
      "In this tutorial we will use dataset with two input variables ( X1 and X2) and one output\n",
      "variable (Y). The input variables are real-valued random numbers drawn from a Gaussian\n",
      "distribution. The output variable has two values, making the problem a binary classiﬁcation\n",
      "problem. The raw data is listed below.\n",
      "X1 X2 Y\n",
      "2.309572387 1.168959634 0\n",
      "1.500958319 2.535482186 0\n",
      "3.107545266 2.162569456 0\n",
      "4.090032824 3.123409313 0\n",
      "5.38660215 2.109488166 0\n",
      "6.451823468 0.242952387 1\n",
      "6.633669528 2.749508563 1\n",
      "8.749958452 2.676022211 1\n",
      "4.589131161 0.925340325 1\n",
      "6.619322828 3.831050828 1\n",
      "Listing 29.1: Bagging Tutorial Data Set.\n",
      "Below is a plot of the dataset. You see that we cannot draw a straight line to separate the\n",
      "classes.\n",
      "130...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 139, 'page_label': '140', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1071}\n",
      "\n",
      "--- Chunk 1072 ---\n",
      "Content:\n",
      "29.2. Learn the Bagged Decision Tree Model 131\n",
      "Figure 29.1: Bagging Tutorial Dataset Scatter plot.\n",
      "29.2 Learn the Bagged Decision Tree Model\n",
      "Bootstrap Aggregation (or Bagging for short), is a simple and very powerful ensemble method.\n",
      "Bagging works by taking a sub-sample of your training dataset (with replacement) and creating\n",
      "a model, often a decision tree because they have high variance. The process is repeated creating\n",
      "as many trees as you desire. Later when making predictions for new data, the outputs from\n",
      "each tree are combined by taking the average.\n",
      "Decision trees like CART select split points using a greedy algorithm that seeks to minimize\n",
      "a cost function, like the Gini index for classiﬁcation. When the tree is created on a random\n",
      "sample of the training data, the algorithm is likely to choose diﬀerent split points, resulting in\n",
      "diﬀerent trees and in turn diﬀerent predictions. This is where the power for bagging comes from,\n",
      "in combining the predictions from models that have very diﬀerent perspectives on the problem.\n",
      "The interesting part of bagged decision trees is how they are combined to make ensemble\n",
      "predictions.With this in mind, we will contrive 3 decision trees from the training data, each\n",
      "with sub-par accuracy. The split points were chosen manually to demonstrate exactly how\n",
      "diﬀerent perspectives on the same problem can be combined to provide increased performance.\n",
      "A decision tree with one split point is called a decision stump, this is because there is little tree\n",
      "to speak of. We will use three decision stumps. The split points for each are as follows:\n",
      "Model1 : X1 ≤5.38660215\n",
      "Model2 : X1 ≤4.090032824\n",
      "Model3 : X2 ≤0.925340325\n",
      "(29.1)\n",
      "If you are adapting this example to your own problem, you can apply the CART algorithm\n",
      "to each bootstrap sample from the training dataset. Because this is a classiﬁcation problem,\n",
      "you could use the Gini index cost function to minimize in order to choose split points. Also, in\n",
      "this example we are only using 3 models, often you will create tens, hundreds or even thousands\n",
      "of models when bagging. Now that we have our bootstrap models, let’s look at how we can\n",
      "aggregate them....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 140, 'page_label': '141', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1072}\n",
      "\n",
      "--- Chunk 1073 ---\n",
      "Content:\n",
      "29.3. Make Predictions with Bagged Decision Trees 132\n",
      "29.3 Make Predictions with Bagged Decision Trees\n",
      "The predictions from the bootstrap models can be combined to produce more accurate predictions.\n",
      "In this section we look at making predictions with each bootstrapped model, then ﬁnally aggregate\n",
      "the sub-model predictions into more accurate ensemble predictions.\n",
      "29.3.1 Decision Stump Model 1\n",
      "The split point for the ﬁrst model is X1 ≤5.38660215. Using this split point we can separate\n",
      "the training data into two groups:\n",
      "Y Group\n",
      "0 LEFT\n",
      "0 LEFT\n",
      "0 LEFT\n",
      "0 LEFT\n",
      "0 LEFT\n",
      "1 RIGHT\n",
      "1 RIGHT\n",
      "1 RIGHT\n",
      "1 LEFT\n",
      "1 RIGHT\n",
      "Listing 29.2: Grouping of Instances for Model 1.\n",
      "We can see that the LEFT group contains mostly instances for class 0 and the RIGHT group\n",
      "contains mostly instances of class 1. Therefore, instances assigned to the two groups will be\n",
      "classiﬁed:\n",
      " LEFT: class 0\n",
      " RIGHT: class 1\n",
      "We can therefore calculate the prediction for each training instance.\n",
      "Prediction Error Accuracy\n",
      "0 0 90\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "0 1\n",
      "1 0\n",
      "Listing 29.3: Prediction of Instances for Model 1.\n",
      "We can see that the model only got one training instance incorrect (the second last one).\n",
      "This gives the model an accuracy of 90%. Not bad at all, but we can do better....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 141, 'page_label': '142', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1073}\n",
      "\n",
      "--- Chunk 1074 ---\n",
      "Content:\n",
      "29.3. Make Predictions with Bagged Decision Trees 133\n",
      "29.3.2 Decision Stump Model 2\n",
      "The second model uses the split point X1 ≤4.090032824. We can separate the training data\n",
      "into two groups:\n",
      "Y Group\n",
      "0 LEFT\n",
      "0 LEFT\n",
      "0 LEFT\n",
      "0 LEFT\n",
      "0 RIGHT\n",
      "1 RIGHT\n",
      "1 RIGHT\n",
      "1 RIGHT\n",
      "1 RIGHT\n",
      "1 RIGHT\n",
      "Listing 29.4: Grouping of Instances for Model 2.\n",
      "As above, the LEFT group will classify instances as class 0 and the RIGHT as class 1. Using\n",
      "this model, we can make predictions for all instances in the training dataset:\n",
      "Prediction Error Accuracy\n",
      "0 0 90\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 1\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "1 0\n",
      "Listing 29.5: Prediction of Instances for Model 2.\n",
      "Again, like the ﬁrst model, we can see the accuracy is 90% with only one instance misclassiﬁed.\n",
      "This time the 5th instance. Combining just these two models results in some ambiguous\n",
      "predictions where the models conﬂict in their predictions. Adding a third model will clear things\n",
      "up.\n",
      "29.3.3 Decision Stump Model 3\n",
      "The split point used by the third model is X2 ≤0.925340325. Using this split point, we can\n",
      "again sort the training dataset into two groups.\n",
      "Y Group\n",
      "0 RIGHT\n",
      "0 RIGHT\n",
      "0 RIGHT\n",
      "0 RIGHT\n",
      "0 RIGHT\n",
      "1 LEFT\n",
      "1 RIGHT\n",
      "1 RIGHT\n",
      "1 LEFT...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 142, 'page_label': '143', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1074}\n",
      "\n",
      "--- Chunk 1075 ---\n",
      "Content:\n",
      "29.4. Final Predictions 134\n",
      "1 RIGHT\n",
      "Listing 29.6: Grouping of Instances for Model 3.\n",
      "In this model the RIGHT group will classify instances as class 0 and the LEFT group as\n",
      "class 1. Using this simple model, we can again make predictions for each training instance.\n",
      "Prediction Error Accuracy\n",
      "0 0 70\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 1\n",
      "0 1\n",
      "1 0\n",
      "0 1\n",
      "Listing 29.7: Prediction of Instances for Model 3.\n",
      "This model has slightly worse accuracy of 70%. Importantly it correctly predicts both the\n",
      "second last instance and the 5th instance, clearing up any ambiguity if the previous two models\n",
      "were combined.\n",
      "29.4 Final Predictions\n",
      "Now that we have predictions from the bootstrap models we can aggregate the predictions into\n",
      "an ensemble prediction. We can do that by taking the mode of each models prediction for each\n",
      "training instance. The mode is a statistical function that select the most common value. In this\n",
      "case, the most common class value predicted. Using this simple procedure we get the following\n",
      "predictions:\n",
      "Prediction Y Error Accuracy\n",
      "0 0 0 100\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 1 0\n",
      "1 1 0\n",
      "1 1 0\n",
      "1 1 0\n",
      "1 1 0\n",
      "Listing 29.8: Prediction of Instances for Bagged Model.\n",
      "You can see that all 10 training instances were classiﬁed correctly at 100% accuracy.\n",
      "29.5 Summary\n",
      "In this chapter you discovered bagged decision trees. You learned.\n",
      " How to create multiple decision stump models from bootstrap data samples....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 143, 'page_label': '144', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1075}\n",
      "\n",
      "--- Chunk 1076 ---\n",
      "Content:\n",
      "29.5. Summary 135\n",
      " How to aggregate the predictions from multiple decision tree models.\n",
      "You now know how to implement bagged decision trees from scratch. In the next chapter\n",
      "you will discover the boosting ensemble method and the AdaBoost machine learning algorithm....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 144, 'page_label': '145', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1076}\n",
      "\n",
      "--- Chunk 1077 ---\n",
      "Content:\n",
      "Chapter 30\n",
      "Boosting and AdaBoost\n",
      "Boosting is an ensemble technique that attempts to create a strong classiﬁer from a number of\n",
      "weak classiﬁers. In this chapter you will discover the AdaBoost Ensemble method for machine\n",
      "learning. After reading this chapter, you will know:\n",
      " What the boosting ensemble method is and generally how it works.\n",
      " How to learn to boost decision trees using the AdaBoost algorithm.\n",
      " How to make predictions using the learned AdaBoost model.\n",
      " How to best prepare your data for use with the AdaBoost algorithm.\n",
      "Let’s get started.\n",
      "30.1 Boosting Ensemble Method\n",
      "Boosting is a general ensemble method that creates a strong classiﬁer from a number of weak\n",
      "classiﬁers. This is done by building a model from the training data, then creating a second\n",
      "model that attempts to correct the errors from the ﬁrst model. Models are added until the\n",
      "training set is predicted perfectly or a maximum number of models are added. AdaBoost was\n",
      "the ﬁrst really successful boosting algorithm developed for binary classiﬁcation. It is the best\n",
      "starting point for understanding boosting. Modern boosting methods build on AdaBoost, most\n",
      "notably stochastic gradient boosting machines.\n",
      "30.2 Learning An AdaBoost Model From Data\n",
      "AdaBoost is best used to boost the performance of decision trees on binary classiﬁcation\n",
      "problems. AdaBoost was originally called AdaBoost.M1 by the developers of the technique.\n",
      "More recently it may be referred to as discrete AdaBoost because it is used for classiﬁcation\n",
      "rather than regression. AdaBoost can be used to boost the performance of any machine learning\n",
      "algorithm. It is best used with weak learners.\n",
      "These are models that achieve accuracy just above random chance on a classiﬁcation problem.\n",
      "The most suited and therefore most common algorithm used with AdaBoost are decision trees\n",
      "with one level. Because these trees are so short and only contain one decision for classiﬁcation,\n",
      "136...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 145, 'page_label': '146', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1077}\n",
      "\n",
      "--- Chunk 1078 ---\n",
      "Content:\n",
      "30.3. How To Train One Model 137\n",
      "they are often called decision stumps. Each instance in the training dataset is weighted. The\n",
      "initial weight is set to:\n",
      "weight(xi) = 1\n",
      "n (30.1)\n",
      "Where xi is the i’th training instance and n is the number of training instances.\n",
      "30.3 How To Train One Model\n",
      "A weak classiﬁer (decision stump) is prepared on the training data using the weighted samples.\n",
      "Only binary (two-class) classiﬁcation problems are supported, so each decision stump makes one\n",
      "decision on one input variable and outputs a +1.0 or -1.0 value for the ﬁrst or second class value.\n",
      "The misclassiﬁcation rate is calculated for the trained model. Traditionally, this is calculated as:\n",
      "error = correct−N\n",
      "N (30.2)\n",
      "Where erroris the misclassiﬁcation rate, correct are the number of training instance predicted\n",
      "correctly by the model and N is the total number of training instances. For example, if the\n",
      "model predicted 78 of 100 training instances correctly the error or misclassiﬁcation rate would\n",
      "be 78−100\n",
      "100 or 0.22. This is modiﬁed to use the weighting of the training instances:\n",
      "error =\n",
      "∑n\n",
      "i=1(wi ×perrori)∑n\n",
      "i=1 w (30.3)\n",
      "Which is the weighted sum of the misclassiﬁcation rate, where w is the weight for training\n",
      "instance i and perror is the prediction error for training instance i which is 1 if misclassiﬁed\n",
      "and 0 if correctly classiﬁed. For example, if we had 3 training instances with the weights 0.01,\n",
      "0.5 and 0.2. The predicted values were -1, -1 and -1, and the actual output variables in the\n",
      "instances were -1, 1 and -1, then the perror values would be 0, 1, and 0. The misclassiﬁcation\n",
      "rate would be calculated as:\n",
      "error = 0.01 ×0 + 0.5 ×1 + 0.2 ×0\n",
      "0.01 + 0.5 + 0.2\n",
      "error = 0.704\n",
      "(30.4)\n",
      "A stage value is calculated for the trained model which provides a weighting for any predictions\n",
      "that the model makes. The stage value for a trained model is calculated as follows:\n",
      "stage= ln(1 −error\n",
      "error ) (30.5)\n",
      "Where stage is the stage value used to weight predictions from the model, ln() is the natural\n",
      "logarithm and error is the misclassiﬁcation error for the model. The eﬀect of the stage weight\n",
      "is that more accurate models have more weight or contribution to the ﬁnal prediction. The\n",
      "training weights are updated giving more weight to incorrectly predicted instances, and less\n",
      "weight to correctly predicted instances. For example, the weight of one training instance ( w) is\n",
      "updated using:\n",
      "w= w×estage×perror (30.6)...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 146, 'page_label': '147', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1078}\n",
      "\n",
      "--- Chunk 1079 ---\n",
      "Content:\n",
      "30.4. AdaBoost Ensemble 138\n",
      "Where w is the weight for a speciﬁc training instance, e is the numerical constant Euler’s\n",
      "number raised to a power, stage is the misclassiﬁcation rate for the weak classiﬁer and perror\n",
      "is the error the weak classiﬁer made predicting the output variable for the training instance,\n",
      "evaluated as:\n",
      " perror = 0 IF y == p\n",
      " perror = 1 IF y != p\n",
      "Where yis the output variable for the training instance and pis the prediction from the weak\n",
      "learner. This has the eﬀect of not changing the weight if the training instance was classiﬁed\n",
      "correctly and making the weight slightly larger if the weak learner misclassiﬁed the instance.\n",
      "30.4 AdaBoost Ensemble\n",
      "Weak models are added sequentially, trained using the weighted training data. The process\n",
      "continues until a pre-set number of weak learners have been created (a user parameter) or no\n",
      "further improvement can be made on the training dataset. Once completed, you are left with a\n",
      "pool of weak learners each with a stage value.\n",
      "30.5 Making Predictions with AdaBoost\n",
      "Predictions are made by calculating the weighted average of the weak classiﬁers. For a new input\n",
      "instance, each weak learner calculates a predicted value as either +1.0 or -1.0. The predicted\n",
      "values are weighted by each weak learners stage value. The prediction for the ensemble model is\n",
      "taken as a the sum of the weighted predictions. If the sum is positive, then the ﬁrst class is\n",
      "predicted, if negative the second class is predicted.\n",
      "For example, 5 weak classiﬁers may predict the values 1.0, 1.0, -1.0, 1.0, -1.0. From a\n",
      "majority vote, it looks like the model will predict a value of 1.0 or the ﬁrst class. These same 5\n",
      "weak classiﬁers may have the stage values 0.2, 0.5, 0.8, 0.2 and 0.9 respectively. Calculating\n",
      "the weighted sum of these predictions results in an output of -0.8, which would be an ensemble\n",
      "prediction of -1.0 or the second class.\n",
      "30.6 Preparing Data For AdaBoost\n",
      "This section lists some heuristics for best preparing your data for AdaBoost.\n",
      " Quality Data: Because the ensemble method continues to attempt to correct misclas-\n",
      "siﬁcation’s in the training data, you need to be careful that the training data is of a\n",
      "high-quality.\n",
      " Outliers: Outliers will force the ensemble down the rabbit hole of working hard to correct\n",
      "for cases that are unrealistic. These could be removed from the training dataset.\n",
      " Noisy Data: Noisy data, speciﬁcally noise in the output variable can be problematic. If\n",
      "possible, attempt to isolate and clean these from your training dataset....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 147, 'page_label': '148', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1079}\n",
      "\n",
      "--- Chunk 1080 ---\n",
      "Content:\n",
      "30.7. Summary 139\n",
      "30.7 Summary\n",
      "In this chapter you discovered the Boosting ensemble method for machine learning. You learned\n",
      "about:\n",
      " Boosting and how it is a general technique that keeps adding weak learners to correct\n",
      "classiﬁcation errors.\n",
      " AdaBoost as the ﬁrst successful boosting algorithm for binary classiﬁcation problems.\n",
      " Learning the AdaBoost model by weighting training instances and the weak learners\n",
      "themselves.\n",
      " Predicting with AdaBoost by weighting predictions from weak learners.\n",
      " Where to look for more theoretical background on the AdaBoost algorithm.\n",
      "You now know about the boosting ensemble method and the AdaBoost machine learning\n",
      "algorithm. In the next chapter you will discover how to implement the AdaBoost algorithm\n",
      "from scratch....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 148, 'page_label': '149', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1080}\n",
      "\n",
      "--- Chunk 1081 ---\n",
      "Content:\n",
      "Chapter 31\n",
      "AdaBoost Tutorial\n",
      "AdaBoost is one of the ﬁrst boosting ensemble machine learning algorithms. In this chapter you\n",
      "will discover how AdaBoost for machine learning works step-by step. After reading this chapter\n",
      "you will know:\n",
      " How to create decision stumps from training data using weighted training data.\n",
      " How to update the weights in the training data so that more attention is put on diﬃcult\n",
      "to classify instances.\n",
      " How to create a sequence of 3 models one after the other.\n",
      " How to use an AdaBoost model with three weak models to make predictions.\n",
      "Let’s get started.\n",
      "31.1 Classiﬁcation Problem Dataset\n",
      "In this tutorial we will use a dataset with two input variables ( X1 and X2) and one output\n",
      "variable (Y). The input variables are real-valued random numbers drawn from a Gaussian\n",
      "distribution. The output variable has two values, making the problem a binary classiﬁcation\n",
      "problem. The raw data is listed below.\n",
      "X1 X2 Y\n",
      "3.64754035 2.996793259 0\n",
      "2.612663842 4.459457779 0\n",
      "2.363359679 1.506982189 0\n",
      "4.932600453 1.299008795 0\n",
      "3.776154753 3.157451378 0\n",
      "8.673960793 2.122873405 1\n",
      "5.861599451 0.003512817 1\n",
      "8.984677361 1.768161009 1\n",
      "7.467380954 0.187045945 1\n",
      "4.436284412 0.862698005 1\n",
      "Listing 31.1: Boosting Tutorial Data Set.\n",
      "Plotting the raw data, you can see that the classes are not clearly separated.\n",
      "140...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 149, 'page_label': '150', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1081}\n",
      "\n",
      "--- Chunk 1082 ---\n",
      "Content:\n",
      "31.2. Learn AdaBoost Model From Data 141\n",
      "Figure 31.1: Boosting Tutorial Dataset Scatter plot.\n",
      "31.2 Learn AdaBoost Model From Data\n",
      "In this section we are going to learn an AdaBoost model from the training data. This will involve\n",
      "learning 3 models, one after the other so that we can observe the eﬀect on the weightings to the\n",
      "training instances and how the predictions from each of the two models are combined. AdaBoost\n",
      "uses decision stump (one node decision trees) as the internal model. Rather than using the\n",
      "CART algorithm or similar to choose the split points for these decision trees, we will select split\n",
      "points manually. Again, these split points will be picked poorly to create classiﬁcation errors\n",
      "and to demonstrate how the second model can correct the ﬁrst model, and so on.\n",
      "31.3 Decision Stump: Model #1\n",
      "The ﬁrst model will be a decision stump for the X1 input variable split at the value 3.64754035.\n",
      " IF X1 ≤4.932600453 THEN LEFT\n",
      " IF X1 >4.932600453 THEN RIGHT\n",
      "This is a poor split point for this data and was chosen intentionally for this tutorial to\n",
      "create some misclassiﬁed instances. Ideally, you would use the CART algorithm to choose a\n",
      "split point for this dataset with the Gini index as a cost function. If we apply this split point to\n",
      "the training data we can see the data split into the following two groups:\n",
      "Y Group\n",
      "0 LEFT\n",
      "0 LEFT\n",
      "0 LEFT\n",
      "0 LEFT\n",
      "0 LEFT\n",
      "1 RIGHT...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 150, 'page_label': '151', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1082}\n",
      "\n",
      "--- Chunk 1083 ---\n",
      "Content:\n",
      "31.3. Decision Stump: Model #1 142\n",
      "1 RIGHT\n",
      "1 RIGHT\n",
      "1 RIGHT\n",
      "1 LEFT\n",
      "Listing 31.2: Splitting of the Training Dataset by Model 1.\n",
      "Looking at the composition of this group, we can see that the right group is predominately\n",
      "in class 1 and the left group is predominately class 0. These class values will be the predictions\n",
      "made for each group.\n",
      " LEFT: Class 0\n",
      " RIGHT: Class 1\n",
      "Now that we have a decision stump model trained, we can use it to make predictions for the\n",
      "training dataset. The error in the prediction can be calculated using:\n",
      " error = 0 IF Prediction == Y\n",
      " error = 1 IF Prediction != Y\n",
      "We can summarize the predictions using the decision stump and their errors in the table\n",
      "below.\n",
      "Y Prediction Error\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "1 1 0\n",
      "1 1 0\n",
      "1 1 0\n",
      "1 1 0\n",
      "1 0 1\n",
      "Listing 31.3: Predictions of the Training Dataset by Model 1.\n",
      "We can see 0 errors in 10 predictions or an accuracy of 0.9 or 90%. Not bad, but not ideal.\n",
      "When using AdaBoost, we calculate the misclassiﬁcation rate for a weak model like the above\n",
      "decision stump using the following equation:\n",
      "MisclassificationRate =\n",
      "∑n\n",
      "i=1(wi ×errori)∑n\n",
      "i=1 w (31.1)\n",
      "Each instance in the training dataset has the starting weight of 1\n",
      "N where N is the number of\n",
      "training instance, in this case 10 so 1\n",
      "10 = 0.1.\n",
      "weight = 1\n",
      "N\n",
      "weight = 1\n",
      "10\n",
      "weight = 0.1\n",
      "(31.2)...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 151, 'page_label': '152', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1083}\n",
      "\n",
      "--- Chunk 1084 ---\n",
      "Content:\n",
      "31.3. Decision Stump: Model #1 143\n",
      "Using this starting weight and the prediction errors above, we can calculate the weighted\n",
      "error for each prediction.\n",
      "WeightedError = weight×error (31.3)\n",
      "The results are listed below.\n",
      "Weight Error Weighted Error\n",
      "0.1 0 0\n",
      "0.1 0 0\n",
      "0.1 0 0\n",
      "0.1 0 0\n",
      "0.1 0 0\n",
      "0.1 0 0\n",
      "0.1 0 0\n",
      "0.1 0 0\n",
      "0.1 0 0\n",
      "0.1 1 0.1\n",
      "Listing 31.4: Weight Errors For Model 1.\n",
      "Now we can calculate the misclassiﬁcation rate as:\n",
      "MisclassificationRate =\n",
      "∑(WeightedError)∑(weight)\n",
      "MisclassificationRate = 0.2\n",
      "1.0\n",
      "MisclassificationRate = 0.2\n",
      "(31.4)\n",
      "Finally, we can use the misclassiﬁcation rate to calculate the stage for this weak model. The\n",
      "stage is the weight applied to any prediction made by this model later when we use it to actually\n",
      "make predictions. The stage is calculated as:\n",
      "stage= ln(1 −MisclassificationRate\n",
      "MisclassificationRate )\n",
      "stage= ln(1 −0.1\n",
      "0.1 )\n",
      "stage= 2.197224577\n",
      "(31.5)\n",
      "This classiﬁer will have a lot of weight on predictions, which is good because it is 90%\n",
      "accurate.\n",
      "31.3.1 Update Instance Weights\n",
      "Before we can prepare a second boosted model, we must update the instance weights. This is\n",
      "the very core of boosting. The weights are updated so that the next model that is created pays\n",
      "more attention to the training instances that the previous models got wrong and less attention\n",
      "to the instances that it got right. The weight for a training instance is updated using:\n",
      "weight = weight×estage×error (31.6)\n",
      "We know the current weight for each training instance (0.1) and the errors made on each\n",
      "training instance. We now also know the stage. Updating the weights for each training instance...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 152, 'page_label': '153', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1084}\n",
      "\n",
      "--- Chunk 1085 ---\n",
      "Content:\n",
      "31.4. Decision Stump: Model #2 144\n",
      "is therefore quite straightforward. Below are the updated weights for each training instance.\n",
      "You will notice that only the weights for the one instance that the ﬁrst model got wrong are\n",
      "diﬀerence. In fact it is larger so that the next model that is created pays more attention to\n",
      "them.\n",
      "Weight\n",
      "0.1\n",
      "0.1\n",
      "0.1\n",
      "0.1\n",
      "0.1\n",
      "0.1\n",
      "0.1\n",
      "0.1\n",
      "0.1\n",
      "0.124573094\n",
      "Listing 31.5: Updated Instance Weights Training Model 1.\n",
      "31.4 Decision Stump: Model #2\n",
      "Now we can create our second weak model from the weighted training instances. This second\n",
      "model will also be a decision stump, but this time it will make a split on the X2 variable at the\n",
      "value 2.122873405. Again, this is a contrived model intended to have poor accuracy. Ideally,\n",
      "you would use something like the CART algorithm and the Gini index to choose a good quality\n",
      "split point. It should be noted that the CART algorithm would take the instance weight into\n",
      "account and focus on splitting at instances with a larger weight to result in a diﬀerent decision\n",
      "stump. If not, the algorithm will create the same decision stump in model after model and there\n",
      "would be no chance to correct the predictions made from prior models. We will go through the\n",
      "same process using this new split point.\n",
      " IF X2 ≤2.122873405 THEN LEFT\n",
      " IF X2 >2.122873405 THEN RIGHT\n",
      "This results in the following groups:\n",
      "Y Group\n",
      "0 RIGHT\n",
      "0 RIGHT\n",
      "0 LEFT\n",
      "0 LEFT\n",
      "0 RIGHT\n",
      "1 LEFT\n",
      "1 LEFT\n",
      "1 LEFT\n",
      "1 LEFT\n",
      "1 LEFT\n",
      "Listing 31.6: Splitting of the Training Dataset by Model 2.\n",
      "Looking at the composition for each group, the LEFT group has the class value 1 and the\n",
      "RIGHT group has the class value 0....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 153, 'page_label': '154', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1085}\n",
      "\n",
      "--- Chunk 1086 ---\n",
      "Content:\n",
      "31.5. Decision Stump: Model #3 145\n",
      " LEFT: Class 1\n",
      " RIGHT: Class 0\n",
      "Using this simple decision tree, we can now calculate a prediction for each training instance\n",
      "and the error and the weighted error for those predictions.\n",
      "Y Prediction Error Weighted Error\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 1 0.1\n",
      "0 1 1 0.1\n",
      "0 0 0 0\n",
      "1 1 0 0\n",
      "1 1 0 0\n",
      "1 1 0 0\n",
      "1 1 0 0\n",
      "1 1 0 0\n",
      "Listing 31.7: Predictions and Weighted Error for Model 2.\n",
      "A quick back of the envelope check indicates that this model was 80% accurate on the\n",
      "training data. Slightly worse than the ﬁrst model. It also looks like it made diﬀerent mistakes\n",
      "and correctly predicted the instances that the ﬁrst model predicted incorrectly. We can calculate\n",
      "the stage value for this classiﬁer:\n",
      "∑\n",
      "weight = 1.024573094\n",
      "∑\n",
      "WeightedError = 0.2\n",
      "MisclassificationRate = 0.1952032521\n",
      "stage= 1.416548424\n",
      "(31.7)\n",
      "We are not done yet. If we stop there, we will not have a very accurate AdaBoost model. In\n",
      "fact a quick check suggests the model will only have an accuracy of 60% on the training data.\n",
      "You can check this yourself later by changing the next section to only combine the predictions\n",
      "from the ﬁrst two models. We need one more model to make some ﬁnal corrections.\n",
      "31.5 Decision Stump: Model #3\n",
      "Again, we can update the weights for the instances using the same update procedure described\n",
      "above. This gives us the following weights:\n",
      "Weight\n",
      "0.1\n",
      "0.1\n",
      "0.11521789\n",
      "0.11521789\n",
      "0.1\n",
      "0.1\n",
      "0.1\n",
      "0.1\n",
      "0.1\n",
      "0.124573094...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 154, 'page_label': '155', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1086}\n",
      "\n",
      "--- Chunk 1087 ---\n",
      "Content:\n",
      "31.5. Decision Stump: Model #3 146\n",
      "Listing 31.8: Weight Values Prior to Training Model 3.\n",
      "This model will choose a split point for X2 at 0.862698005.\n",
      " IF X2 ≤0.862698005 THEN LEFT\n",
      " IF X2 >0.862698005 THEN RIGHT\n",
      "This split point separates the dataset into the following groups:\n",
      "Y Group\n",
      "0 RIGHT\n",
      "0 RIGHT\n",
      "0 RIGHT\n",
      "0 RIGHT\n",
      "0 RIGHT\n",
      "1 RIGHT\n",
      "1 LEFT\n",
      "1 RIGHT\n",
      "1 LEFT\n",
      "1 LEFT\n",
      "Listing 31.9: Splitting of the Training Dataset by Model 3.\n",
      "Again, looking at the composition for each group, the LEFT group has the class value 1 and\n",
      "the RIGHT group has the class value 0.\n",
      " LEFT: Class 1\n",
      " RIGHT: Class 0\n",
      "Using this ﬁnal simple decision tree, we can now calculate a prediction for each training\n",
      "instance and the error and the weighted error for those predictions.\n",
      "Y Prediction Error Weighted Error\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "1 0 1 0.1\n",
      "1 1 0 0\n",
      "1 0 1 0.1\n",
      "1 1 0 0\n",
      "1 1 0 0\n",
      "Listing 31.10: Predictions and Weighted Error for Model 3.\n",
      "This model too makes 2 errors (or has an accuracy of 80%). Again, we can calculate the\n",
      "stage value for this classiﬁer:\n",
      "∑\n",
      "(weight) = 1.024573094\n",
      "∑\n",
      "WeightedError = 0.2\n",
      "MisclassificationRate = 0.1952032521\n",
      "stage= 1.416548424\n",
      "(31.8)...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 155, 'page_label': '156', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1087}\n",
      "\n",
      "--- Chunk 1088 ---\n",
      "Content:\n",
      "31.6. Make Predictions with AdaBoost Model 147\n",
      "Now we are done. Let’s look at how we can use this series of boosted models to make\n",
      "predictions.\n",
      "31.6 Make Predictions with AdaBoost Model\n",
      "We can now make predictions for the training dataset using the AdaBoost model. Predictions\n",
      "for a single classiﬁer are either +1 or -1 and weighted by the stage value for the model. For\n",
      "example for this problem we will use:\n",
      "prediction= stage×(IF (output== 0) THEN −1 ELSE +1) (31.9)\n",
      "Using the three models above, we can make weighted predictions given the input values\n",
      "from the training data. This could just as easily be new data for which we would like to make\n",
      "predictions.\n",
      "X1 X2 Model 1 Model 2 Model 3\n",
      "3.64754035 2.996793259 -2.197224577 -1.416548424 -1.45279448\n",
      "2.612663842 4.459457779 -2.197224577 -1.416548424 -1.45279448\n",
      "2.363359679 1.506982189 -2.197224577 1.416548424 -1.45279448\n",
      "4.932600453 1.299008795 -2.197224577 1.416548424 -1.45279448\n",
      "3.776154753 3.157451378 -2.197224577 -1.416548424 -1.45279448\n",
      "8.673960793 2.122873405 2.197224577 1.416548424 -1.45279448\n",
      "5.861599451 0.003512817 2.197224577 1.416548424 1.45279448\n",
      "8.984677361 1.768161009 2.197224577 1.416548424 -1.45279448\n",
      "7.467380954 0.187045945 2.197224577 1.416548424 1.45279448\n",
      "4.436284412 0.862698005 -2.197224577 1.416548424 1.45279448\n",
      "Listing 31.11: Predictions from All 3 Models.\n",
      "We can sum the predictions for each model to give a ﬁnal outcome. If an outcome is less\n",
      "than 0 then the 0 class is predicted, and if an outcome is greater than 0 the 1 class is predicted.\n",
      "We can now calculate the ﬁnal predictions from the AdaBoost model.\n",
      "Sum Prediction Y Error Accuracy\n",
      "-5.066567482 0 0 0 100\n",
      "-5.066567482 0 0 0\n",
      "-2.233470634 0 0 0\n",
      "-2.233470634 0 0 0\n",
      "-5.066567482 0 0 0\n",
      "2.160978521 1 1 0\n",
      "5.066567482 1 1 0\n",
      "2.160978521 1 1 0\n",
      "5.066567482 1 1 0\n",
      "0.672118327 1 1 0\n",
      "Listing 31.12: Ensemble Prediction from AdaBoost Model.\n",
      "We can see that the predictions match the expected Y values perfectly. Or stated another\n",
      "way, the AdaBoost model achieved an accuracy of 100% on the training data....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 156, 'page_label': '157', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1088}\n",
      "\n",
      "--- Chunk 1089 ---\n",
      "Content:\n",
      "31.7. Summary 148\n",
      "31.7 Summary\n",
      "In this chapter you discovered how to work through implementing an AdaBoost model from\n",
      "scratch step-by-step. You learned:\n",
      " How to create decision stumps from training data using weighted training data.\n",
      " How to update the weights in the training data so that more attention is put on diﬃcult\n",
      "to classify instances.\n",
      " How to create a sequence of 3 models one after the other.\n",
      " How to use an AdaBoost model with three weak models to make predictions.\n",
      "You now know how to implement the AdaBoost machine learning algorithm from scratch.\n",
      "This concludes your introduction to ensemble machine learning algorithms....\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 157, 'page_label': '158', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1089}\n",
      "\n",
      "--- Chunk 1090 ---\n",
      "Content:\n",
      "Part VI\n",
      "Conclusions\n",
      "149...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 158, 'page_label': '159', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1090}\n",
      "\n",
      "--- Chunk 1091 ---\n",
      "Content:\n",
      "Chapter 32\n",
      "How Far You Have Come\n",
      "You made it. Well done.Take a moment and look back at how far you have come.\n",
      "1. You started oﬀ with an interest in machine learning algorithms.\n",
      "2. You learned the underlying principle for all supervised machine learning algorithms that\n",
      "they are estimating a mapping function from input to output variables.\n",
      "3. You discovered the diﬀerence between parametric and nonparametric algorithms, supervised\n",
      "and unsupervised algorithms and error introduced from bias and variance.\n",
      "4. You discovered and implemented linear machine learning algorithms including linear\n",
      "regression, logistic regression and linear discriminant analysis.\n",
      "5. You implemented from scratch nonlinear machine learning algorithms including classiﬁca-\n",
      "tion and regression trees, naive Bayes, k-nearest neighbors, learning vector quantization\n",
      "and support vector machines.\n",
      "6. Finally, you discovered and implemented two of the most popular ensemble algorithms\n",
      "bagging with decision trees and boosting with adaboost.\n",
      "Don’t make light of this. You have come a long way in a short amount of time. You\n",
      "have developed the important and valuable skill of being able to implement machine learning\n",
      "algorithms from scratch in a spreadsheet. There is nowhere to hide in a spreadsheet, you either\n",
      "understand the algorithm and it works or you don’t and you don’t get results.\n",
      "I want to take a moment and sincerely thank you for letting me help you start your journey\n",
      "with machine learning algorithms. I hope you keep learning and have fun as you continue to\n",
      "master machine learning.\n",
      "150...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 159, 'page_label': '160', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1091}\n",
      "\n",
      "--- Chunk 1092 ---\n",
      "Content:\n",
      "Chapter 33\n",
      "Getting More Help\n",
      "This is just the beginning of your journey with machine learning algorithms. As you start to\n",
      "work on new algorithms or expand your existing knowledge of algorithms you may need help.\n",
      "This chapter points out some of the best sources of help on machine learning algorithms you\n",
      "can ﬁnd.\n",
      "33.1 Machine Learning Books\n",
      "This book contains everything that you need to get started with machine learning algorithms,\n",
      "but if you are like me, then you love books. There are many machine learning books available,\n",
      "but below are a small selection that I recommend as the next step.\n",
      " An Introduction to Statistical Learning . Excellent coverage of machine learning\n",
      "algorithms from a statistical perspective. Recommended as the next step.\n",
      "http://amzn.to/1pgirl0\n",
      " Applied Predictive Modeling. An excellent introduction to predictive modeling with\n",
      "coverage of a large number of algorithms. This book is better for breadth rather than\n",
      "depth on any one algorithm.\n",
      "http://amzn.to/1n5MSsq\n",
      " Artiﬁcial Intelligence: A Modern Approach . An excellent book on artiﬁcial intelli-\n",
      "gence in general, but the chapters on machine learning give a superb computer science\n",
      "perspective of the algorithms covered in this book.\n",
      "http://amzn.to/1TGk1rr\n",
      "33.2 Forums and Q&A Websites\n",
      "Question and answer sites are perhaps the best way to get answers to your speciﬁc technical\n",
      "questions about machine learning. You can search them for similar questions, browse through\n",
      "topics to learn about solutions to common problems and ask your own technical questions. The\n",
      "best Q&A sites I would recommend for your machine learning algorithm questions are:\n",
      " Cross Validated: http://stats.stackexchange.com/\n",
      "151...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 160, 'page_label': '161', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1092}\n",
      "\n",
      "--- Chunk 1093 ---\n",
      "Content:\n",
      "33.3. Contact the Author 152\n",
      " Stack Overﬂow: http://stackoverflow.com/questions/tagged/machine-learning\n",
      " Data Science: http://datascience.stackexchange.com/\n",
      " Reddit Machine Learning: http://www.reddit.com/r/machinelearning\n",
      " Quora Machine Learning: https://www.quora.com/topic/Machine-Learning/top_stories\n",
      "Make heavy use of the search feature on these sites. Also note the list of Related questions\n",
      "in the right-hand navigation bar when viewing a speciﬁc question on Cross Validated. These\n",
      "are often relevant and useful.\n",
      "33.3 Contact the Author\n",
      "If you ever have any questions about machine learning algorithms or this book, please contact\n",
      "me directly. I will do my best to help.\n",
      "Jason Brownlee\n",
      "Jason@MachineLearningMastery.com...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 161, 'page_label': '162', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1093}\n",
      "\n",
      "--- Chunk 1094 ---\n",
      "Content:\n",
      "Errata\n",
      "This section lists changes between diﬀerent revisions of this book.\n",
      "Revision 1.0\n",
      " First edition of this book.\n",
      "Revision 1.1\n",
      " Fixed formatting Chapter 2, pages 7-8.\n",
      "153...\n",
      "Metadata:\n",
      "{'producer': 'PDFium', 'creator': 'PDFium', 'creationdate': 'D:20170731014854', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'total_pages': 163, 'page': 162, 'page_label': '163', 'rel_path': 'machine_learning/Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1094}\n",
      "\n",
      "--- Chunk 1095 ---\n",
      "Content:\n",
      "The\n",
      "Hundred-\n",
      "Page\n",
      "Machine\n",
      "Learning\n",
      "Book\n",
      "Andriy Burkov...\n",
      "Metadata:\n",
      "{'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'PyPDF', 'creationdate': '2018-12-18T05:07:46+00:00', 'moddate': '2019-01-22T19:51:34+00:00', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Burkov_the_hundred-page_Machine_Learning.pdf', 'total_pages': 152, 'page': 0, 'page_label': '1', 'rel_path': 'machine_learning/Burkov_the_hundred-page_Machine_Learning.pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1095}\n",
      "\n",
      "--- Chunk 1096 ---\n",
      "Content:\n",
      "“All models are wrong, but some are useful.”\n",
      "— George Box\n",
      "The book is distributed on the “read ﬁrst, buy later” principle.\n",
      "Andriy Burkov The Hundred-Page Machine Learning Book - Draft...\n",
      "Metadata:\n",
      "{'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'PyPDF', 'creationdate': '2018-12-18T05:07:46+00:00', 'moddate': '2019-01-22T19:51:34+00:00', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Burkov_the_hundred-page_Machine_Learning.pdf', 'total_pages': 152, 'page': 1, 'page_label': '2', 'rel_path': 'machine_learning/Burkov_the_hundred-page_Machine_Learning.pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1096}\n",
      "\n",
      "--- Chunk 1097 ---\n",
      "Content:\n",
      "Preface\n",
      "Let’s start by telling the truth: machines don’t learn. What a typical “learning machine”\n",
      "does, is ﬁnding a mathematical formula, which, when applied to a collection of inputs (called\n",
      "“training data”), produces the desired outputs. This mathematical formula also generates the\n",
      "correct outputs for most other inputs (distinct from the training data) on the condition that\n",
      "those inputs come from the same or a similar statistical distribution as the one the training\n",
      "data was drawn from.\n",
      "Why isn’t that learning? Because if you slightly distort the inputs, the output is very likely\n",
      "to become completely wrong. It’s not how learning in animals works. If you learned to play\n",
      "a video game by looking straight at the screen, you would still be a good player if someone\n",
      "rotates the screen slightly. A machine learning algorithm, if it was trained by “looking”\n",
      "straight at the screen, unless it was also trained to recognize rotation, will fail to play the\n",
      "game on a rotated screen.\n",
      "So why the name “machine learning” then? The reason, as is often the case, is marketing:\n",
      "Arthur Samuel, an American pioneer in the ﬁeld of computer gaming and artiﬁcial intelligence,\n",
      "coined the term in 1959 while at IBM. Similarly to how in the 2010s IBM tried to market\n",
      "the term “cognitive computing” to stand out from competition, in the 1960s, IBM used the\n",
      "new cool term “machine learning” to attract both clients and talented employees.\n",
      "As you can see, just like artiﬁcial intelligence is not intelligence, machine learning is not\n",
      "learning. However, machine learning is a universally recognized term that usually refers\n",
      "to the science and engineering of building machines capable of doing various useful things\n",
      "without being explicitly programmed to do so. So, the word “learning” in the term is used\n",
      "by analogy with the learning in animals rather than literally.\n",
      "Who This Book is For\n",
      "This book contains only those parts of the vast body of material on machine learning developed\n",
      "since the 1960s that have proven to have a signiﬁcant practical value. A beginner in machine\n",
      "learning will ﬁnd in this book just enough details to get a comfortable level of understanding\n",
      "of the ﬁeld and start asking the right questions.\n",
      "Practitioners with experience can use this book as a collection of directions for further\n",
      "self-improvement. The book also comes in handy when brainstorming at the beginning of a\n",
      "project, when you try to answer the question whether a given technical or business problem\n",
      "is “machine-learnable” and, if yes, which techniques you should try to solve it.\n",
      "How to Use This Book\n",
      "If you are about to start learning machine learning, you should read this book from the\n",
      "beginning to the end. (It’s just a hundred pages, not a big deal.) If you are interested\n",
      "Andriy Burkov The Hundred-Page Machine Learning Book - Draft 3...\n",
      "Metadata:\n",
      "{'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'PyPDF', 'creationdate': '2018-12-18T05:07:46+00:00', 'moddate': '2019-01-22T19:51:34+00:00', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Burkov_the_hundred-page_Machine_Learning.pdf', 'total_pages': 152, 'page': 2, 'page_label': '3', 'rel_path': 'machine_learning/Burkov_the_hundred-page_Machine_Learning.pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1097}\n",
      "\n",
      "--- Chunk 1098 ---\n",
      "Content:\n",
      "in a speciﬁc topic covered in the book and want to know more, most sections have a QR\n",
      "code. By scanning one of those QR codes with your phone, you will get a link to a page on\n",
      "the book’s companion wiki theMLbook.com with additional materials: recommended reads,\n",
      "videos, Q&As, code snippets, tutorials, and other bonuses.\n",
      "The book’s wiki is continuously updated with contributions from the book’s author himself\n",
      "as well as volunteers from all over the world. So this book, like a good wine, keeps getting\n",
      "better after you buy it.\n",
      "Scan the QR code below with your phone to get to the book’s wiki:\n",
      "Some sections don’t have a QR code, but they still most likely have a wiki page. You can\n",
      "ﬁnd it by submitting the section’s title to the wiki’s search engine.\n",
      "Should You Buy This Book?\n",
      "This book is distributed on the “read ﬁrst, buy later” principle. I ﬁrmly believe that paying\n",
      "for the content before consuming it is buying a pig in a poke. You can see and try a car in a\n",
      "dealership before you buy it. You can try on a shirt or a dress in a department store. You\n",
      "have to be able to read a book before paying for it.\n",
      "The read ﬁrst, buy laterprinciple implies that you can freely download the book, read it and\n",
      "share it with your friends and colleagues. If you liked the book, only then you have to buy it.\n",
      "Now you are all set. Enjoy your reading!\n",
      "Andriy Burkov The Hundred-Page Machine Learning Book - Draft 4...\n",
      "Metadata:\n",
      "{'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'PyPDF', 'creationdate': '2018-12-18T05:07:46+00:00', 'moddate': '2019-01-22T19:51:34+00:00', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Burkov_the_hundred-page_Machine_Learning.pdf', 'total_pages': 152, 'page': 3, 'page_label': '4', 'rel_path': 'machine_learning/Burkov_the_hundred-page_Machine_Learning.pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1098}\n",
      "\n",
      "--- Chunk 1099 ---\n",
      "Content:\n",
      "TheHundred-Page\n",
      "MachineLearning\n",
      "Book\n",
      "Andriy Burkov...\n",
      "Metadata:\n",
      "{'producer': '3-Heights(TM) PDF Optimization Shell 4.8.25.2 (http://www.pdf-tools.com)', 'creator': 'PyPDF', 'creationdate': '2018-12-18T05:07:46+00:00', 'moddate': '2019-01-22T19:51:34+00:00', 'source': '/home/bengtegard/github/data-science-rag/data/machine_learning/Burkov_the_hundred-page_Machine_Learning.pdf', 'total_pages': 152, 'page': 4, 'page_label': '5', 'rel_path': 'machine_learning/Burkov_the_hundred-page_Machine_Learning.pdf', 'category': 'machine_learning', 'content_category': 'machine_learning', 'chunk_id': 1099}\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks[1000:1100], start=1000):  \n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        print(f\"Content:\\n{chunk.page_content}...\")  \n",
    "        print(f\"Metadata:\\n{chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ef11d",
   "metadata": {},
   "source": [
    "### Embedding Creation: Convert the text chunks into numerical representations using an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96694e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bengtegard/github/data-science-rag/.rag/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# model for scientific/coding fine-tuning\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\", \n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f4767",
   "metadata": {},
   "source": [
    "Testing the embedding model and inspect the vectors for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63b7e049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1:\n",
      "Text: Hands-On Machine Learning\n",
      "with Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "THIRD EDITION\n",
      "Concepts, Tools, an...\n",
      "Vector (dim=768): [0.02266613207757473, 0.020945105701684952, -0.038166940212249756, 0.04023122414946556, 0.001040332019329071]...\n",
      "\n",
      "Chunk 2:\n",
      "Text: Hands-On Machine Learning with Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "by Aurélien Géron\n",
      "Copyright © 202...\n",
      "Vector (dim=768): [0.022976914420723915, 0.047215186059474945, -0.048667144030332565, 0.010753879323601723, -0.02228672429919243]...\n",
      "\n",
      "Chunk 3:\n",
      "Text: 2022-02-08: First Release\n",
      "2022-03-14: Second Release\n",
      "2022-04-18: Third Release\n",
      "See http://oreilly.co...\n",
      "Vector (dim=768): [0.006109155248850584, 0.014928262680768967, -0.04056369885802269, 0.01669466681778431, -0.045528244227170944]...\n",
      "\n",
      "Chunk 4:\n",
      "Text: Preface\n",
      "The Machine Learning Tsunami\n",
      "In 2006, Geoffrey Hinton et al. published a paper showing how t...\n",
      "Vector (dim=768): [-0.004002333153039217, 0.0928729772567749, -0.0468711219727993, 0.03137785568833351, -0.012745135463774204]...\n",
      "\n",
      "Chunk 5:\n",
      "Text: Or maybe your company has tons of data (user logs, financial data,\n",
      "production data, machine sensor d...\n",
      "Vector (dim=768): [0.0361751988530159, 0.06532702594995499, -0.05443568527698517, 0.0013704417506232858, -0.030319856479763985]...\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [doc.page_content for doc in chunks[:5]]\n",
    "embeddings = embedding_model.embed_documents(sample_texts)\n",
    "\n",
    "for i, (text, vector) in enumerate(zip(sample_texts, embeddings)):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Text: {text[:100]}...\")\n",
    "    print(f\"Vector (dim={len(vector)}): {vector[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a3254a",
   "metadata": {},
   "source": [
    "## Create FAISS vector database\n",
    "\n",
    "First initalize the RAGEngine class, then define the path for the vector database and then create it from the chunks. Later on, I can load the vector database without recomputing the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b259cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_engine import RAGEngine\n",
    "\n",
    "# Initialize the RAG engine\n",
    "rag = RAGEngine(embedding_model, config)\n",
    "\n",
    "# Define where to persist the vector database\n",
    "persist_dir = os.path.abspath(\"../vector_db\")\n",
    "\n",
    "# Now create and persist the vector database\n",
    "rag.create_vector_db(documents=chunks, persist_dir=persist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98cf98e",
   "metadata": {},
   "source": [
    "### Response Generation: Use a language model to generate a response based on retrieved text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2a211eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import rag_engine\n",
    "from rag_engine import RAGEngine\n",
    "from utils import load_config\n",
    "\n",
    "importlib.reload(rag_engine)\n",
    "\n",
    "# Load config.yml\n",
    "config = load_config('/home/bengtegard/github/data-science-rag/config.yml')\n",
    "\n",
    "# Define where to persist the vector database\n",
    "persist_dir = os.path.abspath(\"../vector_db\")\n",
    "\n",
    "# Initialize the RAG engine\n",
    "rag = RAGEngine(embedding_model, config)\n",
    "\n",
    "# Load vector database\n",
    "rag.load_vector_db(persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7c133310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 6 docs using 4445 tokens (limit: 30720)\n",
      "<think>\n",
      "Okay, I need to explain why the Central Limit Theorem (CLT) is important. Let me start by recalling what the CLT is about. From what I remember, it's a statistical theorem that says that if you take a large number of samples from a population, the distribution of the sample means will be approximately normal, even if the population itself isn't normally distributed. That sounds pretty useful.\n",
      "\n",
      "So, why is this important? Well, in real-world scenarios, we often don't know the exact distribution of the population data. Maybe it's skewed or has some other non-normal shape. But with the CLT, even if the population isn't normal, as long as we have a large enough sample size, we can assume that the sample means are normally distributed. This is a big deal because many statistical tests assume normality.\n",
      "\n",
      "I remember that the CLT allows us to use normal distribution models for statistical inference. That must be because, with a large sample size, the sampling distribution becomes normal, making it easier to apply tests that rely on the normal distribution, like t-tests or ANOVA. Even if the original data isn't normal, the sample means will be, so we can still use those tests.\n",
      "\n",
      "Another point is about standard error. The CLT helps us estimate the standard error, which is the standard deviation of the sampling distribution. The formula is SE = s/√n, where s is the sample standard deviation and n is the sample size. This is crucial for constructing confidence intervals and conducting hypothesis tests. Without knowing the population standard deviation, we can still estimate it using the sample standard deviation, thanks to the CLT.\n",
      "\n",
      "The CLT is foundational for many statistical methods. It underpins confidence intervals and hypothesis testing, which are essential in research. For example, in medicine, when testing a new drug, researchers take samples of patients and use the CLT to make inferences about the entire population. This allows them to determine if the drug is effective with a certain level of confidence.\n",
      "\n",
      "Practical applications are vast. In business, the CLT can be used in quality control to monitor product defects. By sampling a large number of products, companies can infer whether their processes are within acceptable limits. In finance, it's used to analyze market trends and make predictions based on sample data.\n",
      "\n",
      "I also remember that the CLT influenced the development of statistics significantly. William S. Gosset's work on using the sample standard deviation instead of the population standard deviation led to the t-distribution, which is used when the population standard deviation is unknown. This was a big transformation in statistical methods.\n",
      "\n",
      "In summary, the CLT is important because it allows us to make inferences about populations using sample data, even when the population isn't normally distributed. It provides a way to estimate standard errors, construct confidence intervals, and conduct hypothesis tests. Its applications are widespread across various fields, making it a cornerstone of modern statistics.\n",
      "</think>\n",
      "\n",
      "The Central Limit Theorem (CLT) holds significant importance in statistics and data analysis due to its profound implications for inferential statistics. Here's a structured explanation of its importance:\n",
      "\n",
      "1. **Foundation for Statistical Inference**: The CLT enables the use of normal distribution models for statistical inference, even when the population distribution is unknown or non-normal. This is particularly advantageous in real-world scenarios where population data may be skewed or otherwise distributed.\n",
      "\n",
      "2. **Application of Statistical Tests**: By ensuring that the sampling distribution of the sample mean is approximately normal for large sample sizes, the CLT allows the application of statistical tests like t-tests and ANOVA, which assume normality. This facilitates reliable hypothesis testing and confidence interval construction.\n",
      "\n",
      "3. **Estimation of Standard Error**: The CLT provides a method to estimate the standard error (SE) as SE = s/√n, where s is the sample standard deviation and n is the sample size. This is crucial for assessing the variability of sample means and making precise inferences about population parameters.\n",
      "\n",
      "4. **Broad Practical Applications**: The CLT is instrumental in various fields such as medicine, business, and social sciences. It aids in quality control, financial analysis, and medical research by enabling inferences about populations based on sample data, thereby informing decision-making processes.\n",
      "\n",
      "5. **Transformation of Statistical Methods**: The CLT's recognition of using sample standard deviation instead of population standard deviation led to the development of the t-distribution by William S. Gosset. This advancement was pivotal in transforming statistical practices, especially when population parameters are unknown.\n",
      "\n",
      "In essence, the CLT is a cornerstone of modern statistics, providing a robust framework for making accurate inferences about populations using sample data, thus underpinning a wide array of statistical methods and applications.\n",
      "\n",
      "This answer was based on the following sources:\n",
      "\n",
      "- Stats Data and Models, 3rd Edition- Richard D de Veaux.pdf (pages: 461, 468, 476, 477, 478, 580) [statistics]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import rag_engine\n",
    "\n",
    "importlib.reload(rag_engine)\n",
    "\n",
    "# Ask a question\n",
    "response = rag.ask(\"Explain why central limit theorem is important\")\n",
    "\n",
    "# Extract the response and sources\n",
    "response_text = response[\"answer\"]\n",
    "sources = response['sources']\n",
    "\n",
    "# Format the complete response\n",
    "formatted_response = f\"{response_text}\\n\\n{rag.format_sources(sources)}\"\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3e97883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 6 docs using 2685 tokens (limit: 30720)\n",
      "<think>\n",
      "Okay, so I'm trying to figure out what a parameter and a hyperparameter are. I remember from my stats class that parameters have something to do with populations, and statistics are about samples. Let me see if I can connect that.\n",
      "\n",
      "In the extracted information, there's a section that talks about parameters and statistics. It says that a parameter is a characteristic of the entire population, like the mean age of all women in the US who own cats. A statistic is like the mean age from a sample of those women. So, parameters are about the whole group, and statistics are estimates based on a part of the group.\n",
      "\n",
      "Now, switching over to machine learning. There's a chapter about parametric and nonparametric algorithms. Parametric models have a fixed number of parameters, regardless of the data size. They assume a specific form, like linear regression. Nonparametric models don't make such assumptions and can adjust based on data size.\n",
      "\n",
      "Then, hyperparameters are mentioned in the context of tuning. Hyperparameters are set before training, like learning rate, batch size, or the number of hidden layers. They're different from model parameters because they aren't learned during training but are chosen by the data analyst.\n",
      "\n",
      "So, to sum up: parameters are model internals learned from data, hyperparameters are settings chosen before training, and in stats, parameters describe populations while statistics describe samples.\n",
      "</think>\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "In both statistics and machine learning, the terms *parameter* and *hyperparameter* have distinct meanings. Here's a clear breakdown:\n",
      "\n",
      "### **In Statistics:**\n",
      "- **Parameter:** Refers to a numerical characteristic of an entire population, such as the true mean (μ) or proportion (p). It represents a fixed value that describes the whole dataset.\n",
      "- **Statistic:** A numerical characteristic of a sample drawn from the population, used to estimate the parameter. For example, the sample mean (\\(\\bar{x}\\)) is a statistic that estimates the population mean (μ).\n",
      "\n",
      "### **In Machine Learning:**\n",
      "- **Parameter:** These are the variables learned from the training data by the model itself. Examples include the weights (w) and biases (b) in a linear regression model, \\(y = w x + b\\). These are optimized during the training process to fit the data.\n",
      "- **Hyperparameter:** These are parameters set by the data analyst before training the model. They control the learning process and are not directly learned from the data. Examples include the learning rate, regularization strength (C), batch size, and the number of hidden layers in a neural network. Hyperparameters are typically tuned to optimize model performance.\n",
      "\n",
      "### **Key Differences:**\n",
      "- **Scope:** Parameters are intrinsic to the model and learned from data, while hyperparameters are external settings chosen by the analyst.\n",
      "- **Optimization:** Parameters are optimized during training, whereas hyperparameters are tuned before or during training to improve model performance.\n",
      "\n",
      "### **Example:**\n",
      "In linear regression, the coefficients (weights) are *parameters* learned from the data. The learning rate used by the optimizer (e.g., gradient descent) is a *hyperparameter* set by the analyst to influence how quickly the model learns these parameters.\n",
      "\n",
      "Understanding the distinction between parameters and hyperparameters is crucial for effectively training and tuning machine learning models.\n",
      "\n",
      "This answer was based on the following sources:\n",
      "\n",
      "- Burkov_the_hundred-page_Machine_Learning.pdf (pages: 69, 70) [machine_learning]\n",
      "- Hands-On Machine Learning with Scikit-Learn, Keras, and - Aurélien Géron _3, 2022 _O'Reilly Media.pdf (pages: 586, 665) [machine_learning]\n",
      "- Master Machine Learning Algorithms - Discover how they work and Implement Them From Scratch by Jason Brownlee (z-lib.org).pdf (pages: 22) [machine_learning]\n",
      "- The Book of R_ A First Course in Programming and Statistics ( PDFDrive.com ).pdf (pages: 298) [r]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "response_2 = rag.ask(\"What is a parameter and a hyperparameter?\")\n",
    "\n",
    "# Extract the response and sources\n",
    "response_text = response_2[\"answer\"]\n",
    "sources = response_2['sources']\n",
    "\n",
    "# Format the complete response\n",
    "formatted_response = f\"{response_text}\\n\\n{rag.format_sources(sources)}\"\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9087f2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 6 docs using 3572 tokens (limit: 30720)\n",
      "<think>\n",
      "Okay, so I need to figure out what the layered grammar of graphics is from Hadley Wickham. I remember that he created ggplot2, which is a popular data visualization library in R. From the extracted context, it seems like the layered grammar is a framework for creating graphics. Let me go through each section to understand the components.\n",
      "\n",
      "First, the grammar has components like data, aesthetic mappings, geometric objects, scales, facet specifications, statistical transformations, and coordinate systems. Each of these plays a role in building a plot. Layers are made up of these elements, and a plot can have multiple layers, such as a scatterplot with a regression line.\n",
      "\n",
      "The scales handle how data is mapped to visual elements like color or position. They can be adjusted before or after statistical transformations. Faceting is about splitting data into subsets to create small multiples, which helps in comparing different conditions. The coordinate system defines how data points are positioned on the plot, like Cartesian or polar coordinates.\n",
      "\n",
      "Statistical transformations allow for operations like smoothing or binning. Position adjustments handle how elements are arranged, such as stacking or dodging. Defaults and embedding in a programming language like R make the grammar flexible and user-friendly, allowing for concise code.\n",
      "\n",
      "The grammar's strengths include its flexibility and the ability to combine elements in various ways. However, it has limitations, especially with certain plot types like area plots and interactive visualizations. It also doesn't handle objects beyond data frames well.\n",
      "\n",
      "Putting this all together, the layered grammar is a structured approach to creating graphics by breaking them down into components and allowing layers to be built up. It's both powerful and flexible, making it easier to produce a wide range of visualizations systematically.\n",
      "</think>\n",
      "\n",
      "The layered grammar of graphics, as conceptualized by Hadley Wickham, is a structured framework for creating visualizations by decomposing them into distinct, combinable components. This approach, implemented in the ggplot2 library, emphasizes modularity and flexibility, allowing users to build complex graphics incrementally. Below is an organized explanation of its key components and their interplay:\n",
      "\n",
      "### Components of the Layered Grammar\n",
      "\n",
      "1. **Data and Aesthetic Mappings**:\n",
      "   - **Data**: The dataset to be visualized.\n",
      "   - **Aesthetic Mappings**: Define how variables in the data map to visual elements (aesthetics) such as position, color, shape, and size.\n",
      "\n",
      "2. **Geometric Objects (Geoms)**:\n",
      "   - These are the visual elements (e.g., points, lines, bars) that represent data points.\n",
      "\n",
      "3. **Scales**:\n",
      "   - Determine how data values map to visual aesthetics. They can be applied before or after statistical transformations, offering flexibility in data presentation.\n",
      "\n",
      "4. **Facet Specification (Faceting)**:\n",
      "   - Allows the dataset to be split into subsets, creating small multiples of plots. This facilitates comparison across different conditions or categories.\n",
      "\n",
      "5. **Statistical Transformations**:\n",
      "   - Apply statistical operations (e.g., smoothing, binning) to data before or after mapping, enabling visualizations like regression lines or histograms.\n",
      "\n",
      "6. **Position Adjustments**:\n",
      "   - Control how geoms are arranged within a plot, such as stacking or dodging elements to avoid overplotting.\n",
      "\n",
      "7. **Coordinate System**:\n",
      "   - Defines how data coordinates are mapped to the plot's plane. Common systems include Cartesian and polar coordinates, each affecting the appearance of geoms.\n",
      "\n",
      "### Layered Structure\n",
      "\n",
      "A **layer** in this grammar is a combination of data, aesthetic mappings, a geom, statistical transformation, and position adjustment. Plots can have multiple layers, enabling the overlay of different elements (e.g., points and a regression line) to convey complex information.\n",
      "\n",
      "### Defaults and Embedding\n",
      "\n",
      "The grammar incorporates a hierarchy of defaults to simplify plot creation, reducing the need to specify every component explicitly. Embedded within programming languages like R, it allows for concise and expressive code, enhancing productivity.\n",
      "\n",
      "### Strengths and Limitations\n",
      "\n",
      "- **Strengths**: Offers flexibility and systematic approach to visualization, supporting a wide range of graphics through combinable components.\n",
      "- **Limitations**: Less insightful for certain plot types (e.g., area plots) and lacks support for interactive or non-data frame objects.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Wickham's layered grammar provides a powerful, flexible framework for data visualization, emphasizing decomposition and systematic combination of components. This approach, while not exhaustive, offers a robust foundation for creating insightful and aesthetically pleasing visualizations, as seen in the capabilities of ggplot2.\n",
      "\n",
      "This answer was based on the following sources:\n",
      "\n",
      "- layered-grammar.pdf (pages: 5, 10, 11, 14, 23) [data_science]\n",
      "- R for Data Science - Hadley Wickham & Garrett Grolemund - (Data Science Books)-1.pdf (pages: 4) [r]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "response_4 = rag.ask(\"What is the layered grammar of graphics from Hadley Wickham?\")\n",
    "\n",
    "# Extract the response and sources\n",
    "response_text = response_4[\"answer\"]\n",
    "sources = response_4['sources']\n",
    "\n",
    "# Format the complete response\n",
    "formatted_response = f\"{response_text}\\n\\n{rag.format_sources(sources)}\"\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f804a233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 6 docs using 3317 tokens (limit: 30720)\n",
      "<think>\n",
      "Alright, I need to explain how a convolutional neural network works. Let me start by recalling what I know from the provided documents. \n",
      "\n",
      "First, I remember that CNNs are designed to process data with grid-like topology, such as images. They use convolution and pooling layers to extract features. The key idea is that these layers help the network detect local patterns and then combine them into more complex features.\n",
      "\n",
      "Looking at the documents, I see that convolution layers apply filters to small regions of the input. These filters are like templates that slide over the image, performing a dot product to check for the presence of specific features. The filters are learned during training, which means they adapt to the specific features relevant to the task, like edges or textures in images.\n",
      "\n",
      "Then there are pooling layers. The most common type is max pooling. This layer downsamples the feature maps by taking the maximum value across each patch, which helps reduce the spatial dimensions and makes the network less sensitive to the exact position of features. This is useful for capturing more abstract representations.\n",
      "\n",
      "The documents also mention that CNNs stack multiple convolution and pooling layers. Each layer builds on the previous one, starting with simple features like edges and moving towards more complex structures like object parts. This hierarchical structure is similar to how the human visual cortex processes information.\n",
      "\n",
      "I should explain the process step by step, starting from the input image, through the initial feature extraction, combining into higher-level features, and finally to the classification. It's important to highlight how the network learns these features automatically, which is a key advantage over traditional methods that rely on hand-engineered features.\n",
      "\n",
      "Additionally, the documents discuss the architecture of a CNN, including how the input is a 3D feature map (height, width, channels), and how each convolution filter operates on small regions across all channels. The output of these operations forms feature maps that are then downsampled by pooling layers.\n",
      "\n",
      "I should also touch on the use of activation functions, like ReLU, which introduce non-linearity into the model, allowing it to learn more complex patterns. After several layers, the feature maps are flattened into a vector and passed through fully connected layers to produce the final output, often using a softmax activation for classification tasks.\n",
      "\n",
      "Regularization techniques like dropout and ridge regularization are mentioned to prevent overfitting, which is crucial when dealing with the large number of parameters in CNNs compared to the training data size.\n",
      "\n",
      "To make this explanation clear, I'll structure it into sections: introduction, convolution layers, pooling layers, architecture, feature learning, and regularization. I'll include equations to illustrate the convolution operation and describe the overall flow from input to output.\n",
      "\n",
      "I need to ensure the explanation is concise, precise, and connects theoretical concepts to practical implementations. Using real-world analogies, like how humans recognize objects by their features, will help in gaining intuition. I'll avoid making up any information and strictly base my answer on the provided context.\n",
      "\n",
      "Finally, I'll summarize the key points to reinforce the explanation, highlighting the efficiency and effectiveness of CNNs in image classification tasks.\n",
      "</think>\n",
      "\n",
      "**How Does a Convolutional Neural Network Work?**\n",
      "\n",
      "Convolutional Neural Networks (CNNs) are specialized neural networks designed to process data with grid-like topology, such as images. They excel in image classification tasks by leveraging convolution and pooling operations to extract and combine local features hierarchically.\n",
      "\n",
      "### 1. **Introduction**\n",
      "CNNs mimic the human visual cortex's hierarchical processing of visual information. They automatically learn spatial hierarchies of features, from simple edges to complex objects, directly from data.\n",
      "\n",
      "### 2. **Convolution Layers**\n",
      "- **Operation**: A convolution layer applies multiple small filters to the input image. Each filter is a small matrix that slides over the image, performing a dot product at each position to detect local features.\n",
      "- **Equation**: For an image \\( I \\) and filter \\( F \\), the convolution output at position \\( (i, j) \\) is:\n",
      "  \\[\n",
      "  (I * F)(i, j) = \\sum_{k=1}^{m} \\sum_{l=1}^{n} I[i+k, j+l] \\cdot F[k, l]\n",
      "  \\]\n",
      "  where \\( m \\times n \\) is the filter size.\n",
      "- **Filters**: Each filter learns to detect specific features (e.g., edges, textures). The outputs from all filters form feature maps.\n",
      "\n",
      "### 3. **Pooling Layers**\n",
      "- **Operation**: Pooling layers downsample the feature maps to reduce spatial dimensions and retain important features. Max pooling is common, where each patch's maximum value is taken.\n",
      "- **Effect**: Reduces sensitivity to feature position, helping the network generalize.\n",
      "\n",
      "### 4. **Architecture**\n",
      "- **Input**: A 3D feature map (height, width, channels). For color images, channels are RGB.\n",
      "- **Layers**: Alternating convolution and pooling layers extract features at increasing complexity. Multiple layers allow the network to detect edges, textures, and finally, objects.\n",
      "- **Flattening**: After several layers, feature maps are flattened into a vector for fully connected layers, leading to the output.\n",
      "\n",
      "### 5. **Feature Learning**\n",
      "- **Hierarchical Learning**: Early layers learn low-level features (edges), while deeper layers combine these into higher-level features (object parts).\n",
      "- **Shared Weights**: Filters are reused across the entire image, reducing parameters and enabling efficient learning.\n",
      "\n",
      "### 6. **Activation and Regularization**\n",
      "- **Non-linearity**: ReLU activation is applied after each convolution to introduce non-linearity.\n",
      "- **Regularization**: Techniques like dropout and ridge regularization prevent overfitting by adding noise or penalizing large weights.\n",
      "\n",
      "### 7. **Output**\n",
      "- **Classification**: The final layer typically uses softmax activation to produce probabilities over classes.\n",
      "\n",
      "### Summary\n",
      "CNNs process images through convolution and pooling to hierarchically extract features, enabling efficient and effective image classification. Their architecture automatically learns relevant features, making them powerful tools in computer vision.\n",
      "\n",
      "This answer was based on the following sources:\n",
      "\n",
      "- Burkov_the_hundred-page_Machine_Learning.pdf (pages: 80) [machine_learning]\n",
      "- ISLRv2_corrected_June_2023.pdf (pages: 421, 422, 423, 424, 426) [r]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "response_4 = rag.ask(\"How does a convolutional neural network work?\")\n",
    "\n",
    "# Extract the response and sources\n",
    "response_text = response_4[\"answer\"]\n",
    "sources = response_4['sources']\n",
    "\n",
    "# Format the complete response\n",
    "formatted_response = f\"{response_text}\\n\\n{rag.format_sources(sources)}\"\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276dd691",
   "metadata": {},
   "source": [
    "## Automated LLM-Based Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c60cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation scoring system constants\n",
    "SCORE_FULL = 1.0     # Complete match or fully satisfactory\n",
    "SCORE_PARTIAL = 0.5  # Partial match or somewhat satisfactory\n",
    "SCORE_NONE = 0.0     # No match or unsatisfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46b20db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define strict evaluation prompt templates\n",
    "FAITHFULNESS_PROMPT_TEMPLATE = \"\"\"\n",
    "Evaluate the faithfulness of the AI response compared to the true answer.\n",
    "User Query: {question}\n",
    "AI Response: {response}\n",
    "True Answer: {true_answer}\n",
    "\n",
    "Faithfulness measures how well the AI response aligns with facts in the true answer, without hallucinations.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Score STRICTLY using only these values:\n",
    "    * {full} = Completely faithful, no contradictions with true answer\n",
    "    * {partial} = Partially faithful, minor contradictions\n",
    "    * {none} = Not faithful, major contradictions or hallucinations\n",
    "- Return ONLY the numerical score ({full}, {partial}, or {none}) with no explanation or additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANCY_PROMPT_TEMPLATE = \"\"\"\n",
    "Evaluate the relevancy of the AI response to the user query.\n",
    "User Query: {question}\n",
    "AI Response: {response}\n",
    "\n",
    "Relevancy measures how well the response addresses the user's question.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Score STRICTLY using only these values:\n",
    "    * {full} = Completely relevant, directly addresses the query\n",
    "    * {partial} = Partially relevant, addresses some aspects\n",
    "    * {none} = Not relevant, fails to address the query\n",
    "- Return ONLY the numerical score ({full}, {partial}, or {none}) with no explanation or additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a352fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score: 0.5\n",
      "Relevancy Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "def evaluate_response(question, response, true_answer):\n",
    "        \"\"\"\n",
    "        Evaluates the quality of an AI-generated response based on faithfulness and relevancy.\n",
    "\n",
    "        Args:\n",
    "        question (str): The user's original question.\n",
    "        response (str): The AI-generated response being evaluated.\n",
    "        true_answer (str): The correct answer used as ground truth.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[float, float]: A tuple containing (faithfulness_score, relevancy_score).\n",
    "                                                Each score is one of: 1.0 (full), 0.5 (partial), or 0.0 (none).\n",
    "        \"\"\"\n",
    "        # Format the evaluation prompts\n",
    "        faithfulness_prompt = FAITHFULNESS_PROMPT_TEMPLATE.format(\n",
    "                question=question, \n",
    "                response=response, \n",
    "                true_answer=true_answer,\n",
    "                full=SCORE_FULL,\n",
    "                partial=SCORE_PARTIAL,\n",
    "                none=SCORE_NONE\n",
    "        )\n",
    "        \n",
    "        relevancy_prompt = RELEVANCY_PROMPT_TEMPLATE.format(\n",
    "                question=question, \n",
    "                response=response,\n",
    "                full=SCORE_FULL,\n",
    "                partial=SCORE_PARTIAL,\n",
    "                none=SCORE_NONE\n",
    "        )\n",
    "\n",
    "        # Request faithfulness evaluation from the model\n",
    "        faithfulness_response = client.chat.completions.create(\n",
    "               model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an objective evaluator. Return ONLY the numerical score.\"},\n",
    "                        {\"role\": \"user\", \"content\": faithfulness_prompt}\n",
    "                ]\n",
    "        )\n",
    "        \n",
    "        # Request relevancy evaluation from the model\n",
    "        relevancy_response = client.chat.completions.create(\n",
    "                model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an objective evaluator. Return ONLY the numerical score.\"},\n",
    "                        {\"role\": \"user\", \"content\": relevancy_prompt}\n",
    "                ]\n",
    "        )\n",
    "        \n",
    "        # Extract scores and handle potential parsing errors\n",
    "        try:\n",
    "                faithfulness_score = float(faithfulness_response.choices[0].message.content.strip())\n",
    "        except ValueError:\n",
    "                print(\"Warning: Could not parse faithfulness score, defaulting to 0\")\n",
    "                faithfulness_score = 0.0\n",
    "                \n",
    "        try:\n",
    "                relevancy_score = float(relevancy_response.choices[0].message.content.strip())\n",
    "        except ValueError:\n",
    "                print(\"Warning: Could not parse relevancy score, defaulting to 0\")\n",
    "                relevancy_score = 0.0\n",
    "\n",
    "        return faithfulness_score, relevancy_score\n",
    "\n",
    "# True answer for the validation data\n",
    "true_answer = data[1][\"ideal_answer\"]\n",
    "\n",
    "# Evaluate response\n",
    "faithfulness, relevancy = evaluate_response(query, ai_response, true_answer)\n",
    "\n",
    "# print the evaluation scores\n",
    "print(f\"Faithfulness Score: {faithfulness}\")\n",
    "print(f\"Relevancy Score: {relevancy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ed92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three common techniques to prevent overfitting in deep learning models are: 1) Regularization: Adding penalties to the loss function to constrain model weights. L1 regularization adds the sum of absolute weights, promoting sparsity. L2 regularization adds the sum of squared weights, keeping weights small and diffuse. 2) Dropout: Randomly 'dropping out' (setting to zero) a percentage of neurons during training. This prevents neurons from co-adapting too much and forces the network to learn more robust features. Each training iteration uses a different random subset of neurons. 3) Early Stopping: Monitoring performance on a validation set during training and stopping when validation performance begins to degrade. This prevents the model from learning noise in the training data by capturing the point where generalization is optimal. Other effective techniques include data augmentation, batch normalization, and transfer learning.\n"
     ]
    }
   ],
   "source": [
    "# True answer for the first validation data\n",
    "true_answer = data[1]['ideal_answer']\n",
    "\n",
    "print(true_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
