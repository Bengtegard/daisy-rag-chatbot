[
  {
    "id": "q1",
    "question": "What is gradient descent and how does it help train neural networks?",
    "ideal_answer": "Gradient descent is an optimization algorithm used to minimize the cost function in neural networks. It works by iteratively adjusting the model's parameters (weights and biases) in the direction of the negative gradient of the cost function. The process works as follows: 1) Initialize the network with random weights, 2) Calculate the prediction error using the cost function, 3) Compute the gradient (partial derivatives) of the cost function with respect to each parameter, 4) Update each parameter by subtracting a small portion of its gradient (controlled by the learning rate), 5) Repeat steps 2-4 until convergence. Gradient descent helps train neural networks by finding the parameter values that minimize prediction errors. There are several variants including batch gradient descent (uses entire dataset), stochastic gradient descent (uses single random instance), and mini-batch gradient descent (uses small random batches of instances). The learning rate hyperparameter is crucial - if too large, training may diverge; if too small, training will be slow and may get stuck in local minima."
  },
  {
    "id": "q2",
    "question": "What are three common techniques to prevent overfitting in deep learning models?",
    "ideal_answer": "Three common techniques to prevent overfitting in deep learning models are: 1) Regularization: Adding penalties to the loss function to constrain model weights. L1 regularization adds the sum of absolute weights, promoting sparsity. L2 regularization adds the sum of squared weights, keeping weights small and diffuse. 2) Dropout: Randomly 'dropping out' (setting to zero) a percentage of neurons during training. This prevents neurons from co-adapting too much and forces the network to learn more robust features. Each training iteration uses a different random subset of neurons. 3) Early Stopping: Monitoring performance on a validation set during training and stopping when validation performance begins to degrade. This prevents the model from learning noise in the training data by capturing the point where generalization is optimal. Other effective techniques include data augmentation, batch normalization, and transfer learning."
  },
  {
    "id": "q3",
    "question": "What are word embeddings and why are they important in NLP?",
    "ideal_answer": "Word embeddings are dense vector representations of words in a continuous vector space where semantically similar words are mapped to nearby points. Unlike one-hot encoding, embeddings capture semantic relationships between words. Word embeddings are important in NLP for several reasons: 1) Dimensionality reduction: They represent words in a much lower-dimensional space than one-hot encodings, making models more efficient. 2) Semantic relationships: They capture meaningful semantic and syntactic relationships between words. In well-trained embeddings, vector operations reveal semantic relationships (e.g., vector('king') - vector('man') + vector('woman') ≈ vector('queen')). 3) Transfer learning: Pre-trained embeddings like Word2Vec, GloVe, or FastText can transfer knowledge from large corpora to specific NLP tasks, improving performance when training data is limited. 4) Improved generalization: By grouping similar words together in the vector space, models can generalize better to words or contexts not seen during training. Modern NLP systems typically use contextual embeddings from transformer models like BERT, which generate different embeddings for the same word based on its context."
  },
  {
    "id": "q4",
    "question": "Explain what regularization is and name two regularization methods discussed in the book.",
    "ideal_answer": "Regularization is a set of techniques used to prevent overfitting in machine learning models by adding constraints or penalties that discourage learning overly complex patterns. It helps models generalize better to unseen data by reducing variance without substantially increasing bias. Two regularization methods discussed in the book are: 1) L1 Regularization (Lasso): Adds a penalty equal to the sum of the absolute values of the model's weights to the loss function. This encourages sparse models by pushing some weights exactly to zero, effectively performing feature selection. The regularization term is λ∑|w|, where λ controls regularization strength. 2) L2 Regularization (Ridge): Adds a penalty equal to the sum of the squared values of the model's weights to the loss function. This constrains the weights to be small but rarely exactly zero, distributing the importance across all features. The regularization term is λ∑w², making the model less sensitive to input variations. The book also covers other regularization techniques like dropout, early stopping, and batch normalization specifically for neural networks."
  },
  {
    "id": "q5",
    "question": "What is the difference between TensorFlow and Keras?",
    "ideal_answer": "TensorFlow and Keras represent different layers of abstraction in the deep learning ecosystem. TensorFlow is a comprehensive, low-level, open-source machine learning framework developed by Google. It provides fine-grained control over model creation, with features like: building computational graphs, distributed computing capabilities, deployment options across various platforms, TensorFlow Extended (TFX) for production ML pipelines, and lower-level operations requiring more code but offering more flexibility. Keras is a high-level neural networks API that can run on top of TensorFlow (as well as other backends historically). It focuses on user-friendliness and rapid experimentation with features like: simple, consistent interface for common model architectures, minimalist, human-readable code, built-in support for common layers, objectives, and optimizers, sensible defaults for hyperparameters, and extensive documentation and developer guides. Since TensorFlow 2.0, Keras has been integrated as TensorFlow's official high-level API (tf.keras), combining Keras' ease of use with TensorFlow's powerful features and ecosystem. Users can seamlessly mix high-level Keras layers with custom TensorFlow operations when needed."
  }
]